{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d6f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%pip install optuna-integration[lightgbm]\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea074aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca73795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "df_pendientes = pd.read_parquet('./data/l_vm_completa_train_pendientes.parquet', engine='fastparquet')\n",
    "# Reunir los DataFrames df_full y df_pendientes por PRODUCT_ID, CUSTOMER_ID y PERIODO, agregar las \n",
    "# columnas de df_pendientes a df_full\n",
    "df_full = df_full.merge(df_pendientes, on=['PRODUCT_ID', 'CUSTOMER_ID', 'PERIODO'], how='left', suffixes=('', '_features'))\n",
    "# Imprimir las columnas de df_full\n",
    "print(\"Columnas de df_full después de la unión con df_pendientes:\")\n",
    "print(df_full.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "df_full['MES_PROBLEMATICO'] = df_full['PERIODO'].apply(lambda x: 1 if x in [201906, 201908, 201910] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e06b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizar tipos de datos numéricos\n",
    "for col in df_full.select_dtypes(include=['int64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='integer')\n",
    "for col in df_full.select_dtypes(include=['float64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='float')\n",
    "# Variables categóricas\n",
    "# categorical_features = ['ANIO','MES','TRIMESTRE','ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','SKU_SIZE','CUSTOMER_ID','PRODUCT_ID','PLAN_PRECIOS_CUIDADOS']\n",
    "categorical_features = ['ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','PLAN_PRECIOS_CUIDADOS','MES_PROBLEMATICO','A_PREDECIR']\n",
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_full[col] = df_full[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables predictoras y objetivo\n",
    "# filtrar que en X el periodo sea menor o igual a 201910\n",
    "# En x eliminar la columna 'CLASE' y 'CLASE_DELTA'\n",
    "X = df_full[df_full['PERIODO'] <= 201910].drop(columns=['CLASE', 'CLASE_DELTA']) \n",
    "# Filtrar en y que el periodo sea menor o igual a 201910\n",
    "y = df_full[df_full['PERIODO'] <= 201910]['CLASE_DELTA']\n",
    "# Eliminar df_full para liberar memoria\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los periodos de validación 201910\n",
    "#periodos_valid = [201910]\n",
    "periodos_valid = [201910]\n",
    "\n",
    "# Separar train y cinco conjuntos de validación respetando la secuencia temporal\n",
    "X_train = X[X['PERIODO'] < periodos_valid[0]]\n",
    "y_train = y[X['PERIODO'] < periodos_valid[0]]\n",
    "X_val_list = [X[X['PERIODO'] == p] for p in periodos_valid]\n",
    "y_val_list = [y[X['PERIODO'] == p] for p in periodos_valid]\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "import os\n",
    "\n",
    "# === Usamos solo el primer período de validación ===\n",
    "X_val = X_val_list[0]\n",
    "y_val = y_val_list[0]\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',  # alias de l1\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 1024, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.2, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 24),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 1.0),\n",
    "        'verbose': -1,\n",
    "        'feature_pre_filter': False,\n",
    "        'bagging_seed': 42,\n",
    "        'feature_fraction_seed': 42\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],  # No usar valid_names\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=100),\n",
    "            LightGBMPruningCallback(trial, 'l1')  # Este es el nombre correcto\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    best_score = model.best_score['valid_0']['l1']\n",
    "    print(f\"Trial {trial.number}: MAE = {best_score:.5f}\")\n",
    "    return best_score\n",
    "\n",
    "# Optuna\n",
    "storage_url = \"sqlite:///./modelos/optuna.db\"\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name=\"mae_delta_lgbm_regression_todos_los_productos_con_pendientes\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True\n",
    ")\n",
    "study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "# Mostrar mejores hiperparámetros\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Entrenamiento final\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'mae'\n",
    "best_params['verbose'] = -1\n",
    "\n",
    "model_reg = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=50000,\n",
    "    valid_sets=[val_data],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=500),\n",
    "        lgb.log_evaluation(period=500)\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.makedirs('./modelos', exist_ok=True)\n",
    "model_reg.save_model('./modelos/lgbm_model_reg_todos_los_productos.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la importancia de cada variable\n",
    "importancia = model_reg.feature_importance(importance_type='gain')\n",
    "nombres = X_train.columns\n",
    "\n",
    "# Crear un DataFrame ordenado por importancia\n",
    "df_importancia = pd.DataFrame({'feature': nombres, 'importance': importancia})\n",
    "df_importancia = df_importancia.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Mostrar las variables más importantes\n",
    "print(df_importancia.head(20))\n",
    "\n",
    "\n",
    "# Si quieres visualizarlo gráficamente:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(df_importancia['feature'], df_importancia['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Importancia de variables LightGBM')\n",
    "plt.xlabel('Importancia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c421266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entreno nuevamente el modelo con los mejores hiperparámetros y el conjunto completo de datos\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')\n",
    "# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "df_pendientes = pd.read_parquet('./data/l_vm_completa_train_pendientes.parquet', engine='fastparquet')\n",
    "# Reunir los DataFrames df_full y df_pendientes por PRODUCT_ID, CUSTOMER_ID y PERIODO, agregar las \n",
    "# columnas de df_pendientes a df_full\n",
    "df_full = df_full.merge(df_pendientes, on=['PRODUCT_ID', 'CUSTOMER_ID', 'PERIODO'], how='left', suffixes=('', '_features'))\n",
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "df_full['MES_PROBLEMATICO'] = df_full['PERIODO'].apply(lambda x: 1 if x in [201906, 201908] else 0)\n",
    "# Optimizar tipos de datos numéricos\n",
    "for col in df_full.select_dtypes(include=['int64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='integer')\n",
    "for col in df_full.select_dtypes(include=['float64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='float')\n",
    "# Variables categóricas\n",
    "# categorical_features = ['ANIO','MES','TRIMESTRE','ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','SKU_SIZE','CUSTOMER_ID','PRODUCT_ID','PLAN_PRECIOS_CUIDADOS']\n",
    "categorical_features = ['ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','PLAN_PRECIOS_CUIDADOS','MES_PROBLEMATICO','A_PREDECIR']\n",
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_full[col] = df_full[col].astype('category')\n",
    "# Variables predictoras y objetivo\n",
    "# filtrar que en X el periodo sea menor o igual a 201910\n",
    "# En x eliminar la columna 'CLASE' y 'CLASE_DELTA'\n",
    "X = df_full[df_full['PERIODO'] <= 201910].drop(columns=['CLASE', 'CLASE_DELTA']) \n",
    "# Filtrar en y que el periodo sea menor o igual a 201910\n",
    "y = df_full[df_full['PERIODO'] <= 201910]['CLASE_DELTA']\n",
    "# Eliminar df_full para liberar memoria\n",
    "del df_full\n",
    "gc.collect()\n",
    "# Separar train y cinco conjuntos de validación respetando la secuencia temporal\n",
    "X_train = X\n",
    "y_train = y\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528be593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 159,\n",
       " 'learning_rate': 0.15976562141122164,\n",
       " 'feature_fraction': 0.9902705425545245,\n",
       " 'bagging_fraction': 0.9641034133841936,\n",
       " 'bagging_freq': 3,\n",
       " 'min_data_in_leaf': 311,\n",
       " 'max_depth': 18,\n",
       " 'lambda_l1': 2.4322841274280815,\n",
       " 'lambda_l2': 0.4206330467495403,\n",
       " 'min_gain_to_split': 0.9020185871164503}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener los mejores hiperparámetros del estudio de la base de datos SQLite\n",
    "storage_url = \"sqlite:///./modelos/optuna.db\"\n",
    "study1 = optuna.load_study(\n",
    "    study_name=\"mae_delta_lgbm_regression_todos_los_productos_con_pendientes\",\n",
    "    storage=storage_url\n",
    ")\n",
    "best_params1 = study1.best_params\n",
    "study1.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "\n",
    "# Entrenamiento final\n",
    "best_params = study1.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'mae'\n",
    "best_params['verbose'] = -1\n",
    "\n",
    "model_reg = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=1000 \n",
    ")\n",
    "\n",
    "os.makedirs('./modelos', exist_ok=True)\n",
    "model_reg.save_model('./modelos/lgbm_model_reg_todos_los_productos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6453469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos sobre los que quiero hacer predicciones\n",
    "\n",
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')\n",
    "df_full = df_full[df_full['PERIODO'] == 201910].drop(columns=['CLASE', 'CLASE_DELTA'])\n",
    "df_pendientes = pd.read_parquet('./data/l_vm_completa_train_pendientes.parquet', engine='fastparquet')\n",
    "df_pendientes = df_pendientes[df_pendientes['PERIODO'] == 201910]\n",
    "df_pred_full = df_full.merge(df_pendientes, on=['PRODUCT_ID', 'CUSTOMER_ID', 'PERIODO'], how='left', suffixes=('', '_features'))\n",
    "# Filtrar solo los que tengan la columna A_PREDECIR con valor 1\n",
    "df_pred_full = df_pred_full[df_pred_full['A_PREDECIR'] == 1]\n",
    "# Agregar a df_pred_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "df_pred_full['MES_PROBLEMATICO'] = df_pred_full['PERIODO'].apply(lambda x: 1 if x in [201906, 201908] else 0)\n",
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_pred_full[col] = df_pred_full[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a0458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar del dataframe df_pred_full la columna 'PREDICCIONES'\n",
    "if 'PREDICCIONES' in df_pred_full.columns:\n",
    "    df_pred_full.drop(columns=['PREDICCIONES'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model_reg.predict(df_pred_full) \n",
    "df_pred_full['PREDICCIONES'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar Dataframe que contenga por cada PRODUCT_ID la suma de las predicciones y la suma de la clase observada\n",
    "df_comp = df_pred_full.groupby('PRODUCT_ID').agg({'PREDICCIONES': 'sum', 'CLASE': 'sum'}).reset_index()\n",
    "df_comp['DIF_ABS'] = np.abs(df_comp['PREDICCIONES'] - df_comp['CLASE'])\n",
    "# ordernar por la diferencia absoluta\n",
    "df_comp = df_comp.sort_values(by='DIF_ABS', ascending=False)\n",
    "df_comp.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para los primeros 30 PRODUCT_ID de df_comp generar graficos como el anterior agregando resaltado en el gráfico en el periodo 201912\n",
    "# el valor PREDICCIONES de DF_COMP para el PRODUCT_ID en el título del gráfico\n",
    "for product_id in df_comp['PRODUCT_ID'].head(30):\n",
    "    df_prod = df_full[df_full['PRODUCT_ID'] == product_id].groupby('PERIODO').agg({'TN': 'sum'}).reset_index()\n",
    "    df_prod['PERIODO'] = df_prod['PERIODO'].astype(str)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_prod, x='PERIODO', y='TN', marker='o')\n",
    "    pred_value = df_comp[df_comp[\"PRODUCT_ID\"] == product_id][\"PREDICCIONES\"].values[0]\n",
    "    dif = df_comp[df_comp[\"PRODUCT_ID\"] == product_id][\"DIF_ABS\"].values[0]\n",
    "    plt.title(f'Evolución de las TN para PRODUCT_ID {product_id} - Predicciones: {pred_value:.2f} - Dif Abs: {dif:.2f}')\n",
    "    plt.xlabel('Periodo')\n",
    "    plt.ylabel('Total de TN')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, df_prod['TN'].max() * 1.1)  # Ajustar el eje y para que comience en cero\n",
    "    plt.scatter('201912', pred_value, color='red', s=100, zorder=5, label='Predicho')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el DataFrame de salida que contiene PRODUCT_ID y la suma de las predicciones por PRODUCT_ID\n",
    "df_output = df_pred_full.groupby('PRODUCT_ID')['PREDICCIONES'].sum().reset_index()\n",
    "# Hacer que la columna de predicciones sea mayor que cero\n",
    "df_output['PREDICCIONES'] = np.where(df_output['PREDICCIONES'] < 0, 0, df_output['PREDICCIONES'])\n",
    "df_output.head(10)\n",
    "# Renombrar las columnas como product_id y tn\n",
    "df_output.columns = ['product_id', 'tn']\n",
    "# Guardar el DataFrame de salida en un archivo CSV\n",
    "df_output.to_csv('./modelos/optuna_lgbm_predictions_sin_val_sin_negativos.csv', index=False)\n",
    "# Contar los valores negativos en df_output\n",
    "negativos = df_output[df_output['tn'] < 0].shape[0]\n",
    "print(f\"Número de valores negativos en las predicciones: {negativos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaboIII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
