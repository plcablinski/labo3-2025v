{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c9b116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-26 20:04:28,871\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "import sqlite3\n",
    "import ray\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d80a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "df_pendientes = pd.read_parquet('./data/l_vm_completa_train_pendientes.parquet', engine='fastparquet')\n",
    "# Reunir los DataFrames df_full y df_pendientes por PRODUCT_ID, CUSTOMER_ID y PERIODO, agregar las \n",
    "# columnas de df_pendientes a df_full\n",
    "df_full = df_full.merge(df_pendientes, on=['PRODUCT_ID', 'CUSTOMER_ID', 'PERIODO'], how='left', suffixes=('', '_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9d3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "df_full['MES_PROBLEMATICO'] = df_full['PERIODO'].apply(lambda x: 1 if x in [201906, 201908] else 0)\n",
    "# Optimizar tipos de datos numéricos\n",
    "for col in df_full.select_dtypes(include=['int64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='integer')\n",
    "for col in df_full.select_dtypes(include=['float64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='float')\n",
    "# Variables categóricas\n",
    "# categorical_features = ['ANIO','MES','TRIMESTRE','ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','SKU_SIZE','CUSTOMER_ID','PRODUCT_ID','PLAN_PRECIOS_CUIDADOS']\n",
    "categorical_features = ['ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','PLAN_PRECIOS_CUIDADOS','MES_PROBLEMATICO']\n",
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_full[col] = df_full[col].astype('category')\n",
    "# Hacer que A_PREDECIR sea boolean si es 'S' vale True, si es 'N' False\n",
    "df_full['A_PREDECIR'] = df_full['A_PREDECIR'].map({'S': True, 'N': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea2c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar filas donde el target es válido (ni NaN ni Inf)\n",
    "df_full = df_full[df_full['CLASE_DELTA'].notnull() & np.isfinite(df_full['CLASE_DELTA'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c2e2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables predictoras y objetivo\n",
    "# filtrar que en X el periodo sea menor o igual a 201910\n",
    "# En x eliminar la columna 'CLASE' y 'CLASE_DELTA' si existen\n",
    "cols_to_drop = [col for col in ['CLASE', 'CLASE_DELTA'] if col in df_full.columns]\n",
    "X = df_full[df_full['PERIODO'] <= 201910].drop(columns=cols_to_drop)\n",
    "# Filtrar en y que el periodo sea menor o igual a 201910 y que la columna exista\n",
    "y = df_full[df_full['PERIODO'] <= 201910]['CLASE_DELTA']\n",
    "# Eliminar df_full para liberar memoria\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd36925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir los periodos de validación 201910\n",
    "#periodos_valid = [201910]\n",
    "periodos_valid = [201910]\n",
    "\n",
    "# Separar train y cinco conjuntos de validación respetando la secuencia temporal\n",
    "X_train = X[X['PERIODO'] < periodos_valid[0]]\n",
    "y_train = y[X['PERIODO'] < periodos_valid[0]]\n",
    "X_val_list = [X[X['PERIODO'] == p] for p in periodos_valid]\n",
    "y_val_list = [y[X['PERIODO'] == p] for p in periodos_valid]\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af1a4473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250626_230517\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.23\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2\n",
      "CPU Count:          28\n",
      "Memory Avail:       103.08 GB / 125.58 GB (82.1%)\n",
      "Disk Space Avail:   419.32 GB / 543.17 GB (77.2%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to False. Reason: Skip dynamic_stacking when use_bag_holdout is enabled. (use_bag_holdout=True)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"/home/pablo/Documentos/labo3-2025v/AutogluonModels/ag-20250626_230517\"\n",
      "Train Data Rows:    15084267\n",
      "Train Data Columns: 95\n",
      "Tuning Data Rows:    544570\n",
      "Tuning Data Columns: 95\n",
      "Label Column:       CLASE_DELTA\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    99718.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4933.50 MB (4.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])     :  1 | ['A_PREDECIR']\n",
      "\t\t('category', []) :  6 | ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'PLAN_PRECIOS_CUIDADOS', ...]\n",
      "\t\t('float', [])    : 76 | ['MES_SIN', 'MES_COS', 'CUST_REQUEST_TN', 'TN', 'STOCK_FINAL', ...]\n",
      "\t\t('int', [])      : 12 | ['PERIODO', 'ANIO', 'MES', 'TRIMESTRE', 'SKU_SIZE', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  4 | ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND']\n",
      "\t\t('float', [])     : 76 | ['MES_SIN', 'MES_COS', 'CUST_REQUEST_TN', 'TN', 'STOCK_FINAL', ...]\n",
      "\t\t('int', [])       : 12 | ['PERIODO', 'ANIO', 'MES', 'TRIMESTRE', 'SKU_SIZE', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['PLAN_PRECIOS_CUIDADOS', 'A_PREDECIR', 'MES_PROBLEMATICO']\n",
      "\t34.0s = Fit runtime\n",
      "\t95 features in original data used to generate 95 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4933.50 MB (5.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 37.97s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 574.54s of the 862.02s of remaining time.\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 20.635 GB out of 104.933 GB available memory (19.665%)... (20.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.36 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 127200.81s compared to 717.02s of available time.\n",
      "\tTime limit exceeded... Skipping KNeighborsUnif_BAG_L1.\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 542.13s of the 829.61s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 20.635 GB out of 99.615 GB available memory (20.715%)... (20.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.09 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 528.47s of the 815.96s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 30.81% memory usage per fold, 61.63%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=14, gpus=0, memory=30.81%)\n",
      "2025-06-26 20:06:59,950\tERROR services.py:1350 -- Failed to start the dashboard , return code 0\n",
      "2025-06-26 20:06:59,954\tERROR services.py:1375 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2025-06-26 20:06:59,955\tERROR services.py:1419 -- \n",
      "The last 20 lines of /tmp/ray/session_2025-06-26_20-06-59_251577_73078/logs/dashboard.log (it contains the error message from the dashboard): \n",
      "2025-06-26 20:06:59,855\tWARNING dashboard.py:264 -- The dashboard on node pablo-i7-linux failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/ray/dashboard/dashboard.py\", line 255, in <module>\n",
      "    loop.run_until_complete(dashboard.run())\n",
      "  File \"/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/ray/dashboard/dashboard.py\", line 78, in run\n",
      "    await self.dashboard_head.run()\n",
      "  File \"/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/ray/dashboard/head.py\", line 291, in run\n",
      "    await self._configure_http_server(modules)\n",
      "  File \"/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/ray/dashboard/head.py\", line 138, in _configure_http_server\n",
      "    self.http_server = HttpServerDashboardHead(\n",
      "  File \"/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/ray/dashboard/http_server_head.py\", line 103, in __init__\n",
      "    raise ex\n",
      "  File \"/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/ray/dashboard/http_server_head.py\", line 94, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/ray/dashboard/http_server_head.py\", line 54, in setup_static_dir\n",
      "    raise dashboard_utils.FrontendNotFoundError(\n",
      "ray.dashboard.utils.FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm ci && npm run build): '/home/pablo/anaconda3/envs/LaboIII/lib/python3.9/site-packages/ray/dashboard/client/build'\n",
      "\t-0.8205999732017517\t = Validation score   (-root_mean_squared_error)\n",
      "\t428.06s\t = Training   runtime\n",
      "\t2.84s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 77.95s of the 365.43s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.91% memory usage per fold, 65.83%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=14, gpus=0, memory=32.91%)\n",
      "\t-1.0841000080108643\t = Validation score   (-root_mean_squared_error)\n",
      "\t99.59s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 247.24s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.8205999732017517\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 247.14s of the 246.36s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.97% memory usage per fold, 71.94%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=14, gpus=0, memory=35.97%)\n",
      "\t-0.7957000136375427\t = Validation score   (-root_mean_squared_error)\n",
      "\t198.9s\t = Training   runtime\n",
      "\t1.56s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 26.77s of the 26.00s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.73% memory usage per fold, 71.46%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=14, gpus=0, memory=35.73%)\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -2.24s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 1.0}\n",
      "\t-0.7957000136375427\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 905.12s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 107276.6 rows/s (544570 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/pablo/Documentos/labo3-2025v/AutogluonModels/ag-20250626_230517\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:20:27,925\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "#ray.init(include_dashboard=False)\n",
    "\n",
    "# Dataset de entrenamiento (PERIODO < 201910)\n",
    "df_train = X_train.copy()\n",
    "df_train['CLASE_DELTA'] = y_train\n",
    "\n",
    "# Dataset de validación (PERIODO == 201910)\n",
    "df_val = X_val_list[0].copy()\n",
    "df_val['CLASE_DELTA'] = y_val_list[0]\n",
    "\n",
    "# Entrenamiento con AutoGluon\n",
    "predictor = TabularPredictor(\n",
    "    label='CLASE_DELTA',\n",
    "    problem_type='regression'\n",
    ").fit(\n",
    "    train_data=df_train,\n",
    "    tuning_data=df_val,\n",
    "    time_limit=900,\n",
    "    presets='best_quality',\n",
    "    use_bag_holdout=True  # <--- esta línea resuelve tu error\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32fcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos sobre los que quiero hacer predicciones\n",
    "gc.collect()\n",
    "df_pred_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')\n",
    "df_pred_full = df_pred_full[df_pred_full['PERIODO'] == 201910].drop(columns=['CLASE', 'CLASE_DELTA'])\n",
    "df_pendientes = pd.read_parquet('./data/l_vm_completa_train_pendientes.parquet', engine='fastparquet')\n",
    "df_pendientes = df_pendientes[df_pendientes['PERIODO'] == 201910]\n",
    "df_pred_full = df_pred_full.merge(df_pendientes, on=['PRODUCT_ID', 'CUSTOMER_ID', 'PERIODO'], how='left', suffixes=('', '_features'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c80478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo los que tengan la columna A_PREDECIR con valor 1\n",
    "df_pred_full = df_pred_full[df_pred_full['A_PREDECIR'] == 'S']\n",
    "# Hacer que A_PREDECIR sea boolean si es 'S' vale True, si es 'N' False\n",
    "df_pred_full['A_PREDECIR'] = df_pred_full['A_PREDECIR'].map({'S': True, 'N': False})\n",
    "\n",
    "# Agregar a df_pred_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "df_pred_full['MES_PROBLEMATICO'] = df_pred_full['PERIODO'].apply(lambda x: 1 if x in [201906, 201908] else 0)\n",
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_pred_full[col] = df_pred_full[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52dc5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar del dataframe df_pred_full la columna 'PREDICCIONES'\n",
    "if 'PREDICCIONES' in df_pred_full.columns:\n",
    "    df_pred_full.drop(columns=['PREDICCIONES'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac20b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_pred_full.empty:\n",
    "    # Eliminar la columna target si está presente (puede venir con NaN)\n",
    "    df_pred_full = df_pred_full.drop(columns=[\"CLASE_DELTA\"], errors=\"ignore\")\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions = predictor.predict(df_pred_full)\n",
    "    \n",
    "    # Guardar las predicciones en el DataFrame\n",
    "    df_pred_full[\"PREDICCIONES\"] = predictions\n",
    "else:\n",
    "    print(\"df_pred_full está vacío, no se generaron predicciones.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaboIII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
