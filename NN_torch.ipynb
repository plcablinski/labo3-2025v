{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c9b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import optuna\n",
    "import sqlite3\n",
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Rutas de entrada y salida\n",
    "input_path = './data/l_vm_completa_normalizada_fe.parquet'\n",
    "output_train = './data/df_train.parquet'\n",
    "output_val = './data/df_val.parquet'\n",
    "\n",
    "# Periodos para división\n",
    "periodo_train_max = 201908\n",
    "periodos_val = [201909, 201910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4257dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_limpio = './data/df_train_limpio.parquet'\n",
    "output_val_limpio = './data/df_val_limpio.parquet'\n",
    "\n",
    "df_train_limpio = pd.read_parquet(output_train_limpio, engine='fastparquet')\n",
    "df_val_limpio = pd.read_parquet(output_val_limpio, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data for TabularNeuralNetTorchModel has: 6730977 examples, 380 features (380 vector, 0 embedding)\n",
      "Training on GPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=380, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training tabular neural network for up to 40 epochs...\n",
      "Epoch 1 (Update 3286).\tTrain loss: 0.3348, Val mean_absolute_error: -0.2515, Best Epoch: 1\n",
      "Epoch 2 (Update 6572).\tTrain loss: 0.3095, Val mean_absolute_error: -0.2561, Best Epoch: 1\n",
      "Epoch 3 (Update 9858).\tTrain loss: 0.3022, Val mean_absolute_error: -0.2468, Best Epoch: 3\n",
      "Epoch 4 (Update 13144).\tTrain loss: 0.2976, Val mean_absolute_error: -0.2477, Best Epoch: 3\n",
      "Epoch 5 (Update 16430).\tTrain loss: 0.2948, Val mean_absolute_error: -0.2563, Best Epoch: 3\n",
      "Epoch 6 (Update 19716).\tTrain loss: 0.293, Val mean_absolute_error: -0.2444, Best Epoch: 6\n",
      "Epoch 7 (Update 23002).\tTrain loss: 0.291, Val mean_absolute_error: -0.2481, Best Epoch: 6\n",
      "Epoch 8 (Update 26288).\tTrain loss: 0.2896, Val mean_absolute_error: -0.2481, Best Epoch: 6\n",
      "Epoch 9 (Update 29574).\tTrain loss: 0.2883, Val mean_absolute_error: -0.2468, Best Epoch: 6\n",
      "Epoch 10 (Update 32860).\tTrain loss: 0.2882, Val mean_absolute_error: -0.244, Best Epoch: 10\n",
      "Epoch 11 (Update 36146).\tTrain loss: 0.2872, Val mean_absolute_error: -0.2458, Best Epoch: 10\n",
      "Epoch 12 (Update 39432).\tTrain loss: 0.2868, Val mean_absolute_error: -0.2525, Best Epoch: 10\n",
      "Epoch 13 (Update 42718).\tTrain loss: 0.2863, Val mean_absolute_error: -0.2557, Best Epoch: 10\n",
      "Epoch 14 (Update 46004).\tTrain loss: 0.286, Val mean_absolute_error: -0.2539, Best Epoch: 10\n",
      "Epoch 15 (Update 49290).\tTrain loss: 0.2849, Val mean_absolute_error: -0.2517, Best Epoch: 10\n",
      "Epoch 16 (Update 52576).\tTrain loss: 0.2849, Val mean_absolute_error: -0.2506, Best Epoch: 10\n",
      "Epoch 17 (Update 55862).\tTrain loss: 0.2848, Val mean_absolute_error: -0.2503, Best Epoch: 10\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label='CLASE_DELTA_ZSCORE',\n",
    "    problem_type='regression',\n",
    "    eval_metric='mean_absolute_error',\n",
    "    verbosity=4,\n",
    "    path='AutogluonModels/nn_gpu_full_train'\n",
    ").fit(\n",
    "    train_data=df_train_limpio,         # tus 7 millones\n",
    "    tuning_data=df_val_limpio,          # tus 0.5 millones separados temporalmente\n",
    "    time_limit=7200,                    # 2 horas (ajustable según complejidad)\n",
    "    use_bag_holdout=False,              # obligatorio si usás tuning_data\n",
    "    presets='medium',                   # no activa stacking ni mezcla datos\n",
    "    hyperparameters={\n",
    "        'NN_TORCH': [{\n",
    "            'num_epochs': 40,\n",
    "            'learning_rate': 0.005,\n",
    "            'dropout_prob': 0.1,\n",
    "            'batch_size': 2048,         # mayor batch para mejor uso de GPU\n",
    "            'hidden_size': 256,         # red más potente\n",
    "            'ag_args': {'name_suffix': 'GPU_Full'},\n",
    "            'ag_args_fit': {'num_gpus': 1}\n",
    "        }]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea892c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor = TabularPredictor(\n",
    "    label='CLASE_DELTA_ZSCORE',\n",
    "    problem_type='regression',\n",
    "    eval_metric='mean_absolute_error',\n",
    "    verbosity=4,\n",
    "    path='AutogluonModels/nn_lightfast_test_15min'\n",
    ").fit(\n",
    "    train_data=df_entrenamiento_total,\n",
    "    time_limit=900,  # 15 minutos = 900 segundos\n",
    "    hyperparameters={\n",
    "        'NN_TORCH': {\n",
    "            'num_epochs': 5,              # reducir epochs acelera bastante\n",
    "            'learning_rate': 0.01,\n",
    "            'dropout_prob': 0.1,\n",
    "            'batch_size': 1024,           # más batch size = mejor para GPU\n",
    "            'hidden_size': 64,            # red más chica = más rápida\n",
    "            'ag_args': {'name_suffix': 'GPU_15min'},\n",
    "            'ag_args_fit': {'num_gpus': 1}\n",
    "        }\n",
    "    },\n",
    "    presets='medium_quality',\n",
    "    use_bag_holdout=False  # sin bagging ni tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9368a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label='CLASE_DELTA_ZSCORE',\n",
    "    problem_type='regression',\n",
    "    eval_metric='mean_absolute_error',\n",
    "    verbosity=4,\n",
    "    path='AutogluonModels/nn_lightfast_test'\n",
    ").fit(\n",
    "    train_data=df_entrenamiento_total,\n",
    "    time_limit=1800,  # 30 minutos\n",
    "    hyperparameters={\n",
    "        'NN_TORCH': {\n",
    "            'num_epochs': 10,\n",
    "            'learning_rate': 0.01,\n",
    "            'dropout_prob': 0.1,\n",
    "            'batch_size': 512,\n",
    "            'hidden_size': 128,\n",
    "            'ag_args': {'name_suffix': 'GPU_Basic'},\n",
    "            'ag_args_fit': {'num_gpus': 1}\n",
    "        }\n",
    "    },\n",
    "    presets='medium_quality',\n",
    "    use_bag_holdout=False  # sin bagging, directo\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" target_col = 'CLASE_DELTA_ZSCORE'  # ajustá si tu columna objetivo es otra\n",
    "\n",
    "hyperparameters = {\n",
    "    'NN_TORCH': [\n",
    "        {'ag_args': {'name_suffix': 'LightFast'}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "predictor_nn_lightfast = TabularPredictor(\n",
    "    label='CLASE_DELTA_ZSCORE',\n",
    "    problem_type='regression',\n",
    "    eval_metric='mean_absolute_error',\n",
    "    verbosity=3,\n",
    "    path='AutogluonModels/nn_lightfast_retrain'\n",
    ").fit(\n",
    "    train_data=df_train_limpio,\n",
    "    tuning_data=df_val_limpio,\n",
    "    time_limit=7200,\n",
    "    # hyperparameters={\n",
    "    #     'NN_TORCH': [{\n",
    "    #         'num_epochs': 8,\n",
    "    #         'learning_rate': 0.01,\n",
    "    #         'dropout_prob': 0.1,\n",
    "    #         'weight_decay': 1e-5,\n",
    "    #         'batch_size': 256,\n",
    "    #         'hidden_size': 128,\n",
    "    #         'ag_args': {'name_suffix': 'NN_LightFast'},\n",
    "    #         'ag_args_fit': {'num_gpus': 1}\n",
    "    #     }]\n",
    "    # },\n",
    "    hyperparameters={\n",
    "        'NN_TORCH': {}\n",
    "    },\n",
    "    hyperparameter_tune_kwargs='auto',\n",
    "    presets='best_quality',\n",
    "    use_bag_holdout=False\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" target_col = 'CLASE_DELTA_ZSCORE'  # Ajustá si es necesario\n",
    "\n",
    "df_entrenamiento_total = pd.concat([df_train_limpio, df_val_limpio], axis=0)\n",
    "del df_train_limpio, df_val_limpio\n",
    "gc.collect()\n",
    "\n",
    "predictor_nn_lightfast = TabularPredictor(\n",
    "    label=target_col,\n",
    "    problem_type='regression',\n",
    "    eval_metric='mean_absolute_error',\n",
    "    verbosity=3,\n",
    "    path='AutogluonModels/nn_lightfast_retrain'\n",
    ").fit(\n",
    "    train_data=df_entrenamiento_total,\n",
    "    #train_data=df_train_limpio,\n",
    "    #tuning_data=df_val_limpio,\n",
    "    time_limit=7200,\n",
    "    presets='best_quality',\n",
    "    hyperparameters={\n",
    "        'NN_TORCH': {\n",
    "            'ag_args': {'name_suffix': 'NN_LightFast'},\n",
    "            'ag_args_fit': {'num_gpus': 1}\n",
    "        }\n",
    "    },\n",
    "    hyperparameter_tune_kwargs='auto',\n",
    "    use_bag_holdout=True\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50069cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 3. Evaluación del modelo ===\n",
    "\n",
    "lb = predictor_nn_lightfast.leaderboard(df_val_limpio, extra_metrics=['mean_absolute_error', 'median_absolute_error', 'r2'], silent=False)\n",
    "\n",
    "# === 4. Importancia de características ===\n",
    "\n",
    "feature_importance = predictor_nn_lightfast.feature_importance(df_val_limpio)\n",
    "\n",
    "# === 5. Guardado del modelo ya está hecho con `path=...` ===\n",
    "\n",
    "print(\"✅ Reentrenamiento completado y modelo guardado en 'AutogluonModels/nn_lightfast_retrain'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce344269",
   "metadata": {},
   "source": [
    "✅ Paso 1: Calcular errores de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Predicciones y error absoluto\n",
    "df_val['y_true'] = df_val['CLASE_DELTA_ZSCORE']\n",
    "df_val['y_pred'] = predictor.predict(df_val)\n",
    "df_val['error_abs'] = abs(df_val['y_true'] - df_val['y_pred'])\n",
    "df_val['error_signed'] = df_val['y_pred'] - df_val['y_true']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47114e7",
   "metadata": {},
   "source": [
    "📊 Paso 2: Histogramas de error absoluto y error signado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df_val['error_abs'], bins=50, kde=True)\n",
    "plt.title('Distribución del Error Absoluto')\n",
    "plt.xlabel('|y_pred - y_true|')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df_val['error_signed'], bins=50, kde=True)\n",
    "plt.title('Distribución del Error Signado')\n",
    "plt.xlabel('y_pred - y_true')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18881d9e",
   "metadata": {},
   "source": [
    "📈 Paso 3: Error vs. Valor real (dispersión)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df_val['y_true'], y=df_val['error_signed'], alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Error Signado vs. Valor Real')\n",
    "plt.xlabel('Valor real')\n",
    "plt.ylabel('Error (pred - real)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c603b",
   "metadata": {},
   "source": [
    "🧩 Paso 4: Promedio de error por grupo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_errores_producto = df_val.groupby('PRODUCT_ID')['error_abs'].mean().sort_values(ascending=False).head(20)\n",
    "top_errores_producto.plot(kind='bar', figsize=(10,4), title='Top 20 PRODUCT_ID con mayor error promedio')\n",
    "plt.ylabel('Error Absoluto Medio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_errores_producto = df_val.groupby('CUSTOMER_ID')['error_abs'].mean().sort_values(ascending=False).head(20)\n",
    "top_errores_producto.plot(kind='bar', figsize=(10,4), title='Top 20 CUSTOMER_ID con mayor error promedio')\n",
    "plt.ylabel('Error Absoluto Medio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_train_limpio, df_val_limpio], ignore_index=True)\n",
    "del df_train_limpio, df_val_limpio\n",
    "gc.collect()\n",
    "predictor_full = predictor_nn_lightfast.refit_full(train_data=df_full)\n",
    "predictor_full.save(\"AutogluonModels/nn_lightfast_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac970579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Combinar entrenamiento + validación\n",
    "df_full = pd.concat([df_train, df_val], axis=0)\n",
    "del df_train, df_val\n",
    "gc.collect() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31acb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reentrenar el mejor modelo con TODOS los datos disponibles\n",
    "# predictor_full = predictor.refit_full(train_data=df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar los modelos disponibles (el mejor ahora tiene el sufijo '_FULL')\n",
    "print(\"Modelos disponibles luego del refit completo:\")\n",
    "print(predictor.leaderboard(silent=True)['model'].tolist())\n",
    "# Eliminar modelos intermedios para liberar espacio\n",
    "predictor.delete_models(models_to_keep='best', dry_run=False)\n",
    "\n",
    "# Confirmar que solo queda el modelo reentrenado\n",
    "print(\"\\nModelos restantes después de eliminar los intermedios:\")\n",
    "print(predictor.leaderboard(silent=True)['model'].tolist())\n",
    "\n",
    "# (Opcional) Guardar el predictor final si querés usarlo luego sin volver a cargar todo\n",
    "predictor.save('./data/modelo_final_autogluon')\n",
    "\n",
    "# ---  Liberar memoria ---\n",
    "del df_full\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos sobre los que quiero hacer predicciones\n",
    "df_pred_full = pd.read_parquet('./data/l_vm_completa_normalizada_fe.parquet', engine='fastparquet')\n",
    "# Dejo solo los datos del periodo 201910 y que A_PREDECIR sea True\n",
    "# Filtrar solo los datos del periodo 201910 y donde A_PREDECIR sea True\n",
    "df_pred_full = df_pred_full[\n",
    "    (df_pred_full['PERIODO'] == 201910) & (df_pred_full['A_PREDECIR'] == True)\n",
    "].drop(columns=['CLASE_ZSCORE', 'CLASE_DELTA_ZSCORE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7da41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar las predicciones usando el predictor original\n",
    "predictions = predictor.predict(df_pred_full)\n",
    "# Agregar las predicciones al DataFrame original\n",
    "df_pred_full['CLASE_DELTA_ZSCORE'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la lista de columas del DataFrame con las predicciones\n",
    "print(\"Columnas del DataFrame con las predicciones:\")\n",
    "print(df_pred_full.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d459184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dernormalizar la columna CLASE_DELTA_ZSCORE\n",
    "df_pred_full['CLASE_DELTA'] = df_pred_full['CLASE_DELTA_ZSCORE'] * df_pred_full['CLASE_DELTA_STD'] + df_pred_full['CLASE_DELTA_MEAN']\n",
    "df_pred_full['TN'] = df_pred_full['TN_ZSCORE'] * df_pred_full['TN_STD'] + df_pred_full['TN_MEAN']\n",
    "# Agregar la columna TN_PREDICT que sea la suma de TN y CLASE_DELTA y si es menor que cero, poner cero\n",
    "df_pred_full['TN_PREDICT'] = df_pred_full['TN'] + df_pred_full['CLASE_DELTA']\n",
    "df_pred_full['TN_PREDICT'] = df_pred_full['TN_PREDICT'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d822ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar Dataframe que contenga por cada PRODUCT_ID la suma de TN_PREDICT\n",
    "df_final = df_pred_full.groupby('PRODUCT_ID').agg({'TN_PREDICT': 'sum'}).reset_index()\n",
    "df_final = df_final.rename(columns={'PRODUCT_ID': 'product_id', 'TN_PREDICT': 'tn'})\n",
    "# Guardar el DataFrame df_final en un archivo CSV\n",
    "df_final.to_csv('./modelos/autoglun_normalizando_clase_delta.csv', index=False)\n",
    "df_final.shape\n",
    "\n",
    "# Para instalar AutoGluon con soporte para `autogluon.core.space` en conda, ejecuta:\n",
    "# \n",
    "# conda install -c conda-forge autogluon\n",
    "# \n",
    "# O si prefieres usar pip dentro de tu entorno conda:\n",
    "# \n",
    "# pip install autogluon\n",
    "# \n",
    "# Luego podrás usar:\n",
    "# from autogluon.core import space as ag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaboIII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
