{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d6f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import optuna\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea074aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e06b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizar tipos de datos numéricos\n",
    "for col in df_full.select_dtypes(include=['int64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='integer')\n",
    "for col in df_full.select_dtypes(include=['float64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='float')\n",
    "# Variables categóricas\n",
    "# categorical_features = ['ANIO','MES','TRIMESTRE','ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','SKU_SIZE','CUSTOMER_ID','PRODUCT_ID','PLAN_PRECIOS_CUIDADOS']\n",
    "categorical_features = ['ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','PLAN_PRECIOS_CUIDADOS']\n",
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_full[col] = df_full[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e06cb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables predictoras y objetivo\n",
    "# filtrar que en X el periodo sea menor o igual a 201910\n",
    "# En x eliminar la columna 'CLASE' y 'CLASE_DELTA'\n",
    "X = df_full[df_full['PERIODO'] <= 201910].drop(columns=['CLASE', 'CLASE_DELTA']) \n",
    "# Filtrar en y que el periodo sea menor o igual a 201910\n",
    "y = df_full[df_full['PERIODO'] <= 201910]['CLASE']\n",
    "# Agrega una columna 'CLASE_BIN' que sea 1 si 'CLASE' es mayor a 0, y 0 en caso contrario para clasificación binaria\n",
    "# El tipo es cero o uno por lo que se convierte a int del menor tamaño posible\n",
    "df_full['CLASE_BIN'] = (df_full['CLASE'] > 0).astype('int8')\n",
    "y_bin = df_full[df_full['PERIODO'] <= 201910]['CLASE_BIN']\n",
    "\n",
    "# Eliminar df_full para liberar memoria\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec1811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir los periodos de validación 201910\n",
    "periodos_valid = [201910]\n",
    "\n",
    "# Separar train y cinco conjuntos de validación respetando la secuencia temporal\n",
    "X_train = X[X['PERIODO'] < periodos_valid[0]]\n",
    "y_train = y[X['PERIODO'] < periodos_valid[0]]\n",
    "y_bin_train = y_bin[X['PERIODO'] < periodos_valid[0]]\n",
    "\n",
    "X_val_list = [X[X['PERIODO'] == p] for p in periodos_valid]\n",
    "y_val_list = [y[X['PERIODO'] == p] for p in periodos_valid]\n",
    "y_bin_val_list = [y_bin[X['PERIODO'] == p] for p in periodos_valid]\n",
    "\n",
    "del X, y, y_bin\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca270248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 12:05:53,826] Using an existing study with name 'lgbm_regression' instead of creating a new one.\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.125399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9638\n",
      "[LightGBM] [Info] Number of data points in the train set: 10864094, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 0.095022\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[30]\tvalidation_201910's mape_sum: 1.40363\n",
      "[60]\tvalidation_201910's mape_sum: 1.12076\n",
      "[90]\tvalidation_201910's mape_sum: 0.910108\n",
      "[120]\tvalidation_201910's mape_sum: 0.751255\n",
      "[150]\tvalidation_201910's mape_sum: 0.631179\n",
      "[180]\tvalidation_201910's mape_sum: 0.537409\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalidation_201910's mape_sum: 0.489242\n",
      "Evaluated only: mape_sum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.264714:   2%|▏         | 1/50 [05:32<4:31:32, 332.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2: mape_sum=0.48924\n",
      "[I 2025-06-11 12:11:26,331] Trial 2 finished with value: 0.4892419997986056 and parameters: {'num_leaves': 312, 'learning_rate': 0.008050660937447149, 'feature_fraction': 0.6936674808518878, 'bagging_fraction': 0.7040158293402152, 'bagging_freq': 2, 'min_data_in_leaf': 106, 'max_depth': 12, 'lambda_l1': 2.6892767082927276, 'lambda_l2': 2.3180461917118245, 'min_gain_to_split': 0.350817222537568}. Best is trial 0 with value: 0.26471426940189174.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.036041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9638\n",
      "[LightGBM] [Info] Number of data points in the train set: 10864094, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 0.095022\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    }
   ],
   "source": [
    "# # Hacer un try-except para cargar el modelo de LightGBM para regresión\n",
    "# try:\n",
    "#     # Verificar si el archivo del modelo de regresión existe\n",
    "#     if not os.path.exists('./modelos/lgbm_model_reg.txt'):\n",
    "#         raise FileNotFoundError(\"El modelo de regresión no se encuentra en la ruta especificada.\")\n",
    "#     model_reg = lgb.Booster(model_file='./modelos/lgbm_model_reg.txt')\n",
    "#     print(\"Modelo de regresión cargado exitosamente.\")\n",
    "# except FileNotFoundError:\n",
    "#     model_reg = None\n",
    "# Crear los datasets de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "val_data_list = []\n",
    "for i in range(len(periodos_valid)):\n",
    "    val_dataset = lgb.Dataset(X_val_list[i], label=y_val_list[i], categorical_feature=categorical_features)\n",
    "    # Inyectar PRODUCT_ID como atributo extra\n",
    "    val_dataset.PRODUCT_ID = X_val_list[i]['PRODUCT_ID'].values\n",
    "    val_data_list.append(val_dataset)\n",
    "\n",
    "def mape_sum_lgb(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    product_id = getattr(dataset, 'PRODUCT_ID', None)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    if denom == 0:\n",
    "        return 'mape_sum', 0.0, False\n",
    "    if product_id is not None:\n",
    "        df_pred = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'PRODUCT_ID': product_id})\n",
    "        mape = np.sum(np.abs(df_pred.groupby('PRODUCT_ID')['y_true'].sum() - df_pred.groupby('PRODUCT_ID')['y_pred'].sum())) / denom\n",
    "    else:\n",
    "        mape = np.sum(np.abs(y_true.sum() - y_pred.sum())) / denom\n",
    "    mape = np.nan_to_num(mape, nan=0.0)\n",
    "    return 'mape_sum', mape, False  # False: menor es mejor\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'None',  # Solo métrica personalizada\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 512),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 16),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 1.0),\n",
    "        'verbose': 1,\n",
    "        'feature_pre_filter': False\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=200,\n",
    "        valid_sets=val_data_list,\n",
    "        valid_names=[f'validation_{p}' for p in periodos_valid],\n",
    "        feval=mape_sum_lgb,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=30, first_metric_only=True),\n",
    "            lgb.log_evaluation(period=30)\n",
    "        ]\n",
    "    )\n",
    "    best_score = model.best_score[f'validation_{periodos_valid[0]}']['mape_sum']\n",
    "    print(f\"Trial {trial.number}: mape_sum={best_score:.5f}\")\n",
    "    return best_score\n",
    "\n",
    "# Guardar resultados en base de datos sqlite\n",
    "storage_url = \"sqlite:///./modelos/optuna.db\"\n",
    "study = optuna.create_study(direction='minimize', study_name=\"lgbm_regression\", storage=storage_url, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)  # Puedes ajustar n_trials\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Entrena el modelo final con los mejores hiperparámetros encontrados\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'None'\n",
    "\n",
    "model_reg = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=val_data_list,\n",
    "    valid_names=[f'validation_{p}' for p in periodos_valid],\n",
    "    feval=mape_sum_lgb,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50, first_metric_only=True),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.makedirs('./modelos', exist_ok=True)\n",
    "model_reg.save_model('./modelos/lgbm_model_reg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la importancia de cada variable\n",
    "importancia = model_reg.feature_importance(importance_type='gain')\n",
    "nombres = X_train.columns\n",
    "\n",
    "# Crear un DataFrame ordenado por importancia\n",
    "df_importancia = pd.DataFrame({'feature': nombres, 'importance': importancia})\n",
    "df_importancia = df_importancia.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Mostrar las variables más importantes\n",
    "print(df_importancia)\n",
    "\n",
    "# Si quieres visualizarlo gráficamente:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(df_importancia['feature'], df_importancia['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Importancia de variables LightGBM')\n",
    "plt.xlabel('Importancia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREDICCIÓN Y EVALUACIÓN por periodo ---\n",
    "for i, (X_val, y_val, periodo) in enumerate(zip(X_val_list, y_val_list, periodos_valid)):\n",
    "    proba_no_cero = model_clasif.predict(X_val)\n",
    "    umbral = 0.25\n",
    "    pred_bin = (proba_no_cero > umbral)\n",
    "    pred_reg = np.zeros(len(X_val))\n",
    "    if pred_bin.sum() > 0:\n",
    "        pred_reg[pred_bin] = model_reg.predict(X_val[pred_bin])\n",
    "    y_val_real = y_val.values\n",
    "    # WAPE solo en no-cero\n",
    "    mask_nocero = y_val_real != 0\n",
    "    if mask_nocero.sum() > 0:\n",
    "        wape_nocero = np.sum(np.abs(y_val_real[mask_nocero] - pred_reg[mask_nocero])) / np.sum(np.abs(y_val_real[mask_nocero]))\n",
    "        print(f\"WAPE (no-cero) periodo {periodo}: {wape_nocero:.4f}\")\n",
    "    else:\n",
    "        print(f\"WAPE (no-cero) periodo {periodo}: N/A (no hay valores no-cero)\")\n",
    "    # También puedes seguir mostrando el WAPE global\n",
    "    wape = np.sum(np.abs(y_val_real - pred_reg)) / np.sum(np.abs(y_val_real))\n",
    "    print(f\"WAPE global periodo {periodo}: {wape:.4f}\")\n",
    "    print(f\"Valores distintos de cero en pred_reg: {(pred_reg != 0).sum()} de {len(pred_reg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armar listado de las 100 predicioones con mayor diferencia entre predicción y valor real con product_id, customer_id \n",
    "df_val = X_val_list[0].copy()\n",
    "df_val['CLASE_REAL'] = y_val_list[0].values\n",
    "df_val['CLASE_PRED'] = pred_reg\n",
    "df_val['DIF'] = np.abs(df_val['CLASE_REAL'] - df_val['CLASE_PRED'])\n",
    "df_val = df_val.sort_values(by='DIF', ascending=False).head(50)\n",
    "df_val[['PRODUCT_ID', 'CUSTOMER_ID', 'CLASE_REAL', 'CLASE_PRED', 'DIF']]\n",
    "# Agrupar las diferencias por PRODUCT_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dif_product = df_val.groupby('PRODUCT_ID')['DIF'].sum().reset_index()\n",
    "print(\"Diferencias promedio por PRODUCT_ID:\")   \n",
    "df_dif_product.sort_values(by='DIF', ascending=False).head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1dbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar las diferencias por CUSTOMER_ID\n",
    "df_dif_customer = df_val.groupby('CUSTOMER_ID')['DIF'].sum().reset_index()\n",
    "print(\"Diferencias promedio por CUSTOMER_ID:\")\n",
    "df_dif_customer.sort_values(by='DIF', ascending=False).head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6453469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos sobre los que quiero hacer predicciones\n",
    "\n",
    "conn =  cx_Oracle.connect(user=\"pc\",password=\"p201404\",dsn=\"siatchdesa\")\n",
    "query = \"select * from L_VM_COMPLETA where periodo = 201912\" \n",
    "df_pred = pd.read_sql(query, conn, chunksize=1000000)  # Lee en chunks para no llenar la RAM\n",
    "\n",
    "# Para concatenar todos los chunks en un solo DataFrame (si tienes suficiente RAM)\n",
    "df_pred_full = pd.concat(df_pred, ignore_index=True)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_pred_full[col] = df_pred_full[col].astype('category')\n",
    "\n",
    "# Con el modelo entrenado, hacemos predicciones \n",
    "X_pred = df_pred_full[['PERIODO','CUSTOMER_ID','PRODUCT_ID',\n",
    "'TN_DELTA_01','TN_DELTA_02','TN_DELTA_03','TN_DELTA_06','TN_DELTA_12']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.predict(X_pred)\n",
    "# Agregar las predicciones al DataFrame original\n",
    "df_pred_full['PREDICCIONES'] = predictions\n",
    "# Imprimir las primeras filas del DataFrame con las predicciones\n",
    "print(df_pred_full.head())\n",
    "# Guardar el DataFrame con las predicciones en un archivo CSV\n",
    "df_pred_full.to_csv('predicciones.csv', index=False)\n",
    "# Imprimir el número de filas y columnas del DataFrame con las predicciones\n",
    "print(f\"Número de filas: {df_pred_full.shape[0]}, Número de columnas: {df_pred_full.shape[1]} con predicciones.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con el DataFrame de predicción, actualizamos la base de datos\n",
    "# el criterio es actualizar la tabla L_VM_COMPLETA_PREDICCIONES con las nuevas predicciones\n",
    "# la columnna PREDICCIONES se debe actualizar con los nuevos valores\n",
    "# la clave primaria es (PERIODO, CUSTOMER_ID, PRODUCT_ID)\n",
    "# Hacer commit cada 10000 filas para evitar problemas de memoria\n",
    "\n",
    "# Conectar a la base de datos para actualizar los datos de predicción\n",
    "conn = cx_Oracle.connect(user=\"pc\", password=\"p201404\", dsn=\"siatchdesa\")\n",
    "# Crear un cursor para ejecutar las actualizaciones\n",
    "cursor = conn.cursor()\n",
    "update_query = \"\"\"\n",
    "    UPDATE L_VM_COMPLETA_PREDICCIONES\n",
    "    SET PREDICCION = NULL\n",
    "\"\"\"\n",
    "cursor.execute(update_query)\n",
    "# Hacer commit para aplicar el cambio de NULL\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Imprimir mensaje de inicio de actualización\n",
    "print(\"Iniciando actualización de la tabla L_VM_COMPLETA_PREDICCIONES con las nuevas predicciones.\")\n",
    "\n",
    "# Iterar sobre las filas del DataFrame con las predicciones\n",
    "for index, row in df_pred_full.iterrows():\n",
    "    periodo = row['PERIODO']\n",
    "    customer_id = row['CUSTOMER_ID']\n",
    "    product_id = row['PRODUCT_ID']\n",
    "    prediccion = row['TN'] + row['PREDICCIONES']\n",
    "   # prediccion = row['PREDICCIONES']\n",
    "    \n",
    "    # Actualizar la tabla L_DATOS_PREDICCION con la nueva predicción\n",
    "    update_query = \"\"\"\n",
    "        UPDATE L_VM_COMPLETA_PREDICCIONES\n",
    "        SET PREDICCION = :prediccion\n",
    "        WHERE PERIODO = :periodo AND CUSTOMER_ID = :customer_id AND PRODUCT_ID = :product_id\n",
    "    \"\"\"\n",
    "    cursor.execute(update_query, {'prediccion': prediccion, 'periodo': periodo, 'customer_id': customer_id, 'product_id': product_id})  \n",
    "    # Hacer commit cada 10000 filas para evitar problemas de memoria\n",
    "    if index % 10000 == 0:\n",
    "        conn.commit()\n",
    "        print(f\"Actualizadas {index} filas de L_VM_COMPLETA_PREDICCIONES con las nuevas predicciones.\")\n",
    "# Confirmar los cambios en la base de datos\n",
    "conn.commit()\n",
    "# Cerrar el cursor y la conexión\n",
    "cursor.close()\n",
    "conn.close()\n",
    "# Imprimir mensaje de finalización\n",
    "print(\"Actualización de la tabla  completada con las nuevas predicciones.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oracle_parquet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
