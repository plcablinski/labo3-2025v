{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41eccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9628af",
   "metadata": {},
   "source": [
    "El objetivo de este Notebook es preparar los datos para modelar con redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducir memoria automáticamente\n",
    "def optimizar_memoria(df):\n",
    "    for col in df.select_dtypes(include=['int64', 'int32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    for col in df.select_dtypes(include=['float64', 'float32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11bc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar de df_agrupado las filas donde la columna A_PREDECIR sea 'N'\n",
    "df_full = df_full[df_full['A_PREDECIR'] != 'N']\n",
    "df_full = df_full.drop(columns=['A_PREDECIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado = (\n",
    "    df_full.groupby([\n",
    "        'ORDINAL','PERIODO', 'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE',\n",
    "        'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'PRODUCT_ID'\n",
    "    ], as_index=False)[['TN', 'CLASE']].sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las filas de df_agrupado ordernadas por TN de menor a mayor\n",
    "df_agrupado = df_agrupado.sort_values(by='TN', ascending=True)\n",
    "# Mostrar las primeras 10 filas de df_agrupado\n",
    "print(df_agrupado.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar una columna que indique la diferencia en ORDINAL entre el ORDINAL actual y el ORDINAL anterior donde TN sea mayor a 0\n",
    "# para ese PRODUCT_ID\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_grupo(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_PRODUCT_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_parallel(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_ID'] = 36  # valor inicial\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_ID'] = df['MESES_SIN_COMPRAR_PRODUCT_ID'].astype('int16')\n",
    "\n",
    "    # Agrupar por cliente y producto\n",
    "    grupos = list(df.groupby(['PRODUCT_ID']))\n",
    "\n",
    "    # Procesar en paralelo\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_grupo)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    # Concatenar todos los resultados\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "\n",
    "\n",
    "df_agrupado = agregar_diferencia_ordinal_parallel(df_agrupado, n_jobs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "# Calcular los días del mes usando las columnas ANIO y MES\n",
    "\n",
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si ANIO==2019 y MES en [6, 8, 10], y 0 en caso contrario\n",
    "df_agrupado['MES_PROBLEMATICO'] = np.where(\n",
    "       (df_agrupado['ANIO'] == 2019) & (df_agrupado['MES'].isin([6, 8, 10])),\n",
    "       1., 0.0\n",
    ")\n",
    "df_agrupado['MES_PROBLEMATICO'] = df_agrupado['MES_PROBLEMATICO'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calcular_pendientes_grupo(group, periodos_list):\n",
    "    group = group.sort_values(by='PERIODO').copy()\n",
    "    n = len(group)\n",
    "    y_values = group['TN'].values\n",
    "    for cant in periodos_list:\n",
    "        means = np.zeros(n)\n",
    "        pendientes = np.zeros(n)\n",
    "        ewmas = np.zeros(n)\n",
    "        medians = np.zeros(n)\n",
    "        minimo = np.zeros(n)\n",
    "        maximo = np.zeros(n)\n",
    "        stds = np.zeros(n)\n",
    "        skews = np.zeros(n)\n",
    "        kurts = np.zeros(n)\n",
    "        growths = np.zeros(n)\n",
    "        iqrs = np.zeros(n)\n",
    "        sums = np.zeros(n)\n",
    "        count_pos = np.zeros(n)\n",
    "        pct_zero = np.zeros(n)\n",
    "        last = np.zeros(n)\n",
    "        last_diff = np.zeros(n)\n",
    "\n",
    "        coef_var = np.zeros(n)\n",
    "        maxmin_ratio = np.zeros(n)\n",
    "        rango = np.zeros(n)\n",
    "        rango_rel = np.zeros(n)\n",
    "        last_vs_median = np.zeros(n)\n",
    "        cambio_ventana = np.zeros(n)\n",
    "        zeros_end = np.zeros(n)\n",
    "        last_pct_sum = np.zeros(n)\n",
    "        pct_90 = np.zeros(n)\n",
    "        pct_10 = np.zeros(n)\n",
    "        pct_width = np.zeros(n)\n",
    "\n",
    "        # --- Comparaciones TN vs rolling stats ---\n",
    "        tn_minus_mean = np.zeros(n)\n",
    "        tn_minus_median = np.zeros(n)\n",
    "        tn_minus_ewma = np.zeros(n)\n",
    "        tn_over_mean = np.zeros(n)\n",
    "        tn_over_median = np.zeros(n)\n",
    "        tn_over_ewma = np.zeros(n)\n",
    "\n",
    "        x = np.arange(cant)\n",
    "        for i in range(n):\n",
    "            # Solo procesar si la ventana está completa\n",
    "            if i >= cant - 1:\n",
    "                y = y_values[i - (cant - 1): i + 1]\n",
    "                means[i] = np.mean(y)\n",
    "                try:\n",
    "                    pendientes[i] = np.polyfit(x, y, 1)[0]\n",
    "                except:\n",
    "                    pendientes[i] = 0\n",
    "                ewmas[i] = pd.Series(y).ewm(span=cant, adjust=False).mean().iloc[-1]\n",
    "                medians[i] = np.median(y)\n",
    "                minimo[i] = np.min(y)\n",
    "                maximo[i] = np.max(y)\n",
    "                std_y = np.std(y)\n",
    "                mean_y = means[i]\n",
    "                stds[i] = std_y\n",
    "                skews[i] = skew(y) if std_y > 0 else 0\n",
    "                kurts[i] = kurtosis(y) if std_y > 0 else 0\n",
    "                growths[i] = (y[-1] - y[0]) / (y[0] + 1e-8) if y[0] != 0 else 0\n",
    "                iqrs[i] = np.percentile(y, 75) - np.percentile(y, 25)\n",
    "                sums[i] = np.sum(y)\n",
    "                count_pos[i] = np.sum(y > 0)\n",
    "                pct_zero[i] = np.mean(y == 0)\n",
    "                last[i] = y[-1]\n",
    "                last_diff[i] = y[-1] - y[-2] if cant > 1 else 0\n",
    "\n",
    "                coef_var[i] = std_y / (mean_y + 1e-8) if mean_y != 0 else 0\n",
    "                maxmin_ratio[i] = maximo[i] / (minimo[i] + 1e-8) if minimo[i] != 0 else 0\n",
    "                rango[i] = maximo[i] - minimo[i]\n",
    "                rango_rel[i] = rango[i] / (minimo[i] + 1e-8) if minimo[i] != 0 else 0\n",
    "                last_vs_median[i] = y[-1] - np.median(y)\n",
    "                # Cambio de ventana anterior a actual\n",
    "                if i >= 2 * cant - 1:\n",
    "                    y_prev = y_values[i - (2 * cant - 1):i - cant + 1]\n",
    "                    mean_prev = np.mean(y_prev)\n",
    "                    cambio_ventana[i] = (mean_y - mean_prev) / (mean_prev + 1e-8) if mean_prev != 0 else 0\n",
    "                zeros_end[i] = np.argmax(y[::-1] != 0)\n",
    "                last_pct_sum[i] = y[-1] / (np.sum(y) + 1e-8) if np.sum(y) != 0 else 0\n",
    "                pct_90[i] = np.percentile(y, 90)\n",
    "                pct_10[i] = np.percentile(y, 10)\n",
    "                pct_width[i] = pct_90[i] - pct_10[i]\n",
    "\n",
    "                # Comparaciones TN vs rolling stats\n",
    "                tn_actual = y_values[i]\n",
    "                tn_minus_mean[i] = tn_actual - means[i]\n",
    "                tn_minus_median[i] = tn_actual - medians[i]\n",
    "                tn_minus_ewma[i] = tn_actual - ewmas[i]\n",
    "                tn_over_mean[i] = tn_actual / (means[i] + 1e-8) if means[i] != 0 else 0\n",
    "                tn_over_median[i] = tn_actual / (medians[i] + 1e-8) if medians[i] != 0 else 0\n",
    "                tn_over_ewma[i] = tn_actual / (ewmas[i] + 1e-8) if ewmas[i] != 0 else 0\n",
    "            # Si la ventana no está completa, todo queda en cero (ya inicializado)\n",
    "\n",
    "        group[f'TN_MEAN_{str(cant).zfill(2)}'] = means\n",
    "        group[f'PENDIENTE_TENDENCIA_{cant}'] = pendientes\n",
    "        group[f'TN_EWMA_{str(cant).zfill(2)}'] = ewmas\n",
    "        group[f'TN_MEDIAN_{str(cant).zfill(2)}'] = medians\n",
    "        group[f'TN_MIN_{str(cant).zfill(2)}'] = minimo\n",
    "        group[f'TN_MAX_{str(cant).zfill(2)}'] = maximo\n",
    "        group[f'TN_STD_{str(cant).zfill(2)}'] = stds\n",
    "        group[f'TN_SKEW_{str(cant).zfill(2)}'] = skews\n",
    "        group[f'TN_KURT_{str(cant).zfill(2)}'] = kurts\n",
    "        group[f'TN_GROWTH_{str(cant).zfill(2)}'] = growths\n",
    "        group[f'TN_IQR_{str(cant).zfill(2)}'] = iqrs\n",
    "        group[f'TN_SUM_{str(cant).zfill(2)}'] = sums\n",
    "        group[f'TN_COUNT_POS_{str(cant).zfill(2)}'] = count_pos\n",
    "        group[f'TN_PCT_ZERO_{str(cant).zfill(2)}'] = pct_zero\n",
    "        group[f'TN_LAST_{str(cant).zfill(2)}'] = last\n",
    "        group[f'TN_LAST_DIFF_{str(cant).zfill(2)}'] = last_diff\n",
    "\n",
    "        group[f'TN_COEF_VAR_{cant}'] = coef_var\n",
    "        group[f'TN_MAXMIN_RATIO_{cant}'] = maxmin_ratio\n",
    "        group[f'TN_RANGO_{cant}'] = rango\n",
    "        group[f'TN_RANGO_REL_{cant}'] = rango_rel\n",
    "        group[f'TN_LAST_VS_MEDIAN_{cant}'] = last_vs_median\n",
    "        group[f'TN_CHANGE_PREV_WINDOW_{cant}'] = cambio_ventana\n",
    "        group[f'TN_ZEROS_END_{cant}'] = zeros_end\n",
    "        group[f'TN_LAST_PCT_SUM_{cant}'] = last_pct_sum\n",
    "        group[f'TN_PCT90_{cant}'] = pct_90\n",
    "        group[f'TN_PCT10_{cant}'] = pct_10\n",
    "        group[f'TN_PCT_WIDTH_{cant}'] = pct_width\n",
    "\n",
    "        # Comparaciones TN vs rolling stats\n",
    "        group[f'TN_MINUS_MEAN_{str(cant).zfill(2)}'] = tn_minus_mean\n",
    "        group[f'TN_MINUS_MEDIAN_{str(cant).zfill(2)}'] = tn_minus_median\n",
    "        group[f'TN_MINUS_EWMA_{str(cant).zfill(2)}'] = tn_minus_ewma\n",
    "        group[f'TN_OVER_MEAN_{str(cant).zfill(2)}'] = tn_over_mean\n",
    "        group[f'TN_OVER_MEDIAN_{str(cant).zfill(2)}'] = tn_over_median\n",
    "        group[f'TN_OVER_EWMA_{str(cant).zfill(2)}'] = tn_over_ewma\n",
    "\n",
    "    return group\n",
    "\n",
    "def calcular_pendientes_parallel(df, periodos_list=[6, 12], n_jobs=20):\n",
    "    group_cols = [col for col in df.columns if col != 'TN']\n",
    "    print(\"Agrupando por columnas:\", group_cols)\n",
    "    grupos = [group for _, group in df.groupby(group_cols)]\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', verbose=10)(\n",
    "        delayed(calcular_pendientes_grupo)(group, periodos_list) for group in grupos\n",
    "    )\n",
    "    df_final = pd.concat(resultados, ignore_index=True)\n",
    "    df_final.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    df_final.fillna(0, inplace=True)\n",
    "    return df_final\n",
    "\n",
    "# --- Script principal ---\n",
    "# Calcular pendientes, medias ponderadas y medianas para los periodos indicados en paralelo\n",
    "df_agrupado = calcular_pendientes_parallel(df_agrupado, periodos_list=[3, 6, 9, 12, 18], n_jobs=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_agrupado.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer un EDA de la columna  CLASE en  df_agrupado\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Estadísticas descriptivas ---\n",
    "print(\"Estadísticas básicas de CLASE:\")\n",
    "print(df_agrupado['CLASE'].describe())\n",
    "\n",
    "print(\"\\nPrimeros valores únicos (si son pocos):\", df_agrupado['CLASE'].unique()[:10])\n",
    "print(\"Cantidad de valores únicos:\", df_agrupado['CLASE'].nunique())\n",
    "\n",
    "print(\"\\nCantidad de nulos:\", df_agrupado['CLASE'].isna().sum())\n",
    "\n",
    "# --- Histograma ---\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df_agrupado['CLASE'], bins=30, kde=True)\n",
    "plt.title(\"Histograma de CLASE\")\n",
    "plt.xlabel(\"CLASE\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n",
    "\n",
    "# --- Boxplot para ver outliers ---\n",
    "plt.figure(figsize=(8,2))\n",
    "sns.boxplot(x=df_agrupado['CLASE'])\n",
    "plt.title(\"Boxplot de CLASE\")\n",
    "plt.show()\n",
    "\n",
    "# --- Distribución log-transformada (útil si hay muchos valores chicos y pocos grandes) ---\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(np.log1p(df_agrupado['CLASE']), bins=30, kde=True)\n",
    "plt.title(\"Histograma de log(1+CLASE)\")\n",
    "plt.xlabel(\"log(1+CLASE)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n",
    "\n",
    "# --- Percentiles y cuantiles ---\n",
    "print(\"\\nPercentiles de CLASE:\")\n",
    "print(df_agrupado['CLASE'].quantile([0, 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99, 1]))\n",
    "\n",
    "# --- Outliers extremos (más allá de 3 desviaciones estándar) ---\n",
    "media = df_agrupado['CLASE'].mean()\n",
    "std = df_agrupado['CLASE'].std()\n",
    "outliers = df_agrupado[(df_agrupado['CLASE'] > media + 3*std) | (df_agrupado['CLASE'] < media - 3*std)]\n",
    "print(f\"\\nCantidad de outliers (>3 desvíos): {len(outliers)}\")\n",
    "\n",
    "# --- Relación con otras variables (ejemplo: PERIODO) ---\n",
    "if 'PERIODO' in df_agrupado.columns:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.lineplot(x='PERIODO', y='CLASE', data=df_agrupado, ci=None)\n",
    "    plt.title(\"CLASE a lo largo del tiempo (PERIODO)\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado['CLASE_LOG1P'] = np.log1p(df_agrupado['CLASE'])\n",
    "# Y para invertir:\n",
    "#df_agrupado['CLASE'] = np.expm1(df_full['CLASE_LOG1P'])\n",
    "\n",
    "# Eliminar la columna CLASE \n",
    "df_agrupado = df_agrupado.drop(columns=['CLASE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b395843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# FIT scaler solo sobre entrenamiento\n",
    "mask_train = df_agrupado['PERIODO'] <= 201909\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(df_agrupado.loc[mask_train, ['CLASE_LOG1P']])\n",
    "\n",
    "# Aplicar la transformación a todo df_full\n",
    "df_agrupado['CLASE_LOG1P_Z'] = scaler_y.transform(df_agrupado[['CLASE_LOG1P']])\n",
    "\n",
    "# Guardar media y std\n",
    "media_y = scaler_y.mean_[0]\n",
    "std_y = scaler_y.scale_[0]\n",
    "print(f\"Media usada (entrenamiento): {media_y}\")\n",
    "print(f\"Std usada (entrenamiento): {std_y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd37067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar a disco\n",
    "joblib.dump(scaler_y, 'scaler_y_CLASE_LOG1P.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdda666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Cargar desde disco\n",
    "#scaler_y = joblib.load('scaler_y_CLASE_LOG1P.joblib')\n",
    "#y_pred_log1p = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas CLASE y CLASE_LOG1P para evitar confusiones\n",
    "df_agrupado.drop(columns=['CLASE', 'CLASE_LOG1P'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_agrupado.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame en parquet\n",
    "df_agrupado.to_parquet('./data/product_interm_NN_TORCH.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "92d75fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado = pd.read_parquet('./data/product_interm_NN_TORCH.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "92f799e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Excluir la variable de clase\n",
    "cols_to_check = df_agrupado.columns.difference(['CLASE_LOG1P_Z'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_agrupado[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ae6c73c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DataFrame final con 22,375 filas y 182 columnas:\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 DataFrame final con {df_agrupado.shape[0]:,} filas y {df_agrupado.shape[1]} columnas:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bbc780ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia de seguridad del DataFrame\n",
    "df = df_agrupado.copy()\n",
    "\n",
    "# === Binning (en deciles) ===\n",
    "df['PRODUCT_RANK_BIN'] = pd.qcut(df['PRODUCT_ID'], q=10, labels=False)\n",
    "df['PRODUCT_RANK_BIN'] = df['PRODUCT_RANK_BIN'].astype('category')\n",
    "\n",
    "# Reemplazar en el DataFrame principal\n",
    "df_agrupado = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "788f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE','MES_PROBLEMATICO','PRODUCT_RANK_BIN']\n",
    "\n",
    "for col in cat_cols:\n",
    "    if df_agrupado[col].isnull().any():\n",
    "        df_agrupado[col] = df_agrupado[col].fillna(\"missing\")\n",
    "\n",
    "\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_agrupado.loc[:, col] = le.fit_transform(df_agrupado[col]).astype(int)\n",
    "    encoders[col] = le  # para guardar los mapeos por si necesitás revertirlos\n",
    "\n",
    "# Guardo los encoders en archivos .pkl\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('encoders', exist_ok=True)\n",
    "\n",
    "for col, le in encoders.items():\n",
    "    joblib.dump(le, f'encoders/{col}_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b288b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas excluidas del escalado: ['PERIODO', 'CLASE_LOG1P_Z', 'MES_SIN', 'MES_COS', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'MES_PROBLEMATICO', 'PRODUCT_RANK_BIN']\n",
      "Columnas numéricas a escalar: ['ORDINAL', 'ANIO', 'MES', 'TRIMESTRE', 'PRODUCT_ID', 'TN', 'MESES_SIN_COMPRAR_PRODUCT_ID', 'TN_MEAN_03', 'PENDIENTE_TENDENCIA_3', 'TN_EWMA_03', 'TN_MEDIAN_03', 'TN_MIN_03', 'TN_MAX_03', 'TN_STD_03', 'TN_SKEW_03', 'TN_KURT_03', 'TN_GROWTH_03', 'TN_IQR_03', 'TN_SUM_03', 'TN_COUNT_POS_03', 'TN_PCT_ZERO_03', 'TN_LAST_03', 'TN_LAST_DIFF_03', 'TN_COEF_VAR_3', 'TN_MAXMIN_RATIO_3', 'TN_RANGO_3', 'TN_RANGO_REL_3', 'TN_LAST_VS_MEDIAN_3', 'TN_CHANGE_PREV_WINDOW_3', 'TN_ZEROS_END_3', 'TN_LAST_PCT_SUM_3', 'TN_PCT90_3', 'TN_PCT10_3', 'TN_PCT_WIDTH_3', 'TN_MINUS_MEAN_03', 'TN_MINUS_MEDIAN_03', 'TN_MINUS_EWMA_03', 'TN_OVER_MEAN_03', 'TN_OVER_MEDIAN_03', 'TN_OVER_EWMA_03', 'TN_MEAN_06', 'PENDIENTE_TENDENCIA_6', 'TN_EWMA_06', 'TN_MEDIAN_06', 'TN_MIN_06', 'TN_MAX_06', 'TN_STD_06', 'TN_SKEW_06', 'TN_KURT_06', 'TN_GROWTH_06', 'TN_IQR_06', 'TN_SUM_06', 'TN_COUNT_POS_06', 'TN_PCT_ZERO_06', 'TN_LAST_06', 'TN_LAST_DIFF_06', 'TN_COEF_VAR_6', 'TN_MAXMIN_RATIO_6', 'TN_RANGO_6', 'TN_RANGO_REL_6', 'TN_LAST_VS_MEDIAN_6', 'TN_CHANGE_PREV_WINDOW_6', 'TN_ZEROS_END_6', 'TN_LAST_PCT_SUM_6', 'TN_PCT90_6', 'TN_PCT10_6', 'TN_PCT_WIDTH_6', 'TN_MINUS_MEAN_06', 'TN_MINUS_MEDIAN_06', 'TN_MINUS_EWMA_06', 'TN_OVER_MEAN_06', 'TN_OVER_MEDIAN_06', 'TN_OVER_EWMA_06', 'TN_MEAN_09', 'PENDIENTE_TENDENCIA_9', 'TN_EWMA_09', 'TN_MEDIAN_09', 'TN_MIN_09', 'TN_MAX_09', 'TN_STD_09', 'TN_SKEW_09', 'TN_KURT_09', 'TN_GROWTH_09', 'TN_IQR_09', 'TN_SUM_09', 'TN_COUNT_POS_09', 'TN_PCT_ZERO_09', 'TN_LAST_09', 'TN_LAST_DIFF_09', 'TN_COEF_VAR_9', 'TN_MAXMIN_RATIO_9', 'TN_RANGO_9', 'TN_RANGO_REL_9', 'TN_LAST_VS_MEDIAN_9', 'TN_CHANGE_PREV_WINDOW_9', 'TN_ZEROS_END_9', 'TN_LAST_PCT_SUM_9', 'TN_PCT90_9', 'TN_PCT10_9', 'TN_PCT_WIDTH_9', 'TN_MINUS_MEAN_09', 'TN_MINUS_MEDIAN_09', 'TN_MINUS_EWMA_09', 'TN_OVER_MEAN_09', 'TN_OVER_MEDIAN_09', 'TN_OVER_EWMA_09', 'TN_MEAN_12', 'PENDIENTE_TENDENCIA_12', 'TN_EWMA_12', 'TN_MEDIAN_12', 'TN_MIN_12', 'TN_MAX_12', 'TN_STD_12', 'TN_SKEW_12', 'TN_KURT_12', 'TN_GROWTH_12', 'TN_IQR_12', 'TN_SUM_12', 'TN_COUNT_POS_12', 'TN_PCT_ZERO_12', 'TN_LAST_12', 'TN_LAST_DIFF_12', 'TN_COEF_VAR_12', 'TN_MAXMIN_RATIO_12', 'TN_RANGO_12', 'TN_RANGO_REL_12', 'TN_LAST_VS_MEDIAN_12', 'TN_CHANGE_PREV_WINDOW_12', 'TN_ZEROS_END_12', 'TN_LAST_PCT_SUM_12', 'TN_PCT90_12', 'TN_PCT10_12', 'TN_PCT_WIDTH_12', 'TN_MINUS_MEAN_12', 'TN_MINUS_MEDIAN_12', 'TN_MINUS_EWMA_12', 'TN_OVER_MEAN_12', 'TN_OVER_MEDIAN_12', 'TN_OVER_EWMA_12', 'TN_MEAN_18', 'PENDIENTE_TENDENCIA_18', 'TN_EWMA_18', 'TN_MEDIAN_18', 'TN_MIN_18', 'TN_MAX_18', 'TN_STD_18', 'TN_SKEW_18', 'TN_KURT_18', 'TN_GROWTH_18', 'TN_IQR_18', 'TN_SUM_18', 'TN_COUNT_POS_18', 'TN_PCT_ZERO_18', 'TN_LAST_18', 'TN_LAST_DIFF_18', 'TN_COEF_VAR_18', 'TN_MAXMIN_RATIO_18', 'TN_RANGO_18', 'TN_RANGO_REL_18', 'TN_LAST_VS_MEDIAN_18', 'TN_CHANGE_PREV_WINDOW_18', 'TN_ZEROS_END_18', 'TN_LAST_PCT_SUM_18', 'TN_PCT90_18', 'TN_PCT10_18', 'TN_PCT_WIDTH_18', 'TN_MINUS_MEAN_18', 'TN_MINUS_MEDIAN_18', 'TN_MINUS_EWMA_18', 'TN_OVER_MEAN_18', 'TN_OVER_MEDIAN_18', 'TN_OVER_EWMA_18']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Copia del DataFrame original\n",
    "df = df_agrupado.copy()\n",
    "\n",
    "# 1. Columnas categóricas (auto si no existen)\n",
    "if 'cat_cols' not in locals():\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in ['PRODUCT_RANK_BIN']:\n",
    "    if col not in cat_cols:\n",
    "        cat_cols.append(col)\n",
    "\n",
    "# 2. Columnas a excluir del escalado\n",
    "excluir = ['PERIODO', 'CLASE_LOG1P_Z', 'MES_SIN', 'MES_COS'] + cat_cols\n",
    "print(\"Columnas excluidas del escalado:\", excluir)\n",
    "\n",
    "# 3. Columnas numéricas a escalar\n",
    "cols_a_escalar = [\n",
    "    col for col in df.columns\n",
    "    if col not in excluir and pd.api.types.is_numeric_dtype(df[col])\n",
    "]\n",
    "print(\"Columnas numéricas a escalar:\", cols_a_escalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fc2e50a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler guardado en './encoders/scaler_features_numericas.pkl'\n",
      "Shape final del DataFrame: (22375, 184)\n",
      "Primeras columnas escaladas:\n",
      "    ORDINAL_Z    ANIO_Z     MES_Z  TRIMESTRE_Z  PRODUCT_ID_Z      TN_Z  \\\n",
      "0  -1.819956 -1.299285 -1.613484    -1.339623      0.403997 -0.355391   \n",
      "1  -1.819956 -1.299285 -1.613484    -1.339623     -0.449898 -0.311829   \n",
      "2  -1.819956 -1.299285 -1.613484    -1.339623     -0.528071 -0.302478   \n",
      "3  -1.819956 -1.299285 -1.613484    -1.339623     -0.952012 -0.106872   \n",
      "4  -1.819956 -1.299285 -1.613484    -1.339623     -1.069272  0.042260   \n",
      "\n",
      "   MESES_SIN_COMPRAR_PRODUCT_ID_Z  TN_MEAN_03_Z  PENDIENTE_TENDENCIA_3_Z  \\\n",
      "0                        5.067999           0.0                      0.0   \n",
      "1                        5.067999           0.0                      0.0   \n",
      "2                        5.067999           0.0                      0.0   \n",
      "3                        5.067999           0.0                      0.0   \n",
      "4                        5.067999           0.0                      0.0   \n",
      "\n",
      "   TN_EWMA_03_Z  ...  TN_LAST_PCT_SUM_18_Z  TN_PCT90_18_Z  TN_PCT10_18_Z  \\\n",
      "0           0.0  ...                   0.0            0.0            0.0   \n",
      "1           0.0  ...                   0.0            0.0            0.0   \n",
      "2           0.0  ...                   0.0            0.0            0.0   \n",
      "3           0.0  ...                   0.0            0.0            0.0   \n",
      "4           0.0  ...                   0.0            0.0            0.0   \n",
      "\n",
      "   TN_PCT_WIDTH_18_Z  TN_MINUS_MEAN_18_Z  TN_MINUS_MEDIAN_18_Z  \\\n",
      "0                0.0                 0.0                   0.0   \n",
      "1                0.0                 0.0                   0.0   \n",
      "2                0.0                 0.0                   0.0   \n",
      "3                0.0                 0.0                   0.0   \n",
      "4                0.0                 0.0                   0.0   \n",
      "\n",
      "   TN_MINUS_EWMA_18_Z  TN_OVER_MEAN_18_Z  TN_OVER_MEDIAN_18_Z  \\\n",
      "0                 0.0                0.0                  0.0   \n",
      "1                 0.0                0.0                  0.0   \n",
      "2                 0.0                0.0                  0.0   \n",
      "3                 0.0                0.0                  0.0   \n",
      "4                 0.0                0.0                  0.0   \n",
      "\n",
      "   TN_OVER_EWMA_18_Z  \n",
      "0                0.0  \n",
      "1                0.0  \n",
      "2                0.0  \n",
      "3                0.0  \n",
      "4                0.0  \n",
      "\n",
      "[5 rows x 172 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Entrenamiento del scaler SOLO con datos de entrenamiento\n",
    "df_entrenamiento = df[df['PERIODO'] <= 201910].copy()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_entrenamiento[cols_a_escalar])\n",
    "\n",
    "# 5. Aplicación del scaler a TODO el dataset\n",
    "valores_escalados = scaler.transform(df[cols_a_escalar])\n",
    "df_scaled = pd.DataFrame(\n",
    "    valores_escalados,\n",
    "    columns=[col + '_Z' for col in cols_a_escalar],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# 6. Eliminar columnas originales escaladas (excepto IDs)\n",
    "cols_a_borrar = [col for col in cols_a_escalar if col not in ['CUSTOMER_ID', 'PRODUCT_ID']]\n",
    "df.drop(columns=cols_a_borrar, inplace=True)\n",
    "\n",
    "# 7. Concatenar columnas escaladas\n",
    "df = pd.concat([df, df_scaled], axis=1)\n",
    "\n",
    "# 8. Guardar el scaler\n",
    "os.makedirs('./encoders', exist_ok=True)\n",
    "joblib.dump(scaler, './encoders/scaler_features_numericas.pkl')\n",
    "print(\"Scaler guardado en './encoders/scaler_features_numericas.pkl'\")\n",
    "\n",
    "# 9. Actualizar df_full\n",
    "df_agrupado = df\n",
    "\n",
    "print(\"Shape final del DataFrame:\", df_agrupado.shape)\n",
    "print(\"Primeras columnas escaladas:\\n\", df_agrupado[[col + '_Z' for col in cols_a_escalar]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "33c9a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERIODO', 'MES_SIN', 'MES_COS', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'PRODUCT_ID', 'MES_PROBLEMATICO', 'CLASE_LOG1P_Z', 'PRODUCT_RANK_BIN', 'ORDINAL_Z', 'ANIO_Z', 'MES_Z', 'TRIMESTRE_Z', 'PRODUCT_ID_Z', 'TN_Z', 'MESES_SIN_COMPRAR_PRODUCT_ID_Z', 'TN_MEAN_03_Z', 'PENDIENTE_TENDENCIA_3_Z', 'TN_EWMA_03_Z', 'TN_MEDIAN_03_Z', 'TN_MIN_03_Z', 'TN_MAX_03_Z', 'TN_STD_03_Z', 'TN_SKEW_03_Z', 'TN_KURT_03_Z', 'TN_GROWTH_03_Z', 'TN_IQR_03_Z', 'TN_SUM_03_Z', 'TN_COUNT_POS_03_Z', 'TN_PCT_ZERO_03_Z', 'TN_LAST_03_Z', 'TN_LAST_DIFF_03_Z', 'TN_COEF_VAR_3_Z', 'TN_MAXMIN_RATIO_3_Z', 'TN_RANGO_3_Z', 'TN_RANGO_REL_3_Z', 'TN_LAST_VS_MEDIAN_3_Z', 'TN_CHANGE_PREV_WINDOW_3_Z', 'TN_ZEROS_END_3_Z', 'TN_LAST_PCT_SUM_3_Z', 'TN_PCT90_3_Z', 'TN_PCT10_3_Z', 'TN_PCT_WIDTH_3_Z', 'TN_MINUS_MEAN_03_Z', 'TN_MINUS_MEDIAN_03_Z', 'TN_MINUS_EWMA_03_Z', 'TN_OVER_MEAN_03_Z', 'TN_OVER_MEDIAN_03_Z', 'TN_OVER_EWMA_03_Z', 'TN_MEAN_06_Z', 'PENDIENTE_TENDENCIA_6_Z', 'TN_EWMA_06_Z', 'TN_MEDIAN_06_Z', 'TN_MIN_06_Z', 'TN_MAX_06_Z', 'TN_STD_06_Z', 'TN_SKEW_06_Z', 'TN_KURT_06_Z', 'TN_GROWTH_06_Z', 'TN_IQR_06_Z', 'TN_SUM_06_Z', 'TN_COUNT_POS_06_Z', 'TN_PCT_ZERO_06_Z', 'TN_LAST_06_Z', 'TN_LAST_DIFF_06_Z', 'TN_COEF_VAR_6_Z', 'TN_MAXMIN_RATIO_6_Z', 'TN_RANGO_6_Z', 'TN_RANGO_REL_6_Z', 'TN_LAST_VS_MEDIAN_6_Z', 'TN_CHANGE_PREV_WINDOW_6_Z', 'TN_ZEROS_END_6_Z', 'TN_LAST_PCT_SUM_6_Z', 'TN_PCT90_6_Z', 'TN_PCT10_6_Z', 'TN_PCT_WIDTH_6_Z', 'TN_MINUS_MEAN_06_Z', 'TN_MINUS_MEDIAN_06_Z', 'TN_MINUS_EWMA_06_Z', 'TN_OVER_MEAN_06_Z', 'TN_OVER_MEDIAN_06_Z', 'TN_OVER_EWMA_06_Z', 'TN_MEAN_09_Z', 'PENDIENTE_TENDENCIA_9_Z', 'TN_EWMA_09_Z', 'TN_MEDIAN_09_Z', 'TN_MIN_09_Z', 'TN_MAX_09_Z', 'TN_STD_09_Z', 'TN_SKEW_09_Z', 'TN_KURT_09_Z', 'TN_GROWTH_09_Z', 'TN_IQR_09_Z', 'TN_SUM_09_Z', 'TN_COUNT_POS_09_Z', 'TN_PCT_ZERO_09_Z', 'TN_LAST_09_Z', 'TN_LAST_DIFF_09_Z', 'TN_COEF_VAR_9_Z', 'TN_MAXMIN_RATIO_9_Z', 'TN_RANGO_9_Z', 'TN_RANGO_REL_9_Z', 'TN_LAST_VS_MEDIAN_9_Z', 'TN_CHANGE_PREV_WINDOW_9_Z', 'TN_ZEROS_END_9_Z', 'TN_LAST_PCT_SUM_9_Z', 'TN_PCT90_9_Z', 'TN_PCT10_9_Z', 'TN_PCT_WIDTH_9_Z', 'TN_MINUS_MEAN_09_Z', 'TN_MINUS_MEDIAN_09_Z', 'TN_MINUS_EWMA_09_Z', 'TN_OVER_MEAN_09_Z', 'TN_OVER_MEDIAN_09_Z', 'TN_OVER_EWMA_09_Z', 'TN_MEAN_12_Z', 'PENDIENTE_TENDENCIA_12_Z', 'TN_EWMA_12_Z', 'TN_MEDIAN_12_Z', 'TN_MIN_12_Z', 'TN_MAX_12_Z', 'TN_STD_12_Z', 'TN_SKEW_12_Z', 'TN_KURT_12_Z', 'TN_GROWTH_12_Z', 'TN_IQR_12_Z', 'TN_SUM_12_Z', 'TN_COUNT_POS_12_Z', 'TN_PCT_ZERO_12_Z', 'TN_LAST_12_Z', 'TN_LAST_DIFF_12_Z', 'TN_COEF_VAR_12_Z', 'TN_MAXMIN_RATIO_12_Z', 'TN_RANGO_12_Z', 'TN_RANGO_REL_12_Z', 'TN_LAST_VS_MEDIAN_12_Z', 'TN_CHANGE_PREV_WINDOW_12_Z', 'TN_ZEROS_END_12_Z', 'TN_LAST_PCT_SUM_12_Z', 'TN_PCT90_12_Z', 'TN_PCT10_12_Z', 'TN_PCT_WIDTH_12_Z', 'TN_MINUS_MEAN_12_Z', 'TN_MINUS_MEDIAN_12_Z', 'TN_MINUS_EWMA_12_Z', 'TN_OVER_MEAN_12_Z', 'TN_OVER_MEDIAN_12_Z', 'TN_OVER_EWMA_12_Z', 'TN_MEAN_18_Z', 'PENDIENTE_TENDENCIA_18_Z', 'TN_EWMA_18_Z', 'TN_MEDIAN_18_Z', 'TN_MIN_18_Z', 'TN_MAX_18_Z', 'TN_STD_18_Z', 'TN_SKEW_18_Z', 'TN_KURT_18_Z', 'TN_GROWTH_18_Z', 'TN_IQR_18_Z', 'TN_SUM_18_Z', 'TN_COUNT_POS_18_Z', 'TN_PCT_ZERO_18_Z', 'TN_LAST_18_Z', 'TN_LAST_DIFF_18_Z', 'TN_COEF_VAR_18_Z', 'TN_MAXMIN_RATIO_18_Z', 'TN_RANGO_18_Z', 'TN_RANGO_REL_18_Z', 'TN_LAST_VS_MEDIAN_18_Z', 'TN_CHANGE_PREV_WINDOW_18_Z', 'TN_ZEROS_END_18_Z', 'TN_LAST_PCT_SUM_18_Z', 'TN_PCT90_18_Z', 'TN_PCT10_18_Z', 'TN_PCT_WIDTH_18_Z', 'TN_MINUS_MEAN_18_Z', 'TN_MINUS_MEDIAN_18_Z', 'TN_MINUS_EWMA_18_Z', 'TN_OVER_MEAN_18_Z', 'TN_OVER_MEDIAN_18_Z', 'TN_OVER_EWMA_18_Z']\n"
     ]
    }
   ],
   "source": [
    "print(df_agrupado.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0d7910d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Excluir la variable de clase\n",
    "cols_to_check = df_agrupado.columns.difference(['CLASE_LOG1P_Z'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_agrupado[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ed5652d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22375, 184)\n"
     ]
    }
   ],
   "source": [
    "print(df_agrupado.shape)\n",
    "# Guardar el DataFrame resultante en un archivo parquet\n",
    "df_agrupado.to_parquet('./data/product_train_val_NN_TORCH.parquet', engine='fastparquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "777f0d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'MES_PROBLEMATICO', 'PRODUCT_RANK_BIN']\n"
     ]
    }
   ],
   "source": [
    "print(cat_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
