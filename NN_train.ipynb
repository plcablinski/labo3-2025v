{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2250dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8275aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/train_val_NN_TORCH.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e3938cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar conjuntos\n",
    "df_train = df_full[df_full['PERIODO'] <= 201908].copy()\n",
    "df_val = df_full[(df_full['PERIODO'] >= 201909) & (df_full['PERIODO'] <= 201910)].copy()\n",
    "df_pred = df_full[df_full['PERIODO'] == 201912].copy()\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "454ffc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "target_col = 'CLASE_DELTA_LOG1P_Z'\n",
    "\n",
    "# Columnas categ√≥ricas a embeddings\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', \n",
    "            'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', \n",
    "            'PRODUCT_RANK_BIN']\n",
    "\n",
    "# Codificaci√≥n para embeddings\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "    clases_entrenadas = set(le.classes_)\n",
    "    df_val[col] = df_val[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    df_pred[col] = df_pred[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "embedding_sizes = [\n",
    "    (df_train[col].nunique() + 1, min(50, (df_train[col].nunique() + 1) // 2))\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Excluir columnas que no deben ir al modelo\n",
    "excluir = ['PERIODO', 'CUSTOMER_ID', 'PRODUCT_ID', 'CLASE_DELTA_LOG1P_Z', 'ORDINAL']\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in excluir and col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efcc2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No los incluyas en ninguna de estas dos listas\n",
    "assert 'CUSTOMER_ID' not in feature_cols\n",
    "assert 'CUSTOMER_ID' not in cat_cols\n",
    "assert 'PRODUCT_ID' not in feature_cols\n",
    "assert 'PRODUCT_ID' not in cat_cols\n",
    "assert 'PERIODO' not in feature_cols\n",
    "assert 'PERIODO' not in cat_cols\n",
    "assert 'CLASE_DELTA_LOG1P_Z' not in feature_cols\n",
    "assert 'CLASE_DELTA_LOG1P_Z' not in cat_cols\n",
    "assert 'ORDINAL' not in feature_cols\n",
    "assert 'ORDINAL' not in cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb83a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col=None):\n",
    "        self.cat_data = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
    "        self.num_data = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
    "        self.has_target = target_col is not None\n",
    "        if self.has_target:\n",
    "            self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.cat_data[idx], self.num_data[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.cat_data[idx], self.num_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50723c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' class WeightedMSELossMulti(nn.Module):\\n    def __init__(self, penalty_indices, alpha=0.5):\\n        super().__init__()\\n        self.penalty_indices = penalty_indices\\n        self.alpha = alpha\\n\\n    def forward(self, preds, targets, x_num):\\n        penalty = 1 + self.alpha * sum(x_num[:, i].abs() for i in self.penalty_indices)\\n        error = (preds.squeeze() - targets.squeeze()) ** 2\\n        return (penalty * error).mean() '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" class WeightedMSELossMulti(nn.Module):\n",
    "    def __init__(self, penalty_indices, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.penalty_indices = penalty_indices\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, preds, targets, x_num):\n",
    "        penalty = 1 + self.alpha * sum(x_num[:, i].abs() for i in self.penalty_indices)\n",
    "        error = (preds.squeeze() - targets.squeeze()) ** 2\n",
    "        return (penalty * error).mean() \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b15c6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class WeightedMSELossMulti(nn.Module):\n",
    "#     def __init__(self, penalty_indices, coefficients, alpha=0.5):\n",
    "#         \"\"\"\n",
    "#         penalty_indices: lista de √≠ndices de columnas en x_num (por ejemplo, [10, 11, ..., 20])\n",
    "#         coefficients: lista de coeficientes (ordenados igual que penalty_indices)\n",
    "#         alpha: fuerza de penalizaci√≥n\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.penalty_indices = penalty_indices\n",
    "#         self.coefficients = coefficients\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def forward(self, preds, targets, x_num):\n",
    "#         # Calcula penalizaci√≥n por muestra\n",
    "#         penalty = torch.ones_like(targets).float()  # shape: [batch_size, 1]\n",
    "\n",
    "#         for idx, coef in zip(self.penalty_indices, self.coefficients):\n",
    "#             penalty += self.alpha * coef * x_num[:, idx:idx+1].abs()  # ensure shape [batch_size, 1]\n",
    "\n",
    "#         error = (preds - targets) ** 2  # both [batch_size, 1]\n",
    "#         return (penalty * error).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec1183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightedMSELossMultiStable(nn.Module):\n",
    "    def __init__(self, penalty_indices, coefficients, alpha=0.1, debug=False):\n",
    "        \"\"\"\n",
    "        penalty_indices: lista de √≠ndices de columnas en x_num (por ejemplo, [10, 11, ..., 20])\n",
    "        coefficients: lista de coeficientes (ordenados igual que penalty_indices)\n",
    "        alpha: fuerza de penalizaci√≥n (m√°s bajo por estabilidad en redes grandes)\n",
    "        debug: si True, imprime el promedio del peso de penalizaci√≥n ocasionalmente\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.penalty_indices = penalty_indices\n",
    "        self.coefficients = coefficients\n",
    "        self.alpha = alpha\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, preds, targets, x_num):\n",
    "        # Penalizaci√≥n por muestra (shape: [batch_size, 1])\n",
    "        penalty = torch.ones_like(targets).float()\n",
    "\n",
    "        for idx, coef in zip(self.penalty_indices, self.coefficients):\n",
    "            val = x_num[:, idx:idx+1]  # shape: [batch_size, 1]\n",
    "            safe_val = torch.tanh(val)  # limitar la magnitud para evitar outliers\n",
    "            penalty += self.alpha * coef * safe_val.abs()  # siempre positivo\n",
    "\n",
    "        if self.debug and torch.rand(1).item() < 0.01:\n",
    "            print(f\"[LossDebug] Mean penalty: {penalty.mean().item():.4f}\")\n",
    "\n",
    "        error = (preds - targets) ** 2\n",
    "        weighted_error = penalty * error\n",
    "\n",
    "        return weighted_error.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aad019ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z', 'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z', 'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z']\n",
      "[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n"
     ]
    }
   ],
   "source": [
    "penalty_cols = ['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z','TN_LAG_05_Z','TN_LAG_06_Z','TN_LAG_07_Z',\n",
    "'TN_LAG_08_Z','TN_LAG_09_Z','TN_LAG_10_Z','TN_LAG_11_Z']\n",
    "penalty_indices = [feature_cols.index(col) for col in penalty_cols]\n",
    "print(penalty_cols)\n",
    "print(penalty_indices)\n",
    "coefficients = [\n",
    "    0.236558,\n",
    "    0.178208,\n",
    "   -0.060031,\n",
    "   -0.161875,\n",
    "   -0.007775,\n",
    "    0.151936,\n",
    "    0.043933,\n",
    "    0.142839,\n",
    "    0.103804,\n",
    "    0.119211,\n",
    "    0.073671\n",
    "]\n",
    "# loss_fn = WeightedMSELossMulti(penalty_indices, coefficients, alpha=0.5)\n",
    "\n",
    "loss_fn = WeightedMSELossMultiStable(\n",
    "    penalty_indices=penalty_indices,\n",
    "    coefficients=coefficients,\n",
    "    alpha=0.1,      # m√°s suave\n",
    "    debug=True      # activalo si quer√©s monitorear internamente\n",
    ")\n",
    "\n",
    "#loss_fn = WeightedMSELossMulti(penalty_indices=penalty_cols, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45c637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(col in df_train.columns for col in cat_cols), \"Faltan columnas categ√≥ricas\"\n",
    "assert all(col in df_train.columns for col in feature_cols), \"Faltan columnas num√©ricas\"\n",
    "assert target_col in df_train.columns, \"Falta la variable objetivo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd112c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "\n",
    "train_dataset = TabularDataset(df_train, cat_cols, feature_cols, target_col)\n",
    "val_dataset = TabularDataset(df_val, cat_cols, feature_cols, target_col)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6d04175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN']\n",
      "['MES_SIN', 'MES_COS', 'CUSTOMER_ID_Z', 'PRODUCT_ID_Z', 'CUST_REQUEST_QTY_Z', 'CUST_REQUEST_TN_Z', 'TN_Z', 'MEDIA_MOVIL_3M_CLI_PROD_Z', 'MEDIA_MOVIL_6M_CLI_PROD_Z', 'MEDIA_MOVIL_12M_CLI_PROD_Z', 'DESVIO_MOVIL_3M_CLI_PROD_Z', 'DESVIO_MOVIL_6M_CLI_PROD_Z', 'DESVIO_MOVIL_12M_CLI_PROD_Z', 'MEDIA_MOVIL_3M_PROD_Z', 'MEDIA_MOVIL_6M_PROD_Z', 'MEDIA_MOVIL_12M_PROD_Z', 'DESVIO_MOVIL_3M_PROD_Z', 'DESVIO_MOVIL_6M_PROD_Z', 'DESVIO_MOVIL_12M_PROD_Z', 'MEDIA_MOVIL_3M_CLI_Z', 'MEDIA_MOVIL_6M_CLI_Z', 'MEDIA_MOVIL_12M_CLI_Z', 'DESVIO_MOVIL_3M_CLI_Z', 'DESVIO_MOVIL_6M_CLI_Z', 'DESVIO_MOVIL_12M_CLI_Z', 'TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z', 'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z', 'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z', 'TN_LAG_12_Z', 'TN_LAG_13_Z', 'TN_LAG_14_Z', 'TN_LAG_15_Z', 'ORDINAL_Z', 'TN_DELTA_01_Z', 'TN_DELTA_02_Z', 'TN_DELTA_03_Z', 'TN_DELTA_04_Z', 'TN_DELTA_05_Z', 'TN_DELTA_06_Z', 'TN_DELTA_07_Z', 'TN_DELTA_08_Z', 'TN_DELTA_09_Z', 'TN_DELTA_10_Z', 'TN_DELTA_11_Z', 'TN_DELTA_12_Z', 'TN_DELTA_13_Z', 'TN_DELTA_14_Z', 'TN_DELTA_15_Z', 'ANTIG_CLIENTE_Z', 'ANTIG_PRODUCTO_Z', 'CANT_PROD_CLI_PER_Z', 'MEDIA_PROD_PER_Z', 'MEDIA_PROD_Z', 'MEDIA_PER_Z', 'PENDIENTE_TENDENCIA_3_Z', 'TN_EWMA_03_Z', 'TN_MEDIAN_03_Z', 'TN_MIN_03_Z', 'TN_MAX_03_Z', 'PENDIENTE_TENDENCIA_6_Z', 'TN_EWMA_06_Z', 'TN_MEDIAN_06_Z', 'TN_MIN_06_Z', 'TN_MAX_06_Z', 'PENDIENTE_TENDENCIA_9_Z', 'TN_EWMA_09_Z', 'TN_MEDIAN_09_Z', 'TN_MIN_09_Z', 'TN_MAX_09_Z', 'PENDIENTE_TENDENCIA_12_Z', 'TN_EWMA_12_Z', 'TN_MEDIAN_12_Z', 'TN_MIN_12_Z', 'TN_MAX_12_Z', 'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID_Z', 'MESES_SIN_COMPRAR_PRODUCT_ID_Z', 'MESES_SIN_COMPRAR_CUSTOMER_ID_Z']\n",
      "CLASE_DELTA_LOG1P_Z\n"
     ]
    }
   ],
   "source": [
    "print(cat_cols)\n",
    "print(feature_cols)\n",
    "print(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dff8de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TabularNNImproved(nn.Module):\n",
    "    def __init__(self, embedding_sizes, num_numerical, hidden_sizes=[512, 512, 256, 128], dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(ni, nf) for ni, nf in embedding_sizes\n",
    "        ])\n",
    "        embedding_dim = sum([nf for _, nf in embedding_sizes])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Total input size after embedding + numerical\n",
    "        input_size = embedding_dim + num_numerical\n",
    "\n",
    "        # Hidden layers\n",
    "        layers = []\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = h\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        x = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = torch.cat([x, x_num], dim=1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e66d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabularNNImproved(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(5, 2)\n",
      "    (1): Embedding(16, 8)\n",
      "    (2): Embedding(84, 42)\n",
      "    (3): Embedding(36, 18)\n",
      "    (4): Embedding(67, 33)\n",
      "    (5): Embedding(4, 2)\n",
      "    (6): Embedding(13, 6)\n",
      "    (7): Embedding(5, 2)\n",
      "    (8): Embedding(3, 1)\n",
      "    (9-10): 2 x Embedding(11, 5)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=209, out_features=4096, bias=True)\n",
      "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): GELU(approximate='none')\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): GELU(approximate='none')\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): GELU(approximate='none')\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): GELU(approximate='none')\n",
      "    (19): Dropout(p=0.3, inplace=False)\n",
      "    (20): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (21): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): GELU(approximate='none')\n",
      "    (23): Dropout(p=0.3, inplace=False)\n",
      "    (24): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (25): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): GELU(approximate='none')\n",
      "    (27): Dropout(p=0.3, inplace=False)\n",
      "    (28): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Detectar si hay GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Crear el modelo\n",
    "model = TabularNNImproved(\n",
    "    embedding_sizes=embedding_sizes,\n",
    "    num_numerical=len(feature_cols),\n",
    "    hidden_sizes=[4096,2048,1024,512, 512, 256, 128],\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e8314c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loader, val_loader, n_epochs=20, lr=1e-3, alpha=0.5, patience=3, penalty_indices=None, coefficients=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #criterion = CustomWeightedLoss(tn_index=7, alpha=0.5)\n",
    "    #criterion = WeightedMSELossMulti(penalty_indices=penalty_indices, coefficients=coefficients, alpha=0.5)\n",
    "    #criterion = NonlinearWeightedMSELoss(alpha=0.5)  # pod√©s ajustar alpha  #WeightedMSELoss(alpha=alpha)\n",
    "    criterion = WeightedMSELossMultiStable(penalty_indices=penalty_indices, coefficients=coefficients, alpha=0.5)\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for cats, conts, y in train_loader:\n",
    "            cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(cats, conts)\n",
    "            loss = criterion(y_pred, y, conts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validaci√≥n\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for cats, conts, y in val_loader:\n",
    "                cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "                y_pred = model(cats, conts)\n",
    "                loss = criterion(y_pred, y, conts)\n",
    "                val_loss += loss.item() * y.size(0)\n",
    "\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_pred_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        y_true = np.concatenate(y_true_list)\n",
    "        y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | MAE: {mae:.4f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"üî¥ Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Restaurar el mejor modelo\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Retornar valores verdaderos y predichos del √∫ltimo paso\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b6b5a",
   "metadata": {},
   "source": [
    "# B√∫squeda de hiperpar√°metros (Grid Search)\n",
    "Probamos distintas combinaciones de hiperpar√°metros y seleccionamos la que da mejor MAE en validaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040764c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.7968 | Val Loss: 0.6248 | MAE: 0.2065 | R¬≤: 0.3953\n",
      "Epoch 2/8 | Train Loss: 0.7511 | Val Loss: 0.6281 | MAE: 0.1831 | R¬≤: 0.3936\n",
      "Epoch 3/8 | Train Loss: 0.7396 | Val Loss: 0.6224 | MAE: 0.1760 | R¬≤: 0.3980\n",
      "Epoch 4/8 | Train Loss: 0.7312 | Val Loss: 0.6126 | MAE: 0.1908 | R¬≤: 0.4089\n",
      "Epoch 5/8 | Train Loss: 0.7254 | Val Loss: 0.6153 | MAE: 0.1883 | R¬≤: 0.4058\n",
      "Epoch 6/8 | Train Loss: 0.7186 | Val Loss: 0.6340 | MAE: 0.1884 | R¬≤: 0.3901\n",
      "üî¥ Early stopping triggered\n",
      "‚úÖ MAE = 0.1884\n",
      "üíæ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "üîß Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7843 | Val Loss: 0.6339 | MAE: 0.1860 | R¬≤: 0.3894\n",
      "Epoch 2/8 | Train Loss: 0.7495 | Val Loss: 0.6370 | MAE: 0.1983 | R¬≤: 0.3853\n",
      "Epoch 3/8 | Train Loss: 0.7405 | Val Loss: 0.6216 | MAE: 0.1925 | R¬≤: 0.4005\n",
      "Epoch 4/8 | Train Loss: 0.7318 | Val Loss: 0.6312 | MAE: 0.1948 | R¬≤: 0.3907\n",
      "Epoch 5/8 | Train Loss: 0.7253 | Val Loss: 0.6190 | MAE: 0.1933 | R¬≤: 0.4027\n",
      "Epoch 6/8 | Train Loss: 0.7202 | Val Loss: 0.6238 | MAE: 0.1869 | R¬≤: 0.3981\n",
      "Epoch 7/8 | Train Loss: 0.7142 | Val Loss: 0.6203 | MAE: 0.1850 | R¬≤: 0.4017\n",
      "üî¥ Early stopping triggered\n",
      "‚úÖ MAE = 0.1850\n",
      "üíæ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "üîß Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.7898 | Val Loss: 0.6937 | MAE: 0.2064 | R¬≤: 0.3370\n",
      "Epoch 2/8 | Train Loss: 0.7499 | Val Loss: 0.6283 | MAE: 0.1896 | R¬≤: 0.3932\n",
      "Epoch 3/8 | Train Loss: 0.7385 | Val Loss: 0.6460 | MAE: 0.1974 | R¬≤: 0.3777\n",
      "Epoch 4/8 | Train Loss: 0.7317 | Val Loss: 0.6191 | MAE: 0.1975 | R¬≤: 0.4015\n",
      "Epoch 5/8 | Train Loss: 0.7257 | Val Loss: 0.6266 | MAE: 0.1783 | R¬≤: 0.3955\n",
      "Epoch 6/8 | Train Loss: 0.7195 | Val Loss: 0.6257 | MAE: 0.1933 | R¬≤: 0.3956\n",
      "üî¥ Early stopping triggered\n",
      "‚úÖ MAE = 0.1933\n",
      "\n",
      "üîß Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.7899 | Val Loss: 0.6600 | MAE: 0.1828 | R¬≤: 0.3678\n",
      "Epoch 2/8 | Train Loss: 0.7498 | Val Loss: 0.6169 | MAE: 0.2111 | R¬≤: 0.4045\n",
      "Epoch 3/8 | Train Loss: 0.7406 | Val Loss: 0.6194 | MAE: 0.1872 | R¬≤: 0.4022\n",
      "Epoch 4/8 | Train Loss: 0.7329 | Val Loss: 0.6226 | MAE: 0.1885 | R¬≤: 0.3990\n",
      "üî¥ Early stopping triggered\n",
      "‚úÖ MAE = 0.1885\n",
      "\n",
      "üîß Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.7859 | Val Loss: 0.6313 | MAE: 0.1885 | R¬≤: 0.3912\n",
      "Epoch 2/8 | Train Loss: 0.7482 | Val Loss: 0.6374 | MAE: 0.1979 | R¬≤: 0.3852\n",
      "Epoch 3/8 | Train Loss: 0.7371 | Val Loss: 0.6113 | MAE: 0.2037 | R¬≤: 0.4091\n",
      "Epoch 4/8 | Train Loss: 0.7307 | Val Loss: 0.6146 | MAE: 0.1928 | R¬≤: 0.4062\n",
      "Epoch 5/8 | Train Loss: 0.7239 | Val Loss: 0.6120 | MAE: 0.2080 | R¬≤: 0.4072\n",
      "üî¥ Early stopping triggered\n",
      "‚úÖ MAE = 0.2080\n",
      "\n",
      "üîß Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.7937 | Val Loss: 0.6356 | MAE: 0.2509 | R¬≤: 0.3813\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6148 | MAE: 0.1847 | R¬≤: 0.4058\n",
      "Epoch 3/8 | Train Loss: 0.7391 | Val Loss: 0.6165 | MAE: 0.1854 | R¬≤: 0.4049\n",
      "Epoch 4/8 | Train Loss: 0.7313 | Val Loss: 0.6100 | MAE: 0.1771 | R¬≤: 0.4091\n",
      "Epoch 5/8 | Train Loss: 0.7250 | Val Loss: 0.6197 | MAE: 0.1970 | R¬≤: 0.4017\n",
      "Epoch 6/8 | Train Loss: 0.7202 | Val Loss: 0.6190 | MAE: 0.1858 | R¬≤: 0.4016\n",
      "üî¥ Early stopping triggered\n",
      "‚úÖ MAE = 0.1858\n",
      "\n",
      "üîß Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.7868 | Val Loss: 0.6234 | MAE: 0.1849 | R¬≤: 0.3969\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6197 | MAE: 0.1833 | R¬≤: 0.4027\n",
      "Epoch 3/8 | Train Loss: 0.7386 | Val Loss: 0.6117 | MAE: 0.1912 | R¬≤: 0.4084\n",
      "Epoch 4/8 | Train Loss: 0.7310 | Val Loss: 0.6200 | MAE: 0.1897 | R¬≤: 0.4005\n",
      "Epoch 5/8 | Train Loss: 0.7233 | Val Loss: 0.6406 | MAE: 0.1863 | R¬≤: 0.3835\n",
      "üî¥ Early stopping triggered\n",
      "‚úÖ MAE = 0.1863\n",
      "\n",
      "üîß Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7951 | Val Loss: 0.6690 | MAE: 0.1758 | R¬≤: 0.3593\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6436 | MAE: 0.1943 | R¬≤: 0.3804\n",
      "Epoch 3/8 | Train Loss: 0.7387 | Val Loss: 0.6429 | MAE: 0.1833 | R¬≤: 0.3786\n",
      "Epoch 4/8 | Train Loss: 0.7330 | Val Loss: 0.6271 | MAE: 0.1727 | R¬≤: 0.3937\n",
      "Epoch 5/8 | Train Loss: 0.7262 | Val Loss: 0.6174 | MAE: 0.1919 | R¬≤: 0.4038\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Definir el espacio de b√∫squeda\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'hidden_sizes': [\n",
    "        [1024, 512, 256],\n",
    "        [2048, 1024, 512, 256]\n",
    "    ],\n",
    "    'alpha': [0,0.1,0.3, 0.5, 0.7,0.9]\n",
    "}\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "param_combinations = list(product(\n",
    "    param_grid['lr'],\n",
    "    param_grid['dropout'],\n",
    "    param_grid['hidden_sizes'],\n",
    "    param_grid['alpha']\n",
    "))\n",
    "\n",
    "results = []\n",
    "best_mae = float('inf')\n",
    "\n",
    "# Loop de entrenamiento por combinaci√≥n\n",
    "for lr, dropout, hidden_sizes, alpha in param_combinations:\n",
    "    print(f\"\\nüîß Entrenando con: lr={lr}, dropout={dropout}, hidden_sizes={hidden_sizes}, alpha={alpha}\")\n",
    "\n",
    "    # Crear modelo y mover a dispositivo\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrenamiento corto para tuning\n",
    "    y_true_gs, y_pred_gs = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        n_epochs=8, lr=lr, alpha=alpha, patience=2,penalty_indices=penalty_indices,coefficients=coefficients\n",
    "    )\n",
    "\n",
    "    mae = mean_absolute_error(y_true_gs, y_pred_gs)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'dropout': dropout,\n",
    "        'hidden_sizes': hidden_sizes,\n",
    "        'alpha': alpha,\n",
    "        'mae': mae\n",
    "    })\n",
    "\n",
    "    print(f\"‚úÖ MAE = {mae:.4f}\")\n",
    "\n",
    "    # Guardar modelo si es el mejor\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        torch.save(model.state_dict(), f\"best_model_mae{mae:.4f}_lr{lr}_do{dropout}_a{alpha}.pth\")\n",
    "        print(\"üíæ Modelo guardado (mejor hasta ahora)\")\n",
    "\n",
    "    # Limpiar memoria GPU\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Convertir a DataFrame y mostrar top 5\n",
    "results_df = pd.DataFrame(results).sort_values(by='mae')\n",
    "print(\"\\nüìä Mejores combinaciones:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Guardar resultados a disco\n",
    "results_df.to_csv(\"gridsearch_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Asumimos que estas funciones/clases est√°n definidas en tu entorno:\n",
    "# - train_model(model, train_loader, val_loader, n_epochs, lr, loss_fn, patience, scheduler, weight_decay)\n",
    "# - WeightedMSELossMulti(penalty_indices, coefficients, alpha)\n",
    "# - MLP(input_dim, hidden_sizes, dropout)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Espacio de b√∫squeda\n",
    "    hidden_sizes = trial.suggest_categorical(\"hidden_sizes\", [\n",
    "        [512, 256],\n",
    "        [1024, 512, 256],\n",
    "        [2048, 1024, 512, 256]\n",
    "    ])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.0, 1.0)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    scheduler_gamma = trial.suggest_float(\"scheduler_gamma\", 0.7, 0.99)\n",
    "\n",
    "    # P√©rdida\n",
    "    if alpha > 0:\n",
    "        penalty_cols = ['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z',\n",
    "                        'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z',\n",
    "                        'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z']\n",
    "        penalty_indices = [feature_cols.index(col) for col in penalty_cols]\n",
    "        coefficients = [0.236558, 0.178208, -0.060031, -0.161875, -0.007775,\n",
    "                        0.151936, 0.043933, 0.142839, 0.103804, 0.119211, 0.073671]\n",
    "        loss_fn = WeightedMSELossMulti(penalty_indices, coefficients, alpha=alpha)\n",
    "    else:\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Modelo\n",
    "    model = MLP(input_dim=len(feature_cols), hidden_sizes=hidden_sizes, dropout=dropout)\n",
    "\n",
    "    # Scheduler (opcional)\n",
    "    scheduler_config = {\n",
    "        'type': 'StepLR',\n",
    "        'step_size': 2,\n",
    "        'gamma': scheduler_gamma\n",
    "    }\n",
    "\n",
    "    # Entrenamiento\n",
    "    y_true, y_pred = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        n_epochs=8,\n",
    "        lr=lr,\n",
    "        loss_fn=loss_fn,\n",
    "        patience=2,\n",
    "        scheduler_config=scheduler_config,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # M√©tricas\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    trial.set_user_attr(\"mae\", mae)\n",
    "    trial.set_user_attr(\"r2\", r2)\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Ejecutar b√∫squeda\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nüîç Mejor configuraci√≥n encontrada:\")\n",
    "print(study.best_trial.params)\n",
    "print(f\"‚úÖ MAE: {study.best_value:.4f}\")\n",
    "print(f\"üìà R¬≤: {study.best_trial.user_attrs['r2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleccionar las 3 mejores combinaci√≥n de cara a un ensemble\n",
    "results.sort(key=lambda x: x['mae'])\n",
    "print(\"\\nResultados ordenados por MAE:\")\n",
    "for res in results:\n",
    "    print(f\"Params: lr={res['lr']}, dropout={res['dropout']}, hidden_sizes={res['hidden_sizes']}, alpha={res['alpha']} -> MAE={res['mae']:.4f}\")\n",
    "#best_params = min(results, key=lambda x: x['mae'])\n",
    "#print(\"Mejores hiperpar√°metros encontrados:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11892c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# üîß Tus par√°metros finales para los 3 mejores modelos\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"modelo_m1\",\n",
    "        \"hidden_sizes\": [1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.001,\n",
    "        \"alpha\": 0.7\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"modelo_m2\",\n",
    "        \"hidden_sizes\": [2048, 1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.0005,\n",
    "        \"alpha\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"modelo_m3\",\n",
    "        \"hidden_sizes\": [1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.0005,\n",
    "        \"alpha\": 0.3\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "# üì¶ Dataset completo ya procesado\n",
    "# Usamos el mismo dataset de entrenamiento ya creado\n",
    "# Concatenar train_dataset y val_dataset\n",
    "train_val_dataset = ConcatDataset([train_dataset, val_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5155bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_full = DataLoader(train_val_dataset, batch_size=16384, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d59636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß† Funci√≥n de p√©rdida personalizada\n",
    "class CustomLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mae = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        base_loss = self.mae(preds, targets)\n",
    "        penalty = torch.mean(torch.abs(targets))  # o cualquier criterio adicional\n",
    "        return (1 - self.alpha) * base_loss + self.alpha * penalty\n",
    "\n",
    "# üöÇ Funci√≥n de entrenamiento final sin validaci√≥n\n",
    "def train_final_model(model, train_loader, n_epochs=20, lr=0.001, alpha=0.3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = CustomLoss(alpha=alpha)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_cat, X_num, y_batch in train_loader:\n",
    "            X_cat, X_num, y_batch = X_cat.to(device), X_num.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_cat, X_num)\n",
    "            #loss = loss_fn(preds.squeeze(), y_batch)\n",
    "            loss = loss_fn(preds.squeeze(), y_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"[{model.__class__.__name__}] Epoch {epoch+1}/{n_epochs} | Train Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ‚öôÔ∏è Entrenamiento de los 3 modelos\n",
    "for cfg in model_configs:\n",
    "    print(f\"\\nüîµ Entrenando {cfg['name']}...\")\n",
    "\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg[\"hidden_sizes\"],\n",
    "        dropout=cfg[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    model = train_final_model(\n",
    "        model,\n",
    "        train_loader=train_loader_full,\n",
    "        n_epochs=20,\n",
    "        lr=cfg[\"lr\"],\n",
    "        alpha=cfg[\"alpha\"]\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{cfg['name']}.pth\")\n",
    "    print(f\"‚úÖ Modelo {cfg['name']} guardado.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b12318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# üìå Filtrar periodo 201912\n",
    "df_pred = df_full[df_full[\"PERIODO\"] == 201912].copy()\n",
    "print(df_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üöß Preparar inputs\n",
    "X_cat_pred = torch.tensor(df_pred[cat_cols].values, dtype=torch.long)\n",
    "X_num_pred = torch.tensor(df_pred[numerical_cols].values, dtype=torch.float)\n",
    "\n",
    "# üì¶ Dataset y DataLoader sin target\n",
    "pred_dataset = TensorDataset(X_cat_pred, X_num_pred)\n",
    "pred_loader = DataLoader(pred_dataset, batch_size=65536, shuffle=False)\n",
    "\n",
    "# üìÅ Modelos a cargar\n",
    "model_paths = [\n",
    "    (\"modelo_m1\", [1024, 512, 256]),\n",
    "    (\"modelo_m2\", [2048, 1024, 512, 256]),\n",
    "    (\"modelo_m3\", [1024, 512, 256]),\n",
    "]\n",
    "\n",
    "# üß† Clase del modelo: asegurate de tener TabularNNImproved definido\n",
    "\n",
    "# üì§ Funci√≥n para predecir\n",
    "def predict_model(path, hidden_sizes):\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(numerical_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"{path}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_cat_batch, X_num_batch in pred_loader:\n",
    "            X_cat_batch = X_cat_batch.to(device)\n",
    "            X_num_batch = X_num_batch.to(device)\n",
    "            outputs = model(X_cat_batch, X_num_batch).squeeze().cpu().numpy()\n",
    "            preds.extend(outputs)\n",
    "    return np.array(preds)\n",
    "\n",
    "# üîÅ Predecir con los 3 modelos\n",
    "preds_dict = {}\n",
    "for name, h_sizes in model_paths:\n",
    "    print(f\"üì° Prediciendo con {name}...\")\n",
    "    preds_dict[name] = predict_model(name, h_sizes)\n",
    "\n",
    "# üîÄ Ensemble (promedio)\n",
    "ensemble_pred = np.mean(np.stack(list(preds_dict.values()), axis=0), axis=0)\n",
    "\n",
    "# ‚úÖ Guardar predicciones\n",
    "df_pred[\"PRED_LOG1P_Z\"] = ensemble_pred\n",
    "\n",
    "# (Opcional) si quer√©s ver distribuci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ensemble_pred, bins=100)\n",
    "plt.title(\"Distribuci√≥n de predicciones (log1p z-score)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
