{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2250dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8275aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/train_val_NN_TORCH.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e3938cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar conjuntos\n",
    "df_train = df_full[df_full['PERIODO'] <= 201908].copy()\n",
    "df_val = df_full[(df_full['PERIODO'] >= 201909) & (df_full['PERIODO'] <= 201910)].copy()\n",
    "df_pred = df_full[df_full['PERIODO'] == 201912].copy()\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "454ffc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "target_col = 'CLASE_DELTA_LOG1P_Z'\n",
    "\n",
    "# Columnas categÃ³ricas a embeddings\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', \n",
    "            'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', \n",
    "            'PRODUCT_RANK_BIN']\n",
    "\n",
    "# CodificaciÃ³n para embeddings\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "    clases_entrenadas = set(le.classes_)\n",
    "    df_val[col] = df_val[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    df_pred[col] = df_pred[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "embedding_sizes = [\n",
    "    (df_train[col].nunique() + 1, min(50, (df_train[col].nunique() + 1) // 2))\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Excluir columnas que no deben ir al modelo\n",
    "excluir = ['PERIODO', 'CUSTOMER_ID', 'PRODUCT_ID', 'CLASE_DELTA_LOG1P_Z', 'ORDINAL']\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in excluir and col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efcc2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No los incluyas en ninguna de estas dos listas\n",
    "assert 'CUSTOMER_ID' not in feature_cols\n",
    "assert 'CUSTOMER_ID' not in cat_cols\n",
    "assert 'PRODUCT_ID' not in feature_cols\n",
    "assert 'PRODUCT_ID' not in cat_cols\n",
    "assert 'PERIODO' not in feature_cols\n",
    "assert 'PERIODO' not in cat_cols\n",
    "assert 'CLASE_DELTA_LOG1P_Z' not in feature_cols\n",
    "assert 'CLASE_DELTA_LOG1P_Z' not in cat_cols\n",
    "assert 'ORDINAL' not in feature_cols\n",
    "assert 'ORDINAL' not in cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb83a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col=None):\n",
    "        self.cat_data = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
    "        self.num_data = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
    "        self.has_target = target_col is not None\n",
    "        if self.has_target:\n",
    "            self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.cat_data[idx], self.num_data[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.cat_data[idx], self.num_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50723c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' class WeightedMSELossMulti(nn.Module):\\n    def __init__(self, penalty_indices, alpha=0.5):\\n        super().__init__()\\n        self.penalty_indices = penalty_indices\\n        self.alpha = alpha\\n\\n    def forward(self, preds, targets, x_num):\\n        penalty = 1 + self.alpha * sum(x_num[:, i].abs() for i in self.penalty_indices)\\n        error = (preds.squeeze() - targets.squeeze()) ** 2\\n        return (penalty * error).mean() '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" class WeightedMSELossMulti(nn.Module):\n",
    "    def __init__(self, penalty_indices, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.penalty_indices = penalty_indices\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, preds, targets, x_num):\n",
    "        penalty = 1 + self.alpha * sum(x_num[:, i].abs() for i in self.penalty_indices)\n",
    "        error = (preds.squeeze() - targets.squeeze()) ** 2\n",
    "        return (penalty * error).mean() \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b15c6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class WeightedMSELossMulti(nn.Module):\n",
    "#     def __init__(self, penalty_indices, coefficients, alpha=0.5):\n",
    "#         \"\"\"\n",
    "#         penalty_indices: lista de Ã­ndices de columnas en x_num (por ejemplo, [10, 11, ..., 20])\n",
    "#         coefficients: lista de coeficientes (ordenados igual que penalty_indices)\n",
    "#         alpha: fuerza de penalizaciÃ³n\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.penalty_indices = penalty_indices\n",
    "#         self.coefficients = coefficients\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def forward(self, preds, targets, x_num):\n",
    "#         # Calcula penalizaciÃ³n por muestra\n",
    "#         penalty = torch.ones_like(targets).float()  # shape: [batch_size, 1]\n",
    "\n",
    "#         for idx, coef in zip(self.penalty_indices, self.coefficients):\n",
    "#             penalty += self.alpha * coef * x_num[:, idx:idx+1].abs()  # ensure shape [batch_size, 1]\n",
    "\n",
    "#         error = (preds - targets) ** 2  # both [batch_size, 1]\n",
    "#         return (penalty * error).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec1183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightedMSELossMultiStable(nn.Module):\n",
    "    def __init__(self, penalty_indices, coefficients, alpha=0.1, debug=False):\n",
    "        \"\"\"\n",
    "        penalty_indices: lista de Ã­ndices de columnas en x_num (por ejemplo, [10, 11, ..., 20])\n",
    "        coefficients: lista de coeficientes (ordenados igual que penalty_indices)\n",
    "        alpha: fuerza de penalizaciÃ³n (mÃ¡s bajo por estabilidad en redes grandes)\n",
    "        debug: si True, imprime el promedio del peso de penalizaciÃ³n ocasionalmente\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.penalty_indices = penalty_indices\n",
    "        self.coefficients = coefficients\n",
    "        self.alpha = alpha\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, preds, targets, x_num):\n",
    "        # PenalizaciÃ³n por muestra (shape: [batch_size, 1])\n",
    "        penalty = torch.ones_like(targets).float()\n",
    "\n",
    "        for idx, coef in zip(self.penalty_indices, self.coefficients):\n",
    "            val = x_num[:, idx:idx+1]  # shape: [batch_size, 1]\n",
    "            safe_val = torch.tanh(val)  # limitar la magnitud para evitar outliers\n",
    "            penalty += self.alpha * coef * safe_val.abs()  # siempre positivo\n",
    "\n",
    "        if self.debug and torch.rand(1).item() < 0.01:\n",
    "            print(f\"[LossDebug] Mean penalty: {penalty.mean().item():.4f}\")\n",
    "\n",
    "        error = (preds - targets) ** 2\n",
    "        weighted_error = penalty * error\n",
    "\n",
    "        return weighted_error.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aad019ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z', 'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z', 'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z']\n",
      "[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n"
     ]
    }
   ],
   "source": [
    "penalty_cols = ['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z','TN_LAG_05_Z','TN_LAG_06_Z','TN_LAG_07_Z',\n",
    "'TN_LAG_08_Z','TN_LAG_09_Z','TN_LAG_10_Z','TN_LAG_11_Z']\n",
    "penalty_indices = [feature_cols.index(col) for col in penalty_cols]\n",
    "print(penalty_cols)\n",
    "print(penalty_indices)\n",
    "coefficients = [\n",
    "    0.236558,\n",
    "    0.178208,\n",
    "   -0.060031,\n",
    "   -0.161875,\n",
    "   -0.007775,\n",
    "    0.151936,\n",
    "    0.043933,\n",
    "    0.142839,\n",
    "    0.103804,\n",
    "    0.119211,\n",
    "    0.073671\n",
    "]\n",
    "# loss_fn = WeightedMSELossMulti(penalty_indices, coefficients, alpha=0.5)\n",
    "\n",
    "loss_fn = WeightedMSELossMultiStable(\n",
    "    penalty_indices=penalty_indices,\n",
    "    coefficients=coefficients,\n",
    "    alpha=0.1,      # mÃ¡s suave\n",
    "    debug=True      # activalo si querÃ©s monitorear internamente\n",
    ")\n",
    "\n",
    "#loss_fn = WeightedMSELossMulti(penalty_indices=penalty_cols, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45c637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(col in df_train.columns for col in cat_cols), \"Faltan columnas categÃ³ricas\"\n",
    "assert all(col in df_train.columns for col in feature_cols), \"Faltan columnas numÃ©ricas\"\n",
    "assert target_col in df_train.columns, \"Falta la variable objetivo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd112c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "\n",
    "train_dataset = TabularDataset(df_train, cat_cols, feature_cols, target_col)\n",
    "val_dataset = TabularDataset(df_val, cat_cols, feature_cols, target_col)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6d04175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN']\n",
      "['MES_SIN', 'MES_COS', 'CUSTOMER_ID_Z', 'PRODUCT_ID_Z', 'CUST_REQUEST_QTY_Z', 'CUST_REQUEST_TN_Z', 'TN_Z', 'MEDIA_MOVIL_3M_CLI_PROD_Z', 'MEDIA_MOVIL_6M_CLI_PROD_Z', 'MEDIA_MOVIL_12M_CLI_PROD_Z', 'DESVIO_MOVIL_3M_CLI_PROD_Z', 'DESVIO_MOVIL_6M_CLI_PROD_Z', 'DESVIO_MOVIL_12M_CLI_PROD_Z', 'MEDIA_MOVIL_3M_PROD_Z', 'MEDIA_MOVIL_6M_PROD_Z', 'MEDIA_MOVIL_12M_PROD_Z', 'DESVIO_MOVIL_3M_PROD_Z', 'DESVIO_MOVIL_6M_PROD_Z', 'DESVIO_MOVIL_12M_PROD_Z', 'MEDIA_MOVIL_3M_CLI_Z', 'MEDIA_MOVIL_6M_CLI_Z', 'MEDIA_MOVIL_12M_CLI_Z', 'DESVIO_MOVIL_3M_CLI_Z', 'DESVIO_MOVIL_6M_CLI_Z', 'DESVIO_MOVIL_12M_CLI_Z', 'TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z', 'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z', 'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z', 'TN_LAG_12_Z', 'TN_LAG_13_Z', 'TN_LAG_14_Z', 'TN_LAG_15_Z', 'ORDINAL_Z', 'TN_DELTA_01_Z', 'TN_DELTA_02_Z', 'TN_DELTA_03_Z', 'TN_DELTA_04_Z', 'TN_DELTA_05_Z', 'TN_DELTA_06_Z', 'TN_DELTA_07_Z', 'TN_DELTA_08_Z', 'TN_DELTA_09_Z', 'TN_DELTA_10_Z', 'TN_DELTA_11_Z', 'TN_DELTA_12_Z', 'TN_DELTA_13_Z', 'TN_DELTA_14_Z', 'TN_DELTA_15_Z', 'ANTIG_CLIENTE_Z', 'ANTIG_PRODUCTO_Z', 'CANT_PROD_CLI_PER_Z', 'MEDIA_PROD_PER_Z', 'MEDIA_PROD_Z', 'MEDIA_PER_Z', 'PENDIENTE_TENDENCIA_3_Z', 'TN_EWMA_03_Z', 'TN_MEDIAN_03_Z', 'TN_MIN_03_Z', 'TN_MAX_03_Z', 'PENDIENTE_TENDENCIA_6_Z', 'TN_EWMA_06_Z', 'TN_MEDIAN_06_Z', 'TN_MIN_06_Z', 'TN_MAX_06_Z', 'PENDIENTE_TENDENCIA_9_Z', 'TN_EWMA_09_Z', 'TN_MEDIAN_09_Z', 'TN_MIN_09_Z', 'TN_MAX_09_Z', 'PENDIENTE_TENDENCIA_12_Z', 'TN_EWMA_12_Z', 'TN_MEDIAN_12_Z', 'TN_MIN_12_Z', 'TN_MAX_12_Z', 'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID_Z', 'MESES_SIN_COMPRAR_PRODUCT_ID_Z', 'MESES_SIN_COMPRAR_CUSTOMER_ID_Z']\n",
      "CLASE_DELTA_LOG1P_Z\n"
     ]
    }
   ],
   "source": [
    "print(cat_cols)\n",
    "print(feature_cols)\n",
    "print(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dff8de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TabularNNImproved(nn.Module):\n",
    "    def __init__(self, embedding_sizes, num_numerical, hidden_sizes=[512, 512, 256, 128], dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(ni, nf) for ni, nf in embedding_sizes\n",
    "        ])\n",
    "        embedding_dim = sum([nf for _, nf in embedding_sizes])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Total input size after embedding + numerical\n",
    "        input_size = embedding_dim + num_numerical\n",
    "\n",
    "        # Hidden layers\n",
    "        layers = []\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = h\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        x = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = torch.cat([x, x_num], dim=1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e66d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabularNNImproved(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(5, 2)\n",
      "    (1): Embedding(16, 8)\n",
      "    (2): Embedding(84, 42)\n",
      "    (3): Embedding(36, 18)\n",
      "    (4): Embedding(67, 33)\n",
      "    (5): Embedding(4, 2)\n",
      "    (6): Embedding(13, 6)\n",
      "    (7): Embedding(5, 2)\n",
      "    (8): Embedding(3, 1)\n",
      "    (9-10): 2 x Embedding(11, 5)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=209, out_features=4096, bias=True)\n",
      "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): GELU(approximate='none')\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): GELU(approximate='none')\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): GELU(approximate='none')\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): GELU(approximate='none')\n",
      "    (19): Dropout(p=0.3, inplace=False)\n",
      "    (20): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (21): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): GELU(approximate='none')\n",
      "    (23): Dropout(p=0.3, inplace=False)\n",
      "    (24): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (25): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): GELU(approximate='none')\n",
      "    (27): Dropout(p=0.3, inplace=False)\n",
      "    (28): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Detectar si hay GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Crear el modelo\n",
    "model = TabularNNImproved(\n",
    "    embedding_sizes=embedding_sizes,\n",
    "    num_numerical=len(feature_cols),\n",
    "    hidden_sizes=[4096,2048,1024,512, 512, 256, 128],\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e8314c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loader, val_loader, n_epochs=20, lr=1e-3, alpha=0.5, patience=3, penalty_indices=None, coefficients=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #criterion = CustomWeightedLoss(tn_index=7, alpha=0.5)\n",
    "    #criterion = WeightedMSELossMulti(penalty_indices=penalty_indices, coefficients=coefficients, alpha=0.5)\n",
    "    #criterion = NonlinearWeightedMSELoss(alpha=0.5)  # podÃ©s ajustar alpha  #WeightedMSELoss(alpha=alpha)\n",
    "    criterion = WeightedMSELossMultiStable(penalty_indices=penalty_indices, coefficients=coefficients, alpha=0.5)\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for cats, conts, y in train_loader:\n",
    "            cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(cats, conts)\n",
    "            loss = criterion(y_pred, y, conts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # ValidaciÃ³n\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for cats, conts, y in val_loader:\n",
    "                cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "                y_pred = model(cats, conts)\n",
    "                loss = criterion(y_pred, y, conts)\n",
    "                val_loss += loss.item() * y.size(0)\n",
    "\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_pred_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        y_true = np.concatenate(y_true_list)\n",
    "        y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | MAE: {mae:.4f} | RÂ²: {r2:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"ðŸ”´ Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Restaurar el mejor modelo\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Retornar valores verdaderos y predichos del Ãºltimo paso\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b6b5a",
   "metadata": {},
   "source": [
    "# BÃºsqueda de hiperparÃ¡metros (Grid Search)\n",
    "Probamos distintas combinaciones de hiperparÃ¡metros y seleccionamos la que da mejor MAE en validaciÃ³n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "040764c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.7968 | Val Loss: 0.6248 | MAE: 0.2065 | RÂ²: 0.3953\n",
      "Epoch 2/8 | Train Loss: 0.7511 | Val Loss: 0.6281 | MAE: 0.1831 | RÂ²: 0.3936\n",
      "Epoch 3/8 | Train Loss: 0.7396 | Val Loss: 0.6224 | MAE: 0.1760 | RÂ²: 0.3980\n",
      "Epoch 4/8 | Train Loss: 0.7312 | Val Loss: 0.6126 | MAE: 0.1908 | RÂ²: 0.4089\n",
      "Epoch 5/8 | Train Loss: 0.7254 | Val Loss: 0.6153 | MAE: 0.1883 | RÂ²: 0.4058\n",
      "Epoch 6/8 | Train Loss: 0.7186 | Val Loss: 0.6340 | MAE: 0.1884 | RÂ²: 0.3901\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1884\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7843 | Val Loss: 0.6339 | MAE: 0.1860 | RÂ²: 0.3894\n",
      "Epoch 2/8 | Train Loss: 0.7495 | Val Loss: 0.6370 | MAE: 0.1983 | RÂ²: 0.3853\n",
      "Epoch 3/8 | Train Loss: 0.7405 | Val Loss: 0.6216 | MAE: 0.1925 | RÂ²: 0.4005\n",
      "Epoch 4/8 | Train Loss: 0.7318 | Val Loss: 0.6312 | MAE: 0.1948 | RÂ²: 0.3907\n",
      "Epoch 5/8 | Train Loss: 0.7253 | Val Loss: 0.6190 | MAE: 0.1933 | RÂ²: 0.4027\n",
      "Epoch 6/8 | Train Loss: 0.7202 | Val Loss: 0.6238 | MAE: 0.1869 | RÂ²: 0.3981\n",
      "Epoch 7/8 | Train Loss: 0.7142 | Val Loss: 0.6203 | MAE: 0.1850 | RÂ²: 0.4017\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1850\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.7898 | Val Loss: 0.6937 | MAE: 0.2064 | RÂ²: 0.3370\n",
      "Epoch 2/8 | Train Loss: 0.7499 | Val Loss: 0.6283 | MAE: 0.1896 | RÂ²: 0.3932\n",
      "Epoch 3/8 | Train Loss: 0.7385 | Val Loss: 0.6460 | MAE: 0.1974 | RÂ²: 0.3777\n",
      "Epoch 4/8 | Train Loss: 0.7317 | Val Loss: 0.6191 | MAE: 0.1975 | RÂ²: 0.4015\n",
      "Epoch 5/8 | Train Loss: 0.7257 | Val Loss: 0.6266 | MAE: 0.1783 | RÂ²: 0.3955\n",
      "Epoch 6/8 | Train Loss: 0.7195 | Val Loss: 0.6257 | MAE: 0.1933 | RÂ²: 0.3956\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1933\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.7899 | Val Loss: 0.6600 | MAE: 0.1828 | RÂ²: 0.3678\n",
      "Epoch 2/8 | Train Loss: 0.7498 | Val Loss: 0.6169 | MAE: 0.2111 | RÂ²: 0.4045\n",
      "Epoch 3/8 | Train Loss: 0.7406 | Val Loss: 0.6194 | MAE: 0.1872 | RÂ²: 0.4022\n",
      "Epoch 4/8 | Train Loss: 0.7329 | Val Loss: 0.6226 | MAE: 0.1885 | RÂ²: 0.3990\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1885\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.7859 | Val Loss: 0.6313 | MAE: 0.1885 | RÂ²: 0.3912\n",
      "Epoch 2/8 | Train Loss: 0.7482 | Val Loss: 0.6374 | MAE: 0.1979 | RÂ²: 0.3852\n",
      "Epoch 3/8 | Train Loss: 0.7371 | Val Loss: 0.6113 | MAE: 0.2037 | RÂ²: 0.4091\n",
      "Epoch 4/8 | Train Loss: 0.7307 | Val Loss: 0.6146 | MAE: 0.1928 | RÂ²: 0.4062\n",
      "Epoch 5/8 | Train Loss: 0.7239 | Val Loss: 0.6120 | MAE: 0.2080 | RÂ²: 0.4072\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.2080\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.7937 | Val Loss: 0.6356 | MAE: 0.2509 | RÂ²: 0.3813\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6148 | MAE: 0.1847 | RÂ²: 0.4058\n",
      "Epoch 3/8 | Train Loss: 0.7391 | Val Loss: 0.6165 | MAE: 0.1854 | RÂ²: 0.4049\n",
      "Epoch 4/8 | Train Loss: 0.7313 | Val Loss: 0.6100 | MAE: 0.1771 | RÂ²: 0.4091\n",
      "Epoch 5/8 | Train Loss: 0.7250 | Val Loss: 0.6197 | MAE: 0.1970 | RÂ²: 0.4017\n",
      "Epoch 6/8 | Train Loss: 0.7202 | Val Loss: 0.6190 | MAE: 0.1858 | RÂ²: 0.4016\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1858\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.7868 | Val Loss: 0.6234 | MAE: 0.1849 | RÂ²: 0.3969\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6197 | MAE: 0.1833 | RÂ²: 0.4027\n",
      "Epoch 3/8 | Train Loss: 0.7386 | Val Loss: 0.6117 | MAE: 0.1912 | RÂ²: 0.4084\n",
      "Epoch 4/8 | Train Loss: 0.7310 | Val Loss: 0.6200 | MAE: 0.1897 | RÂ²: 0.4005\n",
      "Epoch 5/8 | Train Loss: 0.7233 | Val Loss: 0.6406 | MAE: 0.1863 | RÂ²: 0.3835\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1863\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7951 | Val Loss: 0.6690 | MAE: 0.1758 | RÂ²: 0.3593\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6436 | MAE: 0.1943 | RÂ²: 0.3804\n",
      "Epoch 3/8 | Train Loss: 0.7387 | Val Loss: 0.6429 | MAE: 0.1833 | RÂ²: 0.3786\n",
      "Epoch 4/8 | Train Loss: 0.7330 | Val Loss: 0.6271 | MAE: 0.1727 | RÂ²: 0.3937\n",
      "Epoch 5/8 | Train Loss: 0.7262 | Val Loss: 0.6174 | MAE: 0.1919 | RÂ²: 0.4038\n",
      "Epoch 6/8 | Train Loss: 0.7190 | Val Loss: 0.6186 | MAE: 0.1726 | RÂ²: 0.4026\n",
      "Epoch 7/8 | Train Loss: 0.7131 | Val Loss: 0.6328 | MAE: 0.1804 | RÂ²: 0.3883\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1804\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.7929 | Val Loss: 0.7244 | MAE: 0.1991 | RÂ²: 0.3134\n",
      "Epoch 2/8 | Train Loss: 0.7476 | Val Loss: 0.6184 | MAE: 0.1884 | RÂ²: 0.4013\n",
      "Epoch 3/8 | Train Loss: 0.7388 | Val Loss: 0.6300 | MAE: 0.1963 | RÂ²: 0.3908\n",
      "Epoch 4/8 | Train Loss: 0.7322 | Val Loss: 0.6170 | MAE: 0.1940 | RÂ²: 0.4040\n",
      "Epoch 5/8 | Train Loss: 0.7248 | Val Loss: 0.6454 | MAE: 0.1734 | RÂ²: 0.3787\n",
      "Epoch 6/8 | Train Loss: 0.7198 | Val Loss: 0.6260 | MAE: 0.1914 | RÂ²: 0.3941\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1914\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.7908 | Val Loss: 0.6909 | MAE: 0.1955 | RÂ²: 0.3343\n",
      "Epoch 2/8 | Train Loss: 0.7479 | Val Loss: 0.6348 | MAE: 0.1862 | RÂ²: 0.3867\n",
      "Epoch 3/8 | Train Loss: 0.7386 | Val Loss: 0.6397 | MAE: 0.1855 | RÂ²: 0.3850\n",
      "Epoch 4/8 | Train Loss: 0.7310 | Val Loss: 0.6397 | MAE: 0.1821 | RÂ²: 0.3842\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1821\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.7911 | Val Loss: 0.6117 | MAE: 0.1777 | RÂ²: 0.4086\n",
      "Epoch 2/8 | Train Loss: 0.7485 | Val Loss: 0.6288 | MAE: 0.1875 | RÂ²: 0.3928\n",
      "Epoch 3/8 | Train Loss: 0.7386 | Val Loss: 0.6139 | MAE: 0.1739 | RÂ²: 0.4066\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1739\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.7915 | Val Loss: 0.6395 | MAE: 0.1999 | RÂ²: 0.3853\n",
      "Epoch 2/8 | Train Loss: 0.7473 | Val Loss: 0.6218 | MAE: 0.2027 | RÂ²: 0.3999\n",
      "Epoch 3/8 | Train Loss: 0.7376 | Val Loss: 0.6159 | MAE: 0.1821 | RÂ²: 0.4039\n",
      "Epoch 4/8 | Train Loss: 0.7304 | Val Loss: 0.6159 | MAE: 0.1782 | RÂ²: 0.4044\n",
      "Epoch 5/8 | Train Loss: 0.7247 | Val Loss: 0.6154 | MAE: 0.1830 | RÂ²: 0.4058\n",
      "Epoch 6/8 | Train Loss: 0.7189 | Val Loss: 0.6337 | MAE: 0.2004 | RÂ²: 0.3902\n",
      "Epoch 7/8 | Train Loss: 0.7119 | Val Loss: 0.6231 | MAE: 0.1834 | RÂ²: 0.3994\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1834\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.8016 | Val Loss: 0.6470 | MAE: 0.2030 | RÂ²: 0.3772\n",
      "Epoch 2/8 | Train Loss: 0.7593 | Val Loss: 0.6737 | MAE: 0.2045 | RÂ²: 0.3555\n",
      "Epoch 3/8 | Train Loss: 0.7499 | Val Loss: 0.6221 | MAE: 0.1810 | RÂ²: 0.4002\n",
      "Epoch 4/8 | Train Loss: 0.7415 | Val Loss: 0.6149 | MAE: 0.1784 | RÂ²: 0.4055\n",
      "Epoch 5/8 | Train Loss: 0.7369 | Val Loss: 0.6101 | MAE: 0.1840 | RÂ²: 0.4103\n",
      "Epoch 6/8 | Train Loss: 0.7307 | Val Loss: 0.6400 | MAE: 0.1825 | RÂ²: 0.3839\n",
      "Epoch 7/8 | Train Loss: 0.7270 | Val Loss: 0.6191 | MAE: 0.1863 | RÂ²: 0.4026\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1863\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7989 | Val Loss: 0.6174 | MAE: 0.1964 | RÂ²: 0.4041\n",
      "Epoch 2/8 | Train Loss: 0.7584 | Val Loss: 0.6272 | MAE: 0.1972 | RÂ²: 0.3948\n",
      "Epoch 3/8 | Train Loss: 0.7476 | Val Loss: 0.6267 | MAE: 0.1915 | RÂ²: 0.3958\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1915\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.8001 | Val Loss: 0.7205 | MAE: 0.1879 | RÂ²: 0.3166\n",
      "Epoch 2/8 | Train Loss: 0.7581 | Val Loss: 0.6250 | MAE: 0.1792 | RÂ²: 0.3956\n",
      "Epoch 3/8 | Train Loss: 0.7488 | Val Loss: 0.6304 | MAE: 0.1835 | RÂ²: 0.3925\n",
      "Epoch 4/8 | Train Loss: 0.7427 | Val Loss: 0.6206 | MAE: 0.1980 | RÂ²: 0.4000\n",
      "Epoch 5/8 | Train Loss: 0.7355 | Val Loss: 0.6262 | MAE: 0.1917 | RÂ²: 0.3956\n",
      "Epoch 6/8 | Train Loss: 0.7323 | Val Loss: 0.6282 | MAE: 0.1729 | RÂ²: 0.3909\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1729\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.7932 | Val Loss: 0.6361 | MAE: 0.2057 | RÂ²: 0.3876\n",
      "Epoch 2/8 | Train Loss: 0.7561 | Val Loss: 0.6279 | MAE: 0.1976 | RÂ²: 0.3933\n",
      "Epoch 3/8 | Train Loss: 0.7481 | Val Loss: 0.6364 | MAE: 0.1946 | RÂ²: 0.3880\n",
      "Epoch 4/8 | Train Loss: 0.7404 | Val Loss: 0.6291 | MAE: 0.1918 | RÂ²: 0.3944\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1918\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.7969 | Val Loss: 0.6312 | MAE: 0.1966 | RÂ²: 0.3909\n",
      "Epoch 2/8 | Train Loss: 0.7571 | Val Loss: 0.6178 | MAE: 0.1900 | RÂ²: 0.4028\n",
      "Epoch 3/8 | Train Loss: 0.7481 | Val Loss: 0.6252 | MAE: 0.2012 | RÂ²: 0.3961\n",
      "Epoch 4/8 | Train Loss: 0.7413 | Val Loss: 0.6157 | MAE: 0.2024 | RÂ²: 0.4049\n",
      "Epoch 5/8 | Train Loss: 0.7355 | Val Loss: 0.6199 | MAE: 0.1777 | RÂ²: 0.4013\n",
      "Epoch 6/8 | Train Loss: 0.7308 | Val Loss: 0.6183 | MAE: 0.1901 | RÂ²: 0.4032\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1901\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.8021 | Val Loss: 0.6686 | MAE: 0.1947 | RÂ²: 0.3607\n",
      "Epoch 2/8 | Train Loss: 0.7575 | Val Loss: 0.6271 | MAE: 0.1972 | RÂ²: 0.3945\n",
      "Epoch 3/8 | Train Loss: 0.7494 | Val Loss: 0.6386 | MAE: 0.1909 | RÂ²: 0.3866\n",
      "Epoch 4/8 | Train Loss: 0.7395 | Val Loss: 0.6239 | MAE: 0.1777 | RÂ²: 0.3987\n",
      "Epoch 5/8 | Train Loss: 0.7376 | Val Loss: 0.6123 | MAE: 0.1883 | RÂ²: 0.4085\n",
      "Epoch 6/8 | Train Loss: 0.7291 | Val Loss: 0.6346 | MAE: 0.1788 | RÂ²: 0.3902\n",
      "Epoch 7/8 | Train Loss: 0.7257 | Val Loss: 0.6135 | MAE: 0.1841 | RÂ²: 0.4072\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1841\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.8019 | Val Loss: 0.6371 | MAE: 0.1791 | RÂ²: 0.3859\n",
      "Epoch 2/8 | Train Loss: 0.7554 | Val Loss: 0.6198 | MAE: 0.1903 | RÂ²: 0.4013\n",
      "Epoch 3/8 | Train Loss: 0.7471 | Val Loss: 0.6303 | MAE: 0.1904 | RÂ²: 0.3897\n",
      "Epoch 4/8 | Train Loss: 0.7404 | Val Loss: 0.6632 | MAE: 0.1875 | RÂ²: 0.3631\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1875\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7984 | Val Loss: 0.6406 | MAE: 0.2076 | RÂ²: 0.3788\n",
      "Epoch 2/8 | Train Loss: 0.7559 | Val Loss: 0.6321 | MAE: 0.1799 | RÂ²: 0.3916\n",
      "Epoch 3/8 | Train Loss: 0.7451 | Val Loss: 0.6305 | MAE: 0.2004 | RÂ²: 0.3914\n",
      "Epoch 4/8 | Train Loss: 0.7384 | Val Loss: 0.6259 | MAE: 0.1983 | RÂ²: 0.3963\n",
      "Epoch 5/8 | Train Loss: 0.7349 | Val Loss: 0.6358 | MAE: 0.1814 | RÂ²: 0.3884\n",
      "Epoch 6/8 | Train Loss: 0.7274 | Val Loss: 0.6206 | MAE: 0.1832 | RÂ²: 0.4019\n",
      "Epoch 7/8 | Train Loss: 0.7245 | Val Loss: 0.6133 | MAE: 0.1848 | RÂ²: 0.4078\n",
      "Epoch 8/8 | Train Loss: 0.7183 | Val Loss: 0.6217 | MAE: 0.1946 | RÂ²: 0.4004\n",
      "âœ… MAE = 0.1946\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.7988 | Val Loss: 0.6291 | MAE: 0.1901 | RÂ²: 0.3919\n",
      "Epoch 2/8 | Train Loss: 0.7564 | Val Loss: 0.6177 | MAE: 0.1792 | RÂ²: 0.4019\n",
      "Epoch 3/8 | Train Loss: 0.7464 | Val Loss: 0.6445 | MAE: 0.1794 | RÂ²: 0.3809\n",
      "Epoch 4/8 | Train Loss: 0.7420 | Val Loss: 0.6160 | MAE: 0.1753 | RÂ²: 0.4053\n",
      "Epoch 5/8 | Train Loss: 0.7355 | Val Loss: 0.6167 | MAE: 0.2123 | RÂ²: 0.4034\n",
      "Epoch 6/8 | Train Loss: 0.7304 | Val Loss: 0.6260 | MAE: 0.1830 | RÂ²: 0.3959\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1830\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.8006 | Val Loss: 0.6520 | MAE: 0.2045 | RÂ²: 0.3737\n",
      "Epoch 2/8 | Train Loss: 0.7556 | Val Loss: 0.6234 | MAE: 0.1834 | RÂ²: 0.3974\n",
      "Epoch 3/8 | Train Loss: 0.7473 | Val Loss: 0.6158 | MAE: 0.1975 | RÂ²: 0.4043\n",
      "Epoch 4/8 | Train Loss: 0.7402 | Val Loss: 0.6288 | MAE: 0.1841 | RÂ²: 0.3941\n",
      "Epoch 5/8 | Train Loss: 0.7338 | Val Loss: 0.6051 | MAE: 0.2075 | RÂ²: 0.4145\n",
      "Epoch 6/8 | Train Loss: 0.7303 | Val Loss: 0.6261 | MAE: 0.1960 | RÂ²: 0.3952\n",
      "Epoch 7/8 | Train Loss: 0.7239 | Val Loss: 0.6242 | MAE: 0.1937 | RÂ²: 0.3979\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1937\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.8032 | Val Loss: 0.6292 | MAE: 0.2186 | RÂ²: 0.3898\n",
      "Epoch 2/8 | Train Loss: 0.7563 | Val Loss: 0.6213 | MAE: 0.1916 | RÂ²: 0.4011\n",
      "Epoch 3/8 | Train Loss: 0.7462 | Val Loss: 0.6150 | MAE: 0.1923 | RÂ²: 0.4063\n",
      "Epoch 4/8 | Train Loss: 0.7396 | Val Loss: 0.6187 | MAE: 0.2060 | RÂ²: 0.4025\n",
      "Epoch 5/8 | Train Loss: 0.7341 | Val Loss: 0.6129 | MAE: 0.1866 | RÂ²: 0.4080\n",
      "Epoch 6/8 | Train Loss: 0.7300 | Val Loss: 0.6192 | MAE: 0.1870 | RÂ²: 0.4025\n",
      "Epoch 7/8 | Train Loss: 0.7242 | Val Loss: 0.6101 | MAE: 0.1901 | RÂ²: 0.4106\n",
      "Epoch 8/8 | Train Loss: 0.7193 | Val Loss: 0.6153 | MAE: 0.1782 | RÂ²: 0.4061\n",
      "âœ… MAE = 0.1782\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.8022 | Val Loss: 0.6286 | MAE: 0.2001 | RÂ²: 0.3933\n",
      "Epoch 2/8 | Train Loss: 0.7543 | Val Loss: 0.6464 | MAE: 0.1878 | RÂ²: 0.3794\n",
      "Epoch 3/8 | Train Loss: 0.7470 | Val Loss: 0.6169 | MAE: 0.1881 | RÂ²: 0.4039\n",
      "Epoch 4/8 | Train Loss: 0.7397 | Val Loss: 0.6245 | MAE: 0.1837 | RÂ²: 0.3970\n",
      "Epoch 5/8 | Train Loss: 0.7337 | Val Loss: 0.6206 | MAE: 0.1832 | RÂ²: 0.4020\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1832\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.7996 | Val Loss: 0.6314 | MAE: 0.2047 | RÂ²: 0.3900\n",
      "Epoch 2/8 | Train Loss: 0.7495 | Val Loss: 0.6210 | MAE: 0.1937 | RÂ²: 0.4009\n",
      "Epoch 3/8 | Train Loss: 0.7393 | Val Loss: 0.6154 | MAE: 0.1782 | RÂ²: 0.4057\n",
      "Epoch 4/8 | Train Loss: 0.7294 | Val Loss: 0.6190 | MAE: 0.1820 | RÂ²: 0.4033\n",
      "Epoch 5/8 | Train Loss: 0.7241 | Val Loss: 0.6306 | MAE: 0.1771 | RÂ²: 0.3913\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1771\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7960 | Val Loss: 0.6280 | MAE: 0.2145 | RÂ²: 0.3936\n",
      "Epoch 2/8 | Train Loss: 0.7499 | Val Loss: 0.6215 | MAE: 0.1889 | RÂ²: 0.4006\n",
      "Epoch 3/8 | Train Loss: 0.7390 | Val Loss: 0.6315 | MAE: 0.1972 | RÂ²: 0.3916\n",
      "Epoch 4/8 | Train Loss: 0.7298 | Val Loss: 0.6353 | MAE: 0.2105 | RÂ²: 0.3873\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.2105\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.7959 | Val Loss: 0.6309 | MAE: 0.1945 | RÂ²: 0.3912\n",
      "Epoch 2/8 | Train Loss: 0.7493 | Val Loss: 0.6301 | MAE: 0.1873 | RÂ²: 0.3911\n",
      "Epoch 3/8 | Train Loss: 0.7373 | Val Loss: 0.6285 | MAE: 0.1968 | RÂ²: 0.3947\n",
      "Epoch 4/8 | Train Loss: 0.7307 | Val Loss: 0.6191 | MAE: 0.1925 | RÂ²: 0.4026\n",
      "Epoch 5/8 | Train Loss: 0.7246 | Val Loss: 0.6230 | MAE: 0.1950 | RÂ²: 0.3993\n",
      "Epoch 6/8 | Train Loss: 0.7171 | Val Loss: 0.6223 | MAE: 0.1803 | RÂ²: 0.3987\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1803\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.7946 | Val Loss: 0.6548 | MAE: 0.2376 | RÂ²: 0.3686\n",
      "Epoch 2/8 | Train Loss: 0.7485 | Val Loss: 0.6233 | MAE: 0.2272 | RÂ²: 0.3973\n",
      "Epoch 3/8 | Train Loss: 0.7393 | Val Loss: 0.6362 | MAE: 0.2133 | RÂ²: 0.3868\n",
      "Epoch 4/8 | Train Loss: 0.7304 | Val Loss: 0.6210 | MAE: 0.2021 | RÂ²: 0.3998\n",
      "Epoch 5/8 | Train Loss: 0.7233 | Val Loss: 0.6297 | MAE: 0.2063 | RÂ²: 0.3931\n",
      "Epoch 6/8 | Train Loss: 0.7192 | Val Loss: 0.6132 | MAE: 0.1961 | RÂ²: 0.4080\n",
      "Epoch 7/8 | Train Loss: 0.7116 | Val Loss: 0.6264 | MAE: 0.1972 | RÂ²: 0.3960\n",
      "Epoch 8/8 | Train Loss: 0.7076 | Val Loss: 0.6198 | MAE: 0.2170 | RÂ²: 0.4011\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.2170\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.7951 | Val Loss: 0.6168 | MAE: 0.2046 | RÂ²: 0.4037\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6266 | MAE: 0.1942 | RÂ²: 0.3963\n",
      "Epoch 3/8 | Train Loss: 0.7387 | Val Loss: 0.6170 | MAE: 0.1900 | RÂ²: 0.4055\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1900\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.7973 | Val Loss: 0.6225 | MAE: 0.1885 | RÂ²: 0.3980\n",
      "Epoch 2/8 | Train Loss: 0.7494 | Val Loss: 0.6229 | MAE: 0.1908 | RÂ²: 0.4000\n",
      "Epoch 3/8 | Train Loss: 0.7383 | Val Loss: 0.6301 | MAE: 0.2104 | RÂ²: 0.3876\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.2104\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.7951 | Val Loss: 0.6375 | MAE: 0.1807 | RÂ²: 0.3824\n",
      "Epoch 2/8 | Train Loss: 0.7466 | Val Loss: 0.6194 | MAE: 0.1897 | RÂ²: 0.4029\n",
      "Epoch 3/8 | Train Loss: 0.7362 | Val Loss: 0.6195 | MAE: 0.1889 | RÂ²: 0.4024\n",
      "Epoch 4/8 | Train Loss: 0.7287 | Val Loss: 0.6521 | MAE: 0.2037 | RÂ²: 0.3702\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.2037\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7959 | Val Loss: 0.6425 | MAE: 0.1889 | RÂ²: 0.3796\n",
      "Epoch 2/8 | Train Loss: 0.7474 | Val Loss: 0.6379 | MAE: 0.1930 | RÂ²: 0.3849\n",
      "Epoch 3/8 | Train Loss: 0.7373 | Val Loss: 0.6356 | MAE: 0.1949 | RÂ²: 0.3864\n",
      "Epoch 4/8 | Train Loss: 0.7291 | Val Loss: 0.6617 | MAE: 0.2037 | RÂ²: 0.3616\n",
      "Epoch 5/8 | Train Loss: 0.7224 | Val Loss: 0.6225 | MAE: 0.1779 | RÂ²: 0.3996\n",
      "Epoch 6/8 | Train Loss: 0.7166 | Val Loss: 0.6347 | MAE: 0.1778 | RÂ²: 0.3899\n",
      "Epoch 7/8 | Train Loss: 0.7100 | Val Loss: 0.6298 | MAE: 0.2053 | RÂ²: 0.3889\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.2053\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.7955 | Val Loss: 0.6383 | MAE: 0.1953 | RÂ²: 0.3816\n",
      "Epoch 2/8 | Train Loss: 0.7478 | Val Loss: 0.6415 | MAE: 0.1724 | RÂ²: 0.3795\n",
      "Epoch 3/8 | Train Loss: 0.7365 | Val Loss: 0.6322 | MAE: 0.1906 | RÂ²: 0.3865\n",
      "Epoch 4/8 | Train Loss: 0.7298 | Val Loss: 0.6237 | MAE: 0.1683 | RÂ²: 0.3966\n",
      "Epoch 5/8 | Train Loss: 0.7216 | Val Loss: 0.6258 | MAE: 0.1724 | RÂ²: 0.3949\n",
      "Epoch 6/8 | Train Loss: 0.7153 | Val Loss: 0.6311 | MAE: 0.1759 | RÂ²: 0.3894\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1759\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.7929 | Val Loss: 0.6757 | MAE: 0.2002 | RÂ²: 0.3513\n",
      "Epoch 2/8 | Train Loss: 0.7467 | Val Loss: 0.6354 | MAE: 0.1950 | RÂ²: 0.3862\n",
      "Epoch 3/8 | Train Loss: 0.7370 | Val Loss: 0.6827 | MAE: 0.2133 | RÂ²: 0.3468\n",
      "Epoch 4/8 | Train Loss: 0.7293 | Val Loss: 0.6436 | MAE: 0.1911 | RÂ²: 0.3788\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1911\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.7952 | Val Loss: 0.6172 | MAE: 0.1830 | RÂ²: 0.4028\n",
      "Epoch 2/8 | Train Loss: 0.7458 | Val Loss: 0.6205 | MAE: 0.1809 | RÂ²: 0.4013\n",
      "Epoch 3/8 | Train Loss: 0.7365 | Val Loss: 0.6225 | MAE: 0.1797 | RÂ²: 0.3996\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1797\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.7940 | Val Loss: 0.6541 | MAE: 0.1996 | RÂ²: 0.3689\n",
      "Epoch 2/8 | Train Loss: 0.7479 | Val Loss: 0.6201 | MAE: 0.1848 | RÂ²: 0.4007\n",
      "Epoch 3/8 | Train Loss: 0.7365 | Val Loss: 0.6302 | MAE: 0.2034 | RÂ²: 0.3909\n",
      "Epoch 4/8 | Train Loss: 0.7277 | Val Loss: 0.6269 | MAE: 0.2005 | RÂ²: 0.3943\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.2005\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.8083 | Val Loss: 0.6388 | MAE: 0.2054 | RÂ²: 0.3817\n",
      "Epoch 2/8 | Train Loss: 0.7580 | Val Loss: 0.6252 | MAE: 0.1879 | RÂ²: 0.3952\n",
      "Epoch 3/8 | Train Loss: 0.7480 | Val Loss: 0.6224 | MAE: 0.1969 | RÂ²: 0.4007\n",
      "Epoch 4/8 | Train Loss: 0.7403 | Val Loss: 0.6163 | MAE: 0.1872 | RÂ²: 0.4045\n",
      "Epoch 5/8 | Train Loss: 0.7356 | Val Loss: 0.6290 | MAE: 0.1819 | RÂ²: 0.3948\n",
      "Epoch 6/8 | Train Loss: 0.7291 | Val Loss: 0.6123 | MAE: 0.1887 | RÂ²: 0.4092\n",
      "Epoch 7/8 | Train Loss: 0.7252 | Val Loss: 0.6156 | MAE: 0.1965 | RÂ²: 0.4048\n",
      "Epoch 8/8 | Train Loss: 0.7204 | Val Loss: 0.6236 | MAE: 0.1866 | RÂ²: 0.3987\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1866\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.8090 | Val Loss: 0.6566 | MAE: 0.1955 | RÂ²: 0.3688\n",
      "Epoch 2/8 | Train Loss: 0.7578 | Val Loss: 0.6216 | MAE: 0.1900 | RÂ²: 0.4004\n",
      "Epoch 3/8 | Train Loss: 0.7486 | Val Loss: 0.6242 | MAE: 0.1940 | RÂ²: 0.3979\n",
      "Epoch 4/8 | Train Loss: 0.7408 | Val Loss: 0.6223 | MAE: 0.1896 | RÂ²: 0.4003\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1896\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.8077 | Val Loss: 0.6128 | MAE: 0.1879 | RÂ²: 0.4073\n",
      "Epoch 2/8 | Train Loss: 0.7606 | Val Loss: 0.6336 | MAE: 0.1850 | RÂ²: 0.3883\n",
      "Epoch 3/8 | Train Loss: 0.7478 | Val Loss: 0.6223 | MAE: 0.1925 | RÂ²: 0.3983\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1925\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.8079 | Val Loss: 0.6493 | MAE: 0.2330 | RÂ²: 0.3713\n",
      "Epoch 2/8 | Train Loss: 0.7582 | Val Loss: 0.6502 | MAE: 0.1853 | RÂ²: 0.3760\n",
      "Epoch 3/8 | Train Loss: 0.7472 | Val Loss: 0.6462 | MAE: 0.1872 | RÂ²: 0.3769\n",
      "Epoch 4/8 | Train Loss: 0.7418 | Val Loss: 0.6326 | MAE: 0.1808 | RÂ²: 0.3910\n",
      "Epoch 5/8 | Train Loss: 0.7355 | Val Loss: 0.6163 | MAE: 0.1752 | RÂ²: 0.4036\n",
      "Epoch 6/8 | Train Loss: 0.7304 | Val Loss: 0.6139 | MAE: 0.1844 | RÂ²: 0.4071\n",
      "Epoch 7/8 | Train Loss: 0.7245 | Val Loss: 0.6264 | MAE: 0.1782 | RÂ²: 0.3962\n",
      "Epoch 8/8 | Train Loss: 0.7195 | Val Loss: 0.6260 | MAE: 0.1778 | RÂ²: 0.3972\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1778\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.8046 | Val Loss: 0.6280 | MAE: 0.1837 | RÂ²: 0.3923\n",
      "Epoch 2/8 | Train Loss: 0.7586 | Val Loss: 0.6321 | MAE: 0.1915 | RÂ²: 0.3886\n",
      "Epoch 3/8 | Train Loss: 0.7469 | Val Loss: 0.6215 | MAE: 0.1860 | RÂ²: 0.3993\n",
      "Epoch 4/8 | Train Loss: 0.7404 | Val Loss: 0.6178 | MAE: 0.1843 | RÂ²: 0.4041\n",
      "Epoch 5/8 | Train Loss: 0.7341 | Val Loss: 0.6161 | MAE: 0.1867 | RÂ²: 0.4040\n",
      "Epoch 6/8 | Train Loss: 0.7286 | Val Loss: 0.6152 | MAE: 0.1861 | RÂ²: 0.4053\n",
      "Epoch 7/8 | Train Loss: 0.7241 | Val Loss: 0.6183 | MAE: 0.1876 | RÂ²: 0.4009\n",
      "Epoch 8/8 | Train Loss: 0.7206 | Val Loss: 0.6193 | MAE: 0.1779 | RÂ²: 0.4002\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1779\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.8055 | Val Loss: 0.6369 | MAE: 0.1921 | RÂ²: 0.3848\n",
      "Epoch 2/8 | Train Loss: 0.7582 | Val Loss: 0.6185 | MAE: 0.1892 | RÂ²: 0.4026\n",
      "Epoch 3/8 | Train Loss: 0.7489 | Val Loss: 0.6201 | MAE: 0.1887 | RÂ²: 0.4018\n",
      "Epoch 4/8 | Train Loss: 0.7416 | Val Loss: 0.6138 | MAE: 0.1808 | RÂ²: 0.4075\n",
      "Epoch 5/8 | Train Loss: 0.7354 | Val Loss: 0.6303 | MAE: 0.1797 | RÂ²: 0.3938\n",
      "Epoch 6/8 | Train Loss: 0.7309 | Val Loss: 0.6193 | MAE: 0.1825 | RÂ²: 0.4022\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1825\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.8050 | Val Loss: 0.6342 | MAE: 0.1862 | RÂ²: 0.3885\n",
      "Epoch 2/8 | Train Loss: 0.7548 | Val Loss: 0.6223 | MAE: 0.1990 | RÂ²: 0.3986\n",
      "Epoch 3/8 | Train Loss: 0.7450 | Val Loss: 0.6177 | MAE: 0.2131 | RÂ²: 0.4031\n",
      "Epoch 4/8 | Train Loss: 0.7394 | Val Loss: 0.6151 | MAE: 0.1868 | RÂ²: 0.4065\n",
      "Epoch 5/8 | Train Loss: 0.7323 | Val Loss: 0.6273 | MAE: 0.1883 | RÂ²: 0.3946\n",
      "Epoch 6/8 | Train Loss: 0.7286 | Val Loss: 0.6159 | MAE: 0.1884 | RÂ²: 0.4059\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1884\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.8079 | Val Loss: 0.6647 | MAE: 0.2317 | RÂ²: 0.3599\n",
      "Epoch 2/8 | Train Loss: 0.7557 | Val Loss: 0.6184 | MAE: 0.1865 | RÂ²: 0.4028\n",
      "Epoch 3/8 | Train Loss: 0.7469 | Val Loss: 0.6197 | MAE: 0.1829 | RÂ²: 0.4027\n",
      "Epoch 4/8 | Train Loss: 0.7397 | Val Loss: 0.6160 | MAE: 0.1782 | RÂ²: 0.4047\n",
      "Epoch 5/8 | Train Loss: 0.7336 | Val Loss: 0.6238 | MAE: 0.1962 | RÂ²: 0.3978\n",
      "Epoch 6/8 | Train Loss: 0.7268 | Val Loss: 0.6329 | MAE: 0.1777 | RÂ²: 0.3912\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1777\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.8043 | Val Loss: 0.6294 | MAE: 0.1837 | RÂ²: 0.3925\n",
      "Epoch 2/8 | Train Loss: 0.7557 | Val Loss: 0.6936 | MAE: 0.1877 | RÂ²: 0.3402\n",
      "Epoch 3/8 | Train Loss: 0.7445 | Val Loss: 0.6423 | MAE: 0.1873 | RÂ²: 0.3809\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1873\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.8103 | Val Loss: 0.6421 | MAE: 0.1998 | RÂ²: 0.3797\n",
      "Epoch 2/8 | Train Loss: 0.7550 | Val Loss: 0.6307 | MAE: 0.1810 | RÂ²: 0.3913\n",
      "Epoch 3/8 | Train Loss: 0.7449 | Val Loss: 0.6279 | MAE: 0.1878 | RÂ²: 0.3933\n",
      "Epoch 4/8 | Train Loss: 0.7393 | Val Loss: 0.6224 | MAE: 0.1786 | RÂ²: 0.3977\n",
      "Epoch 5/8 | Train Loss: 0.7328 | Val Loss: 0.6163 | MAE: 0.1909 | RÂ²: 0.4046\n",
      "Epoch 6/8 | Train Loss: 0.7275 | Val Loss: 0.6342 | MAE: 0.1757 | RÂ²: 0.3902\n",
      "Epoch 7/8 | Train Loss: 0.7238 | Val Loss: 0.6261 | MAE: 0.1848 | RÂ²: 0.3946\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1848\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.8106 | Val Loss: 0.6210 | MAE: 0.1872 | RÂ²: 0.3997\n",
      "Epoch 2/8 | Train Loss: 0.7535 | Val Loss: 0.6271 | MAE: 0.1797 | RÂ²: 0.3953\n",
      "Epoch 3/8 | Train Loss: 0.7447 | Val Loss: 0.6104 | MAE: 0.1899 | RÂ²: 0.4102\n",
      "Epoch 4/8 | Train Loss: 0.7382 | Val Loss: 0.6241 | MAE: 0.1797 | RÂ²: 0.3980\n",
      "Epoch 5/8 | Train Loss: 0.7329 | Val Loss: 0.6361 | MAE: 0.1958 | RÂ²: 0.3847\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1958\n",
      "\n",
      "ðŸ”§ Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.8131 | Val Loss: 0.6363 | MAE: 0.1974 | RÂ²: 0.3879\n",
      "Epoch 2/8 | Train Loss: 0.7547 | Val Loss: 0.6651 | MAE: 0.1879 | RÂ²: 0.3636\n",
      "Epoch 3/8 | Train Loss: 0.7460 | Val Loss: 0.6060 | MAE: 0.1797 | RÂ²: 0.4141\n",
      "Epoch 4/8 | Train Loss: 0.7389 | Val Loss: 0.6356 | MAE: 0.1878 | RÂ²: 0.3876\n",
      "Epoch 5/8 | Train Loss: 0.7333 | Val Loss: 0.6229 | MAE: 0.1876 | RÂ²: 0.3994\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… MAE = 0.1876\n",
      "\n",
      "ðŸ“Š Mejores combinaciones:\n",
      "        lr  dropout            hidden_sizes  alpha       mae\n",
      "14  0.0010      0.3        [1024, 512, 256]    0.3  0.172886\n",
      "10  0.0010      0.2  [2048, 1024, 512, 256]    0.7  0.173905\n",
      "32  0.0005      0.2  [2048, 1024, 512, 256]    0.3  0.175870\n",
      "24  0.0005      0.2        [1024, 512, 256]    0.0  0.177102\n",
      "43  0.0005      0.3  [2048, 1024, 512, 256]    0.1  0.177703\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Definir el espacio de bÃºsqueda\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'hidden_sizes': [\n",
    "        [1024, 512, 256],\n",
    "        [2048, 1024, 512, 256]\n",
    "    ],\n",
    "    'alpha': [0,0.1,0.3, 0.5, 0.7,0.9]\n",
    "}\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "param_combinations = list(product(\n",
    "    param_grid['lr'],\n",
    "    param_grid['dropout'],\n",
    "    param_grid['hidden_sizes'],\n",
    "    param_grid['alpha']\n",
    "))\n",
    "\n",
    "results = []\n",
    "best_mae = float('inf')\n",
    "\n",
    "# Loop de entrenamiento por combinaciÃ³n\n",
    "for lr, dropout, hidden_sizes, alpha in param_combinations:\n",
    "    print(f\"\\nðŸ”§ Entrenando con: lr={lr}, dropout={dropout}, hidden_sizes={hidden_sizes}, alpha={alpha}\")\n",
    "\n",
    "    # Crear modelo y mover a dispositivo\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrenamiento corto para tuning\n",
    "    y_true_gs, y_pred_gs = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        n_epochs=8, lr=lr, alpha=alpha, patience=2,penalty_indices=penalty_indices,coefficients=coefficients\n",
    "    )\n",
    "\n",
    "    mae = mean_absolute_error(y_true_gs, y_pred_gs)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'dropout': dropout,\n",
    "        'hidden_sizes': hidden_sizes,\n",
    "        'alpha': alpha,\n",
    "        'mae': mae\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… MAE = {mae:.4f}\")\n",
    "\n",
    "    # Guardar modelo si es el mejor\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        torch.save(model.state_dict(), f\"best_model_mae{mae:.4f}_lr{lr}_do{dropout}_a{alpha}.pth\")\n",
    "        print(\"ðŸ’¾ Modelo guardado (mejor hasta ahora)\")\n",
    "\n",
    "    # Limpiar memoria GPU\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Convertir a DataFrame y mostrar top 5\n",
    "results_df = pd.DataFrame(results).sort_values(by='mae')\n",
    "print(\"\\nðŸ“Š Mejores combinaciones:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Guardar resultados a disco\n",
    "results_df.to_csv(\"gridsearch_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Asumimos que estas funciones/clases estÃ¡n definidas en tu entorno:\n",
    "# - train_model(model, train_loader, val_loader, n_epochs, lr, loss_fn, patience, scheduler, weight_decay)\n",
    "# - WeightedMSELossMulti(penalty_indices, coefficients, alpha)\n",
    "# - MLP(input_dim, hidden_sizes, dropout)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Espacio de bÃºsqueda\n",
    "    hidden_sizes = trial.suggest_categorical(\"hidden_sizes\", [\n",
    "        [512, 256],\n",
    "        [1024, 512, 256],\n",
    "        [2048, 1024, 512, 256]\n",
    "    ])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.0, 1.0)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    scheduler_gamma = trial.suggest_float(\"scheduler_gamma\", 0.7, 0.99)\n",
    "\n",
    "    # PÃ©rdida\n",
    "    if alpha > 0:\n",
    "        penalty_cols = ['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z',\n",
    "                        'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z',\n",
    "                        'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z']\n",
    "        penalty_indices = [feature_cols.index(col) for col in penalty_cols]\n",
    "        coefficients = [0.236558, 0.178208, -0.060031, -0.161875, -0.007775,\n",
    "                        0.151936, 0.043933, 0.142839, 0.103804, 0.119211, 0.073671]\n",
    "        loss_fn = WeightedMSELossMulti(penalty_indices, coefficients, alpha=alpha)\n",
    "    else:\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Modelo\n",
    "    model = MLP(input_dim=len(feature_cols), hidden_sizes=hidden_sizes, dropout=dropout)\n",
    "\n",
    "    # Scheduler (opcional)\n",
    "    scheduler_config = {\n",
    "        'type': 'StepLR',\n",
    "        'step_size': 2,\n",
    "        'gamma': scheduler_gamma\n",
    "    }\n",
    "\n",
    "    # Entrenamiento\n",
    "    y_true, y_pred = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        n_epochs=8,\n",
    "        lr=lr,\n",
    "        loss_fn=loss_fn,\n",
    "        patience=2,\n",
    "        scheduler_config=scheduler_config,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # MÃ©tricas\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    trial.set_user_attr(\"mae\", mae)\n",
    "    trial.set_user_attr(\"r2\", r2)\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Ejecutar bÃºsqueda\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nðŸ” Mejor configuraciÃ³n encontrada:\")\n",
    "print(study.best_trial.params)\n",
    "print(f\"âœ… MAE: {study.best_value:.4f}\")\n",
    "print(f\"ðŸ“ˆ RÂ²: {study.best_trial.user_attrs['r2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleccionar las 3 mejores combinaciÃ³n de cara a un ensemble\n",
    "results.sort(key=lambda x: x['mae'])\n",
    "print(\"\\nResultados ordenados por MAE:\")\n",
    "for res in results:\n",
    "    print(f\"Params: lr={res['lr']}, dropout={res['dropout']}, hidden_sizes={res['hidden_sizes']}, alpha={res['alpha']} -> MAE={res['mae']:.4f}\")\n",
    "#best_params = min(results, key=lambda x: x['mae'])\n",
    "#print(\"Mejores hiperparÃ¡metros encontrados:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11892c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ðŸ”§ Tus parÃ¡metros finales para los 3 mejores modelos\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"modelo_m1\",\n",
    "        \"hidden_sizes\": [1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.001,\n",
    "        \"alpha\": 0.7\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"modelo_m2\",\n",
    "        \"hidden_sizes\": [2048, 1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.0005,\n",
    "        \"alpha\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"modelo_m3\",\n",
    "        \"hidden_sizes\": [1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.0005,\n",
    "        \"alpha\": 0.3\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "# ðŸ“¦ Dataset completo ya procesado\n",
    "# Usamos el mismo dataset de entrenamiento ya creado\n",
    "# Concatenar train_dataset y val_dataset\n",
    "train_val_dataset = ConcatDataset([train_dataset, val_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5155bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_full = DataLoader(train_val_dataset, batch_size=16384, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d59636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ§  FunciÃ³n de pÃ©rdida personalizada\n",
    "class CustomLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mae = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        base_loss = self.mae(preds, targets)\n",
    "        penalty = torch.mean(torch.abs(targets))  # o cualquier criterio adicional\n",
    "        return (1 - self.alpha) * base_loss + self.alpha * penalty\n",
    "\n",
    "# ðŸš‚ FunciÃ³n de entrenamiento final sin validaciÃ³n\n",
    "def train_final_model(model, train_loader, n_epochs=20, lr=0.001, alpha=0.3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = CustomLoss(alpha=alpha)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_cat, X_num, y_batch in train_loader:\n",
    "            X_cat, X_num, y_batch = X_cat.to(device), X_num.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_cat, X_num)\n",
    "            #loss = loss_fn(preds.squeeze(), y_batch)\n",
    "            loss = loss_fn(preds.squeeze(), y_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"[{model.__class__.__name__}] Epoch {epoch+1}/{n_epochs} | Train Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# âš™ï¸ Entrenamiento de los 3 modelos\n",
    "for cfg in model_configs:\n",
    "    print(f\"\\nðŸ”µ Entrenando {cfg['name']}...\")\n",
    "\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg[\"hidden_sizes\"],\n",
    "        dropout=cfg[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    model = train_final_model(\n",
    "        model,\n",
    "        train_loader=train_loader_full,\n",
    "        n_epochs=20,\n",
    "        lr=cfg[\"lr\"],\n",
    "        alpha=cfg[\"alpha\"]\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{cfg['name']}.pth\")\n",
    "    print(f\"âœ… Modelo {cfg['name']} guardado.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b12318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ“Œ Filtrar periodo 201912\n",
    "df_pred = df_full[df_full[\"PERIODO\"] == 201912].copy()\n",
    "print(df_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸš§ Preparar inputs\n",
    "X_cat_pred = torch.tensor(df_pred[cat_cols].values, dtype=torch.long)\n",
    "X_num_pred = torch.tensor(df_pred[numerical_cols].values, dtype=torch.float)\n",
    "\n",
    "# ðŸ“¦ Dataset y DataLoader sin target\n",
    "pred_dataset = TensorDataset(X_cat_pred, X_num_pred)\n",
    "pred_loader = DataLoader(pred_dataset, batch_size=65536, shuffle=False)\n",
    "\n",
    "# ðŸ“ Modelos a cargar\n",
    "model_paths = [\n",
    "    (\"modelo_m1\", [1024, 512, 256]),\n",
    "    (\"modelo_m2\", [2048, 1024, 512, 256]),\n",
    "    (\"modelo_m3\", [1024, 512, 256]),\n",
    "]\n",
    "\n",
    "# ðŸ§  Clase del modelo: asegurate de tener TabularNNImproved definido\n",
    "\n",
    "# ðŸ“¤ FunciÃ³n para predecir\n",
    "def predict_model(path, hidden_sizes):\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(numerical_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"{path}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_cat_batch, X_num_batch in pred_loader:\n",
    "            X_cat_batch = X_cat_batch.to(device)\n",
    "            X_num_batch = X_num_batch.to(device)\n",
    "            outputs = model(X_cat_batch, X_num_batch).squeeze().cpu().numpy()\n",
    "            preds.extend(outputs)\n",
    "    return np.array(preds)\n",
    "\n",
    "# ðŸ” Predecir con los 3 modelos\n",
    "preds_dict = {}\n",
    "for name, h_sizes in model_paths:\n",
    "    print(f\"ðŸ“¡ Prediciendo con {name}...\")\n",
    "    preds_dict[name] = predict_model(name, h_sizes)\n",
    "\n",
    "# ðŸ”€ Ensemble (promedio)\n",
    "ensemble_pred = np.mean(np.stack(list(preds_dict.values()), axis=0), axis=0)\n",
    "\n",
    "# âœ… Guardar predicciones\n",
    "df_pred[\"PRED_LOG1P_Z\"] = ensemble_pred\n",
    "\n",
    "# (Opcional) si querÃ©s ver distribuciÃ³n\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ensemble_pred, bins=100)\n",
    "plt.title(\"DistribuciÃ³n de predicciones (log1p z-score)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
