{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2250dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8275aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/train_val_NN_TORCH.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e3938cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar conjuntos\n",
    "df_train = df_full[df_full['PERIODO'] <= 201908].copy()\n",
    "df_val = df_full[(df_full['PERIODO'] >= 201909) & (df_full['PERIODO'] <= 201910)].copy()\n",
    "df_pred = df_full[df_full['PERIODO'] == 201912].copy()\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "454ffc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "target_col = 'CLASE_DELTA_LOG1P_Z'\n",
    "\n",
    "# Columnas categóricas a embeddings\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', \n",
    "            'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', \n",
    "            'PRODUCT_RANK_BIN']\n",
    "\n",
    "# Codificación para embeddings\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "    clases_entrenadas = set(le.classes_)\n",
    "    df_val[col] = df_val[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    df_pred[col] = df_pred[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "embedding_sizes = [\n",
    "    (df_train[col].nunique() + 1, min(50, (df_train[col].nunique() + 1) // 2))\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Excluir columnas que no deben ir al modelo\n",
    "excluir = ['PERIODO', 'CUSTOMER_ID', 'PRODUCT_ID', 'CLASE_DELTA_LOG1P_Z', 'ORDINAL']\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in excluir and col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efcc2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No los incluyas en ninguna de estas dos listas\n",
    "assert 'CUSTOMER_ID' not in feature_cols\n",
    "assert 'CUSTOMER_ID' not in cat_cols\n",
    "assert 'PRODUCT_ID' not in feature_cols\n",
    "assert 'PRODUCT_ID' not in cat_cols\n",
    "assert 'PERIODO' not in feature_cols\n",
    "assert 'PERIODO' not in cat_cols\n",
    "assert 'CLASE_DELTA_LOG1P_Z' not in feature_cols\n",
    "assert 'CLASE_DELTA_LOG1P_Z' not in cat_cols\n",
    "assert 'ORDINAL' not in feature_cols\n",
    "assert 'ORDINAL' not in cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb83a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col=None):\n",
    "        self.cat_data = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
    "        self.num_data = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
    "        self.has_target = target_col is not None\n",
    "        if self.has_target:\n",
    "            self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.cat_data[idx], self.num_data[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.cat_data[idx], self.num_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50723c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' class WeightedMSELossMulti(nn.Module):\\n    def __init__(self, penalty_indices, alpha=0.5):\\n        super().__init__()\\n        self.penalty_indices = penalty_indices\\n        self.alpha = alpha\\n\\n    def forward(self, preds, targets, x_num):\\n        penalty = 1 + self.alpha * sum(x_num[:, i].abs() for i in self.penalty_indices)\\n        error = (preds.squeeze() - targets.squeeze()) ** 2\\n        return (penalty * error).mean() '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" class WeightedMSELossMulti(nn.Module):\n",
    "    def __init__(self, penalty_indices, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.penalty_indices = penalty_indices\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, preds, targets, x_num):\n",
    "        penalty = 1 + self.alpha * sum(x_num[:, i].abs() for i in self.penalty_indices)\n",
    "        error = (preds.squeeze() - targets.squeeze()) ** 2\n",
    "        return (penalty * error).mean() \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b15c6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class WeightedMSELossMulti(nn.Module):\n",
    "#     def __init__(self, penalty_indices, coefficients, alpha=0.5):\n",
    "#         \"\"\"\n",
    "#         penalty_indices: lista de índices de columnas en x_num (por ejemplo, [10, 11, ..., 20])\n",
    "#         coefficients: lista de coeficientes (ordenados igual que penalty_indices)\n",
    "#         alpha: fuerza de penalización\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.penalty_indices = penalty_indices\n",
    "#         self.coefficients = coefficients\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def forward(self, preds, targets, x_num):\n",
    "#         # Calcula penalización por muestra\n",
    "#         penalty = torch.ones_like(targets).float()  # shape: [batch_size, 1]\n",
    "\n",
    "#         for idx, coef in zip(self.penalty_indices, self.coefficients):\n",
    "#             penalty += self.alpha * coef * x_num[:, idx:idx+1].abs()  # ensure shape [batch_size, 1]\n",
    "\n",
    "#         error = (preds - targets) ** 2  # both [batch_size, 1]\n",
    "#         return (penalty * error).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec1183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightedMSELossMultiStable(nn.Module):\n",
    "    def __init__(self, penalty_indices, coefficients, alpha=0.1, debug=False):\n",
    "        \"\"\"\n",
    "        penalty_indices: lista de índices de columnas en x_num (por ejemplo, [10, 11, ..., 20])\n",
    "        coefficients: lista de coeficientes (ordenados igual que penalty_indices)\n",
    "        alpha: fuerza de penalización (más bajo por estabilidad en redes grandes)\n",
    "        debug: si True, imprime el promedio del peso de penalización ocasionalmente\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.penalty_indices = penalty_indices\n",
    "        self.coefficients = coefficients\n",
    "        self.alpha = alpha\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, preds, targets, x_num):\n",
    "        # Penalización por muestra (shape: [batch_size, 1])\n",
    "        penalty = torch.ones_like(targets).float()\n",
    "\n",
    "        for idx, coef in zip(self.penalty_indices, self.coefficients):\n",
    "            val = x_num[:, idx:idx+1]  # shape: [batch_size, 1]\n",
    "            safe_val = torch.tanh(val)  # limitar la magnitud para evitar outliers\n",
    "            penalty += self.alpha * coef * safe_val.abs()  # siempre positivo\n",
    "\n",
    "        if self.debug and torch.rand(1).item() < 0.01:\n",
    "            print(f\"[LossDebug] Mean penalty: {penalty.mean().item():.4f}\")\n",
    "\n",
    "        error = (preds - targets) ** 2\n",
    "        weighted_error = penalty * error\n",
    "\n",
    "        return weighted_error.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aad019ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z', 'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z', 'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z']\n",
      "[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n"
     ]
    }
   ],
   "source": [
    "penalty_cols = ['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z','TN_LAG_05_Z','TN_LAG_06_Z','TN_LAG_07_Z',\n",
    "'TN_LAG_08_Z','TN_LAG_09_Z','TN_LAG_10_Z','TN_LAG_11_Z']\n",
    "penalty_indices = [feature_cols.index(col) for col in penalty_cols]\n",
    "print(penalty_cols)\n",
    "print(penalty_indices)\n",
    "coefficients = [\n",
    "    0.236558,\n",
    "    0.178208,\n",
    "   -0.060031,\n",
    "   -0.161875,\n",
    "   -0.007775,\n",
    "    0.151936,\n",
    "    0.043933,\n",
    "    0.142839,\n",
    "    0.103804,\n",
    "    0.119211,\n",
    "    0.073671\n",
    "]\n",
    "# loss_fn = WeightedMSELossMulti(penalty_indices, coefficients, alpha=0.5)\n",
    "\n",
    "loss_fn = WeightedMSELossMultiStable(\n",
    "    penalty_indices=penalty_indices,\n",
    "    coefficients=coefficients,\n",
    "    alpha=0.1,      # más suave\n",
    "    debug=True      # activalo si querés monitorear internamente\n",
    ")\n",
    "\n",
    "#loss_fn = WeightedMSELossMulti(penalty_indices=penalty_cols, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45c637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(col in df_train.columns for col in cat_cols), \"Faltan columnas categóricas\"\n",
    "assert all(col in df_train.columns for col in feature_cols), \"Faltan columnas numéricas\"\n",
    "assert target_col in df_train.columns, \"Falta la variable objetivo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd112c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "\n",
    "train_dataset = TabularDataset(df_train, cat_cols, feature_cols, target_col)\n",
    "val_dataset = TabularDataset(df_val, cat_cols, feature_cols, target_col)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6d04175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN']\n",
      "['MES_SIN', 'MES_COS', 'CUSTOMER_ID_Z', 'PRODUCT_ID_Z', 'CUST_REQUEST_QTY_Z', 'CUST_REQUEST_TN_Z', 'TN_Z', 'MEDIA_MOVIL_3M_CLI_PROD_Z', 'MEDIA_MOVIL_6M_CLI_PROD_Z', 'MEDIA_MOVIL_12M_CLI_PROD_Z', 'DESVIO_MOVIL_3M_CLI_PROD_Z', 'DESVIO_MOVIL_6M_CLI_PROD_Z', 'DESVIO_MOVIL_12M_CLI_PROD_Z', 'MEDIA_MOVIL_3M_PROD_Z', 'MEDIA_MOVIL_6M_PROD_Z', 'MEDIA_MOVIL_12M_PROD_Z', 'DESVIO_MOVIL_3M_PROD_Z', 'DESVIO_MOVIL_6M_PROD_Z', 'DESVIO_MOVIL_12M_PROD_Z', 'MEDIA_MOVIL_3M_CLI_Z', 'MEDIA_MOVIL_6M_CLI_Z', 'MEDIA_MOVIL_12M_CLI_Z', 'DESVIO_MOVIL_3M_CLI_Z', 'DESVIO_MOVIL_6M_CLI_Z', 'DESVIO_MOVIL_12M_CLI_Z', 'TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z', 'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z', 'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z', 'TN_LAG_12_Z', 'TN_LAG_13_Z', 'TN_LAG_14_Z', 'TN_LAG_15_Z', 'ORDINAL_Z', 'TN_DELTA_01_Z', 'TN_DELTA_02_Z', 'TN_DELTA_03_Z', 'TN_DELTA_04_Z', 'TN_DELTA_05_Z', 'TN_DELTA_06_Z', 'TN_DELTA_07_Z', 'TN_DELTA_08_Z', 'TN_DELTA_09_Z', 'TN_DELTA_10_Z', 'TN_DELTA_11_Z', 'TN_DELTA_12_Z', 'TN_DELTA_13_Z', 'TN_DELTA_14_Z', 'TN_DELTA_15_Z', 'ANTIG_CLIENTE_Z', 'ANTIG_PRODUCTO_Z', 'CANT_PROD_CLI_PER_Z', 'MEDIA_PROD_PER_Z', 'MEDIA_PROD_Z', 'MEDIA_PER_Z', 'PENDIENTE_TENDENCIA_3_Z', 'TN_EWMA_03_Z', 'TN_MEDIAN_03_Z', 'TN_MIN_03_Z', 'TN_MAX_03_Z', 'PENDIENTE_TENDENCIA_6_Z', 'TN_EWMA_06_Z', 'TN_MEDIAN_06_Z', 'TN_MIN_06_Z', 'TN_MAX_06_Z', 'PENDIENTE_TENDENCIA_9_Z', 'TN_EWMA_09_Z', 'TN_MEDIAN_09_Z', 'TN_MIN_09_Z', 'TN_MAX_09_Z', 'PENDIENTE_TENDENCIA_12_Z', 'TN_EWMA_12_Z', 'TN_MEDIAN_12_Z', 'TN_MIN_12_Z', 'TN_MAX_12_Z', 'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID_Z', 'MESES_SIN_COMPRAR_PRODUCT_ID_Z', 'MESES_SIN_COMPRAR_CUSTOMER_ID_Z']\n",
      "CLASE_DELTA_LOG1P_Z\n"
     ]
    }
   ],
   "source": [
    "print(cat_cols)\n",
    "print(feature_cols)\n",
    "print(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dff8de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TabularNNImproved(nn.Module):\n",
    "    def __init__(self, embedding_sizes, num_numerical, hidden_sizes=[512, 512, 256, 128], dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(ni, nf) for ni, nf in embedding_sizes\n",
    "        ])\n",
    "        embedding_dim = sum([nf for _, nf in embedding_sizes])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Total input size after embedding + numerical\n",
    "        input_size = embedding_dim + num_numerical\n",
    "\n",
    "        # Hidden layers\n",
    "        layers = []\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = h\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        x = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = torch.cat([x, x_num], dim=1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e66d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabularNNImproved(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(5, 2)\n",
      "    (1): Embedding(16, 8)\n",
      "    (2): Embedding(84, 42)\n",
      "    (3): Embedding(36, 18)\n",
      "    (4): Embedding(67, 33)\n",
      "    (5): Embedding(4, 2)\n",
      "    (6): Embedding(13, 6)\n",
      "    (7): Embedding(5, 2)\n",
      "    (8): Embedding(3, 1)\n",
      "    (9-10): 2 x Embedding(11, 5)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=209, out_features=4096, bias=True)\n",
      "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): GELU(approximate='none')\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): GELU(approximate='none')\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): GELU(approximate='none')\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): GELU(approximate='none')\n",
      "    (19): Dropout(p=0.3, inplace=False)\n",
      "    (20): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (21): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): GELU(approximate='none')\n",
      "    (23): Dropout(p=0.3, inplace=False)\n",
      "    (24): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (25): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): GELU(approximate='none')\n",
      "    (27): Dropout(p=0.3, inplace=False)\n",
      "    (28): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Detectar si hay GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Crear el modelo\n",
    "model = TabularNNImproved(\n",
    "    embedding_sizes=embedding_sizes,\n",
    "    num_numerical=len(feature_cols),\n",
    "    hidden_sizes=[4096,2048,1024,512, 512, 256, 128],\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e8314c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loader, val_loader, n_epochs=20, lr=1e-3, alpha=0.5, patience=3, penalty_indices=None, coefficients=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #criterion = CustomWeightedLoss(tn_index=7, alpha=0.5)\n",
    "    #criterion = WeightedMSELossMulti(penalty_indices=penalty_indices, coefficients=coefficients, alpha=0.5)\n",
    "    #criterion = NonlinearWeightedMSELoss(alpha=0.5)  # podés ajustar alpha  #WeightedMSELoss(alpha=alpha)\n",
    "    criterion = WeightedMSELossMultiStable(penalty_indices=penalty_indices, coefficients=coefficients, alpha=0.5)\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for cats, conts, y in train_loader:\n",
    "            cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(cats, conts)\n",
    "            loss = criterion(y_pred, y, conts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for cats, conts, y in val_loader:\n",
    "                cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "                y_pred = model(cats, conts)\n",
    "                loss = criterion(y_pred, y, conts)\n",
    "                val_loss += loss.item() * y.size(0)\n",
    "\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_pred_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        y_true = np.concatenate(y_true_list)\n",
    "        y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | MAE: {mae:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"🔴 Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Restaurar el mejor modelo\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Retornar valores verdaderos y predichos del último paso\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b6b5a",
   "metadata": {},
   "source": [
    "# Búsqueda de hiperparámetros (Grid Search)\n",
    "Probamos distintas combinaciones de hiperparámetros y seleccionamos la que da mejor MAE en validación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040764c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.7968 | Val Loss: 0.6248 | MAE: 0.2065 | R²: 0.3953\n",
      "Epoch 2/8 | Train Loss: 0.7511 | Val Loss: 0.6281 | MAE: 0.1831 | R²: 0.3936\n",
      "Epoch 3/8 | Train Loss: 0.7396 | Val Loss: 0.6224 | MAE: 0.1760 | R²: 0.3980\n",
      "Epoch 4/8 | Train Loss: 0.7312 | Val Loss: 0.6126 | MAE: 0.1908 | R²: 0.4089\n",
      "Epoch 5/8 | Train Loss: 0.7254 | Val Loss: 0.6153 | MAE: 0.1883 | R²: 0.4058\n",
      "Epoch 6/8 | Train Loss: 0.7186 | Val Loss: 0.6340 | MAE: 0.1884 | R²: 0.3901\n",
      "🔴 Early stopping triggered\n",
      "✅ MAE = 0.1884\n",
      "💾 Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "🔧 Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7843 | Val Loss: 0.6339 | MAE: 0.1860 | R²: 0.3894\n",
      "Epoch 2/8 | Train Loss: 0.7495 | Val Loss: 0.6370 | MAE: 0.1983 | R²: 0.3853\n",
      "Epoch 3/8 | Train Loss: 0.7405 | Val Loss: 0.6216 | MAE: 0.1925 | R²: 0.4005\n",
      "Epoch 4/8 | Train Loss: 0.7318 | Val Loss: 0.6312 | MAE: 0.1948 | R²: 0.3907\n",
      "Epoch 5/8 | Train Loss: 0.7253 | Val Loss: 0.6190 | MAE: 0.1933 | R²: 0.4027\n",
      "Epoch 6/8 | Train Loss: 0.7202 | Val Loss: 0.6238 | MAE: 0.1869 | R²: 0.3981\n",
      "Epoch 7/8 | Train Loss: 0.7142 | Val Loss: 0.6203 | MAE: 0.1850 | R²: 0.4017\n",
      "🔴 Early stopping triggered\n",
      "✅ MAE = 0.1850\n",
      "💾 Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "🔧 Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.3\n",
      "Epoch 1/8 | Train Loss: 0.7898 | Val Loss: 0.6937 | MAE: 0.2064 | R²: 0.3370\n",
      "Epoch 2/8 | Train Loss: 0.7499 | Val Loss: 0.6283 | MAE: 0.1896 | R²: 0.3932\n",
      "Epoch 3/8 | Train Loss: 0.7385 | Val Loss: 0.6460 | MAE: 0.1974 | R²: 0.3777\n",
      "Epoch 4/8 | Train Loss: 0.7317 | Val Loss: 0.6191 | MAE: 0.1975 | R²: 0.4015\n",
      "Epoch 5/8 | Train Loss: 0.7257 | Val Loss: 0.6266 | MAE: 0.1783 | R²: 0.3955\n",
      "Epoch 6/8 | Train Loss: 0.7195 | Val Loss: 0.6257 | MAE: 0.1933 | R²: 0.3956\n",
      "🔴 Early stopping triggered\n",
      "✅ MAE = 0.1933\n",
      "\n",
      "🔧 Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.5\n",
      "Epoch 1/8 | Train Loss: 0.7899 | Val Loss: 0.6600 | MAE: 0.1828 | R²: 0.3678\n",
      "Epoch 2/8 | Train Loss: 0.7498 | Val Loss: 0.6169 | MAE: 0.2111 | R²: 0.4045\n",
      "Epoch 3/8 | Train Loss: 0.7406 | Val Loss: 0.6194 | MAE: 0.1872 | R²: 0.4022\n",
      "Epoch 4/8 | Train Loss: 0.7329 | Val Loss: 0.6226 | MAE: 0.1885 | R²: 0.3990\n",
      "🔴 Early stopping triggered\n",
      "✅ MAE = 0.1885\n",
      "\n",
      "🔧 Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.7\n",
      "Epoch 1/8 | Train Loss: 0.7859 | Val Loss: 0.6313 | MAE: 0.1885 | R²: 0.3912\n",
      "Epoch 2/8 | Train Loss: 0.7482 | Val Loss: 0.6374 | MAE: 0.1979 | R²: 0.3852\n",
      "Epoch 3/8 | Train Loss: 0.7371 | Val Loss: 0.6113 | MAE: 0.2037 | R²: 0.4091\n",
      "Epoch 4/8 | Train Loss: 0.7307 | Val Loss: 0.6146 | MAE: 0.1928 | R²: 0.4062\n",
      "Epoch 5/8 | Train Loss: 0.7239 | Val Loss: 0.6120 | MAE: 0.2080 | R²: 0.4072\n",
      "🔴 Early stopping triggered\n",
      "✅ MAE = 0.2080\n",
      "\n",
      "🔧 Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], alpha=0.9\n",
      "Epoch 1/8 | Train Loss: 0.7937 | Val Loss: 0.6356 | MAE: 0.2509 | R²: 0.3813\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6148 | MAE: 0.1847 | R²: 0.4058\n",
      "Epoch 3/8 | Train Loss: 0.7391 | Val Loss: 0.6165 | MAE: 0.1854 | R²: 0.4049\n",
      "Epoch 4/8 | Train Loss: 0.7313 | Val Loss: 0.6100 | MAE: 0.1771 | R²: 0.4091\n",
      "Epoch 5/8 | Train Loss: 0.7250 | Val Loss: 0.6197 | MAE: 0.1970 | R²: 0.4017\n",
      "Epoch 6/8 | Train Loss: 0.7202 | Val Loss: 0.6190 | MAE: 0.1858 | R²: 0.4016\n",
      "🔴 Early stopping triggered\n",
      "✅ MAE = 0.1858\n",
      "\n",
      "🔧 Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0\n",
      "Epoch 1/8 | Train Loss: 0.7868 | Val Loss: 0.6234 | MAE: 0.1849 | R²: 0.3969\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6197 | MAE: 0.1833 | R²: 0.4027\n",
      "Epoch 3/8 | Train Loss: 0.7386 | Val Loss: 0.6117 | MAE: 0.1912 | R²: 0.4084\n",
      "Epoch 4/8 | Train Loss: 0.7310 | Val Loss: 0.6200 | MAE: 0.1897 | R²: 0.4005\n",
      "Epoch 5/8 | Train Loss: 0.7233 | Val Loss: 0.6406 | MAE: 0.1863 | R²: 0.3835\n",
      "🔴 Early stopping triggered\n",
      "✅ MAE = 0.1863\n",
      "\n",
      "🔧 Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], alpha=0.1\n",
      "Epoch 1/8 | Train Loss: 0.7951 | Val Loss: 0.6690 | MAE: 0.1758 | R²: 0.3593\n",
      "Epoch 2/8 | Train Loss: 0.7487 | Val Loss: 0.6436 | MAE: 0.1943 | R²: 0.3804\n",
      "Epoch 3/8 | Train Loss: 0.7387 | Val Loss: 0.6429 | MAE: 0.1833 | R²: 0.3786\n",
      "Epoch 4/8 | Train Loss: 0.7330 | Val Loss: 0.6271 | MAE: 0.1727 | R²: 0.3937\n",
      "Epoch 5/8 | Train Loss: 0.7262 | Val Loss: 0.6174 | MAE: 0.1919 | R²: 0.4038\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Definir el espacio de búsqueda\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'hidden_sizes': [\n",
    "        [1024, 512, 256],\n",
    "        [2048, 1024, 512, 256]\n",
    "    ],\n",
    "    'alpha': [0,0.1,0.3, 0.5, 0.7,0.9]\n",
    "}\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "param_combinations = list(product(\n",
    "    param_grid['lr'],\n",
    "    param_grid['dropout'],\n",
    "    param_grid['hidden_sizes'],\n",
    "    param_grid['alpha']\n",
    "))\n",
    "\n",
    "results = []\n",
    "best_mae = float('inf')\n",
    "\n",
    "# Loop de entrenamiento por combinación\n",
    "for lr, dropout, hidden_sizes, alpha in param_combinations:\n",
    "    print(f\"\\n🔧 Entrenando con: lr={lr}, dropout={dropout}, hidden_sizes={hidden_sizes}, alpha={alpha}\")\n",
    "\n",
    "    # Crear modelo y mover a dispositivo\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrenamiento corto para tuning\n",
    "    y_true_gs, y_pred_gs = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        n_epochs=8, lr=lr, alpha=alpha, patience=2,penalty_indices=penalty_indices,coefficients=coefficients\n",
    "    )\n",
    "\n",
    "    mae = mean_absolute_error(y_true_gs, y_pred_gs)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'dropout': dropout,\n",
    "        'hidden_sizes': hidden_sizes,\n",
    "        'alpha': alpha,\n",
    "        'mae': mae\n",
    "    })\n",
    "\n",
    "    print(f\"✅ MAE = {mae:.4f}\")\n",
    "\n",
    "    # Guardar modelo si es el mejor\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        torch.save(model.state_dict(), f\"best_model_mae{mae:.4f}_lr{lr}_do{dropout}_a{alpha}.pth\")\n",
    "        print(\"💾 Modelo guardado (mejor hasta ahora)\")\n",
    "\n",
    "    # Limpiar memoria GPU\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Convertir a DataFrame y mostrar top 5\n",
    "results_df = pd.DataFrame(results).sort_values(by='mae')\n",
    "print(\"\\n📊 Mejores combinaciones:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Guardar resultados a disco\n",
    "results_df.to_csv(\"gridsearch_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Asumimos que estas funciones/clases están definidas en tu entorno:\n",
    "# - train_model(model, train_loader, val_loader, n_epochs, lr, loss_fn, patience, scheduler, weight_decay)\n",
    "# - WeightedMSELossMulti(penalty_indices, coefficients, alpha)\n",
    "# - MLP(input_dim, hidden_sizes, dropout)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Espacio de búsqueda\n",
    "    hidden_sizes = trial.suggest_categorical(\"hidden_sizes\", [\n",
    "        [512, 256],\n",
    "        [1024, 512, 256],\n",
    "        [2048, 1024, 512, 256]\n",
    "    ])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.0, 1.0)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    scheduler_gamma = trial.suggest_float(\"scheduler_gamma\", 0.7, 0.99)\n",
    "\n",
    "    # Pérdida\n",
    "    if alpha > 0:\n",
    "        penalty_cols = ['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z',\n",
    "                        'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z',\n",
    "                        'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z']\n",
    "        penalty_indices = [feature_cols.index(col) for col in penalty_cols]\n",
    "        coefficients = [0.236558, 0.178208, -0.060031, -0.161875, -0.007775,\n",
    "                        0.151936, 0.043933, 0.142839, 0.103804, 0.119211, 0.073671]\n",
    "        loss_fn = WeightedMSELossMulti(penalty_indices, coefficients, alpha=alpha)\n",
    "    else:\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Modelo\n",
    "    model = MLP(input_dim=len(feature_cols), hidden_sizes=hidden_sizes, dropout=dropout)\n",
    "\n",
    "    # Scheduler (opcional)\n",
    "    scheduler_config = {\n",
    "        'type': 'StepLR',\n",
    "        'step_size': 2,\n",
    "        'gamma': scheduler_gamma\n",
    "    }\n",
    "\n",
    "    # Entrenamiento\n",
    "    y_true, y_pred = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        n_epochs=8,\n",
    "        lr=lr,\n",
    "        loss_fn=loss_fn,\n",
    "        patience=2,\n",
    "        scheduler_config=scheduler_config,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Métricas\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    trial.set_user_attr(\"mae\", mae)\n",
    "    trial.set_user_attr(\"r2\", r2)\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n🔍 Mejor configuración encontrada:\")\n",
    "print(study.best_trial.params)\n",
    "print(f\"✅ MAE: {study.best_value:.4f}\")\n",
    "print(f\"📈 R²: {study.best_trial.user_attrs['r2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleccionar las 3 mejores combinación de cara a un ensemble\n",
    "results.sort(key=lambda x: x['mae'])\n",
    "print(\"\\nResultados ordenados por MAE:\")\n",
    "for res in results:\n",
    "    print(f\"Params: lr={res['lr']}, dropout={res['dropout']}, hidden_sizes={res['hidden_sizes']}, alpha={res['alpha']} -> MAE={res['mae']:.4f}\")\n",
    "#best_params = min(results, key=lambda x: x['mae'])\n",
    "#print(\"Mejores hiperparámetros encontrados:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11892c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 🔧 Tus parámetros finales para los 3 mejores modelos\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"modelo_m1\",\n",
    "        \"hidden_sizes\": [1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.001,\n",
    "        \"alpha\": 0.7\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"modelo_m2\",\n",
    "        \"hidden_sizes\": [2048, 1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.0005,\n",
    "        \"alpha\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"modelo_m3\",\n",
    "        \"hidden_sizes\": [1024, 512, 256],\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 0.0005,\n",
    "        \"alpha\": 0.3\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "# 📦 Dataset completo ya procesado\n",
    "# Usamos el mismo dataset de entrenamiento ya creado\n",
    "# Concatenar train_dataset y val_dataset\n",
    "train_val_dataset = ConcatDataset([train_dataset, val_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5155bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_full = DataLoader(train_val_dataset, batch_size=16384, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d59636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 🧠 Función de pérdida personalizada\n",
    "class CustomLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mae = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        base_loss = self.mae(preds, targets)\n",
    "        penalty = torch.mean(torch.abs(targets))  # o cualquier criterio adicional\n",
    "        return (1 - self.alpha) * base_loss + self.alpha * penalty\n",
    "\n",
    "# 🚂 Función de entrenamiento final sin validación\n",
    "def train_final_model(model, train_loader, n_epochs=20, lr=0.001, alpha=0.3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = CustomLoss(alpha=alpha)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_cat, X_num, y_batch in train_loader:\n",
    "            X_cat, X_num, y_batch = X_cat.to(device), X_num.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_cat, X_num)\n",
    "            #loss = loss_fn(preds.squeeze(), y_batch)\n",
    "            loss = loss_fn(preds.squeeze(), y_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"[{model.__class__.__name__}] Epoch {epoch+1}/{n_epochs} | Train Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ⚙️ Entrenamiento de los 3 modelos\n",
    "for cfg in model_configs:\n",
    "    print(f\"\\n🔵 Entrenando {cfg['name']}...\")\n",
    "\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg[\"hidden_sizes\"],\n",
    "        dropout=cfg[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    model = train_final_model(\n",
    "        model,\n",
    "        train_loader=train_loader_full,\n",
    "        n_epochs=20,\n",
    "        lr=cfg[\"lr\"],\n",
    "        alpha=cfg[\"alpha\"]\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{cfg['name']}.pth\")\n",
    "    print(f\"✅ Modelo {cfg['name']} guardado.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b12318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# 📌 Filtrar periodo 201912\n",
    "df_pred = df_full[df_full[\"PERIODO\"] == 201912].copy()\n",
    "print(df_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 🚧 Preparar inputs\n",
    "X_cat_pred = torch.tensor(df_pred[cat_cols].values, dtype=torch.long)\n",
    "X_num_pred = torch.tensor(df_pred[numerical_cols].values, dtype=torch.float)\n",
    "\n",
    "# 📦 Dataset y DataLoader sin target\n",
    "pred_dataset = TensorDataset(X_cat_pred, X_num_pred)\n",
    "pred_loader = DataLoader(pred_dataset, batch_size=65536, shuffle=False)\n",
    "\n",
    "# 📁 Modelos a cargar\n",
    "model_paths = [\n",
    "    (\"modelo_m1\", [1024, 512, 256]),\n",
    "    (\"modelo_m2\", [2048, 1024, 512, 256]),\n",
    "    (\"modelo_m3\", [1024, 512, 256]),\n",
    "]\n",
    "\n",
    "# 🧠 Clase del modelo: asegurate de tener TabularNNImproved definido\n",
    "\n",
    "# 📤 Función para predecir\n",
    "def predict_model(path, hidden_sizes):\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(numerical_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"{path}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_cat_batch, X_num_batch in pred_loader:\n",
    "            X_cat_batch = X_cat_batch.to(device)\n",
    "            X_num_batch = X_num_batch.to(device)\n",
    "            outputs = model(X_cat_batch, X_num_batch).squeeze().cpu().numpy()\n",
    "            preds.extend(outputs)\n",
    "    return np.array(preds)\n",
    "\n",
    "# 🔁 Predecir con los 3 modelos\n",
    "preds_dict = {}\n",
    "for name, h_sizes in model_paths:\n",
    "    print(f\"📡 Prediciendo con {name}...\")\n",
    "    preds_dict[name] = predict_model(name, h_sizes)\n",
    "\n",
    "# 🔀 Ensemble (promedio)\n",
    "ensemble_pred = np.mean(np.stack(list(preds_dict.values()), axis=0), axis=0)\n",
    "\n",
    "# ✅ Guardar predicciones\n",
    "df_pred[\"PRED_LOG1P_Z\"] = ensemble_pred\n",
    "\n",
    "# (Opcional) si querés ver distribución\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ensemble_pred, bins=100)\n",
    "plt.title(\"Distribución de predicciones (log1p z-score)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
