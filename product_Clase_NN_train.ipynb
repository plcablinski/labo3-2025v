{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2250dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fe7a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8275aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/product_train_val_NN_TORCH.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3938cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar conjuntos\n",
    "df_train = df_full[df_full['PERIODO'] <= 201909].copy()\n",
    "df_val = df_full[(df_full['PERIODO'] == 201910)].copy()\n",
    "df_pred = df_full[df_full['PERIODO'] == 201912].copy()\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454ffc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'MES_PROBLEMATICO', 'PRODUCT_RANK_BIN', \n",
    "            'IS_FEBRERO', 'ESTOY_PREDICIENDO_FEBRERO', 'CAIDA_ABRUPTA']\n",
    "label_encoders = {}\n",
    "\n",
    "# Cargar los encoders entrenados\n",
    "for col in cat_cols:\n",
    "    le = joblib.load(f'encoders/{col}_encoder.pkl')\n",
    "    label_encoders[col] = le\n",
    "\n",
    "    # Transformar los datasets (train, val, pred) usando ese encoder\n",
    "    for df in [df_train, df_val, df_pred]:\n",
    "        df[col] = df[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else 0)\n",
    "\n",
    "# Definir embedding_sizes (lo mejor es usar el encoder para ver la cantidad de clases)\n",
    "embedding_sizes = [\n",
    "    (len(label_encoders[col].classes_) + 1, min(50, (len(label_encoders[col].classes_) + 1) // 2))\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Excluir columnas que no deben ir al modelo\n",
    "excluir = ['PERIODO', 'CUSTOMER_ID', 'PRODUCT_ID', 'CLASE_LOG1P_Z', 'ORDINAL']\n",
    "feature_cols = [col for col in df.columns if col not in excluir and col not in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efcc2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No los incluyas en ninguna de estas dos listas\n",
    "assert 'CUSTOMER_ID' not in feature_cols\n",
    "assert 'CUSTOMER_ID' not in cat_cols\n",
    "assert 'PRODUCT_ID' not in feature_cols\n",
    "assert 'PRODUCT_ID' not in cat_cols\n",
    "assert 'PERIODO' not in feature_cols\n",
    "assert 'PERIODO' not in cat_cols\n",
    "assert 'CLASE_LOG1P_Z' not in feature_cols\n",
    "assert 'CLASE_LOG1P_Z' not in cat_cols\n",
    "assert 'ORDINAL' not in feature_cols\n",
    "assert 'ORDINAL' not in cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(col in df_train.columns for col in cat_cols), \"Faltan columnas categÃ³ricas\"\n",
    "assert all(col in df_train.columns for col in feature_cols), \"Faltan columnas numÃ©ricas\"\n",
    "assert target_col in df_train.columns, \"Falta la variable objetivo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12892e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col=None):\n",
    "        self.cat_data = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
    "        self.num_data = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
    "        self.has_target = target_col is not None\n",
    "        if self.has_target:\n",
    "            self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.cat_data[idx], self.num_data[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.cat_data[idx], self.num_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd112c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "\n",
    "train_dataset = TabularDataset(df_train, cat_cols, feature_cols, target_col)\n",
    "val_dataset = TabularDataset(df_val, cat_cols, feature_cols, target_col)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d04175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'MES_PROBLEMATICO', 'PRODUCT_RANK_BIN', 'IS_FEBRERO', 'ESTOY_PREDICIENDO_FEBRERO', 'CAIDA_ABRUPTA']\n",
      "['MES_SIN', 'MES_COS', 'ORDINAL_Z', 'ANIO_Z', 'MES_Z', 'TRIMESTRE_Z', 'PRODUCT_ID_Z', 'TN_Z', 'PROM_ULT_3_FEBREROS_Z', 'DIF_TN_VS_FEBREROS_ULT_3_Z', 'TN_MAX_HISTORICO_Z', 'TN_DIST_A_MAX_HIST_Z', 'TN_RATIO_VS_MAX_HIST_Z', 'TN_MEAN_03_Z', 'PENDIENTE_TENDENCIA_3_Z', 'TN_EWMA_03_Z', 'TN_MEDIAN_03_Z', 'TN_MIN_03_Z', 'TN_MAX_03_Z', 'TN_STD_03_Z', 'TN_SKEW_03_Z', 'TN_KURT_03_Z', 'TN_GROWTH_03_Z', 'TN_IQR_03_Z', 'TN_SUM_03_Z', 'TN_COUNT_POS_03_Z', 'TN_PCT_ZERO_03_Z', 'TN_LAST_03_Z', 'TN_LAST_DIFF_03_Z', 'TN_COEF_VAR_3_Z', 'TN_MAXMIN_RATIO_3_Z', 'TN_RANGO_3_Z', 'TN_RANGO_REL_3_Z', 'TN_LAST_VS_MEDIAN_3_Z', 'TN_CHANGE_PREV_WINDOW_3_Z', 'TN_ZEROS_END_3_Z', 'TN_LAST_PCT_SUM_3_Z', 'TN_PCT90_3_Z', 'TN_PCT10_3_Z', 'TN_PCT_WIDTH_3_Z', 'TN_MINUS_MEAN_03_Z', 'TN_MINUS_MEDIAN_03_Z', 'TN_MINUS_EWMA_03_Z', 'TN_OVER_MEAN_03_Z', 'TN_OVER_MEDIAN_03_Z', 'TN_OVER_EWMA_03_Z', 'TN_MEAN_06_Z', 'PENDIENTE_TENDENCIA_6_Z', 'TN_EWMA_06_Z', 'TN_MEDIAN_06_Z', 'TN_MIN_06_Z', 'TN_MAX_06_Z', 'TN_STD_06_Z', 'TN_SKEW_06_Z', 'TN_KURT_06_Z', 'TN_GROWTH_06_Z', 'TN_IQR_06_Z', 'TN_SUM_06_Z', 'TN_COUNT_POS_06_Z', 'TN_PCT_ZERO_06_Z', 'TN_LAST_06_Z', 'TN_LAST_DIFF_06_Z', 'TN_COEF_VAR_6_Z', 'TN_MAXMIN_RATIO_6_Z', 'TN_RANGO_6_Z', 'TN_RANGO_REL_6_Z', 'TN_LAST_VS_MEDIAN_6_Z', 'TN_CHANGE_PREV_WINDOW_6_Z', 'TN_ZEROS_END_6_Z', 'TN_LAST_PCT_SUM_6_Z', 'TN_PCT90_6_Z', 'TN_PCT10_6_Z', 'TN_PCT_WIDTH_6_Z', 'TN_MINUS_MEAN_06_Z', 'TN_MINUS_MEDIAN_06_Z', 'TN_MINUS_EWMA_06_Z', 'TN_OVER_MEAN_06_Z', 'TN_OVER_MEDIAN_06_Z', 'TN_OVER_EWMA_06_Z', 'TN_MEAN_09_Z', 'PENDIENTE_TENDENCIA_9_Z', 'TN_EWMA_09_Z', 'TN_MEDIAN_09_Z', 'TN_MIN_09_Z', 'TN_MAX_09_Z', 'TN_STD_09_Z', 'TN_SKEW_09_Z', 'TN_KURT_09_Z', 'TN_GROWTH_09_Z', 'TN_IQR_09_Z', 'TN_SUM_09_Z', 'TN_COUNT_POS_09_Z', 'TN_PCT_ZERO_09_Z', 'TN_LAST_09_Z', 'TN_LAST_DIFF_09_Z', 'TN_COEF_VAR_9_Z', 'TN_MAXMIN_RATIO_9_Z', 'TN_RANGO_9_Z', 'TN_RANGO_REL_9_Z', 'TN_LAST_VS_MEDIAN_9_Z', 'TN_CHANGE_PREV_WINDOW_9_Z', 'TN_ZEROS_END_9_Z', 'TN_LAST_PCT_SUM_9_Z', 'TN_PCT90_9_Z', 'TN_PCT10_9_Z', 'TN_PCT_WIDTH_9_Z', 'TN_MINUS_MEAN_09_Z', 'TN_MINUS_MEDIAN_09_Z', 'TN_MINUS_EWMA_09_Z', 'TN_OVER_MEAN_09_Z', 'TN_OVER_MEDIAN_09_Z', 'TN_OVER_EWMA_09_Z', 'TN_MEAN_12_Z', 'PENDIENTE_TENDENCIA_12_Z', 'TN_EWMA_12_Z', 'TN_MEDIAN_12_Z', 'TN_MIN_12_Z', 'TN_MAX_12_Z', 'TN_STD_12_Z', 'TN_SKEW_12_Z', 'TN_KURT_12_Z', 'TN_GROWTH_12_Z', 'TN_IQR_12_Z', 'TN_SUM_12_Z', 'TN_COUNT_POS_12_Z', 'TN_PCT_ZERO_12_Z', 'TN_LAST_12_Z', 'TN_LAST_DIFF_12_Z', 'TN_COEF_VAR_12_Z', 'TN_MAXMIN_RATIO_12_Z', 'TN_RANGO_12_Z', 'TN_RANGO_REL_12_Z', 'TN_LAST_VS_MEDIAN_12_Z', 'TN_CHANGE_PREV_WINDOW_12_Z', 'TN_ZEROS_END_12_Z', 'TN_LAST_PCT_SUM_12_Z', 'TN_PCT90_12_Z', 'TN_PCT10_12_Z', 'TN_PCT_WIDTH_12_Z', 'TN_MINUS_MEAN_12_Z', 'TN_MINUS_MEDIAN_12_Z', 'TN_MINUS_EWMA_12_Z', 'TN_OVER_MEAN_12_Z', 'TN_OVER_MEDIAN_12_Z', 'TN_OVER_EWMA_12_Z', 'TN_MEAN_18_Z', 'PENDIENTE_TENDENCIA_18_Z', 'TN_EWMA_18_Z', 'TN_MEDIAN_18_Z', 'TN_MIN_18_Z', 'TN_MAX_18_Z', 'TN_STD_18_Z', 'TN_SKEW_18_Z', 'TN_KURT_18_Z', 'TN_GROWTH_18_Z', 'TN_IQR_18_Z', 'TN_SUM_18_Z', 'TN_COUNT_POS_18_Z', 'TN_PCT_ZERO_18_Z', 'TN_LAST_18_Z', 'TN_LAST_DIFF_18_Z', 'TN_COEF_VAR_18_Z', 'TN_MAXMIN_RATIO_18_Z', 'TN_RANGO_18_Z', 'TN_RANGO_REL_18_Z', 'TN_LAST_VS_MEDIAN_18_Z', 'TN_CHANGE_PREV_WINDOW_18_Z', 'TN_ZEROS_END_18_Z', 'TN_LAST_PCT_SUM_18_Z', 'TN_PCT90_18_Z', 'TN_PCT10_18_Z', 'TN_PCT_WIDTH_18_Z', 'TN_MINUS_MEAN_18_Z', 'TN_MINUS_MEDIAN_18_Z', 'TN_MINUS_EWMA_18_Z', 'TN_OVER_MEAN_18_Z', 'TN_OVER_MEDIAN_18_Z', 'TN_OVER_EWMA_18_Z', 'TN_LAG_03_Z', 'TN_DELTA_03_Z', 'TN_LAG_06_Z', 'TN_DELTA_06_Z', 'TN_LAG_09_Z', 'TN_DELTA_09_Z', 'TN_LAG_12_Z', 'TN_DELTA_12_Z', 'TN_LAG_18_Z', 'TN_DELTA_18_Z', 'TN_COUNT_ZERO_03_Z', 'TN_SIGN_CHANGES_03_Z', 'TN_COEF_VAR_03_Z', 'TN_OUTLIER_PROP_03_Z', 'TN_PCT_POS_03_Z', 'TN_COUNT_ZERO_06_Z', 'TN_SIGN_CHANGES_06_Z', 'TN_COEF_VAR_06_Z', 'TN_OUTLIER_PROP_06_Z', 'TN_PCT_POS_06_Z', 'TN_COUNT_ZERO_09_Z', 'TN_SIGN_CHANGES_09_Z', 'TN_COEF_VAR_09_Z', 'TN_OUTLIER_PROP_09_Z', 'TN_PCT_POS_09_Z', 'TN_COUNT_ZERO_12_Z', 'TN_SIGN_CHANGES_12_Z', 'TN_OUTLIER_PROP_12_Z', 'TN_PCT_POS_12_Z', 'TN_COUNT_ZERO_18_Z', 'TN_SIGN_CHANGES_18_Z', 'TN_OUTLIER_PROP_18_Z', 'TN_PCT_POS_18_Z']\n",
      "CLASE_LOG1P_Z\n"
     ]
    }
   ],
   "source": [
    "print(cat_cols)\n",
    "print(feature_cols)\n",
    "print(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff8de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TabularNNImproved(nn.Module):\n",
    "    def __init__(self, embedding_sizes, num_numerical, hidden_sizes=[512, 512, 256, 128], dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(ni, nf) for ni, nf in embedding_sizes\n",
    "        ])\n",
    "        embedding_dim = sum([nf for _, nf in embedding_sizes])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Total input size after embedding + numerical\n",
    "        input_size = embedding_dim + num_numerical\n",
    "\n",
    "        # Hidden layers\n",
    "        layers = []\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = h\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        x = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = torch.cat([x, x_num], dim=1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e66d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabularNNImproved(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(5, 2)\n",
      "    (1): Embedding(16, 8)\n",
      "    (2): Embedding(85, 42)\n",
      "    (3): Embedding(36, 18)\n",
      "    (4): Embedding(68, 34)\n",
      "    (5): Embedding(3, 1)\n",
      "    (6): Embedding(11, 5)\n",
      "    (7-9): 3 x Embedding(3, 1)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=324, out_features=4096, bias=True)\n",
      "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): GELU(approximate='none')\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): GELU(approximate='none')\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): GELU(approximate='none')\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): GELU(approximate='none')\n",
      "    (19): Dropout(p=0.3, inplace=False)\n",
      "    (20): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (21): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): GELU(approximate='none')\n",
      "    (23): Dropout(p=0.3, inplace=False)\n",
      "    (24): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (25): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): GELU(approximate='none')\n",
      "    (27): Dropout(p=0.3, inplace=False)\n",
      "    (28): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Detectar si hay GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Crear el modelo\n",
    "model = TabularNNImproved(\n",
    "    embedding_sizes=embedding_sizes,\n",
    "    num_numerical=len(feature_cols),\n",
    "    hidden_sizes=[4096,2048,1024,512, 512, 256, 128],\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0ba67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(torch.mean((y_pred - y_true) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e8314c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader=None, n_epochs=20, lr=1e-3, patience=3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = RMSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    best_epoch = 0  # <--- Nuevo: para trackear la mejor epoch\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for cats, conts, y in train_loader:\n",
    "            cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(cats, conts)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # ValidaciÃ³n (solo si hay val_loader)\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            y_true_list = []\n",
    "            y_pred_list = []\n",
    "            with torch.no_grad():\n",
    "                for cats, conts, y in val_loader:\n",
    "                    cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "                    y_pred = model(cats, conts)\n",
    "                    loss = criterion(y_pred, y)\n",
    "                    val_loss += loss.item() * y.size(0)\n",
    "                    y_true_list.append(y.cpu().numpy())\n",
    "                    y_pred_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            y_true = np.concatenate(y_true_list)\n",
    "            y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f} | RÂ²: {r2:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Early stopping y trackeo de la mejor epoch\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                best_epoch = epoch + 1  # <--- Guardamos la mejor epoch (1-based)\n",
    "                best_y_true = y_true\n",
    "                best_y_pred = y_pred\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if patience is not None and epochs_without_improvement >= patience:\n",
    "                    print(\"ðŸ”´ Early stopping triggered\")\n",
    "                    break\n",
    "        else:\n",
    "            # Si no hay val_loader, solo mostrar train_loss\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Restaurar el mejor modelo solo si hubo validaciÃ³n\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Retornar tambiÃ©n la mejor epoch y sus preds\n",
    "    if val_loader is not None:\n",
    "        return best_y_true, best_y_pred, best_epoch\n",
    "    else:\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b6b5a",
   "metadata": {},
   "source": [
    "# BÃºsqueda de hiperparÃ¡metros (Grid Search)\n",
    "Probamos distintas combinaciones de hiperparÃ¡metros y seleccionamos la que da mejor MAE en validaciÃ³n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae473c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ [1/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.9788 | Val Loss: 0.8006 | MAE: 0.6728 | RMSE: 0.8006 | RÂ²: 0.3138\n",
      "Epoch 2/20 | Train Loss: 0.7188 | Val Loss: 0.6375 | MAE: 0.5333 | RMSE: 0.6375 | RÂ²: 0.5649\n",
      "Epoch 3/20 | Train Loss: 0.5721 | Val Loss: 0.5773 | MAE: 0.4549 | RMSE: 0.5773 | RÂ²: 0.6432\n",
      "Epoch 4/20 | Train Loss: 0.5231 | Val Loss: 0.4381 | MAE: 0.3349 | RMSE: 0.4381 | RÂ²: 0.7945\n",
      "Epoch 5/20 | Train Loss: 0.4730 | Val Loss: 0.3365 | MAE: 0.2558 | RMSE: 0.3365 | RÂ²: 0.8788\n",
      "Epoch 6/20 | Train Loss: 0.4506 | Val Loss: 0.3366 | MAE: 0.2480 | RMSE: 0.3366 | RÂ²: 0.8787\n",
      "Epoch 7/20 | Train Loss: 0.4361 | Val Loss: 0.3208 | MAE: 0.2383 | RMSE: 0.3208 | RÂ²: 0.8898\n",
      "Epoch 8/20 | Train Loss: 0.4175 | Val Loss: 0.2978 | MAE: 0.2168 | RMSE: 0.2978 | RÂ²: 0.9051\n",
      "Epoch 9/20 | Train Loss: 0.4062 | Val Loss: 0.2890 | MAE: 0.2056 | RMSE: 0.2890 | RÂ²: 0.9106\n",
      "Epoch 10/20 | Train Loss: 0.3920 | Val Loss: 0.2950 | MAE: 0.2065 | RMSE: 0.2950 | RÂ²: 0.9068\n",
      "Epoch 11/20 | Train Loss: 0.3856 | Val Loss: 0.2794 | MAE: 0.1974 | RMSE: 0.2794 | RÂ²: 0.9164\n",
      "Epoch 12/20 | Train Loss: 0.3837 | Val Loss: 0.2786 | MAE: 0.1951 | RMSE: 0.2786 | RÂ²: 0.9169\n",
      "Epoch 13/20 | Train Loss: 0.3724 | Val Loss: 0.2763 | MAE: 0.1943 | RMSE: 0.2763 | RÂ²: 0.9183\n",
      "Epoch 14/20 | Train Loss: 0.3643 | Val Loss: 0.2751 | MAE: 0.1921 | RMSE: 0.2751 | RÂ²: 0.9190\n",
      "Epoch 15/20 | Train Loss: 0.3638 | Val Loss: 0.2784 | MAE: 0.1941 | RMSE: 0.2784 | RÂ²: 0.9170\n",
      "Epoch 16/20 | Train Loss: 0.3578 | Val Loss: 0.2763 | MAE: 0.1940 | RMSE: 0.2763 | RÂ²: 0.9183\n",
      "Epoch 17/20 | Train Loss: 0.3532 | Val Loss: 0.2758 | MAE: 0.1924 | RMSE: 0.2758 | RÂ²: 0.9186\n",
      "Epoch 18/20 | Train Loss: 0.3471 | Val Loss: 0.2745 | MAE: 0.1920 | RMSE: 0.2745 | RÂ²: 0.9193\n",
      "Epoch 19/20 | Train Loss: 0.3403 | Val Loss: 0.2790 | MAE: 0.1950 | RMSE: 0.2790 | RÂ²: 0.9166\n",
      "Epoch 20/20 | Train Loss: 0.3361 | Val Loss: 0.2803 | MAE: 0.1965 | RMSE: 0.2803 | RÂ²: 0.9159\n",
      "âœ… RMSE = 0.2745 | MAE = 0.1920\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ [2/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8775 | Val Loss: 0.9224 | MAE: 0.7714 | RMSE: 0.9224 | RÂ²: 0.0891\n",
      "Epoch 2/20 | Train Loss: 0.5907 | Val Loss: 0.7105 | MAE: 0.5795 | RMSE: 0.7105 | RÂ²: 0.4595\n",
      "Epoch 3/20 | Train Loss: 0.5138 | Val Loss: 0.5472 | MAE: 0.4278 | RMSE: 0.5472 | RÂ²: 0.6795\n",
      "Epoch 4/20 | Train Loss: 0.4755 | Val Loss: 0.4048 | MAE: 0.2915 | RMSE: 0.4048 | RÂ²: 0.8245\n",
      "Epoch 5/20 | Train Loss: 0.4397 | Val Loss: 0.4084 | MAE: 0.2766 | RMSE: 0.4084 | RÂ²: 0.8214\n",
      "Epoch 6/20 | Train Loss: 0.4047 | Val Loss: 0.3757 | MAE: 0.2725 | RMSE: 0.3757 | RÂ²: 0.8489\n",
      "Epoch 7/20 | Train Loss: 0.3976 | Val Loss: 0.3582 | MAE: 0.2752 | RMSE: 0.3582 | RÂ²: 0.8626\n",
      "Epoch 8/20 | Train Loss: 0.3694 | Val Loss: 0.3835 | MAE: 0.2855 | RMSE: 0.3835 | RÂ²: 0.8425\n",
      "Epoch 9/20 | Train Loss: 0.3553 | Val Loss: 0.3236 | MAE: 0.2432 | RMSE: 0.3236 | RÂ²: 0.8879\n",
      "Epoch 10/20 | Train Loss: 0.3602 | Val Loss: 0.3264 | MAE: 0.2298 | RMSE: 0.3264 | RÂ²: 0.8859\n",
      "Epoch 11/20 | Train Loss: 0.3431 | Val Loss: 0.3174 | MAE: 0.2296 | RMSE: 0.3174 | RÂ²: 0.8922\n",
      "Epoch 12/20 | Train Loss: 0.3342 | Val Loss: 0.3271 | MAE: 0.2240 | RMSE: 0.3271 | RÂ²: 0.8854\n",
      "Epoch 13/20 | Train Loss: 0.3281 | Val Loss: 0.2979 | MAE: 0.2111 | RMSE: 0.2979 | RÂ²: 0.9050\n",
      "Epoch 14/20 | Train Loss: 0.3206 | Val Loss: 0.3070 | MAE: 0.2137 | RMSE: 0.3070 | RÂ²: 0.8991\n",
      "Epoch 15/20 | Train Loss: 0.3154 | Val Loss: 0.2924 | MAE: 0.2005 | RMSE: 0.2924 | RÂ²: 0.9085\n",
      "Epoch 16/20 | Train Loss: 0.3102 | Val Loss: 0.2798 | MAE: 0.1916 | RMSE: 0.2798 | RÂ²: 0.9162\n",
      "Epoch 17/20 | Train Loss: 0.3023 | Val Loss: 0.2891 | MAE: 0.2049 | RMSE: 0.2891 | RÂ²: 0.9105\n",
      "Epoch 18/20 | Train Loss: 0.3000 | Val Loss: 0.2874 | MAE: 0.1979 | RMSE: 0.2874 | RÂ²: 0.9116\n",
      "Epoch 19/20 | Train Loss: 0.2978 | Val Loss: 0.2767 | MAE: 0.1889 | RMSE: 0.2767 | RÂ²: 0.9180\n",
      "Epoch 20/20 | Train Loss: 0.2941 | Val Loss: 0.2711 | MAE: 0.1864 | RMSE: 0.2711 | RÂ²: 0.9213\n",
      "âœ… RMSE = 0.2711 | MAE = 0.1864\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ [3/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.7754 | Val Loss: 0.8705 | MAE: 0.7137 | RMSE: 0.8705 | RÂ²: 0.1888\n",
      "Epoch 2/20 | Train Loss: 0.5269 | Val Loss: 0.4899 | MAE: 0.4019 | RMSE: 0.4899 | RÂ²: 0.7431\n",
      "Epoch 3/20 | Train Loss: 0.4633 | Val Loss: 0.4166 | MAE: 0.3181 | RMSE: 0.4166 | RÂ²: 0.8142\n",
      "Epoch 4/20 | Train Loss: 0.4133 | Val Loss: 0.4370 | MAE: 0.3457 | RMSE: 0.4370 | RÂ²: 0.7956\n",
      "Epoch 5/20 | Train Loss: 0.3981 | Val Loss: 0.3634 | MAE: 0.2903 | RMSE: 0.3634 | RÂ²: 0.8586\n",
      "Epoch 6/20 | Train Loss: 0.3804 | Val Loss: 0.3472 | MAE: 0.2745 | RMSE: 0.3472 | RÂ²: 0.8710\n",
      "Epoch 7/20 | Train Loss: 0.3443 | Val Loss: 0.3396 | MAE: 0.2695 | RMSE: 0.3396 | RÂ²: 0.8765\n",
      "Epoch 8/20 | Train Loss: 0.3309 | Val Loss: 0.3096 | MAE: 0.2282 | RMSE: 0.3096 | RÂ²: 0.8974\n",
      "Epoch 9/20 | Train Loss: 0.3352 | Val Loss: 0.2939 | MAE: 0.2151 | RMSE: 0.2939 | RÂ²: 0.9076\n",
      "Epoch 10/20 | Train Loss: 0.3183 | Val Loss: 0.2674 | MAE: 0.1816 | RMSE: 0.2674 | RÂ²: 0.9235\n",
      "Epoch 11/20 | Train Loss: 0.3023 | Val Loss: 0.2643 | MAE: 0.1807 | RMSE: 0.2643 | RÂ²: 0.9252\n",
      "Epoch 12/20 | Train Loss: 0.2999 | Val Loss: 0.2669 | MAE: 0.1813 | RMSE: 0.2669 | RÂ²: 0.9237\n",
      "Epoch 13/20 | Train Loss: 0.2901 | Val Loss: 0.2559 | MAE: 0.1701 | RMSE: 0.2559 | RÂ²: 0.9299\n",
      "Epoch 14/20 | Train Loss: 0.2865 | Val Loss: 0.2662 | MAE: 0.1807 | RMSE: 0.2662 | RÂ²: 0.9241\n",
      "Epoch 15/20 | Train Loss: 0.2824 | Val Loss: 0.2647 | MAE: 0.1775 | RMSE: 0.2647 | RÂ²: 0.9250\n",
      "Epoch 16/20 | Train Loss: 0.2788 | Val Loss: 0.2711 | MAE: 0.1830 | RMSE: 0.2711 | RÂ²: 0.9213\n",
      "Epoch 17/20 | Train Loss: 0.2725 | Val Loss: 0.2678 | MAE: 0.1803 | RMSE: 0.2678 | RÂ²: 0.9232\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2559 | MAE = 0.1701\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ [4/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[1024, 512, 256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9548 | Val Loss: 0.9750 | MAE: 0.8234 | RMSE: 0.9750 | RÂ²: -0.0177\n",
      "Epoch 2/20 | Train Loss: 0.7270 | Val Loss: 0.9049 | MAE: 0.7663 | RMSE: 0.9049 | RÂ²: 0.1234\n",
      "Epoch 3/20 | Train Loss: 0.6106 | Val Loss: 0.8031 | MAE: 0.6750 | RMSE: 0.8031 | RÂ²: 0.3095\n",
      "Epoch 4/20 | Train Loss: 0.5354 | Val Loss: 0.6415 | MAE: 0.5313 | RMSE: 0.6415 | RÂ²: 0.5594\n",
      "Epoch 5/20 | Train Loss: 0.4910 | Val Loss: 0.4739 | MAE: 0.3836 | RMSE: 0.4739 | RÂ²: 0.7596\n",
      "Epoch 6/20 | Train Loss: 0.4608 | Val Loss: 0.3606 | MAE: 0.2832 | RMSE: 0.3606 | RÂ²: 0.8608\n",
      "Epoch 7/20 | Train Loss: 0.4391 | Val Loss: 0.3138 | MAE: 0.2393 | RMSE: 0.3138 | RÂ²: 0.8946\n",
      "Epoch 8/20 | Train Loss: 0.4112 | Val Loss: 0.2880 | MAE: 0.2124 | RMSE: 0.2880 | RÂ²: 0.9112\n",
      "Epoch 9/20 | Train Loss: 0.4001 | Val Loss: 0.2772 | MAE: 0.2005 | RMSE: 0.2772 | RÂ²: 0.9177\n",
      "Epoch 10/20 | Train Loss: 0.3918 | Val Loss: 0.2780 | MAE: 0.2010 | RMSE: 0.2780 | RÂ²: 0.9173\n",
      "Epoch 11/20 | Train Loss: 0.3794 | Val Loss: 0.2834 | MAE: 0.2030 | RMSE: 0.2834 | RÂ²: 0.9140\n",
      "Epoch 12/20 | Train Loss: 0.3713 | Val Loss: 0.2831 | MAE: 0.1975 | RMSE: 0.2831 | RÂ²: 0.9142\n",
      "Epoch 13/20 | Train Loss: 0.3634 | Val Loss: 0.2779 | MAE: 0.1924 | RMSE: 0.2779 | RÂ²: 0.9173\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2772 | MAE = 0.2005\n",
      "\n",
      "ðŸ”§ [5/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.7718 | Val Loss: 0.7774 | MAE: 0.6470 | RMSE: 0.7774 | RÂ²: 0.3529\n",
      "Epoch 2/20 | Train Loss: 0.4305 | Val Loss: 0.5602 | MAE: 0.4333 | RMSE: 0.5602 | RÂ²: 0.6640\n",
      "Epoch 3/20 | Train Loss: 0.3829 | Val Loss: 0.3517 | MAE: 0.2695 | RMSE: 0.3517 | RÂ²: 0.8676\n",
      "Epoch 4/20 | Train Loss: 0.3306 | Val Loss: 0.4146 | MAE: 0.2889 | RMSE: 0.4146 | RÂ²: 0.8159\n",
      "Epoch 5/20 | Train Loss: 0.3239 | Val Loss: 0.3715 | MAE: 0.2806 | RMSE: 0.3715 | RÂ²: 0.8523\n",
      "Epoch 6/20 | Train Loss: 0.3085 | Val Loss: 0.3466 | MAE: 0.2566 | RMSE: 0.3466 | RÂ²: 0.8714\n",
      "Epoch 7/20 | Train Loss: 0.2928 | Val Loss: 0.2735 | MAE: 0.1884 | RMSE: 0.2735 | RÂ²: 0.9199\n",
      "Epoch 8/20 | Train Loss: 0.2856 | Val Loss: 0.2731 | MAE: 0.1854 | RMSE: 0.2731 | RÂ²: 0.9201\n",
      "Epoch 9/20 | Train Loss: 0.2726 | Val Loss: 0.2696 | MAE: 0.1842 | RMSE: 0.2696 | RÂ²: 0.9222\n",
      "Epoch 10/20 | Train Loss: 0.2728 | Val Loss: 0.2681 | MAE: 0.1733 | RMSE: 0.2681 | RÂ²: 0.9230\n",
      "Epoch 11/20 | Train Loss: 0.2696 | Val Loss: 0.2621 | MAE: 0.1789 | RMSE: 0.2621 | RÂ²: 0.9265\n",
      "Epoch 12/20 | Train Loss: 0.2648 | Val Loss: 0.2757 | MAE: 0.1811 | RMSE: 0.2757 | RÂ²: 0.9186\n",
      "Epoch 13/20 | Train Loss: 0.2611 | Val Loss: 0.2547 | MAE: 0.1734 | RMSE: 0.2547 | RÂ²: 0.9306\n",
      "Epoch 14/20 | Train Loss: 0.2545 | Val Loss: 0.2617 | MAE: 0.1773 | RMSE: 0.2617 | RÂ²: 0.9267\n",
      "Epoch 15/20 | Train Loss: 0.2547 | Val Loss: 0.2707 | MAE: 0.1852 | RMSE: 0.2707 | RÂ²: 0.9216\n",
      "Epoch 16/20 | Train Loss: 0.2491 | Val Loss: 0.2595 | MAE: 0.1747 | RMSE: 0.2595 | RÂ²: 0.9279\n",
      "Epoch 17/20 | Train Loss: 0.2439 | Val Loss: 0.2732 | MAE: 0.1913 | RMSE: 0.2732 | RÂ²: 0.9201\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2547 | MAE = 0.1734\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ [6/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.7114 | Val Loss: 0.9199 | MAE: 0.7694 | RMSE: 0.9199 | RÂ²: 0.0940\n",
      "Epoch 2/20 | Train Loss: 0.3597 | Val Loss: 0.7892 | MAE: 0.6550 | RMSE: 0.7892 | RÂ²: 0.3331\n",
      "Epoch 3/20 | Train Loss: 0.3159 | Val Loss: 0.5625 | MAE: 0.4591 | RMSE: 0.5625 | RÂ²: 0.6613\n",
      "Epoch 4/20 | Train Loss: 0.2916 | Val Loss: 0.3658 | MAE: 0.2929 | RMSE: 0.3658 | RÂ²: 0.8567\n",
      "Epoch 5/20 | Train Loss: 0.2846 | Val Loss: 0.2817 | MAE: 0.2092 | RMSE: 0.2817 | RÂ²: 0.9150\n",
      "Epoch 6/20 | Train Loss: 0.2686 | Val Loss: 0.2525 | MAE: 0.1775 | RMSE: 0.2525 | RÂ²: 0.9318\n",
      "Epoch 7/20 | Train Loss: 0.2618 | Val Loss: 0.2427 | MAE: 0.1661 | RMSE: 0.2427 | RÂ²: 0.9369\n",
      "Epoch 8/20 | Train Loss: 0.2569 | Val Loss: 0.2533 | MAE: 0.1706 | RMSE: 0.2533 | RÂ²: 0.9313\n",
      "Epoch 9/20 | Train Loss: 0.2531 | Val Loss: 0.2480 | MAE: 0.1651 | RMSE: 0.2480 | RÂ²: 0.9342\n",
      "Epoch 10/20 | Train Loss: 0.2454 | Val Loss: 0.2656 | MAE: 0.1816 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "Epoch 11/20 | Train Loss: 0.2497 | Val Loss: 0.2639 | MAE: 0.1782 | RMSE: 0.2639 | RÂ²: 0.9254\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2427 | MAE = 0.1661\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ [7/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[512, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8756 | Val Loss: 0.9198 | MAE: 0.7760 | RMSE: 0.9198 | RÂ²: 0.0943\n",
      "Epoch 2/20 | Train Loss: 0.6098 | Val Loss: 0.8183 | MAE: 0.6901 | RMSE: 0.8183 | RÂ²: 0.2832\n",
      "Epoch 3/20 | Train Loss: 0.4797 | Val Loss: 0.6633 | MAE: 0.5564 | RMSE: 0.6633 | RÂ²: 0.5290\n",
      "Epoch 4/20 | Train Loss: 0.4207 | Val Loss: 0.5028 | MAE: 0.4174 | RMSE: 0.5028 | RÂ²: 0.7294\n",
      "Epoch 5/20 | Train Loss: 0.3842 | Val Loss: 0.4046 | MAE: 0.3283 | RMSE: 0.4046 | RÂ²: 0.8247\n",
      "Epoch 6/20 | Train Loss: 0.3682 | Val Loss: 0.3465 | MAE: 0.2754 | RMSE: 0.3465 | RÂ²: 0.8715\n",
      "Epoch 7/20 | Train Loss: 0.3476 | Val Loss: 0.3083 | MAE: 0.2361 | RMSE: 0.3083 | RÂ²: 0.8983\n",
      "Epoch 8/20 | Train Loss: 0.3341 | Val Loss: 0.2767 | MAE: 0.2027 | RMSE: 0.2767 | RÂ²: 0.9181\n",
      "Epoch 9/20 | Train Loss: 0.3248 | Val Loss: 0.2698 | MAE: 0.1964 | RMSE: 0.2698 | RÂ²: 0.9221\n",
      "Epoch 10/20 | Train Loss: 0.3170 | Val Loss: 0.2699 | MAE: 0.1942 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 11/20 | Train Loss: 0.3072 | Val Loss: 0.2719 | MAE: 0.1946 | RMSE: 0.2719 | RÂ²: 0.9208\n",
      "Epoch 12/20 | Train Loss: 0.3029 | Val Loss: 0.2803 | MAE: 0.1989 | RMSE: 0.2803 | RÂ²: 0.9159\n",
      "Epoch 13/20 | Train Loss: 0.2993 | Val Loss: 0.2741 | MAE: 0.1964 | RMSE: 0.2741 | RÂ²: 0.9195\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2698 | MAE = 0.1964\n",
      "\n",
      "ðŸ”§ [8/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 1.0117 | Val Loss: 0.8966 | MAE: 0.7581 | RMSE: 0.8966 | RÂ²: 0.1395\n",
      "Epoch 2/20 | Train Loss: 0.8768 | Val Loss: 0.7778 | MAE: 0.6499 | RMSE: 0.7778 | RÂ²: 0.3524\n",
      "Epoch 3/20 | Train Loss: 0.8067 | Val Loss: 0.6656 | MAE: 0.5489 | RMSE: 0.6656 | RÂ²: 0.5257\n",
      "Epoch 4/20 | Train Loss: 0.7477 | Val Loss: 0.5692 | MAE: 0.4680 | RMSE: 0.5692 | RÂ²: 0.6532\n",
      "Epoch 5/20 | Train Loss: 0.7203 | Val Loss: 0.5032 | MAE: 0.4089 | RMSE: 0.5032 | RÂ²: 0.7289\n",
      "Epoch 6/20 | Train Loss: 0.6825 | Val Loss: 0.4546 | MAE: 0.3632 | RMSE: 0.4546 | RÂ²: 0.7787\n",
      "Epoch 7/20 | Train Loss: 0.6522 | Val Loss: 0.4115 | MAE: 0.3214 | RMSE: 0.4115 | RÂ²: 0.8187\n",
      "Epoch 8/20 | Train Loss: 0.6290 | Val Loss: 0.3795 | MAE: 0.2919 | RMSE: 0.3795 | RÂ²: 0.8458\n",
      "Epoch 9/20 | Train Loss: 0.6125 | Val Loss: 0.3571 | MAE: 0.2714 | RMSE: 0.3571 | RÂ²: 0.8635\n",
      "Epoch 10/20 | Train Loss: 0.5947 | Val Loss: 0.3472 | MAE: 0.2608 | RMSE: 0.3472 | RÂ²: 0.8709\n",
      "Epoch 11/20 | Train Loss: 0.5738 | Val Loss: 0.3371 | MAE: 0.2516 | RMSE: 0.3371 | RÂ²: 0.8783\n",
      "Epoch 12/20 | Train Loss: 0.5730 | Val Loss: 0.3296 | MAE: 0.2457 | RMSE: 0.3296 | RÂ²: 0.8837\n",
      "Epoch 13/20 | Train Loss: 0.5609 | Val Loss: 0.3277 | MAE: 0.2438 | RMSE: 0.3277 | RÂ²: 0.8850\n",
      "Epoch 14/20 | Train Loss: 0.5451 | Val Loss: 0.3293 | MAE: 0.2436 | RMSE: 0.3293 | RÂ²: 0.8839\n",
      "Epoch 15/20 | Train Loss: 0.5364 | Val Loss: 0.3297 | MAE: 0.2406 | RMSE: 0.3297 | RÂ²: 0.8836\n",
      "Epoch 16/20 | Train Loss: 0.5223 | Val Loss: 0.3216 | MAE: 0.2337 | RMSE: 0.3216 | RÂ²: 0.8893\n",
      "Epoch 17/20 | Train Loss: 0.5183 | Val Loss: 0.3145 | MAE: 0.2294 | RMSE: 0.3145 | RÂ²: 0.8941\n",
      "Epoch 18/20 | Train Loss: 0.5073 | Val Loss: 0.3130 | MAE: 0.2277 | RMSE: 0.3130 | RÂ²: 0.8951\n",
      "Epoch 19/20 | Train Loss: 0.4999 | Val Loss: 0.3110 | MAE: 0.2257 | RMSE: 0.3110 | RÂ²: 0.8965\n",
      "Epoch 20/20 | Train Loss: 0.4982 | Val Loss: 0.3115 | MAE: 0.2243 | RMSE: 0.3115 | RÂ²: 0.8962\n",
      "âœ… RMSE = 0.3110 | MAE = 0.2257\n",
      "\n",
      "ðŸ”§ [9/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[256, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8474 | Val Loss: 0.8136 | MAE: 0.6889 | RMSE: 0.8136 | RÂ²: 0.2914\n",
      "Epoch 2/20 | Train Loss: 0.5759 | Val Loss: 0.6302 | MAE: 0.5222 | RMSE: 0.6302 | RÂ²: 0.5749\n",
      "Epoch 3/20 | Train Loss: 0.4716 | Val Loss: 0.4731 | MAE: 0.3742 | RMSE: 0.4731 | RÂ²: 0.7604\n",
      "Epoch 4/20 | Train Loss: 0.4233 | Val Loss: 0.3536 | MAE: 0.2739 | RMSE: 0.3536 | RÂ²: 0.8661\n",
      "Epoch 5/20 | Train Loss: 0.3919 | Val Loss: 0.3545 | MAE: 0.2571 | RMSE: 0.3545 | RÂ²: 0.8654\n",
      "Epoch 6/20 | Train Loss: 0.3682 | Val Loss: 0.2925 | MAE: 0.2147 | RMSE: 0.2925 | RÂ²: 0.9084\n",
      "Epoch 7/20 | Train Loss: 0.3505 | Val Loss: 0.2857 | MAE: 0.2061 | RMSE: 0.2857 | RÂ²: 0.9126\n",
      "Epoch 8/20 | Train Loss: 0.3376 | Val Loss: 0.2761 | MAE: 0.1956 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 9/20 | Train Loss: 0.3236 | Val Loss: 0.2712 | MAE: 0.1906 | RMSE: 0.2712 | RÂ²: 0.9213\n",
      "Epoch 10/20 | Train Loss: 0.3166 | Val Loss: 0.2750 | MAE: 0.1851 | RMSE: 0.2750 | RÂ²: 0.9190\n",
      "Epoch 11/20 | Train Loss: 0.3103 | Val Loss: 0.2709 | MAE: 0.1884 | RMSE: 0.2709 | RÂ²: 0.9215\n",
      "Epoch 12/20 | Train Loss: 0.3041 | Val Loss: 0.2613 | MAE: 0.1805 | RMSE: 0.2613 | RÂ²: 0.9269\n",
      "Epoch 13/20 | Train Loss: 0.3008 | Val Loss: 0.2752 | MAE: 0.1839 | RMSE: 0.2752 | RÂ²: 0.9189\n",
      "Epoch 14/20 | Train Loss: 0.2955 | Val Loss: 0.2659 | MAE: 0.1835 | RMSE: 0.2659 | RÂ²: 0.9243\n",
      "Epoch 15/20 | Train Loss: 0.2873 | Val Loss: 0.2647 | MAE: 0.1838 | RMSE: 0.2647 | RÂ²: 0.9250\n",
      "Epoch 16/20 | Train Loss: 0.2843 | Val Loss: 0.2702 | MAE: 0.1879 | RMSE: 0.2702 | RÂ²: 0.9219\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2613 | MAE = 0.1805\n",
      "\n",
      "ðŸ”§ [10/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[1024, 256, 1024], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9126 | Val Loss: 0.8778 | MAE: 0.7429 | RMSE: 0.8778 | RÂ²: 0.1752\n",
      "Epoch 2/20 | Train Loss: 0.6324 | Val Loss: 0.7548 | MAE: 0.6261 | RMSE: 0.7548 | RÂ²: 0.3901\n",
      "Epoch 3/20 | Train Loss: 0.5495 | Val Loss: 0.5803 | MAE: 0.4694 | RMSE: 0.5803 | RÂ²: 0.6395\n",
      "Epoch 4/20 | Train Loss: 0.4975 | Val Loss: 0.4624 | MAE: 0.3565 | RMSE: 0.4624 | RÂ²: 0.7711\n",
      "Epoch 5/20 | Train Loss: 0.4658 | Val Loss: 0.3305 | MAE: 0.2472 | RMSE: 0.3305 | RÂ²: 0.8831\n",
      "Epoch 6/20 | Train Loss: 0.4453 | Val Loss: 0.3301 | MAE: 0.2449 | RMSE: 0.3301 | RÂ²: 0.8833\n",
      "Epoch 7/20 | Train Loss: 0.4316 | Val Loss: 0.3099 | MAE: 0.2177 | RMSE: 0.3099 | RÂ²: 0.8972\n",
      "Epoch 8/20 | Train Loss: 0.4244 | Val Loss: 0.3140 | MAE: 0.2201 | RMSE: 0.3140 | RÂ²: 0.8944\n",
      "Epoch 9/20 | Train Loss: 0.4119 | Val Loss: 0.3030 | MAE: 0.2110 | RMSE: 0.3030 | RÂ²: 0.9017\n",
      "Epoch 10/20 | Train Loss: 0.3975 | Val Loss: 0.2934 | MAE: 0.2035 | RMSE: 0.2934 | RÂ²: 0.9078\n",
      "Epoch 11/20 | Train Loss: 0.3923 | Val Loss: 0.2919 | MAE: 0.2050 | RMSE: 0.2919 | RÂ²: 0.9088\n",
      "Epoch 12/20 | Train Loss: 0.3859 | Val Loss: 0.2894 | MAE: 0.2010 | RMSE: 0.2894 | RÂ²: 0.9104\n",
      "Epoch 13/20 | Train Loss: 0.3702 | Val Loss: 0.2922 | MAE: 0.2012 | RMSE: 0.2922 | RÂ²: 0.9086\n",
      "Epoch 14/20 | Train Loss: 0.3732 | Val Loss: 0.2916 | MAE: 0.2028 | RMSE: 0.2916 | RÂ²: 0.9090\n",
      "Epoch 15/20 | Train Loss: 0.3665 | Val Loss: 0.2948 | MAE: 0.1992 | RMSE: 0.2948 | RÂ²: 0.9070\n",
      "Epoch 16/20 | Train Loss: 0.3590 | Val Loss: 0.2911 | MAE: 0.1981 | RMSE: 0.2911 | RÂ²: 0.9093\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2894 | MAE = 0.2010\n",
      "\n",
      "ðŸ”§ [11/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9065 | Val Loss: 0.8007 | MAE: 0.6757 | RMSE: 0.8007 | RÂ²: 0.3137\n",
      "Epoch 2/20 | Train Loss: 0.6597 | Val Loss: 0.5751 | MAE: 0.4667 | RMSE: 0.5751 | RÂ²: 0.6459\n",
      "Epoch 3/20 | Train Loss: 0.5765 | Val Loss: 0.4291 | MAE: 0.3148 | RMSE: 0.4291 | RÂ²: 0.8029\n",
      "Epoch 4/20 | Train Loss: 0.5254 | Val Loss: 0.3370 | MAE: 0.2515 | RMSE: 0.3370 | RÂ²: 0.8784\n",
      "Epoch 5/20 | Train Loss: 0.4762 | Val Loss: 0.3869 | MAE: 0.2954 | RMSE: 0.3869 | RÂ²: 0.8397\n",
      "Epoch 6/20 | Train Loss: 0.4468 | Val Loss: 0.4131 | MAE: 0.3276 | RMSE: 0.4131 | RÂ²: 0.8173\n",
      "Epoch 7/20 | Train Loss: 0.4297 | Val Loss: 0.4138 | MAE: 0.3213 | RMSE: 0.4138 | RÂ²: 0.8167\n",
      "Epoch 8/20 | Train Loss: 0.3978 | Val Loss: 0.3789 | MAE: 0.2984 | RMSE: 0.3789 | RÂ²: 0.8463\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3370 | MAE = 0.2515\n",
      "\n",
      "ðŸ”§ [12/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[512, 512, 512], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9861 | Val Loss: 0.8518 | MAE: 0.7178 | RMSE: 0.8518 | RÂ²: 0.2233\n",
      "Epoch 2/20 | Train Loss: 0.5470 | Val Loss: 0.6201 | MAE: 0.5062 | RMSE: 0.6201 | RÂ²: 0.5884\n",
      "Epoch 3/20 | Train Loss: 0.4639 | Val Loss: 0.3780 | MAE: 0.3015 | RMSE: 0.3780 | RÂ²: 0.8470\n",
      "Epoch 4/20 | Train Loss: 0.4264 | Val Loss: 0.3228 | MAE: 0.2303 | RMSE: 0.3228 | RÂ²: 0.8884\n",
      "Epoch 5/20 | Train Loss: 0.3837 | Val Loss: 0.2801 | MAE: 0.2057 | RMSE: 0.2801 | RÂ²: 0.9160\n",
      "Epoch 6/20 | Train Loss: 0.3607 | Val Loss: 0.3041 | MAE: 0.2291 | RMSE: 0.3041 | RÂ²: 0.9010\n",
      "Epoch 7/20 | Train Loss: 0.3460 | Val Loss: 0.2875 | MAE: 0.2047 | RMSE: 0.2875 | RÂ²: 0.9115\n",
      "Epoch 8/20 | Train Loss: 0.3350 | Val Loss: 0.2932 | MAE: 0.2111 | RMSE: 0.2932 | RÂ²: 0.9080\n",
      "Epoch 9/20 | Train Loss: 0.3194 | Val Loss: 0.2748 | MAE: 0.2012 | RMSE: 0.2748 | RÂ²: 0.9192\n",
      "Epoch 10/20 | Train Loss: 0.3089 | Val Loss: 0.2578 | MAE: 0.1788 | RMSE: 0.2578 | RÂ²: 0.9289\n",
      "Epoch 11/20 | Train Loss: 0.3035 | Val Loss: 0.2621 | MAE: 0.1834 | RMSE: 0.2621 | RÂ²: 0.9265\n",
      "Epoch 12/20 | Train Loss: 0.2945 | Val Loss: 0.2683 | MAE: 0.1915 | RMSE: 0.2683 | RÂ²: 0.9229\n",
      "Epoch 13/20 | Train Loss: 0.2847 | Val Loss: 0.2608 | MAE: 0.1779 | RMSE: 0.2608 | RÂ²: 0.9272\n",
      "Epoch 14/20 | Train Loss: 0.2800 | Val Loss: 0.2726 | MAE: 0.1937 | RMSE: 0.2726 | RÂ²: 0.9205\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2578 | MAE = 0.1788\n",
      "\n",
      "ðŸ”§ [13/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 1.0216 | Val Loss: 0.9356 | MAE: 0.7873 | RMSE: 0.9356 | RÂ²: 0.0628\n",
      "Epoch 2/20 | Train Loss: 0.8457 | Val Loss: 0.8358 | MAE: 0.7090 | RMSE: 0.8358 | RÂ²: 0.2522\n",
      "Epoch 3/20 | Train Loss: 0.7705 | Val Loss: 0.7307 | MAE: 0.6215 | RMSE: 0.7307 | RÂ²: 0.4285\n",
      "Epoch 4/20 | Train Loss: 0.6965 | Val Loss: 0.6362 | MAE: 0.5370 | RMSE: 0.6362 | RÂ²: 0.5666\n",
      "Epoch 5/20 | Train Loss: 0.6402 | Val Loss: 0.5596 | MAE: 0.4556 | RMSE: 0.5596 | RÂ²: 0.6647\n",
      "Epoch 6/20 | Train Loss: 0.6032 | Val Loss: 0.4826 | MAE: 0.3755 | RMSE: 0.4826 | RÂ²: 0.7506\n",
      "Epoch 7/20 | Train Loss: 0.5633 | Val Loss: 0.4115 | MAE: 0.3098 | RMSE: 0.4115 | RÂ²: 0.8188\n",
      "Epoch 8/20 | Train Loss: 0.5474 | Val Loss: 0.3709 | MAE: 0.2759 | RMSE: 0.3709 | RÂ²: 0.8527\n",
      "Epoch 9/20 | Train Loss: 0.5306 | Val Loss: 0.3598 | MAE: 0.2660 | RMSE: 0.3598 | RÂ²: 0.8614\n",
      "Epoch 10/20 | Train Loss: 0.5107 | Val Loss: 0.3500 | MAE: 0.2582 | RMSE: 0.3500 | RÂ²: 0.8689\n",
      "Epoch 11/20 | Train Loss: 0.4915 | Val Loss: 0.3404 | MAE: 0.2497 | RMSE: 0.3404 | RÂ²: 0.8760\n",
      "Epoch 12/20 | Train Loss: 0.4869 | Val Loss: 0.3333 | MAE: 0.2426 | RMSE: 0.3333 | RÂ²: 0.8811\n",
      "Epoch 13/20 | Train Loss: 0.4820 | Val Loss: 0.3240 | MAE: 0.2356 | RMSE: 0.3240 | RÂ²: 0.8876\n",
      "Epoch 14/20 | Train Loss: 0.4691 | Val Loss: 0.3172 | MAE: 0.2307 | RMSE: 0.3172 | RÂ²: 0.8923\n",
      "Epoch 15/20 | Train Loss: 0.4562 | Val Loss: 0.3138 | MAE: 0.2276 | RMSE: 0.3138 | RÂ²: 0.8946\n",
      "Epoch 16/20 | Train Loss: 0.4521 | Val Loss: 0.3118 | MAE: 0.2257 | RMSE: 0.3118 | RÂ²: 0.8959\n",
      "Epoch 17/20 | Train Loss: 0.4429 | Val Loss: 0.3121 | MAE: 0.2245 | RMSE: 0.3121 | RÂ²: 0.8957\n",
      "Epoch 18/20 | Train Loss: 0.4428 | Val Loss: 0.3104 | MAE: 0.2233 | RMSE: 0.3104 | RÂ²: 0.8969\n",
      "Epoch 19/20 | Train Loss: 0.4343 | Val Loss: 0.3077 | MAE: 0.2209 | RMSE: 0.3077 | RÂ²: 0.8986\n",
      "Epoch 20/20 | Train Loss: 0.4266 | Val Loss: 0.3059 | MAE: 0.2192 | RMSE: 0.3059 | RÂ²: 0.8998\n",
      "âœ… RMSE = 0.3059 | MAE = 0.2192\n",
      "\n",
      "ðŸ”§ [14/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[256, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.7299 | Val Loss: 0.7635 | MAE: 0.6293 | RMSE: 0.7635 | RÂ²: 0.3759\n",
      "Epoch 2/20 | Train Loss: 0.4459 | Val Loss: 0.4786 | MAE: 0.3973 | RMSE: 0.4786 | RÂ²: 0.7547\n",
      "Epoch 3/20 | Train Loss: 0.3729 | Val Loss: 0.3936 | MAE: 0.3123 | RMSE: 0.3936 | RÂ²: 0.8342\n",
      "Epoch 4/20 | Train Loss: 0.3270 | Val Loss: 0.3027 | MAE: 0.2284 | RMSE: 0.3027 | RÂ²: 0.9019\n",
      "Epoch 5/20 | Train Loss: 0.3034 | Val Loss: 0.2839 | MAE: 0.2099 | RMSE: 0.2839 | RÂ²: 0.9137\n",
      "Epoch 6/20 | Train Loss: 0.2865 | Val Loss: 0.2645 | MAE: 0.1872 | RMSE: 0.2645 | RÂ²: 0.9251\n",
      "Epoch 7/20 | Train Loss: 0.2719 | Val Loss: 0.2685 | MAE: 0.1868 | RMSE: 0.2685 | RÂ²: 0.9228\n",
      "Epoch 8/20 | Train Loss: 0.2684 | Val Loss: 0.2577 | MAE: 0.1806 | RMSE: 0.2577 | RÂ²: 0.9289\n",
      "Epoch 9/20 | Train Loss: 0.2606 | Val Loss: 0.2526 | MAE: 0.1762 | RMSE: 0.2526 | RÂ²: 0.9317\n",
      "Epoch 10/20 | Train Loss: 0.2523 | Val Loss: 0.2621 | MAE: 0.1821 | RMSE: 0.2621 | RÂ²: 0.9265\n",
      "Epoch 11/20 | Train Loss: 0.2542 | Val Loss: 0.2797 | MAE: 0.1948 | RMSE: 0.2797 | RÂ²: 0.9162\n",
      "Epoch 12/20 | Train Loss: 0.2514 | Val Loss: 0.2756 | MAE: 0.1995 | RMSE: 0.2756 | RÂ²: 0.9187\n",
      "Epoch 13/20 | Train Loss: 0.2498 | Val Loss: 0.2676 | MAE: 0.1871 | RMSE: 0.2676 | RÂ²: 0.9233\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2526 | MAE = 0.1762\n",
      "\n",
      "ðŸ”§ [15/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7771 | Val Loss: 0.8616 | MAE: 0.7181 | RMSE: 0.8616 | RÂ²: 0.2052\n",
      "Epoch 2/20 | Train Loss: 0.4597 | Val Loss: 0.6108 | MAE: 0.5109 | RMSE: 0.6108 | RÂ²: 0.6006\n",
      "Epoch 3/20 | Train Loss: 0.3985 | Val Loss: 0.4295 | MAE: 0.3524 | RMSE: 0.4295 | RÂ²: 0.8025\n",
      "Epoch 4/20 | Train Loss: 0.3495 | Val Loss: 0.2784 | MAE: 0.2079 | RMSE: 0.2784 | RÂ²: 0.9170\n",
      "Epoch 5/20 | Train Loss: 0.3281 | Val Loss: 0.2568 | MAE: 0.1819 | RMSE: 0.2568 | RÂ²: 0.9294\n",
      "Epoch 6/20 | Train Loss: 0.3161 | Val Loss: 0.2633 | MAE: 0.1861 | RMSE: 0.2633 | RÂ²: 0.9258\n",
      "Epoch 7/20 | Train Loss: 0.2984 | Val Loss: 0.3128 | MAE: 0.2039 | RMSE: 0.3128 | RÂ²: 0.8952\n",
      "Epoch 8/20 | Train Loss: 0.2975 | Val Loss: 0.2591 | MAE: 0.1812 | RMSE: 0.2591 | RÂ²: 0.9282\n",
      "Epoch 9/20 | Train Loss: 0.2906 | Val Loss: 0.2766 | MAE: 0.1836 | RMSE: 0.2766 | RÂ²: 0.9181\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2568 | MAE = 0.1819\n",
      "\n",
      "ðŸ”§ [16/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[256, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.1448 | Val Loss: 0.9823 | MAE: 0.8276 | RMSE: 0.9823 | RÂ²: -0.0331\n",
      "Epoch 2/20 | Train Loss: 0.9948 | Val Loss: 0.8788 | MAE: 0.7428 | RMSE: 0.8788 | RÂ²: 0.1733\n",
      "Epoch 3/20 | Train Loss: 0.9055 | Val Loss: 0.7961 | MAE: 0.6600 | RMSE: 0.7961 | RÂ²: 0.3215\n",
      "Epoch 4/20 | Train Loss: 0.8624 | Val Loss: 0.7375 | MAE: 0.5890 | RMSE: 0.7375 | RÂ²: 0.4176\n",
      "Epoch 5/20 | Train Loss: 0.8354 | Val Loss: 0.6726 | MAE: 0.5332 | RMSE: 0.6726 | RÂ²: 0.5157\n",
      "Epoch 6/20 | Train Loss: 0.7916 | Val Loss: 0.6076 | MAE: 0.4885 | RMSE: 0.6076 | RÂ²: 0.6047\n",
      "Epoch 7/20 | Train Loss: 0.7705 | Val Loss: 0.5614 | MAE: 0.4563 | RMSE: 0.5614 | RÂ²: 0.6626\n",
      "Epoch 8/20 | Train Loss: 0.7541 | Val Loss: 0.5304 | MAE: 0.4298 | RMSE: 0.5304 | RÂ²: 0.6988\n",
      "Epoch 9/20 | Train Loss: 0.7170 | Val Loss: 0.5022 | MAE: 0.4033 | RMSE: 0.5022 | RÂ²: 0.7300\n",
      "Epoch 10/20 | Train Loss: 0.7043 | Val Loss: 0.4775 | MAE: 0.3797 | RMSE: 0.4775 | RÂ²: 0.7559\n",
      "Epoch 11/20 | Train Loss: 0.6852 | Val Loss: 0.4551 | MAE: 0.3584 | RMSE: 0.4551 | RÂ²: 0.7783\n",
      "Epoch 12/20 | Train Loss: 0.6680 | Val Loss: 0.4336 | MAE: 0.3383 | RMSE: 0.4336 | RÂ²: 0.7987\n",
      "Epoch 13/20 | Train Loss: 0.6544 | Val Loss: 0.4173 | MAE: 0.3239 | RMSE: 0.4173 | RÂ²: 0.8136\n",
      "Epoch 14/20 | Train Loss: 0.6448 | Val Loss: 0.4055 | MAE: 0.3144 | RMSE: 0.4055 | RÂ²: 0.8240\n",
      "Epoch 15/20 | Train Loss: 0.6257 | Val Loss: 0.4020 | MAE: 0.3105 | RMSE: 0.4020 | RÂ²: 0.8270\n",
      "Epoch 16/20 | Train Loss: 0.6186 | Val Loss: 0.3931 | MAE: 0.3027 | RMSE: 0.3931 | RÂ²: 0.8346\n",
      "Epoch 17/20 | Train Loss: 0.6043 | Val Loss: 0.3827 | MAE: 0.2946 | RMSE: 0.3827 | RÂ²: 0.8432\n",
      "Epoch 18/20 | Train Loss: 0.6059 | Val Loss: 0.3739 | MAE: 0.2871 | RMSE: 0.3739 | RÂ²: 0.8503\n",
      "Epoch 19/20 | Train Loss: 0.5982 | Val Loss: 0.3676 | MAE: 0.2818 | RMSE: 0.3676 | RÂ²: 0.8553\n",
      "Epoch 20/20 | Train Loss: 0.5822 | Val Loss: 0.3618 | MAE: 0.2765 | RMSE: 0.3618 | RÂ²: 0.8599\n",
      "âœ… RMSE = 0.3618 | MAE = 0.2765\n",
      "\n",
      "ðŸ”§ [17/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[1024, 256, 1024], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8484 | Val Loss: 0.9376 | MAE: 0.7936 | RMSE: 0.9376 | RÂ²: 0.0588\n",
      "Epoch 2/20 | Train Loss: 0.5763 | Val Loss: 0.8482 | MAE: 0.7136 | RMSE: 0.8482 | RÂ²: 0.2298\n",
      "Epoch 3/20 | Train Loss: 0.4612 | Val Loss: 0.6793 | MAE: 0.5703 | RMSE: 0.6793 | RÂ²: 0.5059\n",
      "Epoch 4/20 | Train Loss: 0.4217 | Val Loss: 0.5476 | MAE: 0.4537 | RMSE: 0.5476 | RÂ²: 0.6790\n",
      "Epoch 5/20 | Train Loss: 0.3839 | Val Loss: 0.4456 | MAE: 0.3635 | RMSE: 0.4456 | RÂ²: 0.7875\n",
      "Epoch 6/20 | Train Loss: 0.3595 | Val Loss: 0.3809 | MAE: 0.3029 | RMSE: 0.3809 | RÂ²: 0.8447\n",
      "Epoch 7/20 | Train Loss: 0.3445 | Val Loss: 0.3107 | MAE: 0.2386 | RMSE: 0.3107 | RÂ²: 0.8966\n",
      "Epoch 8/20 | Train Loss: 0.3308 | Val Loss: 0.2793 | MAE: 0.2040 | RMSE: 0.2793 | RÂ²: 0.9165\n",
      "Epoch 9/20 | Train Loss: 0.3260 | Val Loss: 0.2724 | MAE: 0.1969 | RMSE: 0.2724 | RÂ²: 0.9206\n",
      "Epoch 10/20 | Train Loss: 0.3149 | Val Loss: 0.2668 | MAE: 0.1913 | RMSE: 0.2668 | RÂ²: 0.9238\n",
      "Epoch 11/20 | Train Loss: 0.3006 | Val Loss: 0.2773 | MAE: 0.1977 | RMSE: 0.2773 | RÂ²: 0.9177\n",
      "Epoch 12/20 | Train Loss: 0.2993 | Val Loss: 0.2754 | MAE: 0.1930 | RMSE: 0.2754 | RÂ²: 0.9188\n",
      "Epoch 13/20 | Train Loss: 0.2965 | Val Loss: 0.2703 | MAE: 0.1898 | RMSE: 0.2703 | RÂ²: 0.9218\n",
      "Epoch 14/20 | Train Loss: 0.2957 | Val Loss: 0.2725 | MAE: 0.1918 | RMSE: 0.2725 | RÂ²: 0.9205\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2668 | MAE = 0.1913\n",
      "\n",
      "ðŸ”§ [18/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[512, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8562 | Val Loss: 0.8118 | MAE: 0.6798 | RMSE: 0.8118 | RÂ²: 0.2944\n",
      "Epoch 2/20 | Train Loss: 0.4707 | Val Loss: 0.5915 | MAE: 0.4867 | RMSE: 0.5915 | RÂ²: 0.6255\n",
      "Epoch 3/20 | Train Loss: 0.3913 | Val Loss: 0.3435 | MAE: 0.2659 | RMSE: 0.3435 | RÂ²: 0.8737\n",
      "Epoch 4/20 | Train Loss: 0.3431 | Val Loss: 0.3015 | MAE: 0.2296 | RMSE: 0.3015 | RÂ²: 0.9027\n",
      "Epoch 5/20 | Train Loss: 0.3262 | Val Loss: 0.3037 | MAE: 0.2252 | RMSE: 0.3037 | RÂ²: 0.9012\n",
      "Epoch 6/20 | Train Loss: 0.3092 | Val Loss: 0.2756 | MAE: 0.2026 | RMSE: 0.2756 | RÂ²: 0.9187\n",
      "Epoch 7/20 | Train Loss: 0.2987 | Val Loss: 0.2781 | MAE: 0.1991 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 8/20 | Train Loss: 0.2889 | Val Loss: 0.2713 | MAE: 0.1914 | RMSE: 0.2713 | RÂ²: 0.9212\n",
      "Epoch 9/20 | Train Loss: 0.2764 | Val Loss: 0.2551 | MAE: 0.1761 | RMSE: 0.2551 | RÂ²: 0.9303\n",
      "Epoch 10/20 | Train Loss: 0.2746 | Val Loss: 0.2885 | MAE: 0.1994 | RMSE: 0.2885 | RÂ²: 0.9109\n",
      "Epoch 11/20 | Train Loss: 0.2732 | Val Loss: 0.2552 | MAE: 0.1702 | RMSE: 0.2552 | RÂ²: 0.9303\n",
      "Epoch 12/20 | Train Loss: 0.2683 | Val Loss: 0.2653 | MAE: 0.1805 | RMSE: 0.2653 | RÂ²: 0.9246\n",
      "Epoch 13/20 | Train Loss: 0.2580 | Val Loss: 0.2682 | MAE: 0.1841 | RMSE: 0.2682 | RÂ²: 0.9230\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2551 | MAE = 0.1761\n",
      "\n",
      "ðŸ”§ [19/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[1024, 256, 1024], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8958 | Val Loss: 0.9193 | MAE: 0.7798 | RMSE: 0.9193 | RÂ²: 0.0953\n",
      "Epoch 2/20 | Train Loss: 0.7133 | Val Loss: 0.8138 | MAE: 0.6909 | RMSE: 0.8138 | RÂ²: 0.2910\n",
      "Epoch 3/20 | Train Loss: 0.6150 | Val Loss: 0.6671 | MAE: 0.5606 | RMSE: 0.6671 | RÂ²: 0.5236\n",
      "Epoch 4/20 | Train Loss: 0.5685 | Val Loss: 0.5322 | MAE: 0.4304 | RMSE: 0.5322 | RÂ²: 0.6968\n",
      "Epoch 5/20 | Train Loss: 0.5333 | Val Loss: 0.4217 | MAE: 0.3223 | RMSE: 0.4217 | RÂ²: 0.8096\n",
      "Epoch 6/20 | Train Loss: 0.5060 | Val Loss: 0.3572 | MAE: 0.2665 | RMSE: 0.3572 | RÂ²: 0.8634\n",
      "Epoch 7/20 | Train Loss: 0.4873 | Val Loss: 0.3122 | MAE: 0.2342 | RMSE: 0.3122 | RÂ²: 0.8956\n",
      "Epoch 8/20 | Train Loss: 0.4648 | Val Loss: 0.3080 | MAE: 0.2300 | RMSE: 0.3080 | RÂ²: 0.8984\n",
      "Epoch 9/20 | Train Loss: 0.4611 | Val Loss: 0.3166 | MAE: 0.2318 | RMSE: 0.3166 | RÂ²: 0.8927\n",
      "Epoch 10/20 | Train Loss: 0.4484 | Val Loss: 0.3152 | MAE: 0.2303 | RMSE: 0.3152 | RÂ²: 0.8936\n",
      "Epoch 11/20 | Train Loss: 0.4404 | Val Loss: 0.3026 | MAE: 0.2215 | RMSE: 0.3026 | RÂ²: 0.9020\n",
      "Epoch 12/20 | Train Loss: 0.4247 | Val Loss: 0.3010 | MAE: 0.2153 | RMSE: 0.3010 | RÂ²: 0.9030\n",
      "Epoch 13/20 | Train Loss: 0.4214 | Val Loss: 0.2957 | MAE: 0.2129 | RMSE: 0.2957 | RÂ²: 0.9064\n",
      "Epoch 14/20 | Train Loss: 0.4183 | Val Loss: 0.2915 | MAE: 0.2131 | RMSE: 0.2915 | RÂ²: 0.9090\n",
      "Epoch 15/20 | Train Loss: 0.4048 | Val Loss: 0.2874 | MAE: 0.2082 | RMSE: 0.2874 | RÂ²: 0.9115\n",
      "Epoch 16/20 | Train Loss: 0.4067 | Val Loss: 0.2834 | MAE: 0.2036 | RMSE: 0.2834 | RÂ²: 0.9140\n",
      "Epoch 17/20 | Train Loss: 0.3955 | Val Loss: 0.2817 | MAE: 0.2012 | RMSE: 0.2817 | RÂ²: 0.9151\n",
      "Epoch 18/20 | Train Loss: 0.3916 | Val Loss: 0.2813 | MAE: 0.2000 | RMSE: 0.2813 | RÂ²: 0.9153\n",
      "Epoch 19/20 | Train Loss: 0.3878 | Val Loss: 0.2847 | MAE: 0.2038 | RMSE: 0.2847 | RÂ²: 0.9132\n",
      "Epoch 20/20 | Train Loss: 0.3822 | Val Loss: 0.2804 | MAE: 0.1971 | RMSE: 0.2804 | RÂ²: 0.9158\n",
      "âœ… RMSE = 0.2804 | MAE = 0.1971\n",
      "\n",
      "ðŸ”§ [20/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.7599 | Val Loss: 0.7678 | MAE: 0.6425 | RMSE: 0.7678 | RÂ²: 0.3688\n",
      "Epoch 2/20 | Train Loss: 0.4803 | Val Loss: 0.5394 | MAE: 0.4474 | RMSE: 0.5394 | RÂ²: 0.6885\n",
      "Epoch 3/20 | Train Loss: 0.4160 | Val Loss: 0.4411 | MAE: 0.3554 | RMSE: 0.4411 | RÂ²: 0.7917\n",
      "Epoch 4/20 | Train Loss: 0.3720 | Val Loss: 0.3389 | MAE: 0.2639 | RMSE: 0.3389 | RÂ²: 0.8770\n",
      "Epoch 5/20 | Train Loss: 0.3510 | Val Loss: 0.3074 | MAE: 0.2226 | RMSE: 0.3074 | RÂ²: 0.8989\n",
      "Epoch 6/20 | Train Loss: 0.3345 | Val Loss: 0.2753 | MAE: 0.1931 | RMSE: 0.2753 | RÂ²: 0.9188\n",
      "Epoch 7/20 | Train Loss: 0.3228 | Val Loss: 0.2969 | MAE: 0.2178 | RMSE: 0.2969 | RÂ²: 0.9056\n",
      "Epoch 8/20 | Train Loss: 0.3152 | Val Loss: 0.2729 | MAE: 0.1913 | RMSE: 0.2729 | RÂ²: 0.9202\n",
      "Epoch 9/20 | Train Loss: 0.3043 | Val Loss: 0.2671 | MAE: 0.1861 | RMSE: 0.2671 | RÂ²: 0.9236\n",
      "Epoch 10/20 | Train Loss: 0.2978 | Val Loss: 0.2750 | MAE: 0.1910 | RMSE: 0.2750 | RÂ²: 0.9190\n",
      "Epoch 11/20 | Train Loss: 0.2945 | Val Loss: 0.2856 | MAE: 0.1990 | RMSE: 0.2856 | RÂ²: 0.9127\n",
      "Epoch 12/20 | Train Loss: 0.2884 | Val Loss: 0.2700 | MAE: 0.1888 | RMSE: 0.2700 | RÂ²: 0.9219\n",
      "Epoch 13/20 | Train Loss: 0.2820 | Val Loss: 0.2722 | MAE: 0.1903 | RMSE: 0.2722 | RÂ²: 0.9207\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2671 | MAE = 0.1861\n",
      "\n",
      "ðŸ”§ [21/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8488 | Val Loss: 0.9355 | MAE: 0.7865 | RMSE: 0.9355 | RÂ²: 0.0631\n",
      "Epoch 2/20 | Train Loss: 0.6207 | Val Loss: 0.8300 | MAE: 0.6940 | RMSE: 0.8300 | RÂ²: 0.2625\n",
      "Epoch 3/20 | Train Loss: 0.5214 | Val Loss: 0.6653 | MAE: 0.5462 | RMSE: 0.6653 | RÂ²: 0.5262\n",
      "Epoch 4/20 | Train Loss: 0.4925 | Val Loss: 0.4798 | MAE: 0.3940 | RMSE: 0.4798 | RÂ²: 0.7535\n",
      "Epoch 5/20 | Train Loss: 0.4596 | Val Loss: 0.3605 | MAE: 0.2882 | RMSE: 0.3605 | RÂ²: 0.8609\n",
      "Epoch 6/20 | Train Loss: 0.4374 | Val Loss: 0.3020 | MAE: 0.2290 | RMSE: 0.3020 | RÂ²: 0.9024\n",
      "Epoch 7/20 | Train Loss: 0.4219 | Val Loss: 0.3146 | MAE: 0.2221 | RMSE: 0.3146 | RÂ²: 0.8940\n",
      "Epoch 8/20 | Train Loss: 0.4057 | Val Loss: 0.3047 | MAE: 0.2215 | RMSE: 0.3047 | RÂ²: 0.9006\n",
      "Epoch 9/20 | Train Loss: 0.3997 | Val Loss: 0.3070 | MAE: 0.2214 | RMSE: 0.3070 | RÂ²: 0.8991\n",
      "Epoch 10/20 | Train Loss: 0.3881 | Val Loss: 0.2960 | MAE: 0.2169 | RMSE: 0.2960 | RÂ²: 0.9062\n",
      "Epoch 11/20 | Train Loss: 0.3773 | Val Loss: 0.2978 | MAE: 0.2203 | RMSE: 0.2978 | RÂ²: 0.9050\n",
      "Epoch 12/20 | Train Loss: 0.3655 | Val Loss: 0.2980 | MAE: 0.2201 | RMSE: 0.2980 | RÂ²: 0.9050\n",
      "Epoch 13/20 | Train Loss: 0.3606 | Val Loss: 0.3153 | MAE: 0.2288 | RMSE: 0.3153 | RÂ²: 0.8935\n",
      "Epoch 14/20 | Train Loss: 0.3577 | Val Loss: 0.3102 | MAE: 0.2244 | RMSE: 0.3102 | RÂ²: 0.8970\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2960 | MAE = 0.2169\n",
      "\n",
      "ðŸ”§ [22/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[512, 512, 512], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9026 | Val Loss: 0.8524 | MAE: 0.7142 | RMSE: 0.8524 | RÂ²: 0.2222\n",
      "Epoch 2/20 | Train Loss: 0.6470 | Val Loss: 0.6769 | MAE: 0.5546 | RMSE: 0.6769 | RÂ²: 0.5095\n",
      "Epoch 3/20 | Train Loss: 0.5520 | Val Loss: 0.4629 | MAE: 0.3723 | RMSE: 0.4629 | RÂ²: 0.7707\n",
      "Epoch 4/20 | Train Loss: 0.5057 | Val Loss: 0.3775 | MAE: 0.2844 | RMSE: 0.3775 | RÂ²: 0.8475\n",
      "Epoch 5/20 | Train Loss: 0.4679 | Val Loss: 0.3407 | MAE: 0.2499 | RMSE: 0.3407 | RÂ²: 0.8757\n",
      "Epoch 6/20 | Train Loss: 0.4415 | Val Loss: 0.3376 | MAE: 0.2444 | RMSE: 0.3376 | RÂ²: 0.8780\n",
      "Epoch 7/20 | Train Loss: 0.4267 | Val Loss: 0.3641 | MAE: 0.2729 | RMSE: 0.3641 | RÂ²: 0.8581\n",
      "Epoch 8/20 | Train Loss: 0.4032 | Val Loss: 0.3501 | MAE: 0.2632 | RMSE: 0.3501 | RÂ²: 0.8688\n",
      "Epoch 9/20 | Train Loss: 0.3796 | Val Loss: 0.3444 | MAE: 0.2643 | RMSE: 0.3444 | RÂ²: 0.8730\n",
      "Epoch 10/20 | Train Loss: 0.3736 | Val Loss: 0.3130 | MAE: 0.2296 | RMSE: 0.3130 | RÂ²: 0.8951\n",
      "Epoch 11/20 | Train Loss: 0.3638 | Val Loss: 0.3011 | MAE: 0.2204 | RMSE: 0.3011 | RÂ²: 0.9029\n",
      "Epoch 12/20 | Train Loss: 0.3588 | Val Loss: 0.3384 | MAE: 0.2359 | RMSE: 0.3384 | RÂ²: 0.8774\n",
      "Epoch 13/20 | Train Loss: 0.3467 | Val Loss: 0.2970 | MAE: 0.2116 | RMSE: 0.2970 | RÂ²: 0.9055\n",
      "Epoch 14/20 | Train Loss: 0.3373 | Val Loss: 0.2841 | MAE: 0.1984 | RMSE: 0.2841 | RÂ²: 0.9136\n",
      "Epoch 15/20 | Train Loss: 0.3342 | Val Loss: 0.3091 | MAE: 0.2171 | RMSE: 0.3091 | RÂ²: 0.8977\n",
      "Epoch 16/20 | Train Loss: 0.3264 | Val Loss: 0.2733 | MAE: 0.1869 | RMSE: 0.2733 | RÂ²: 0.9200\n",
      "Epoch 17/20 | Train Loss: 0.3188 | Val Loss: 0.2859 | MAE: 0.2005 | RMSE: 0.2859 | RÂ²: 0.9125\n",
      "Epoch 18/20 | Train Loss: 0.3145 | Val Loss: 0.2905 | MAE: 0.1988 | RMSE: 0.2905 | RÂ²: 0.9097\n",
      "Epoch 19/20 | Train Loss: 0.3120 | Val Loss: 0.2792 | MAE: 0.1891 | RMSE: 0.2792 | RÂ²: 0.9165\n",
      "Epoch 20/20 | Train Loss: 0.3066 | Val Loss: 0.2896 | MAE: 0.2013 | RMSE: 0.2896 | RÂ²: 0.9102\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2733 | MAE = 0.1869\n",
      "\n",
      "ðŸ”§ [23/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0158 | Val Loss: 0.8966 | MAE: 0.7600 | RMSE: 0.8966 | RÂ²: 0.1393\n",
      "Epoch 2/20 | Train Loss: 0.8687 | Val Loss: 0.7850 | MAE: 0.6661 | RMSE: 0.7850 | RÂ²: 0.3403\n",
      "Epoch 3/20 | Train Loss: 0.8076 | Val Loss: 0.6788 | MAE: 0.5755 | RMSE: 0.6788 | RÂ²: 0.5068\n",
      "Epoch 4/20 | Train Loss: 0.7444 | Val Loss: 0.5826 | MAE: 0.4886 | RMSE: 0.5826 | RÂ²: 0.6366\n",
      "Epoch 5/20 | Train Loss: 0.6927 | Val Loss: 0.4923 | MAE: 0.4025 | RMSE: 0.4923 | RÂ²: 0.7405\n",
      "Epoch 6/20 | Train Loss: 0.6597 | Val Loss: 0.4231 | MAE: 0.3313 | RMSE: 0.4231 | RÂ²: 0.8084\n",
      "Epoch 7/20 | Train Loss: 0.6254 | Val Loss: 0.3813 | MAE: 0.2805 | RMSE: 0.3813 | RÂ²: 0.8444\n",
      "Epoch 8/20 | Train Loss: 0.6012 | Val Loss: 0.3678 | MAE: 0.2562 | RMSE: 0.3678 | RÂ²: 0.8552\n",
      "Epoch 9/20 | Train Loss: 0.5844 | Val Loss: 0.3686 | MAE: 0.2496 | RMSE: 0.3686 | RÂ²: 0.8545\n",
      "Epoch 10/20 | Train Loss: 0.5680 | Val Loss: 0.3664 | MAE: 0.2479 | RMSE: 0.3664 | RÂ²: 0.8563\n",
      "Epoch 11/20 | Train Loss: 0.5512 | Val Loss: 0.3645 | MAE: 0.2484 | RMSE: 0.3645 | RÂ²: 0.8578\n",
      "Epoch 12/20 | Train Loss: 0.5404 | Val Loss: 0.3703 | MAE: 0.2498 | RMSE: 0.3703 | RÂ²: 0.8532\n",
      "Epoch 13/20 | Train Loss: 0.5210 | Val Loss: 0.3672 | MAE: 0.2487 | RMSE: 0.3672 | RÂ²: 0.8556\n",
      "Epoch 14/20 | Train Loss: 0.5177 | Val Loss: 0.3669 | MAE: 0.2493 | RMSE: 0.3669 | RÂ²: 0.8559\n",
      "Epoch 15/20 | Train Loss: 0.5056 | Val Loss: 0.3674 | MAE: 0.2500 | RMSE: 0.3674 | RÂ²: 0.8555\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3645 | MAE = 0.2484\n",
      "\n",
      "ðŸ”§ [24/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8289 | Val Loss: 0.8514 | MAE: 0.7051 | RMSE: 0.8514 | RÂ²: 0.2240\n",
      "Epoch 2/20 | Train Loss: 0.5195 | Val Loss: 0.5342 | MAE: 0.4371 | RMSE: 0.5342 | RÂ²: 0.6945\n",
      "Epoch 3/20 | Train Loss: 0.4504 | Val Loss: 0.3973 | MAE: 0.3066 | RMSE: 0.3973 | RÂ²: 0.8310\n",
      "Epoch 4/20 | Train Loss: 0.4042 | Val Loss: 0.3183 | MAE: 0.2413 | RMSE: 0.3183 | RÂ²: 0.8916\n",
      "Epoch 5/20 | Train Loss: 0.3776 | Val Loss: 0.3332 | MAE: 0.2544 | RMSE: 0.3332 | RÂ²: 0.8811\n",
      "Epoch 6/20 | Train Loss: 0.3583 | Val Loss: 0.3030 | MAE: 0.2256 | RMSE: 0.3030 | RÂ²: 0.9017\n",
      "Epoch 7/20 | Train Loss: 0.3481 | Val Loss: 0.2768 | MAE: 0.2047 | RMSE: 0.2768 | RÂ²: 0.9180\n",
      "Epoch 8/20 | Train Loss: 0.3303 | Val Loss: 0.2753 | MAE: 0.2024 | RMSE: 0.2753 | RÂ²: 0.9189\n",
      "Epoch 9/20 | Train Loss: 0.3179 | Val Loss: 0.2568 | MAE: 0.1786 | RMSE: 0.2568 | RÂ²: 0.9294\n",
      "Epoch 10/20 | Train Loss: 0.3081 | Val Loss: 0.2615 | MAE: 0.1853 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 11/20 | Train Loss: 0.3014 | Val Loss: 0.2498 | MAE: 0.1701 | RMSE: 0.2498 | RÂ²: 0.9332\n",
      "Epoch 12/20 | Train Loss: 0.2931 | Val Loss: 0.2544 | MAE: 0.1740 | RMSE: 0.2544 | RÂ²: 0.9307\n",
      "Epoch 13/20 | Train Loss: 0.2860 | Val Loss: 0.2609 | MAE: 0.1766 | RMSE: 0.2609 | RÂ²: 0.9271\n",
      "Epoch 14/20 | Train Loss: 0.2828 | Val Loss: 0.2618 | MAE: 0.1803 | RMSE: 0.2618 | RÂ²: 0.9266\n",
      "Epoch 15/20 | Train Loss: 0.2790 | Val Loss: 0.2557 | MAE: 0.1738 | RMSE: 0.2557 | RÂ²: 0.9300\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2498 | MAE = 0.1701\n",
      "\n",
      "ðŸ”§ [25/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9196 | Val Loss: 0.8763 | MAE: 0.7397 | RMSE: 0.8763 | RÂ²: 0.1780\n",
      "Epoch 2/20 | Train Loss: 0.7344 | Val Loss: 0.7788 | MAE: 0.6575 | RMSE: 0.7788 | RÂ²: 0.3506\n",
      "Epoch 3/20 | Train Loss: 0.6196 | Val Loss: 0.6741 | MAE: 0.5644 | RMSE: 0.6741 | RÂ²: 0.5135\n",
      "Epoch 4/20 | Train Loss: 0.5454 | Val Loss: 0.5468 | MAE: 0.4501 | RMSE: 0.5468 | RÂ²: 0.6799\n",
      "Epoch 5/20 | Train Loss: 0.4967 | Val Loss: 0.4396 | MAE: 0.3516 | RMSE: 0.4396 | RÂ²: 0.7931\n",
      "Epoch 6/20 | Train Loss: 0.4524 | Val Loss: 0.3695 | MAE: 0.2869 | RMSE: 0.3695 | RÂ²: 0.8538\n",
      "Epoch 7/20 | Train Loss: 0.4242 | Val Loss: 0.3359 | MAE: 0.2548 | RMSE: 0.3359 | RÂ²: 0.8792\n",
      "Epoch 8/20 | Train Loss: 0.4099 | Val Loss: 0.3162 | MAE: 0.2352 | RMSE: 0.3162 | RÂ²: 0.8930\n",
      "Epoch 9/20 | Train Loss: 0.3963 | Val Loss: 0.3090 | MAE: 0.2284 | RMSE: 0.3090 | RÂ²: 0.8978\n",
      "Epoch 10/20 | Train Loss: 0.3851 | Val Loss: 0.3118 | MAE: 0.2266 | RMSE: 0.3118 | RÂ²: 0.8959\n",
      "Epoch 11/20 | Train Loss: 0.3701 | Val Loss: 0.2962 | MAE: 0.2145 | RMSE: 0.2962 | RÂ²: 0.9061\n",
      "Epoch 12/20 | Train Loss: 0.3637 | Val Loss: 0.2902 | MAE: 0.2084 | RMSE: 0.2902 | RÂ²: 0.9099\n",
      "Epoch 13/20 | Train Loss: 0.3550 | Val Loss: 0.2929 | MAE: 0.2099 | RMSE: 0.2929 | RÂ²: 0.9081\n",
      "Epoch 14/20 | Train Loss: 0.3526 | Val Loss: 0.2872 | MAE: 0.2062 | RMSE: 0.2872 | RÂ²: 0.9117\n",
      "Epoch 15/20 | Train Loss: 0.3435 | Val Loss: 0.2780 | MAE: 0.1990 | RMSE: 0.2780 | RÂ²: 0.9172\n",
      "Epoch 16/20 | Train Loss: 0.3398 | Val Loss: 0.2802 | MAE: 0.2007 | RMSE: 0.2802 | RÂ²: 0.9159\n",
      "Epoch 17/20 | Train Loss: 0.3371 | Val Loss: 0.2790 | MAE: 0.1995 | RMSE: 0.2790 | RÂ²: 0.9167\n",
      "Epoch 18/20 | Train Loss: 0.3331 | Val Loss: 0.2781 | MAE: 0.1981 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 19/20 | Train Loss: 0.3236 | Val Loss: 0.2793 | MAE: 0.1998 | RMSE: 0.2793 | RÂ²: 0.9165\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2780 | MAE = 0.1990\n",
      "\n",
      "ðŸ”§ [26/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0288 | Val Loss: 0.9471 | MAE: 0.8002 | RMSE: 0.9471 | RÂ²: 0.0397\n",
      "Epoch 2/20 | Train Loss: 0.9228 | Val Loss: 0.8735 | MAE: 0.7405 | RMSE: 0.8735 | RÂ²: 0.1832\n",
      "Epoch 3/20 | Train Loss: 0.8724 | Val Loss: 0.8004 | MAE: 0.6772 | RMSE: 0.8004 | RÂ²: 0.3142\n",
      "Epoch 4/20 | Train Loss: 0.8063 | Val Loss: 0.7337 | MAE: 0.6200 | RMSE: 0.7337 | RÂ²: 0.4237\n",
      "Epoch 5/20 | Train Loss: 0.7587 | Val Loss: 0.6694 | MAE: 0.5629 | RMSE: 0.6694 | RÂ²: 0.5204\n",
      "Epoch 6/20 | Train Loss: 0.7147 | Val Loss: 0.6040 | MAE: 0.5007 | RMSE: 0.6040 | RÂ²: 0.6095\n",
      "Epoch 7/20 | Train Loss: 0.6853 | Val Loss: 0.5385 | MAE: 0.4368 | RMSE: 0.5385 | RÂ²: 0.6895\n",
      "Epoch 8/20 | Train Loss: 0.6462 | Val Loss: 0.4809 | MAE: 0.3830 | RMSE: 0.4809 | RÂ²: 0.7524\n",
      "Epoch 9/20 | Train Loss: 0.6153 | Val Loss: 0.4369 | MAE: 0.3413 | RMSE: 0.4369 | RÂ²: 0.7957\n",
      "Epoch 10/20 | Train Loss: 0.5963 | Val Loss: 0.4121 | MAE: 0.3132 | RMSE: 0.4121 | RÂ²: 0.8182\n",
      "Epoch 11/20 | Train Loss: 0.5826 | Val Loss: 0.3924 | MAE: 0.2911 | RMSE: 0.3924 | RÂ²: 0.8352\n",
      "Epoch 12/20 | Train Loss: 0.5611 | Val Loss: 0.3785 | MAE: 0.2747 | RMSE: 0.3785 | RÂ²: 0.8466\n",
      "Epoch 13/20 | Train Loss: 0.5422 | Val Loss: 0.3687 | MAE: 0.2637 | RMSE: 0.3687 | RÂ²: 0.8545\n",
      "Epoch 14/20 | Train Loss: 0.5411 | Val Loss: 0.3616 | MAE: 0.2563 | RMSE: 0.3616 | RÂ²: 0.8600\n",
      "Epoch 15/20 | Train Loss: 0.5242 | Val Loss: 0.3544 | MAE: 0.2505 | RMSE: 0.3544 | RÂ²: 0.8656\n",
      "Epoch 16/20 | Train Loss: 0.5197 | Val Loss: 0.3533 | MAE: 0.2478 | RMSE: 0.3533 | RÂ²: 0.8664\n",
      "Epoch 17/20 | Train Loss: 0.5097 | Val Loss: 0.3486 | MAE: 0.2439 | RMSE: 0.3486 | RÂ²: 0.8699\n",
      "Epoch 18/20 | Train Loss: 0.5024 | Val Loss: 0.3487 | MAE: 0.2419 | RMSE: 0.3487 | RÂ²: 0.8698\n",
      "Epoch 19/20 | Train Loss: 0.4915 | Val Loss: 0.3499 | MAE: 0.2401 | RMSE: 0.3499 | RÂ²: 0.8689\n",
      "Epoch 20/20 | Train Loss: 0.4825 | Val Loss: 0.3453 | MAE: 0.2364 | RMSE: 0.3453 | RÂ²: 0.8724\n",
      "âœ… RMSE = 0.3453 | MAE = 0.2364\n",
      "\n",
      "ðŸ”§ [27/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[256, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9197 | Val Loss: 0.7567 | MAE: 0.6380 | RMSE: 0.7567 | RÂ²: 0.3870\n",
      "Epoch 2/20 | Train Loss: 0.6829 | Val Loss: 0.6495 | MAE: 0.5348 | RMSE: 0.6495 | RÂ²: 0.5483\n",
      "Epoch 3/20 | Train Loss: 0.5894 | Val Loss: 0.4820 | MAE: 0.3865 | RMSE: 0.4820 | RÂ²: 0.7513\n",
      "Epoch 4/20 | Train Loss: 0.5283 | Val Loss: 0.3471 | MAE: 0.2717 | RMSE: 0.3471 | RÂ²: 0.8710\n",
      "Epoch 5/20 | Train Loss: 0.4994 | Val Loss: 0.3470 | MAE: 0.2581 | RMSE: 0.3470 | RÂ²: 0.8711\n",
      "Epoch 6/20 | Train Loss: 0.4631 | Val Loss: 0.3130 | MAE: 0.2315 | RMSE: 0.3130 | RÂ²: 0.8951\n",
      "Epoch 7/20 | Train Loss: 0.4463 | Val Loss: 0.3000 | MAE: 0.2212 | RMSE: 0.3000 | RÂ²: 0.9037\n",
      "Epoch 8/20 | Train Loss: 0.4283 | Val Loss: 0.3078 | MAE: 0.2206 | RMSE: 0.3078 | RÂ²: 0.8986\n",
      "Epoch 9/20 | Train Loss: 0.4097 | Val Loss: 0.2929 | MAE: 0.2092 | RMSE: 0.2929 | RÂ²: 0.9081\n",
      "Epoch 10/20 | Train Loss: 0.3992 | Val Loss: 0.2788 | MAE: 0.2001 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "Epoch 11/20 | Train Loss: 0.3899 | Val Loss: 0.2828 | MAE: 0.2013 | RMSE: 0.2828 | RÂ²: 0.9144\n",
      "Epoch 12/20 | Train Loss: 0.3816 | Val Loss: 0.2821 | MAE: 0.1984 | RMSE: 0.2821 | RÂ²: 0.9148\n",
      "Epoch 13/20 | Train Loss: 0.3715 | Val Loss: 0.2785 | MAE: 0.1949 | RMSE: 0.2785 | RÂ²: 0.9170\n",
      "Epoch 14/20 | Train Loss: 0.3633 | Val Loss: 0.2785 | MAE: 0.1944 | RMSE: 0.2785 | RÂ²: 0.9170\n",
      "Epoch 15/20 | Train Loss: 0.3571 | Val Loss: 0.2761 | MAE: 0.1931 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 16/20 | Train Loss: 0.3470 | Val Loss: 0.2758 | MAE: 0.1940 | RMSE: 0.2758 | RÂ²: 0.9186\n",
      "Epoch 17/20 | Train Loss: 0.3488 | Val Loss: 0.2750 | MAE: 0.1911 | RMSE: 0.2750 | RÂ²: 0.9191\n",
      "Epoch 18/20 | Train Loss: 0.3393 | Val Loss: 0.2764 | MAE: 0.1918 | RMSE: 0.2764 | RÂ²: 0.9182\n",
      "Epoch 19/20 | Train Loss: 0.3323 | Val Loss: 0.2761 | MAE: 0.1909 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 20/20 | Train Loss: 0.3297 | Val Loss: 0.2832 | MAE: 0.1949 | RMSE: 0.2832 | RÂ²: 0.9141\n",
      "âœ… RMSE = 0.2750 | MAE = 0.1911\n",
      "\n",
      "ðŸ”§ [28/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8814 | Val Loss: 0.5806 | MAE: 0.4829 | RMSE: 0.5806 | RÂ²: 0.6391\n",
      "Epoch 2/20 | Train Loss: 0.4032 | Val Loss: 0.4642 | MAE: 0.3677 | RMSE: 0.4642 | RÂ²: 0.7693\n",
      "Epoch 3/20 | Train Loss: 0.3535 | Val Loss: 0.6171 | MAE: 0.4625 | RMSE: 0.6171 | RÂ²: 0.5923\n",
      "Epoch 4/20 | Train Loss: 0.3207 | Val Loss: 0.5765 | MAE: 0.4972 | RMSE: 0.5765 | RÂ²: 0.6442\n",
      "Epoch 5/20 | Train Loss: 0.3050 | Val Loss: 0.5744 | MAE: 0.3732 | RMSE: 0.5744 | RÂ²: 0.6468\n",
      "Epoch 6/20 | Train Loss: 0.2971 | Val Loss: 0.5028 | MAE: 0.3769 | RMSE: 0.5028 | RÂ²: 0.7293\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.4642 | MAE = 0.3677\n",
      "\n",
      "ðŸ”§ [29/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9216 | Val Loss: 0.7428 | MAE: 0.6226 | RMSE: 0.7428 | RÂ²: 0.4094\n",
      "Epoch 2/20 | Train Loss: 0.6165 | Val Loss: 0.4535 | MAE: 0.3491 | RMSE: 0.4535 | RÂ²: 0.7798\n",
      "Epoch 3/20 | Train Loss: 0.5160 | Val Loss: 0.4251 | MAE: 0.3041 | RMSE: 0.4251 | RÂ²: 0.8065\n",
      "Epoch 4/20 | Train Loss: 0.4672 | Val Loss: 0.4126 | MAE: 0.3282 | RMSE: 0.4126 | RÂ²: 0.8177\n",
      "Epoch 5/20 | Train Loss: 0.4288 | Val Loss: 0.5013 | MAE: 0.4212 | RMSE: 0.5013 | RÂ²: 0.7310\n",
      "Epoch 6/20 | Train Loss: 0.4055 | Val Loss: 0.3952 | MAE: 0.3024 | RMSE: 0.3952 | RÂ²: 0.8328\n",
      "Epoch 7/20 | Train Loss: 0.3773 | Val Loss: 0.3669 | MAE: 0.2894 | RMSE: 0.3669 | RÂ²: 0.8559\n",
      "Epoch 8/20 | Train Loss: 0.3614 | Val Loss: 0.3336 | MAE: 0.2609 | RMSE: 0.3336 | RÂ²: 0.8809\n",
      "Epoch 9/20 | Train Loss: 0.3462 | Val Loss: 0.2929 | MAE: 0.2167 | RMSE: 0.2929 | RÂ²: 0.9081\n",
      "Epoch 10/20 | Train Loss: 0.3372 | Val Loss: 0.2966 | MAE: 0.2223 | RMSE: 0.2966 | RÂ²: 0.9058\n",
      "Epoch 11/20 | Train Loss: 0.3275 | Val Loss: 0.2807 | MAE: 0.2045 | RMSE: 0.2807 | RÂ²: 0.9156\n",
      "Epoch 12/20 | Train Loss: 0.3169 | Val Loss: 0.2682 | MAE: 0.1859 | RMSE: 0.2682 | RÂ²: 0.9230\n",
      "Epoch 13/20 | Train Loss: 0.3077 | Val Loss: 0.2696 | MAE: 0.1932 | RMSE: 0.2696 | RÂ²: 0.9222\n",
      "Epoch 14/20 | Train Loss: 0.3030 | Val Loss: 0.2607 | MAE: 0.1789 | RMSE: 0.2607 | RÂ²: 0.9272\n",
      "Epoch 15/20 | Train Loss: 0.2989 | Val Loss: 0.2749 | MAE: 0.1906 | RMSE: 0.2749 | RÂ²: 0.9191\n",
      "Epoch 16/20 | Train Loss: 0.2982 | Val Loss: 0.2547 | MAE: 0.1745 | RMSE: 0.2547 | RÂ²: 0.9305\n",
      "Epoch 17/20 | Train Loss: 0.2941 | Val Loss: 0.2788 | MAE: 0.1951 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "Epoch 18/20 | Train Loss: 0.2923 | Val Loss: 0.2564 | MAE: 0.1741 | RMSE: 0.2564 | RÂ²: 0.9296\n",
      "Epoch 19/20 | Train Loss: 0.2844 | Val Loss: 0.2672 | MAE: 0.1847 | RMSE: 0.2672 | RÂ²: 0.9236\n",
      "Epoch 20/20 | Train Loss: 0.2809 | Val Loss: 0.2609 | MAE: 0.1790 | RMSE: 0.2609 | RÂ²: 0.9271\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2547 | MAE = 0.1745\n",
      "\n",
      "ðŸ”§ [30/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9966 | Val Loss: 0.8207 | MAE: 0.6943 | RMSE: 0.8207 | RÂ²: 0.2790\n",
      "Epoch 2/20 | Train Loss: 0.8250 | Val Loss: 0.7104 | MAE: 0.5984 | RMSE: 0.7104 | RÂ²: 0.4598\n",
      "Epoch 3/20 | Train Loss: 0.7538 | Val Loss: 0.5972 | MAE: 0.4964 | RMSE: 0.5972 | RÂ²: 0.6182\n",
      "Epoch 4/20 | Train Loss: 0.6821 | Val Loss: 0.4923 | MAE: 0.4026 | RMSE: 0.4923 | RÂ²: 0.7406\n",
      "Epoch 5/20 | Train Loss: 0.6354 | Val Loss: 0.4168 | MAE: 0.3339 | RMSE: 0.4168 | RÂ²: 0.8140\n",
      "Epoch 6/20 | Train Loss: 0.6097 | Val Loss: 0.3506 | MAE: 0.2729 | RMSE: 0.3506 | RÂ²: 0.8684\n",
      "Epoch 7/20 | Train Loss: 0.5775 | Val Loss: 0.3182 | MAE: 0.2363 | RMSE: 0.3182 | RÂ²: 0.8916\n",
      "Epoch 8/20 | Train Loss: 0.5579 | Val Loss: 0.3073 | MAE: 0.2259 | RMSE: 0.3073 | RÂ²: 0.8989\n",
      "Epoch 9/20 | Train Loss: 0.5325 | Val Loss: 0.3114 | MAE: 0.2259 | RMSE: 0.3114 | RÂ²: 0.8962\n",
      "Epoch 10/20 | Train Loss: 0.5089 | Val Loss: 0.3173 | MAE: 0.2264 | RMSE: 0.3173 | RÂ²: 0.8922\n",
      "Epoch 11/20 | Train Loss: 0.4980 | Val Loss: 0.3223 | MAE: 0.2259 | RMSE: 0.3223 | RÂ²: 0.8888\n",
      "Epoch 12/20 | Train Loss: 0.4857 | Val Loss: 0.3193 | MAE: 0.2227 | RMSE: 0.3193 | RÂ²: 0.8908\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3073 | MAE = 0.2259\n",
      "\n",
      "ðŸ”§ [31/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7706 | Val Loss: 0.9173 | MAE: 0.7706 | RMSE: 0.9173 | RÂ²: 0.0992\n",
      "Epoch 2/20 | Train Loss: 0.4756 | Val Loss: 0.7525 | MAE: 0.6342 | RMSE: 0.7525 | RÂ²: 0.3937\n",
      "Epoch 3/20 | Train Loss: 0.3830 | Val Loss: 0.6046 | MAE: 0.5083 | RMSE: 0.6046 | RÂ²: 0.6087\n",
      "Epoch 4/20 | Train Loss: 0.3379 | Val Loss: 0.4484 | MAE: 0.3712 | RMSE: 0.4484 | RÂ²: 0.7848\n",
      "Epoch 5/20 | Train Loss: 0.3156 | Val Loss: 0.3332 | MAE: 0.2622 | RMSE: 0.3332 | RÂ²: 0.8811\n",
      "Epoch 6/20 | Train Loss: 0.3012 | Val Loss: 0.2825 | MAE: 0.2070 | RMSE: 0.2825 | RÂ²: 0.9146\n",
      "Epoch 7/20 | Train Loss: 0.2842 | Val Loss: 0.2576 | MAE: 0.1791 | RMSE: 0.2576 | RÂ²: 0.9290\n",
      "Epoch 8/20 | Train Loss: 0.2823 | Val Loss: 0.2538 | MAE: 0.1772 | RMSE: 0.2538 | RÂ²: 0.9310\n",
      "Epoch 9/20 | Train Loss: 0.2735 | Val Loss: 0.2627 | MAE: 0.1820 | RMSE: 0.2627 | RÂ²: 0.9261\n",
      "Epoch 10/20 | Train Loss: 0.2678 | Val Loss: 0.2531 | MAE: 0.1755 | RMSE: 0.2531 | RÂ²: 0.9314\n",
      "Epoch 11/20 | Train Loss: 0.2643 | Val Loss: 0.2607 | MAE: 0.1793 | RMSE: 0.2607 | RÂ²: 0.9272\n",
      "Epoch 12/20 | Train Loss: 0.2599 | Val Loss: 0.2661 | MAE: 0.1842 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 13/20 | Train Loss: 0.2567 | Val Loss: 0.2759 | MAE: 0.1900 | RMSE: 0.2759 | RÂ²: 0.9185\n",
      "Epoch 14/20 | Train Loss: 0.2565 | Val Loss: 0.2766 | MAE: 0.1891 | RMSE: 0.2766 | RÂ²: 0.9181\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2531 | MAE = 0.1755\n",
      "\n",
      "ðŸ”§ [32/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7795 | Val Loss: 0.7615 | MAE: 0.6363 | RMSE: 0.7615 | RÂ²: 0.3792\n",
      "Epoch 2/20 | Train Loss: 0.4665 | Val Loss: 0.5004 | MAE: 0.4210 | RMSE: 0.5004 | RÂ²: 0.7319\n",
      "Epoch 3/20 | Train Loss: 0.3883 | Val Loss: 0.4226 | MAE: 0.3408 | RMSE: 0.4226 | RÂ²: 0.8088\n",
      "Epoch 4/20 | Train Loss: 0.3421 | Val Loss: 0.3283 | MAE: 0.2537 | RMSE: 0.3283 | RÂ²: 0.8846\n",
      "Epoch 5/20 | Train Loss: 0.3177 | Val Loss: 0.2768 | MAE: 0.2045 | RMSE: 0.2768 | RÂ²: 0.9180\n",
      "Epoch 6/20 | Train Loss: 0.2927 | Val Loss: 0.2657 | MAE: 0.1881 | RMSE: 0.2657 | RÂ²: 0.9244\n",
      "Epoch 7/20 | Train Loss: 0.2837 | Val Loss: 0.2702 | MAE: 0.1906 | RMSE: 0.2702 | RÂ²: 0.9218\n",
      "Epoch 8/20 | Train Loss: 0.2761 | Val Loss: 0.2645 | MAE: 0.1780 | RMSE: 0.2645 | RÂ²: 0.9251\n",
      "Epoch 9/20 | Train Loss: 0.2641 | Val Loss: 0.2692 | MAE: 0.1862 | RMSE: 0.2692 | RÂ²: 0.9224\n",
      "Epoch 10/20 | Train Loss: 0.2595 | Val Loss: 0.2616 | MAE: 0.1797 | RMSE: 0.2616 | RÂ²: 0.9268\n",
      "Epoch 11/20 | Train Loss: 0.2539 | Val Loss: 0.2785 | MAE: 0.1917 | RMSE: 0.2785 | RÂ²: 0.9170\n",
      "Epoch 12/20 | Train Loss: 0.2553 | Val Loss: 0.2687 | MAE: 0.1848 | RMSE: 0.2687 | RÂ²: 0.9227\n",
      "Epoch 13/20 | Train Loss: 0.2519 | Val Loss: 0.2690 | MAE: 0.1851 | RMSE: 0.2690 | RÂ²: 0.9226\n",
      "Epoch 14/20 | Train Loss: 0.2493 | Val Loss: 0.2718 | MAE: 0.1886 | RMSE: 0.2718 | RÂ²: 0.9209\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2616 | MAE = 0.1797\n",
      "\n",
      "ðŸ”§ [33/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[256, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9532 | Val Loss: 0.7876 | MAE: 0.6660 | RMSE: 0.7876 | RÂ²: 0.3360\n",
      "Epoch 2/20 | Train Loss: 0.7600 | Val Loss: 0.6750 | MAE: 0.5609 | RMSE: 0.6750 | RÂ²: 0.5122\n",
      "Epoch 3/20 | Train Loss: 0.6602 | Val Loss: 0.4919 | MAE: 0.4013 | RMSE: 0.4919 | RÂ²: 0.7410\n",
      "Epoch 4/20 | Train Loss: 0.6112 | Val Loss: 0.3969 | MAE: 0.3107 | RMSE: 0.3969 | RÂ²: 0.8314\n",
      "Epoch 5/20 | Train Loss: 0.5608 | Val Loss: 0.4165 | MAE: 0.2989 | RMSE: 0.4165 | RÂ²: 0.8143\n",
      "Epoch 6/20 | Train Loss: 0.5258 | Val Loss: 0.3671 | MAE: 0.2651 | RMSE: 0.3671 | RÂ²: 0.8557\n",
      "Epoch 7/20 | Train Loss: 0.5010 | Val Loss: 0.3733 | MAE: 0.2652 | RMSE: 0.3733 | RÂ²: 0.8508\n",
      "Epoch 8/20 | Train Loss: 0.4802 | Val Loss: 0.3773 | MAE: 0.2641 | RMSE: 0.3773 | RÂ²: 0.8476\n",
      "Epoch 9/20 | Train Loss: 0.4538 | Val Loss: 0.3472 | MAE: 0.2495 | RMSE: 0.3472 | RÂ²: 0.8709\n",
      "Epoch 10/20 | Train Loss: 0.4501 | Val Loss: 0.3433 | MAE: 0.2468 | RMSE: 0.3433 | RÂ²: 0.8738\n",
      "Epoch 11/20 | Train Loss: 0.4341 | Val Loss: 0.3463 | MAE: 0.2419 | RMSE: 0.3463 | RÂ²: 0.8716\n",
      "Epoch 12/20 | Train Loss: 0.4188 | Val Loss: 0.3350 | MAE: 0.2325 | RMSE: 0.3350 | RÂ²: 0.8798\n",
      "Epoch 13/20 | Train Loss: 0.4107 | Val Loss: 0.3358 | MAE: 0.2294 | RMSE: 0.3358 | RÂ²: 0.8793\n",
      "Epoch 14/20 | Train Loss: 0.4014 | Val Loss: 0.3339 | MAE: 0.2262 | RMSE: 0.3339 | RÂ²: 0.8807\n",
      "Epoch 15/20 | Train Loss: 0.3878 | Val Loss: 0.3153 | MAE: 0.2173 | RMSE: 0.3153 | RÂ²: 0.8936\n",
      "Epoch 16/20 | Train Loss: 0.3837 | Val Loss: 0.3162 | MAE: 0.2162 | RMSE: 0.3162 | RÂ²: 0.8929\n",
      "Epoch 17/20 | Train Loss: 0.3777 | Val Loss: 0.3036 | MAE: 0.2110 | RMSE: 0.3036 | RÂ²: 0.9013\n",
      "Epoch 18/20 | Train Loss: 0.3710 | Val Loss: 0.2983 | MAE: 0.2071 | RMSE: 0.2983 | RÂ²: 0.9047\n",
      "Epoch 19/20 | Train Loss: 0.3617 | Val Loss: 0.2943 | MAE: 0.2045 | RMSE: 0.2943 | RÂ²: 0.9072\n",
      "Epoch 20/20 | Train Loss: 0.3591 | Val Loss: 0.2971 | MAE: 0.2064 | RMSE: 0.2971 | RÂ²: 0.9055\n",
      "âœ… RMSE = 0.2943 | MAE = 0.2045\n",
      "\n",
      "ðŸ”§ [34/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[256, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9924 | Val Loss: 0.9175 | MAE: 0.7778 | RMSE: 0.9175 | RÂ²: 0.0988\n",
      "Epoch 2/20 | Train Loss: 0.8654 | Val Loss: 0.8281 | MAE: 0.6993 | RMSE: 0.8281 | RÂ²: 0.2658\n",
      "Epoch 3/20 | Train Loss: 0.7798 | Val Loss: 0.7399 | MAE: 0.6220 | RMSE: 0.7399 | RÂ²: 0.4139\n",
      "Epoch 4/20 | Train Loss: 0.7167 | Val Loss: 0.6520 | MAE: 0.5464 | RMSE: 0.6520 | RÂ²: 0.5450\n",
      "Epoch 5/20 | Train Loss: 0.6643 | Val Loss: 0.5705 | MAE: 0.4751 | RMSE: 0.5705 | RÂ²: 0.6515\n",
      "Epoch 6/20 | Train Loss: 0.6167 | Val Loss: 0.5007 | MAE: 0.4133 | RMSE: 0.5007 | RÂ²: 0.7317\n",
      "Epoch 7/20 | Train Loss: 0.5868 | Val Loss: 0.4457 | MAE: 0.3638 | RMSE: 0.4457 | RÂ²: 0.7873\n",
      "Epoch 8/20 | Train Loss: 0.5588 | Val Loss: 0.4021 | MAE: 0.3226 | RMSE: 0.4021 | RÂ²: 0.8269\n",
      "Epoch 9/20 | Train Loss: 0.5307 | Val Loss: 0.3696 | MAE: 0.2915 | RMSE: 0.3696 | RÂ²: 0.8538\n",
      "Epoch 10/20 | Train Loss: 0.5139 | Val Loss: 0.3463 | MAE: 0.2704 | RMSE: 0.3463 | RÂ²: 0.8716\n",
      "Epoch 11/20 | Train Loss: 0.5086 | Val Loss: 0.3344 | MAE: 0.2586 | RMSE: 0.3344 | RÂ²: 0.8803\n",
      "Epoch 12/20 | Train Loss: 0.4858 | Val Loss: 0.3267 | MAE: 0.2505 | RMSE: 0.3267 | RÂ²: 0.8857\n",
      "Epoch 13/20 | Train Loss: 0.4788 | Val Loss: 0.3231 | MAE: 0.2451 | RMSE: 0.3231 | RÂ²: 0.8882\n",
      "Epoch 14/20 | Train Loss: 0.4656 | Val Loss: 0.3197 | MAE: 0.2404 | RMSE: 0.3197 | RÂ²: 0.8905\n",
      "Epoch 15/20 | Train Loss: 0.4520 | Val Loss: 0.3138 | MAE: 0.2342 | RMSE: 0.3138 | RÂ²: 0.8946\n",
      "Epoch 16/20 | Train Loss: 0.4497 | Val Loss: 0.3101 | MAE: 0.2303 | RMSE: 0.3101 | RÂ²: 0.8970\n",
      "Epoch 17/20 | Train Loss: 0.4387 | Val Loss: 0.3104 | MAE: 0.2296 | RMSE: 0.3104 | RÂ²: 0.8969\n",
      "Epoch 18/20 | Train Loss: 0.4346 | Val Loss: 0.3090 | MAE: 0.2275 | RMSE: 0.3090 | RÂ²: 0.8978\n",
      "Epoch 19/20 | Train Loss: 0.4320 | Val Loss: 0.3050 | MAE: 0.2238 | RMSE: 0.3050 | RÂ²: 0.9004\n",
      "Epoch 20/20 | Train Loss: 0.4232 | Val Loss: 0.3009 | MAE: 0.2198 | RMSE: 0.3009 | RÂ²: 0.9031\n",
      "âœ… RMSE = 0.3009 | MAE = 0.2198\n",
      "\n",
      "ðŸ”§ [35/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[512, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7667 | Val Loss: 0.8592 | MAE: 0.7171 | RMSE: 0.8592 | RÂ²: 0.2097\n",
      "Epoch 2/20 | Train Loss: 0.4864 | Val Loss: 0.6663 | MAE: 0.5570 | RMSE: 0.6663 | RÂ²: 0.5247\n",
      "Epoch 3/20 | Train Loss: 0.4106 | Val Loss: 0.4848 | MAE: 0.4031 | RMSE: 0.4848 | RÂ²: 0.7484\n",
      "Epoch 4/20 | Train Loss: 0.3736 | Val Loss: 0.3821 | MAE: 0.2899 | RMSE: 0.3821 | RÂ²: 0.8437\n",
      "Epoch 5/20 | Train Loss: 0.3507 | Val Loss: 0.2946 | MAE: 0.2239 | RMSE: 0.2946 | RÂ²: 0.9071\n",
      "Epoch 6/20 | Train Loss: 0.3368 | Val Loss: 0.2654 | MAE: 0.1921 | RMSE: 0.2654 | RÂ²: 0.9246\n",
      "Epoch 7/20 | Train Loss: 0.3253 | Val Loss: 0.3013 | MAE: 0.2146 | RMSE: 0.3013 | RÂ²: 0.9028\n",
      "Epoch 8/20 | Train Loss: 0.3149 | Val Loss: 0.2703 | MAE: 0.1941 | RMSE: 0.2703 | RÂ²: 0.9218\n",
      "Epoch 9/20 | Train Loss: 0.2998 | Val Loss: 0.2724 | MAE: 0.1961 | RMSE: 0.2724 | RÂ²: 0.9206\n",
      "Epoch 10/20 | Train Loss: 0.2949 | Val Loss: 0.2712 | MAE: 0.1880 | RMSE: 0.2712 | RÂ²: 0.9213\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2654 | MAE = 0.1921\n",
      "\n",
      "ðŸ”§ [36/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[1024, 256, 1024], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8997 | Val Loss: 0.7944 | MAE: 0.6698 | RMSE: 0.7944 | RÂ²: 0.3243\n",
      "Epoch 2/20 | Train Loss: 0.7052 | Val Loss: 0.6017 | MAE: 0.4979 | RMSE: 0.6017 | RÂ²: 0.6124\n",
      "Epoch 3/20 | Train Loss: 0.6076 | Val Loss: 0.4167 | MAE: 0.3264 | RMSE: 0.4167 | RÂ²: 0.8141\n",
      "Epoch 4/20 | Train Loss: 0.5582 | Val Loss: 0.3494 | MAE: 0.2569 | RMSE: 0.3494 | RÂ²: 0.8693\n",
      "Epoch 5/20 | Train Loss: 0.5214 | Val Loss: 0.3526 | MAE: 0.2519 | RMSE: 0.3526 | RÂ²: 0.8669\n",
      "Epoch 6/20 | Train Loss: 0.4993 | Val Loss: 0.4000 | MAE: 0.2940 | RMSE: 0.4000 | RÂ²: 0.8287\n",
      "Epoch 7/20 | Train Loss: 0.4744 | Val Loss: 0.3937 | MAE: 0.2839 | RMSE: 0.3937 | RÂ²: 0.8340\n",
      "Epoch 8/20 | Train Loss: 0.4528 | Val Loss: 0.3600 | MAE: 0.2761 | RMSE: 0.3600 | RÂ²: 0.8613\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3494 | MAE = 0.2569\n",
      "\n",
      "ðŸ”§ [37/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9309 | Val Loss: 0.8226 | MAE: 0.6738 | RMSE: 0.8226 | RÂ²: 0.2757\n",
      "Epoch 2/20 | Train Loss: 0.4954 | Val Loss: 0.5680 | MAE: 0.4027 | RMSE: 0.5680 | RÂ²: 0.6547\n",
      "Epoch 3/20 | Train Loss: 0.4272 | Val Loss: 0.4020 | MAE: 0.3197 | RMSE: 0.4020 | RÂ²: 0.8270\n",
      "Epoch 4/20 | Train Loss: 0.3881 | Val Loss: 0.3559 | MAE: 0.2752 | RMSE: 0.3559 | RÂ²: 0.8644\n",
      "Epoch 5/20 | Train Loss: 0.3665 | Val Loss: 0.3777 | MAE: 0.3015 | RMSE: 0.3777 | RÂ²: 0.8473\n",
      "Epoch 6/20 | Train Loss: 0.3537 | Val Loss: 0.3709 | MAE: 0.2571 | RMSE: 0.3709 | RÂ²: 0.8528\n",
      "Epoch 7/20 | Train Loss: 0.3329 | Val Loss: 0.3411 | MAE: 0.2284 | RMSE: 0.3411 | RÂ²: 0.8755\n",
      "Epoch 8/20 | Train Loss: 0.3261 | Val Loss: 0.2841 | MAE: 0.1982 | RMSE: 0.2841 | RÂ²: 0.9136\n",
      "Epoch 9/20 | Train Loss: 0.3230 | Val Loss: 0.2605 | MAE: 0.1792 | RMSE: 0.2605 | RÂ²: 0.9274\n",
      "Epoch 10/20 | Train Loss: 0.3110 | Val Loss: 0.2724 | MAE: 0.1899 | RMSE: 0.2724 | RÂ²: 0.9206\n",
      "Epoch 11/20 | Train Loss: 0.3089 | Val Loss: 0.2638 | MAE: 0.1728 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 12/20 | Train Loss: 0.2995 | Val Loss: 0.2614 | MAE: 0.1804 | RMSE: 0.2614 | RÂ²: 0.9269\n",
      "Epoch 13/20 | Train Loss: 0.2929 | Val Loss: 0.2650 | MAE: 0.1821 | RMSE: 0.2650 | RÂ²: 0.9248\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2605 | MAE = 0.1792\n",
      "\n",
      "ðŸ”§ [38/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9140 | Val Loss: 0.8639 | MAE: 0.7307 | RMSE: 0.8639 | RÂ²: 0.2010\n",
      "Epoch 2/20 | Train Loss: 0.7593 | Val Loss: 0.7462 | MAE: 0.6257 | RMSE: 0.7462 | RÂ²: 0.4039\n",
      "Epoch 3/20 | Train Loss: 0.6632 | Val Loss: 0.6310 | MAE: 0.5301 | RMSE: 0.6310 | RÂ²: 0.5737\n",
      "Epoch 4/20 | Train Loss: 0.6016 | Val Loss: 0.5266 | MAE: 0.4353 | RMSE: 0.5266 | RÂ²: 0.7032\n",
      "Epoch 5/20 | Train Loss: 0.5511 | Val Loss: 0.4217 | MAE: 0.3394 | RMSE: 0.4217 | RÂ²: 0.8096\n",
      "Epoch 6/20 | Train Loss: 0.5223 | Val Loss: 0.3606 | MAE: 0.2823 | RMSE: 0.3606 | RÂ²: 0.8608\n",
      "Epoch 7/20 | Train Loss: 0.5008 | Val Loss: 0.3368 | MAE: 0.2620 | RMSE: 0.3368 | RÂ²: 0.8785\n",
      "Epoch 8/20 | Train Loss: 0.4744 | Val Loss: 0.3260 | MAE: 0.2513 | RMSE: 0.3260 | RÂ²: 0.8862\n",
      "Epoch 9/20 | Train Loss: 0.4584 | Val Loss: 0.3180 | MAE: 0.2430 | RMSE: 0.3180 | RÂ²: 0.8917\n",
      "Epoch 10/20 | Train Loss: 0.4449 | Val Loss: 0.3102 | MAE: 0.2338 | RMSE: 0.3102 | RÂ²: 0.8970\n",
      "Epoch 11/20 | Train Loss: 0.4370 | Val Loss: 0.3016 | MAE: 0.2269 | RMSE: 0.3016 | RÂ²: 0.9026\n",
      "Epoch 12/20 | Train Loss: 0.4284 | Val Loss: 0.3030 | MAE: 0.2278 | RMSE: 0.3030 | RÂ²: 0.9017\n",
      "Epoch 13/20 | Train Loss: 0.4132 | Val Loss: 0.3037 | MAE: 0.2268 | RMSE: 0.3037 | RÂ²: 0.9012\n",
      "Epoch 14/20 | Train Loss: 0.4067 | Val Loss: 0.2994 | MAE: 0.2220 | RMSE: 0.2994 | RÂ²: 0.9041\n",
      "Epoch 15/20 | Train Loss: 0.4014 | Val Loss: 0.2974 | MAE: 0.2188 | RMSE: 0.2974 | RÂ²: 0.9053\n",
      "Epoch 16/20 | Train Loss: 0.3977 | Val Loss: 0.2963 | MAE: 0.2183 | RMSE: 0.2963 | RÂ²: 0.9060\n",
      "Epoch 17/20 | Train Loss: 0.3894 | Val Loss: 0.2968 | MAE: 0.2173 | RMSE: 0.2968 | RÂ²: 0.9057\n",
      "Epoch 18/20 | Train Loss: 0.3826 | Val Loss: 0.2964 | MAE: 0.2160 | RMSE: 0.2964 | RÂ²: 0.9060\n",
      "Epoch 19/20 | Train Loss: 0.3804 | Val Loss: 0.2941 | MAE: 0.2140 | RMSE: 0.2941 | RÂ²: 0.9074\n",
      "Epoch 20/20 | Train Loss: 0.3699 | Val Loss: 0.2944 | MAE: 0.2144 | RMSE: 0.2944 | RÂ²: 0.9072\n",
      "âœ… RMSE = 0.2941 | MAE = 0.2140\n",
      "\n",
      "ðŸ”§ [39/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[256, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9842 | Val Loss: 0.8900 | MAE: 0.7551 | RMSE: 0.8900 | RÂ²: 0.1520\n",
      "Epoch 2/20 | Train Loss: 0.7803 | Val Loss: 0.7458 | MAE: 0.6278 | RMSE: 0.7458 | RÂ²: 0.4045\n",
      "Epoch 3/20 | Train Loss: 0.6793 | Val Loss: 0.6230 | MAE: 0.5189 | RMSE: 0.6229 | RÂ²: 0.5845\n",
      "Epoch 4/20 | Train Loss: 0.6015 | Val Loss: 0.5159 | MAE: 0.4191 | RMSE: 0.5159 | RÂ²: 0.7151\n",
      "Epoch 5/20 | Train Loss: 0.5564 | Val Loss: 0.4202 | MAE: 0.3317 | RMSE: 0.4202 | RÂ²: 0.8110\n",
      "Epoch 6/20 | Train Loss: 0.5258 | Val Loss: 0.3686 | MAE: 0.2833 | RMSE: 0.3686 | RÂ²: 0.8545\n",
      "Epoch 7/20 | Train Loss: 0.4895 | Val Loss: 0.3408 | MAE: 0.2582 | RMSE: 0.3408 | RÂ²: 0.8757\n",
      "Epoch 8/20 | Train Loss: 0.4824 | Val Loss: 0.3282 | MAE: 0.2473 | RMSE: 0.3282 | RÂ²: 0.8847\n",
      "Epoch 9/20 | Train Loss: 0.4612 | Val Loss: 0.3208 | MAE: 0.2397 | RMSE: 0.3208 | RÂ²: 0.8899\n",
      "Epoch 10/20 | Train Loss: 0.4582 | Val Loss: 0.3154 | MAE: 0.2348 | RMSE: 0.3154 | RÂ²: 0.8935\n",
      "Epoch 11/20 | Train Loss: 0.4350 | Val Loss: 0.3064 | MAE: 0.2266 | RMSE: 0.3064 | RÂ²: 0.8995\n",
      "Epoch 12/20 | Train Loss: 0.4265 | Val Loss: 0.3082 | MAE: 0.2253 | RMSE: 0.3082 | RÂ²: 0.8983\n",
      "Epoch 13/20 | Train Loss: 0.4153 | Val Loss: 0.3068 | MAE: 0.2222 | RMSE: 0.3068 | RÂ²: 0.8992\n",
      "Epoch 14/20 | Train Loss: 0.4079 | Val Loss: 0.2968 | MAE: 0.2158 | RMSE: 0.2968 | RÂ²: 0.9057\n",
      "Epoch 15/20 | Train Loss: 0.3985 | Val Loss: 0.2956 | MAE: 0.2126 | RMSE: 0.2956 | RÂ²: 0.9064\n",
      "Epoch 16/20 | Train Loss: 0.4001 | Val Loss: 0.2915 | MAE: 0.2109 | RMSE: 0.2915 | RÂ²: 0.9091\n",
      "Epoch 17/20 | Train Loss: 0.3947 | Val Loss: 0.2956 | MAE: 0.2108 | RMSE: 0.2956 | RÂ²: 0.9065\n",
      "Epoch 18/20 | Train Loss: 0.3855 | Val Loss: 0.2880 | MAE: 0.2064 | RMSE: 0.2880 | RÂ²: 0.9112\n",
      "Epoch 19/20 | Train Loss: 0.3791 | Val Loss: 0.2850 | MAE: 0.2037 | RMSE: 0.2850 | RÂ²: 0.9130\n",
      "Epoch 20/20 | Train Loss: 0.3730 | Val Loss: 0.2869 | MAE: 0.2036 | RMSE: 0.2869 | RÂ²: 0.9119\n",
      "âœ… RMSE = 0.2850 | MAE = 0.2037\n",
      "\n",
      "ðŸ”§ [40/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[2048, 1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.9183 | Val Loss: 0.8167 | MAE: 0.6840 | RMSE: 0.8167 | RÂ²: 0.2860\n",
      "Epoch 2/20 | Train Loss: 0.6355 | Val Loss: 0.5310 | MAE: 0.4273 | RMSE: 0.5310 | RÂ²: 0.6982\n",
      "Epoch 3/20 | Train Loss: 0.5500 | Val Loss: 0.3845 | MAE: 0.3011 | RMSE: 0.3845 | RÂ²: 0.8417\n",
      "Epoch 4/20 | Train Loss: 0.4937 | Val Loss: 0.3391 | MAE: 0.2557 | RMSE: 0.3391 | RÂ²: 0.8769\n",
      "Epoch 5/20 | Train Loss: 0.4585 | Val Loss: 0.3963 | MAE: 0.3087 | RMSE: 0.3963 | RÂ²: 0.8319\n",
      "Epoch 6/20 | Train Loss: 0.4277 | Val Loss: 0.4279 | MAE: 0.3510 | RMSE: 0.4279 | RÂ²: 0.8040\n",
      "Epoch 7/20 | Train Loss: 0.4155 | Val Loss: 0.3826 | MAE: 0.3024 | RMSE: 0.3826 | RÂ²: 0.8433\n",
      "Epoch 8/20 | Train Loss: 0.3900 | Val Loss: 0.3550 | MAE: 0.2709 | RMSE: 0.3550 | RÂ²: 0.8651\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3391 | MAE = 0.2557\n",
      "\n",
      "ðŸ”§ [41/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[256, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8551 | Val Loss: 0.7735 | MAE: 0.6580 | RMSE: 0.7735 | RÂ²: 0.3594\n",
      "Epoch 2/20 | Train Loss: 0.5984 | Val Loss: 0.6012 | MAE: 0.5008 | RMSE: 0.6012 | RÂ²: 0.6130\n",
      "Epoch 3/20 | Train Loss: 0.5130 | Val Loss: 0.4482 | MAE: 0.3564 | RMSE: 0.4482 | RÂ²: 0.7849\n",
      "Epoch 4/20 | Train Loss: 0.4630 | Val Loss: 0.3654 | MAE: 0.2816 | RMSE: 0.3654 | RÂ²: 0.8571\n",
      "Epoch 5/20 | Train Loss: 0.4334 | Val Loss: 0.3342 | MAE: 0.2518 | RMSE: 0.3342 | RÂ²: 0.8805\n",
      "Epoch 6/20 | Train Loss: 0.4117 | Val Loss: 0.3355 | MAE: 0.2462 | RMSE: 0.3355 | RÂ²: 0.8795\n",
      "Epoch 7/20 | Train Loss: 0.3907 | Val Loss: 0.3112 | MAE: 0.2267 | RMSE: 0.3112 | RÂ²: 0.8963\n",
      "Epoch 8/20 | Train Loss: 0.3756 | Val Loss: 0.2872 | MAE: 0.2072 | RMSE: 0.2872 | RÂ²: 0.9117\n",
      "Epoch 9/20 | Train Loss: 0.3630 | Val Loss: 0.2742 | MAE: 0.1954 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "Epoch 10/20 | Train Loss: 0.3539 | Val Loss: 0.2739 | MAE: 0.1939 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 11/20 | Train Loss: 0.3498 | Val Loss: 0.2695 | MAE: 0.1912 | RMSE: 0.2695 | RÂ²: 0.9222\n",
      "Epoch 12/20 | Train Loss: 0.3417 | Val Loss: 0.2712 | MAE: 0.1906 | RMSE: 0.2712 | RÂ²: 0.9213\n",
      "Epoch 13/20 | Train Loss: 0.3311 | Val Loss: 0.2711 | MAE: 0.1891 | RMSE: 0.2711 | RÂ²: 0.9213\n",
      "Epoch 14/20 | Train Loss: 0.3242 | Val Loss: 0.2698 | MAE: 0.1894 | RMSE: 0.2698 | RÂ²: 0.9221\n",
      "Epoch 15/20 | Train Loss: 0.3180 | Val Loss: 0.2638 | MAE: 0.1829 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 16/20 | Train Loss: 0.3123 | Val Loss: 0.2658 | MAE: 0.1847 | RMSE: 0.2658 | RÂ²: 0.9244\n",
      "Epoch 17/20 | Train Loss: 0.3098 | Val Loss: 0.2709 | MAE: 0.1887 | RMSE: 0.2709 | RÂ²: 0.9215\n",
      "Epoch 18/20 | Train Loss: 0.3041 | Val Loss: 0.2725 | MAE: 0.1901 | RMSE: 0.2725 | RÂ²: 0.9205\n",
      "Epoch 19/20 | Train Loss: 0.3008 | Val Loss: 0.2684 | MAE: 0.1857 | RMSE: 0.2684 | RÂ²: 0.9229\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2638 | MAE = 0.1829\n",
      "\n",
      "ðŸ”§ [42/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.0132 | Val Loss: 0.9270 | MAE: 0.7830 | RMSE: 0.9270 | RÂ²: 0.0800\n",
      "Epoch 2/20 | Train Loss: 0.8706 | Val Loss: 0.8264 | MAE: 0.7005 | RMSE: 0.8264 | RÂ²: 0.2690\n",
      "Epoch 3/20 | Train Loss: 0.7986 | Val Loss: 0.7308 | MAE: 0.6173 | RMSE: 0.7308 | RÂ²: 0.4282\n",
      "Epoch 4/20 | Train Loss: 0.7550 | Val Loss: 0.6441 | MAE: 0.5420 | RMSE: 0.6441 | RÂ²: 0.5558\n",
      "Epoch 5/20 | Train Loss: 0.6989 | Val Loss: 0.5720 | MAE: 0.4820 | RMSE: 0.5720 | RÂ²: 0.6498\n",
      "Epoch 6/20 | Train Loss: 0.6551 | Val Loss: 0.5172 | MAE: 0.4343 | RMSE: 0.5172 | RÂ²: 0.7136\n",
      "Epoch 7/20 | Train Loss: 0.6302 | Val Loss: 0.4708 | MAE: 0.3915 | RMSE: 0.4708 | RÂ²: 0.7627\n",
      "Epoch 8/20 | Train Loss: 0.5991 | Val Loss: 0.4306 | MAE: 0.3531 | RMSE: 0.4306 | RÂ²: 0.8015\n",
      "Epoch 9/20 | Train Loss: 0.5760 | Val Loss: 0.3938 | MAE: 0.3184 | RMSE: 0.3938 | RÂ²: 0.8340\n",
      "Epoch 10/20 | Train Loss: 0.5579 | Val Loss: 0.3697 | MAE: 0.2945 | RMSE: 0.3697 | RÂ²: 0.8537\n",
      "Epoch 11/20 | Train Loss: 0.5382 | Val Loss: 0.3547 | MAE: 0.2791 | RMSE: 0.3547 | RÂ²: 0.8653\n",
      "Epoch 12/20 | Train Loss: 0.5189 | Val Loss: 0.3453 | MAE: 0.2683 | RMSE: 0.3453 | RÂ²: 0.8723\n",
      "Epoch 13/20 | Train Loss: 0.5068 | Val Loss: 0.3363 | MAE: 0.2579 | RMSE: 0.3363 | RÂ²: 0.8790\n",
      "Epoch 14/20 | Train Loss: 0.4942 | Val Loss: 0.3297 | MAE: 0.2506 | RMSE: 0.3297 | RÂ²: 0.8836\n",
      "Epoch 15/20 | Train Loss: 0.4827 | Val Loss: 0.3248 | MAE: 0.2450 | RMSE: 0.3248 | RÂ²: 0.8871\n",
      "Epoch 16/20 | Train Loss: 0.4760 | Val Loss: 0.3218 | MAE: 0.2408 | RMSE: 0.3218 | RÂ²: 0.8892\n",
      "Epoch 17/20 | Train Loss: 0.4651 | Val Loss: 0.3161 | MAE: 0.2351 | RMSE: 0.3161 | RÂ²: 0.8931\n",
      "Epoch 18/20 | Train Loss: 0.4568 | Val Loss: 0.3101 | MAE: 0.2301 | RMSE: 0.3101 | RÂ²: 0.8971\n",
      "Epoch 19/20 | Train Loss: 0.4538 | Val Loss: 0.3056 | MAE: 0.2262 | RMSE: 0.3056 | RÂ²: 0.9000\n",
      "Epoch 20/20 | Train Loss: 0.4402 | Val Loss: 0.3045 | MAE: 0.2244 | RMSE: 0.3045 | RÂ²: 0.9008\n",
      "âœ… RMSE = 0.3045 | MAE = 0.2244\n",
      "\n",
      "ðŸ”§ [43/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[1024, 256, 1024], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7592 | Val Loss: 0.9230 | MAE: 0.7774 | RMSE: 0.9230 | RÂ²: 0.0881\n",
      "Epoch 2/20 | Train Loss: 0.4826 | Val Loss: 0.7786 | MAE: 0.6541 | RMSE: 0.7786 | RÂ²: 0.3510\n",
      "Epoch 3/20 | Train Loss: 0.4060 | Val Loss: 0.6246 | MAE: 0.5261 | RMSE: 0.6246 | RÂ²: 0.5824\n",
      "Epoch 4/20 | Train Loss: 0.3642 | Val Loss: 0.5290 | MAE: 0.4269 | RMSE: 0.5290 | RÂ²: 0.7004\n",
      "Epoch 5/20 | Train Loss: 0.3441 | Val Loss: 0.3526 | MAE: 0.2817 | RMSE: 0.3526 | RÂ²: 0.8669\n",
      "Epoch 6/20 | Train Loss: 0.3107 | Val Loss: 0.3128 | MAE: 0.2396 | RMSE: 0.3128 | RÂ²: 0.8952\n",
      "Epoch 7/20 | Train Loss: 0.3097 | Val Loss: 0.2638 | MAE: 0.1879 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 8/20 | Train Loss: 0.2978 | Val Loss: 0.2620 | MAE: 0.1813 | RMSE: 0.2620 | RÂ²: 0.9265\n",
      "Epoch 9/20 | Train Loss: 0.2922 | Val Loss: 0.2804 | MAE: 0.1932 | RMSE: 0.2804 | RÂ²: 0.9158\n",
      "Epoch 10/20 | Train Loss: 0.2879 | Val Loss: 0.2575 | MAE: 0.1770 | RMSE: 0.2575 | RÂ²: 0.9290\n",
      "Epoch 11/20 | Train Loss: 0.2835 | Val Loss: 0.2658 | MAE: 0.1820 | RMSE: 0.2658 | RÂ²: 0.9244\n",
      "Epoch 12/20 | Train Loss: 0.2788 | Val Loss: 0.2700 | MAE: 0.1857 | RMSE: 0.2700 | RÂ²: 0.9219\n",
      "Epoch 13/20 | Train Loss: 0.2738 | Val Loss: 0.2744 | MAE: 0.1882 | RMSE: 0.2744 | RÂ²: 0.9194\n",
      "Epoch 14/20 | Train Loss: 0.2711 | Val Loss: 0.2706 | MAE: 0.1889 | RMSE: 0.2706 | RÂ²: 0.9216\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2575 | MAE = 0.1770\n",
      "\n",
      "ðŸ”§ [44/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[512, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8565 | Val Loss: 0.8294 | MAE: 0.6960 | RMSE: 0.8294 | RÂ²: 0.2635\n",
      "Epoch 2/20 | Train Loss: 0.6195 | Val Loss: 0.6515 | MAE: 0.5372 | RMSE: 0.6515 | RÂ²: 0.5455\n",
      "Epoch 3/20 | Train Loss: 0.5455 | Val Loss: 0.4671 | MAE: 0.3701 | RMSE: 0.4671 | RÂ²: 0.7664\n",
      "Epoch 4/20 | Train Loss: 0.4854 | Val Loss: 0.3607 | MAE: 0.2763 | RMSE: 0.3607 | RÂ²: 0.8607\n",
      "Epoch 5/20 | Train Loss: 0.4580 | Val Loss: 0.4039 | MAE: 0.3050 | RMSE: 0.4039 | RÂ²: 0.8253\n",
      "Epoch 6/20 | Train Loss: 0.4215 | Val Loss: 0.3924 | MAE: 0.3004 | RMSE: 0.3924 | RÂ²: 0.8351\n",
      "Epoch 7/20 | Train Loss: 0.3970 | Val Loss: 0.3952 | MAE: 0.3008 | RMSE: 0.3952 | RÂ²: 0.8328\n",
      "Epoch 8/20 | Train Loss: 0.3848 | Val Loss: 0.3748 | MAE: 0.2952 | RMSE: 0.3748 | RÂ²: 0.8496\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3607 | MAE = 0.2763\n",
      "\n",
      "ðŸ”§ [45/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8473 | Val Loss: 0.8102 | MAE: 0.6761 | RMSE: 0.8102 | RÂ²: 0.2972\n",
      "Epoch 2/20 | Train Loss: 0.4930 | Val Loss: 0.5728 | MAE: 0.4664 | RMSE: 0.5728 | RÂ²: 0.6488\n",
      "Epoch 3/20 | Train Loss: 0.4309 | Val Loss: 0.3459 | MAE: 0.2735 | RMSE: 0.3459 | RÂ²: 0.8719\n",
      "Epoch 4/20 | Train Loss: 0.3909 | Val Loss: 0.3754 | MAE: 0.2453 | RMSE: 0.3754 | RÂ²: 0.8491\n",
      "Epoch 5/20 | Train Loss: 0.3739 | Val Loss: 0.2772 | MAE: 0.2003 | RMSE: 0.2772 | RÂ²: 0.9177\n",
      "Epoch 6/20 | Train Loss: 0.3504 | Val Loss: 0.3012 | MAE: 0.2298 | RMSE: 0.3012 | RÂ²: 0.9029\n",
      "Epoch 7/20 | Train Loss: 0.3376 | Val Loss: 0.2615 | MAE: 0.1841 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 8/20 | Train Loss: 0.3264 | Val Loss: 0.2738 | MAE: 0.2000 | RMSE: 0.2738 | RÂ²: 0.9197\n",
      "Epoch 9/20 | Train Loss: 0.3210 | Val Loss: 0.2662 | MAE: 0.1782 | RMSE: 0.2662 | RÂ²: 0.9241\n",
      "Epoch 10/20 | Train Loss: 0.3045 | Val Loss: 0.2709 | MAE: 0.1870 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 11/20 | Train Loss: 0.3041 | Val Loss: 0.2576 | MAE: 0.1772 | RMSE: 0.2576 | RÂ²: 0.9290\n",
      "Epoch 12/20 | Train Loss: 0.2961 | Val Loss: 0.2768 | MAE: 0.1815 | RMSE: 0.2768 | RÂ²: 0.9180\n",
      "Epoch 13/20 | Train Loss: 0.2898 | Val Loss: 0.2705 | MAE: 0.1860 | RMSE: 0.2705 | RÂ²: 0.9217\n",
      "Epoch 14/20 | Train Loss: 0.2912 | Val Loss: 0.2611 | MAE: 0.1786 | RMSE: 0.2611 | RÂ²: 0.9270\n",
      "Epoch 15/20 | Train Loss: 0.2738 | Val Loss: 0.2752 | MAE: 0.1874 | RMSE: 0.2752 | RÂ²: 0.9189\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2576 | MAE = 0.1772\n",
      "\n",
      "ðŸ”§ [46/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[512, 512, 512], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.0238 | Val Loss: 0.9248 | MAE: 0.7831 | RMSE: 0.9248 | RÂ²: 0.0843\n",
      "Epoch 2/20 | Train Loss: 0.9184 | Val Loss: 0.8233 | MAE: 0.6973 | RMSE: 0.8233 | RÂ²: 0.2744\n",
      "Epoch 3/20 | Train Loss: 0.8460 | Val Loss: 0.7330 | MAE: 0.6186 | RMSE: 0.7330 | RÂ²: 0.4248\n",
      "Epoch 4/20 | Train Loss: 0.7909 | Val Loss: 0.6432 | MAE: 0.5362 | RMSE: 0.6432 | RÂ²: 0.5571\n",
      "Epoch 5/20 | Train Loss: 0.7256 | Val Loss: 0.5441 | MAE: 0.4427 | RMSE: 0.5441 | RÂ²: 0.6830\n",
      "Epoch 6/20 | Train Loss: 0.6826 | Val Loss: 0.4686 | MAE: 0.3713 | RMSE: 0.4686 | RÂ²: 0.7649\n",
      "Epoch 7/20 | Train Loss: 0.6640 | Val Loss: 0.4385 | MAE: 0.3292 | RMSE: 0.4385 | RÂ²: 0.7941\n",
      "Epoch 8/20 | Train Loss: 0.6464 | Val Loss: 0.4330 | MAE: 0.3033 | RMSE: 0.4330 | RÂ²: 0.7993\n",
      "Epoch 9/20 | Train Loss: 0.6256 | Val Loss: 0.4205 | MAE: 0.2860 | RMSE: 0.4205 | RÂ²: 0.8107\n",
      "Epoch 10/20 | Train Loss: 0.6105 | Val Loss: 0.4074 | MAE: 0.2744 | RMSE: 0.4074 | RÂ²: 0.8223\n",
      "Epoch 11/20 | Train Loss: 0.5923 | Val Loss: 0.3993 | MAE: 0.2677 | RMSE: 0.3993 | RÂ²: 0.8293\n",
      "Epoch 12/20 | Train Loss: 0.5803 | Val Loss: 0.3893 | MAE: 0.2644 | RMSE: 0.3893 | RÂ²: 0.8377\n",
      "Epoch 13/20 | Train Loss: 0.5771 | Val Loss: 0.3998 | MAE: 0.2652 | RMSE: 0.3998 | RÂ²: 0.8288\n",
      "Epoch 14/20 | Train Loss: 0.5607 | Val Loss: 0.3974 | MAE: 0.2653 | RMSE: 0.3974 | RÂ²: 0.8309\n",
      "Epoch 15/20 | Train Loss: 0.5497 | Val Loss: 0.3876 | MAE: 0.2645 | RMSE: 0.3876 | RÂ²: 0.8392\n",
      "Epoch 16/20 | Train Loss: 0.5429 | Val Loss: 0.3802 | MAE: 0.2617 | RMSE: 0.3802 | RÂ²: 0.8452\n",
      "Epoch 17/20 | Train Loss: 0.5358 | Val Loss: 0.3908 | MAE: 0.2620 | RMSE: 0.3908 | RÂ²: 0.8365\n",
      "Epoch 18/20 | Train Loss: 0.5226 | Val Loss: 0.3941 | MAE: 0.2619 | RMSE: 0.3941 | RÂ²: 0.8337\n",
      "Epoch 19/20 | Train Loss: 0.5239 | Val Loss: 0.3937 | MAE: 0.2619 | RMSE: 0.3937 | RÂ²: 0.8341\n",
      "Epoch 20/20 | Train Loss: 0.5084 | Val Loss: 0.3885 | MAE: 0.2597 | RMSE: 0.3885 | RÂ²: 0.8384\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3802 | MAE = 0.2617\n",
      "\n",
      "ðŸ”§ [47/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[512, 256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8731 | Val Loss: 0.9137 | MAE: 0.7737 | RMSE: 0.9137 | RÂ²: 0.1062\n",
      "Epoch 2/20 | Train Loss: 0.5895 | Val Loss: 0.7889 | MAE: 0.6652 | RMSE: 0.7889 | RÂ²: 0.3338\n",
      "Epoch 3/20 | Train Loss: 0.4526 | Val Loss: 0.6340 | MAE: 0.5236 | RMSE: 0.6340 | RÂ²: 0.5697\n",
      "Epoch 4/20 | Train Loss: 0.3920 | Val Loss: 0.4457 | MAE: 0.3686 | RMSE: 0.4457 | RÂ²: 0.7873\n",
      "Epoch 5/20 | Train Loss: 0.3550 | Val Loss: 0.3626 | MAE: 0.2901 | RMSE: 0.3626 | RÂ²: 0.8592\n",
      "Epoch 6/20 | Train Loss: 0.3310 | Val Loss: 0.3235 | MAE: 0.2505 | RMSE: 0.3235 | RÂ²: 0.8880\n",
      "Epoch 7/20 | Train Loss: 0.3160 | Val Loss: 0.2797 | MAE: 0.2082 | RMSE: 0.2797 | RÂ²: 0.9163\n",
      "Epoch 8/20 | Train Loss: 0.3083 | Val Loss: 0.2745 | MAE: 0.2000 | RMSE: 0.2745 | RÂ²: 0.9193\n",
      "Epoch 9/20 | Train Loss: 0.2992 | Val Loss: 0.2740 | MAE: 0.1954 | RMSE: 0.2740 | RÂ²: 0.9196\n",
      "Epoch 10/20 | Train Loss: 0.2910 | Val Loss: 0.2753 | MAE: 0.1918 | RMSE: 0.2753 | RÂ²: 0.9189\n",
      "Epoch 11/20 | Train Loss: 0.2859 | Val Loss: 0.2742 | MAE: 0.1957 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "Epoch 12/20 | Train Loss: 0.2809 | Val Loss: 0.2673 | MAE: 0.1865 | RMSE: 0.2673 | RÂ²: 0.9235\n",
      "Epoch 13/20 | Train Loss: 0.2752 | Val Loss: 0.2694 | MAE: 0.1868 | RMSE: 0.2694 | RÂ²: 0.9223\n",
      "Epoch 14/20 | Train Loss: 0.2717 | Val Loss: 0.2752 | MAE: 0.1940 | RMSE: 0.2752 | RÂ²: 0.9189\n",
      "Epoch 15/20 | Train Loss: 0.2682 | Val Loss: 0.2743 | MAE: 0.1913 | RMSE: 0.2743 | RÂ²: 0.9195\n",
      "Epoch 16/20 | Train Loss: 0.2629 | Val Loss: 0.2686 | MAE: 0.1882 | RMSE: 0.2686 | RÂ²: 0.9228\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2673 | MAE = 0.1865\n",
      "\n",
      "ðŸ”§ [48/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[512, 512, 512], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9202 | Val Loss: 0.8725 | MAE: 0.7387 | RMSE: 0.8725 | RÂ²: 0.1850\n",
      "Epoch 2/20 | Train Loss: 0.6985 | Val Loss: 0.7925 | MAE: 0.6589 | RMSE: 0.7925 | RÂ²: 0.3277\n",
      "Epoch 3/20 | Train Loss: 0.5826 | Val Loss: 0.6138 | MAE: 0.5057 | RMSE: 0.6138 | RÂ²: 0.5966\n",
      "Epoch 4/20 | Train Loss: 0.5339 | Val Loss: 0.5179 | MAE: 0.3884 | RMSE: 0.5179 | RÂ²: 0.7129\n",
      "Epoch 5/20 | Train Loss: 0.5006 | Val Loss: 0.3474 | MAE: 0.2672 | RMSE: 0.3474 | RÂ²: 0.8708\n",
      "Epoch 6/20 | Train Loss: 0.4747 | Val Loss: 0.3249 | MAE: 0.2465 | RMSE: 0.3249 | RÂ²: 0.8870\n",
      "Epoch 7/20 | Train Loss: 0.4483 | Val Loss: 0.3112 | MAE: 0.2292 | RMSE: 0.3112 | RÂ²: 0.8964\n",
      "Epoch 8/20 | Train Loss: 0.4304 | Val Loss: 0.2985 | MAE: 0.2137 | RMSE: 0.2985 | RÂ²: 0.9046\n",
      "Epoch 9/20 | Train Loss: 0.4111 | Val Loss: 0.3131 | MAE: 0.2188 | RMSE: 0.3131 | RÂ²: 0.8950\n",
      "Epoch 10/20 | Train Loss: 0.4018 | Val Loss: 0.3115 | MAE: 0.2171 | RMSE: 0.3115 | RÂ²: 0.8961\n",
      "Epoch 11/20 | Train Loss: 0.3946 | Val Loss: 0.3005 | MAE: 0.2144 | RMSE: 0.3005 | RÂ²: 0.9034\n",
      "Epoch 12/20 | Train Loss: 0.3863 | Val Loss: 0.3104 | MAE: 0.2196 | RMSE: 0.3104 | RÂ²: 0.8968\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2985 | MAE = 0.2137\n",
      "\n",
      "ðŸ”§ [49/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8250 | Val Loss: 0.9056 | MAE: 0.7637 | RMSE: 0.9056 | RÂ²: 0.1220\n",
      "Epoch 2/20 | Train Loss: 0.5334 | Val Loss: 0.7565 | MAE: 0.6329 | RMSE: 0.7565 | RÂ²: 0.3873\n",
      "Epoch 3/20 | Train Loss: 0.4443 | Val Loss: 0.5674 | MAE: 0.4720 | RMSE: 0.5674 | RÂ²: 0.6554\n",
      "Epoch 4/20 | Train Loss: 0.4020 | Val Loss: 0.4160 | MAE: 0.3417 | RMSE: 0.4160 | RÂ²: 0.8147\n",
      "Epoch 5/20 | Train Loss: 0.3726 | Val Loss: 0.3424 | MAE: 0.2619 | RMSE: 0.3424 | RÂ²: 0.8745\n",
      "Epoch 6/20 | Train Loss: 0.3566 | Val Loss: 0.2930 | MAE: 0.1983 | RMSE: 0.2930 | RÂ²: 0.9081\n",
      "Epoch 7/20 | Train Loss: 0.3378 | Val Loss: 0.2632 | MAE: 0.1821 | RMSE: 0.2632 | RÂ²: 0.9258\n",
      "Epoch 8/20 | Train Loss: 0.3285 | Val Loss: 0.2658 | MAE: 0.1868 | RMSE: 0.2658 | RÂ²: 0.9243\n",
      "Epoch 9/20 | Train Loss: 0.3172 | Val Loss: 0.2674 | MAE: 0.1875 | RMSE: 0.2674 | RÂ²: 0.9235\n",
      "Epoch 10/20 | Train Loss: 0.3110 | Val Loss: 0.2795 | MAE: 0.1924 | RMSE: 0.2795 | RÂ²: 0.9164\n",
      "Epoch 11/20 | Train Loss: 0.3058 | Val Loss: 0.2675 | MAE: 0.1836 | RMSE: 0.2675 | RÂ²: 0.9234\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2632 | MAE = 0.1821\n",
      "\n",
      "ðŸ”§ [50/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[512, 512, 512], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0186 | Val Loss: 0.8699 | MAE: 0.7373 | RMSE: 0.8699 | RÂ²: 0.1899\n",
      "Epoch 2/20 | Train Loss: 0.8664 | Val Loss: 0.7704 | MAE: 0.6525 | RMSE: 0.7704 | RÂ²: 0.3646\n",
      "Epoch 3/20 | Train Loss: 0.7822 | Val Loss: 0.6518 | MAE: 0.5467 | RMSE: 0.6518 | RÂ²: 0.5452\n",
      "Epoch 4/20 | Train Loss: 0.7073 | Val Loss: 0.5131 | MAE: 0.4228 | RMSE: 0.5131 | RÂ²: 0.7182\n",
      "Epoch 5/20 | Train Loss: 0.6599 | Val Loss: 0.4718 | MAE: 0.3522 | RMSE: 0.4718 | RÂ²: 0.7617\n",
      "Epoch 6/20 | Train Loss: 0.6238 | Val Loss: 0.4423 | MAE: 0.2975 | RMSE: 0.4423 | RÂ²: 0.7906\n",
      "Epoch 7/20 | Train Loss: 0.5964 | Val Loss: 0.4016 | MAE: 0.2745 | RMSE: 0.4016 | RÂ²: 0.8273\n",
      "Epoch 8/20 | Train Loss: 0.5754 | Val Loss: 0.4029 | MAE: 0.2764 | RMSE: 0.4029 | RÂ²: 0.8262\n",
      "Epoch 9/20 | Train Loss: 0.5435 | Val Loss: 0.3860 | MAE: 0.2709 | RMSE: 0.3860 | RÂ²: 0.8405\n",
      "Epoch 10/20 | Train Loss: 0.5279 | Val Loss: 0.3902 | MAE: 0.2782 | RMSE: 0.3902 | RÂ²: 0.8370\n",
      "Epoch 11/20 | Train Loss: 0.5147 | Val Loss: 0.4014 | MAE: 0.2815 | RMSE: 0.4014 | RÂ²: 0.8275\n",
      "Epoch 12/20 | Train Loss: 0.5043 | Val Loss: 0.3948 | MAE: 0.2852 | RMSE: 0.3948 | RÂ²: 0.8332\n",
      "Epoch 13/20 | Train Loss: 0.4971 | Val Loss: 0.4175 | MAE: 0.2938 | RMSE: 0.4175 | RÂ²: 0.8134\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3860 | MAE = 0.2709\n",
      "\n",
      "ðŸ”§ [51/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0445 | Val Loss: 0.9230 | MAE: 0.7771 | RMSE: 0.9230 | RÂ²: 0.0879\n",
      "Epoch 2/20 | Train Loss: 0.9633 | Val Loss: 0.8534 | MAE: 0.7181 | RMSE: 0.8534 | RÂ²: 0.2203\n",
      "Epoch 3/20 | Train Loss: 0.9023 | Val Loss: 0.7908 | MAE: 0.6623 | RMSE: 0.7908 | RÂ²: 0.3306\n",
      "Epoch 4/20 | Train Loss: 0.8728 | Val Loss: 0.7344 | MAE: 0.6125 | RMSE: 0.7344 | RÂ²: 0.4227\n",
      "Epoch 5/20 | Train Loss: 0.8211 | Val Loss: 0.6835 | MAE: 0.5689 | RMSE: 0.6835 | RÂ²: 0.4998\n",
      "Epoch 6/20 | Train Loss: 0.7854 | Val Loss: 0.6380 | MAE: 0.5314 | RMSE: 0.6380 | RÂ²: 0.5642\n",
      "Epoch 7/20 | Train Loss: 0.7531 | Val Loss: 0.5960 | MAE: 0.4952 | RMSE: 0.5960 | RÂ²: 0.6197\n",
      "Epoch 8/20 | Train Loss: 0.7311 | Val Loss: 0.5566 | MAE: 0.4602 | RMSE: 0.5566 | RÂ²: 0.6684\n",
      "Epoch 9/20 | Train Loss: 0.7034 | Val Loss: 0.5207 | MAE: 0.4277 | RMSE: 0.5207 | RÂ²: 0.7098\n",
      "Epoch 10/20 | Train Loss: 0.6851 | Val Loss: 0.4895 | MAE: 0.4000 | RMSE: 0.4895 | RÂ²: 0.7435\n",
      "Epoch 11/20 | Train Loss: 0.6724 | Val Loss: 0.4667 | MAE: 0.3793 | RMSE: 0.4667 | RÂ²: 0.7668\n",
      "Epoch 12/20 | Train Loss: 0.6500 | Val Loss: 0.4477 | MAE: 0.3606 | RMSE: 0.4477 | RÂ²: 0.7854\n",
      "Epoch 13/20 | Train Loss: 0.6350 | Val Loss: 0.4309 | MAE: 0.3441 | RMSE: 0.4309 | RÂ²: 0.8013\n",
      "Epoch 14/20 | Train Loss: 0.6203 | Val Loss: 0.4150 | MAE: 0.3287 | RMSE: 0.4150 | RÂ²: 0.8156\n",
      "Epoch 15/20 | Train Loss: 0.6054 | Val Loss: 0.4017 | MAE: 0.3149 | RMSE: 0.4017 | RÂ²: 0.8273\n",
      "Epoch 16/20 | Train Loss: 0.5953 | Val Loss: 0.3916 | MAE: 0.3036 | RMSE: 0.3916 | RÂ²: 0.8358\n",
      "Epoch 17/20 | Train Loss: 0.5975 | Val Loss: 0.3808 | MAE: 0.2923 | RMSE: 0.3808 | RÂ²: 0.8447\n",
      "Epoch 18/20 | Train Loss: 0.5800 | Val Loss: 0.3749 | MAE: 0.2845 | RMSE: 0.3749 | RÂ²: 0.8495\n",
      "Epoch 19/20 | Train Loss: 0.5723 | Val Loss: 0.3744 | MAE: 0.2803 | RMSE: 0.3744 | RÂ²: 0.8499\n",
      "Epoch 20/20 | Train Loss: 0.5596 | Val Loss: 0.3737 | MAE: 0.2758 | RMSE: 0.3737 | RÂ²: 0.8505\n",
      "âœ… RMSE = 0.3737 | MAE = 0.2758\n",
      "\n",
      "ðŸ”§ [52/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9675 | Val Loss: 0.5755 | MAE: 0.4742 | RMSE: 0.5755 | RÂ²: 0.6455\n",
      "Epoch 2/20 | Train Loss: 0.5156 | Val Loss: 0.5207 | MAE: 0.4240 | RMSE: 0.5207 | RÂ²: 0.7097\n",
      "Epoch 3/20 | Train Loss: 0.4561 | Val Loss: 0.7276 | MAE: 0.5870 | RMSE: 0.7276 | RÂ²: 0.4332\n",
      "Epoch 4/20 | Train Loss: 0.4190 | Val Loss: 0.6480 | MAE: 0.5634 | RMSE: 0.6480 | RÂ²: 0.5505\n",
      "Epoch 5/20 | Train Loss: 0.3798 | Val Loss: 0.5074 | MAE: 0.4061 | RMSE: 0.5074 | RÂ²: 0.7244\n",
      "Epoch 6/20 | Train Loss: 0.3795 | Val Loss: 0.4091 | MAE: 0.3182 | RMSE: 0.4091 | RÂ²: 0.8208\n",
      "Epoch 7/20 | Train Loss: 0.3489 | Val Loss: 0.4368 | MAE: 0.2692 | RMSE: 0.4368 | RÂ²: 0.7957\n",
      "Epoch 8/20 | Train Loss: 0.3410 | Val Loss: 0.3292 | MAE: 0.2406 | RMSE: 0.3292 | RÂ²: 0.8840\n",
      "Epoch 9/20 | Train Loss: 0.3292 | Val Loss: 0.2721 | MAE: 0.1861 | RMSE: 0.2721 | RÂ²: 0.9207\n",
      "Epoch 10/20 | Train Loss: 0.3185 | Val Loss: 0.2789 | MAE: 0.1808 | RMSE: 0.2789 | RÂ²: 0.9167\n",
      "Epoch 11/20 | Train Loss: 0.3157 | Val Loss: 0.2784 | MAE: 0.1895 | RMSE: 0.2784 | RÂ²: 0.9170\n",
      "Epoch 12/20 | Train Loss: 0.2982 | Val Loss: 0.2947 | MAE: 0.1933 | RMSE: 0.2947 | RÂ²: 0.9070\n",
      "Epoch 13/20 | Train Loss: 0.2881 | Val Loss: 0.2662 | MAE: 0.1754 | RMSE: 0.2662 | RÂ²: 0.9241\n",
      "Epoch 14/20 | Train Loss: 0.2859 | Val Loss: 0.2822 | MAE: 0.1914 | RMSE: 0.2822 | RÂ²: 0.9147\n",
      "Epoch 15/20 | Train Loss: 0.2766 | Val Loss: 0.2791 | MAE: 0.1917 | RMSE: 0.2791 | RÂ²: 0.9166\n",
      "Epoch 16/20 | Train Loss: 0.2750 | Val Loss: 0.2745 | MAE: 0.1900 | RMSE: 0.2745 | RÂ²: 0.9194\n",
      "Epoch 17/20 | Train Loss: 0.2703 | Val Loss: 0.2719 | MAE: 0.1857 | RMSE: 0.2719 | RÂ²: 0.9209\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2662 | MAE = 0.1754\n",
      "\n",
      "ðŸ”§ [53/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.1036 | Val Loss: 0.7364 | MAE: 0.6035 | RMSE: 0.7364 | RÂ²: 0.4195\n",
      "Epoch 2/20 | Train Loss: 0.5236 | Val Loss: 0.4139 | MAE: 0.2733 | RMSE: 0.4139 | RÂ²: 0.8166\n",
      "Epoch 3/20 | Train Loss: 0.4288 | Val Loss: 0.8002 | MAE: 0.5317 | RMSE: 0.8002 | RÂ²: 0.3145\n",
      "Epoch 4/20 | Train Loss: 0.3966 | Val Loss: 0.5070 | MAE: 0.3921 | RMSE: 0.5070 | RÂ²: 0.7249\n",
      "Epoch 5/20 | Train Loss: 0.3739 | Val Loss: 0.4101 | MAE: 0.3013 | RMSE: 0.4101 | RÂ²: 0.8199\n",
      "Epoch 6/20 | Train Loss: 0.3448 | Val Loss: 0.4642 | MAE: 0.3215 | RMSE: 0.4642 | RÂ²: 0.7693\n",
      "Epoch 7/20 | Train Loss: 0.3277 | Val Loss: 0.2789 | MAE: 0.2002 | RMSE: 0.2789 | RÂ²: 0.9167\n",
      "Epoch 8/20 | Train Loss: 0.3385 | Val Loss: 0.2905 | MAE: 0.2184 | RMSE: 0.2905 | RÂ²: 0.9097\n",
      "Epoch 9/20 | Train Loss: 0.3189 | Val Loss: 0.2686 | MAE: 0.1834 | RMSE: 0.2686 | RÂ²: 0.9228\n",
      "Epoch 10/20 | Train Loss: 0.3101 | Val Loss: 0.2845 | MAE: 0.1859 | RMSE: 0.2845 | RÂ²: 0.9134\n",
      "Epoch 11/20 | Train Loss: 0.3014 | Val Loss: 0.2762 | MAE: 0.1834 | RMSE: 0.2762 | RÂ²: 0.9184\n",
      "Epoch 12/20 | Train Loss: 0.2985 | Val Loss: 0.2587 | MAE: 0.1762 | RMSE: 0.2587 | RÂ²: 0.9283\n",
      "Epoch 13/20 | Train Loss: 0.2951 | Val Loss: 0.3057 | MAE: 0.1853 | RMSE: 0.3057 | RÂ²: 0.8999\n",
      "Epoch 14/20 | Train Loss: 0.2832 | Val Loss: 0.2709 | MAE: 0.1856 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 15/20 | Train Loss: 0.2826 | Val Loss: 0.2629 | MAE: 0.1761 | RMSE: 0.2629 | RÂ²: 0.9260\n",
      "Epoch 16/20 | Train Loss: 0.2731 | Val Loss: 0.2816 | MAE: 0.1969 | RMSE: 0.2816 | RÂ²: 0.9151\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2587 | MAE = 0.1762\n",
      "\n",
      "ðŸ”§ [54/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[256, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 1.0259 | Val Loss: 0.9569 | MAE: 0.8093 | RMSE: 0.9569 | RÂ²: 0.0197\n",
      "Epoch 2/20 | Train Loss: 0.8444 | Val Loss: 0.8475 | MAE: 0.7176 | RMSE: 0.8475 | RÂ²: 0.2310\n",
      "Epoch 3/20 | Train Loss: 0.7515 | Val Loss: 0.7445 | MAE: 0.6222 | RMSE: 0.7445 | RÂ²: 0.4065\n",
      "Epoch 4/20 | Train Loss: 0.6832 | Val Loss: 0.6478 | MAE: 0.5336 | RMSE: 0.6478 | RÂ²: 0.5508\n",
      "Epoch 5/20 | Train Loss: 0.6274 | Val Loss: 0.5638 | MAE: 0.4602 | RMSE: 0.5638 | RÂ²: 0.6597\n",
      "Epoch 6/20 | Train Loss: 0.5792 | Val Loss: 0.4945 | MAE: 0.3976 | RMSE: 0.4945 | RÂ²: 0.7382\n",
      "Epoch 7/20 | Train Loss: 0.5313 | Val Loss: 0.4319 | MAE: 0.3411 | RMSE: 0.4319 | RÂ²: 0.8003\n",
      "Epoch 8/20 | Train Loss: 0.5138 | Val Loss: 0.3921 | MAE: 0.3047 | RMSE: 0.3921 | RÂ²: 0.8354\n",
      "Epoch 9/20 | Train Loss: 0.4869 | Val Loss: 0.3711 | MAE: 0.2856 | RMSE: 0.3711 | RÂ²: 0.8525\n",
      "Epoch 10/20 | Train Loss: 0.4727 | Val Loss: 0.3582 | MAE: 0.2729 | RMSE: 0.3582 | RÂ²: 0.8626\n",
      "Epoch 11/20 | Train Loss: 0.4527 | Val Loss: 0.3479 | MAE: 0.2622 | RMSE: 0.3479 | RÂ²: 0.8704\n",
      "Epoch 12/20 | Train Loss: 0.4404 | Val Loss: 0.3376 | MAE: 0.2541 | RMSE: 0.3376 | RÂ²: 0.8780\n",
      "Epoch 13/20 | Train Loss: 0.4295 | Val Loss: 0.3301 | MAE: 0.2483 | RMSE: 0.3301 | RÂ²: 0.8834\n",
      "Epoch 14/20 | Train Loss: 0.4188 | Val Loss: 0.3238 | MAE: 0.2422 | RMSE: 0.3238 | RÂ²: 0.8877\n",
      "Epoch 15/20 | Train Loss: 0.4116 | Val Loss: 0.3178 | MAE: 0.2358 | RMSE: 0.3178 | RÂ²: 0.8919\n",
      "Epoch 16/20 | Train Loss: 0.4073 | Val Loss: 0.3127 | MAE: 0.2318 | RMSE: 0.3127 | RÂ²: 0.8953\n",
      "Epoch 17/20 | Train Loss: 0.4061 | Val Loss: 0.3102 | MAE: 0.2296 | RMSE: 0.3102 | RÂ²: 0.8970\n",
      "Epoch 18/20 | Train Loss: 0.3964 | Val Loss: 0.3072 | MAE: 0.2270 | RMSE: 0.3072 | RÂ²: 0.8990\n",
      "Epoch 19/20 | Train Loss: 0.3896 | Val Loss: 0.3039 | MAE: 0.2241 | RMSE: 0.3039 | RÂ²: 0.9011\n",
      "Epoch 20/20 | Train Loss: 0.3806 | Val Loss: 0.3012 | MAE: 0.2218 | RMSE: 0.3012 | RÂ²: 0.9029\n",
      "âœ… RMSE = 0.3012 | MAE = 0.2218\n",
      "\n",
      "ðŸ”§ [55/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.7992 | Val Loss: 0.9310 | MAE: 0.7813 | RMSE: 0.9310 | RÂ²: 0.0722\n",
      "Epoch 2/20 | Train Loss: 0.5196 | Val Loss: 0.7402 | MAE: 0.6252 | RMSE: 0.7402 | RÂ²: 0.4135\n",
      "Epoch 3/20 | Train Loss: 0.4739 | Val Loss: 0.5458 | MAE: 0.4446 | RMSE: 0.5458 | RÂ²: 0.6811\n",
      "Epoch 4/20 | Train Loss: 0.4470 | Val Loss: 0.3181 | MAE: 0.2407 | RMSE: 0.3181 | RÂ²: 0.8916\n",
      "Epoch 5/20 | Train Loss: 0.4049 | Val Loss: 0.3286 | MAE: 0.2192 | RMSE: 0.3286 | RÂ²: 0.8844\n",
      "Epoch 6/20 | Train Loss: 0.3887 | Val Loss: 0.3104 | MAE: 0.2295 | RMSE: 0.3104 | RÂ²: 0.8969\n",
      "Epoch 7/20 | Train Loss: 0.3857 | Val Loss: 0.2755 | MAE: 0.1908 | RMSE: 0.2755 | RÂ²: 0.9188\n",
      "Epoch 8/20 | Train Loss: 0.3647 | Val Loss: 0.2633 | MAE: 0.1835 | RMSE: 0.2633 | RÂ²: 0.9258\n",
      "Epoch 9/20 | Train Loss: 0.3562 | Val Loss: 0.2862 | MAE: 0.2126 | RMSE: 0.2862 | RÂ²: 0.9123\n",
      "Epoch 10/20 | Train Loss: 0.3458 | Val Loss: 0.2590 | MAE: 0.1743 | RMSE: 0.2590 | RÂ²: 0.9282\n",
      "Epoch 11/20 | Train Loss: 0.3509 | Val Loss: 0.2935 | MAE: 0.1973 | RMSE: 0.2935 | RÂ²: 0.9078\n",
      "Epoch 12/20 | Train Loss: 0.3346 | Val Loss: 0.2721 | MAE: 0.1890 | RMSE: 0.2721 | RÂ²: 0.9207\n",
      "Epoch 13/20 | Train Loss: 0.3286 | Val Loss: 0.2752 | MAE: 0.1922 | RMSE: 0.2752 | RÂ²: 0.9189\n",
      "Epoch 14/20 | Train Loss: 0.3307 | Val Loss: 0.2727 | MAE: 0.1919 | RMSE: 0.2727 | RÂ²: 0.9204\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2590 | MAE = 0.1743\n",
      "\n",
      "ðŸ”§ [56/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[512, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8982 | Val Loss: 0.8458 | MAE: 0.7014 | RMSE: 0.8458 | RÂ²: 0.2342\n",
      "Epoch 2/20 | Train Loss: 0.5372 | Val Loss: 0.5833 | MAE: 0.4815 | RMSE: 0.5833 | RÂ²: 0.6357\n",
      "Epoch 3/20 | Train Loss: 0.4413 | Val Loss: 0.3656 | MAE: 0.2928 | RMSE: 0.3656 | RÂ²: 0.8569\n",
      "Epoch 4/20 | Train Loss: 0.3976 | Val Loss: 0.3128 | MAE: 0.2275 | RMSE: 0.3128 | RÂ²: 0.8953\n",
      "Epoch 5/20 | Train Loss: 0.3632 | Val Loss: 0.3635 | MAE: 0.2897 | RMSE: 0.3635 | RÂ²: 0.8585\n",
      "Epoch 6/20 | Train Loss: 0.3461 | Val Loss: 0.3076 | MAE: 0.2258 | RMSE: 0.3076 | RÂ²: 0.8987\n",
      "Epoch 7/20 | Train Loss: 0.3259 | Val Loss: 0.3424 | MAE: 0.2668 | RMSE: 0.3424 | RÂ²: 0.8745\n",
      "Epoch 8/20 | Train Loss: 0.3119 | Val Loss: 0.2818 | MAE: 0.2058 | RMSE: 0.2818 | RÂ²: 0.9150\n",
      "Epoch 9/20 | Train Loss: 0.3052 | Val Loss: 0.2841 | MAE: 0.2109 | RMSE: 0.2841 | RÂ²: 0.9136\n",
      "Epoch 10/20 | Train Loss: 0.2963 | Val Loss: 0.2584 | MAE: 0.1827 | RMSE: 0.2584 | RÂ²: 0.9285\n",
      "Epoch 11/20 | Train Loss: 0.2925 | Val Loss: 0.2581 | MAE: 0.1854 | RMSE: 0.2581 | RÂ²: 0.9287\n",
      "Epoch 12/20 | Train Loss: 0.2839 | Val Loss: 0.2708 | MAE: 0.1932 | RMSE: 0.2708 | RÂ²: 0.9215\n",
      "Epoch 13/20 | Train Loss: 0.2792 | Val Loss: 0.2533 | MAE: 0.1747 | RMSE: 0.2533 | RÂ²: 0.9313\n",
      "Epoch 14/20 | Train Loss: 0.2748 | Val Loss: 0.2695 | MAE: 0.1903 | RMSE: 0.2695 | RÂ²: 0.9222\n",
      "Epoch 15/20 | Train Loss: 0.2696 | Val Loss: 0.2583 | MAE: 0.1796 | RMSE: 0.2583 | RÂ²: 0.9286\n",
      "Epoch 16/20 | Train Loss: 0.2670 | Val Loss: 0.2625 | MAE: 0.1840 | RMSE: 0.2625 | RÂ²: 0.9262\n",
      "Epoch 17/20 | Train Loss: 0.2622 | Val Loss: 0.2844 | MAE: 0.2035 | RMSE: 0.2844 | RÂ²: 0.9134\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2533 | MAE = 0.1747\n",
      "\n",
      "ðŸ”§ [57/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[512, 512, 512], seed=1009\n",
      "Epoch 1/20 | Train Loss: 1.0484 | Val Loss: 0.8696 | MAE: 0.7279 | RMSE: 0.8696 | RÂ²: 0.1904\n",
      "Epoch 2/20 | Train Loss: 0.5167 | Val Loss: 0.6723 | MAE: 0.5619 | RMSE: 0.6723 | RÂ²: 0.5160\n",
      "Epoch 3/20 | Train Loss: 0.4033 | Val Loss: 0.3784 | MAE: 0.3002 | RMSE: 0.3784 | RÂ²: 0.8467\n",
      "Epoch 4/20 | Train Loss: 0.3563 | Val Loss: 0.3585 | MAE: 0.2869 | RMSE: 0.3585 | RÂ²: 0.8624\n",
      "Epoch 5/20 | Train Loss: 0.3343 | Val Loss: 0.3059 | MAE: 0.1990 | RMSE: 0.3059 | RÂ²: 0.8998\n",
      "Epoch 6/20 | Train Loss: 0.3178 | Val Loss: 0.2643 | MAE: 0.1866 | RMSE: 0.2643 | RÂ²: 0.9252\n",
      "Epoch 7/20 | Train Loss: 0.3019 | Val Loss: 0.3014 | MAE: 0.2136 | RMSE: 0.3014 | RÂ²: 0.9027\n",
      "Epoch 8/20 | Train Loss: 0.2950 | Val Loss: 0.2558 | MAE: 0.1775 | RMSE: 0.2558 | RÂ²: 0.9300\n",
      "Epoch 9/20 | Train Loss: 0.2856 | Val Loss: 0.2825 | MAE: 0.2023 | RMSE: 0.2825 | RÂ²: 0.9146\n",
      "Epoch 10/20 | Train Loss: 0.2798 | Val Loss: 0.2674 | MAE: 0.1881 | RMSE: 0.2674 | RÂ²: 0.9235\n",
      "Epoch 11/20 | Train Loss: 0.2722 | Val Loss: 0.2603 | MAE: 0.1782 | RMSE: 0.2603 | RÂ²: 0.9275\n",
      "Epoch 12/20 | Train Loss: 0.2700 | Val Loss: 0.2683 | MAE: 0.1876 | RMSE: 0.2683 | RÂ²: 0.9229\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2558 | MAE = 0.1775\n",
      "\n",
      "ðŸ”§ [58/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[512, 256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7978 | Val Loss: 0.9180 | MAE: 0.7752 | RMSE: 0.9180 | RÂ²: 0.0978\n",
      "Epoch 2/20 | Train Loss: 0.5318 | Val Loss: 0.7041 | MAE: 0.5921 | RMSE: 0.7041 | RÂ²: 0.4693\n",
      "Epoch 3/20 | Train Loss: 0.4316 | Val Loss: 0.5008 | MAE: 0.4122 | RMSE: 0.5008 | RÂ²: 0.7315\n",
      "Epoch 4/20 | Train Loss: 0.3840 | Val Loss: 0.3667 | MAE: 0.2905 | RMSE: 0.3667 | RÂ²: 0.8560\n",
      "Epoch 5/20 | Train Loss: 0.3565 | Val Loss: 0.3007 | MAE: 0.2177 | RMSE: 0.3007 | RÂ²: 0.9032\n",
      "Epoch 6/20 | Train Loss: 0.3413 | Val Loss: 0.2763 | MAE: 0.1928 | RMSE: 0.2763 | RÂ²: 0.9183\n",
      "Epoch 7/20 | Train Loss: 0.3289 | Val Loss: 0.2823 | MAE: 0.1985 | RMSE: 0.2823 | RÂ²: 0.9147\n",
      "Epoch 8/20 | Train Loss: 0.3150 | Val Loss: 0.2729 | MAE: 0.1941 | RMSE: 0.2729 | RÂ²: 0.9202\n",
      "Epoch 9/20 | Train Loss: 0.3078 | Val Loss: 0.2674 | MAE: 0.1895 | RMSE: 0.2674 | RÂ²: 0.9234\n",
      "Epoch 10/20 | Train Loss: 0.2951 | Val Loss: 0.2694 | MAE: 0.1838 | RMSE: 0.2694 | RÂ²: 0.9223\n",
      "Epoch 11/20 | Train Loss: 0.2931 | Val Loss: 0.2640 | MAE: 0.1821 | RMSE: 0.2640 | RÂ²: 0.9254\n",
      "Epoch 12/20 | Train Loss: 0.2862 | Val Loss: 0.2722 | MAE: 0.1888 | RMSE: 0.2722 | RÂ²: 0.9207\n",
      "Epoch 13/20 | Train Loss: 0.2819 | Val Loss: 0.2624 | MAE: 0.1790 | RMSE: 0.2624 | RÂ²: 0.9263\n",
      "Epoch 14/20 | Train Loss: 0.2775 | Val Loss: 0.2674 | MAE: 0.1854 | RMSE: 0.2674 | RÂ²: 0.9234\n",
      "Epoch 15/20 | Train Loss: 0.2721 | Val Loss: 0.2703 | MAE: 0.1860 | RMSE: 0.2703 | RÂ²: 0.9218\n",
      "Epoch 16/20 | Train Loss: 0.2724 | Val Loss: 0.2690 | MAE: 0.1870 | RMSE: 0.2690 | RÂ²: 0.9225\n",
      "Epoch 17/20 | Train Loss: 0.2660 | Val Loss: 0.2734 | MAE: 0.1892 | RMSE: 0.2734 | RÂ²: 0.9200\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2624 | MAE = 0.1790\n",
      "\n",
      "ðŸ”§ [59/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[512, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9727 | Val Loss: 0.9274 | MAE: 0.7849 | RMSE: 0.9274 | RÂ²: 0.0792\n",
      "Epoch 2/20 | Train Loss: 0.8342 | Val Loss: 0.8256 | MAE: 0.7012 | RMSE: 0.8256 | RÂ²: 0.2703\n",
      "Epoch 3/20 | Train Loss: 0.7561 | Val Loss: 0.7228 | MAE: 0.6132 | RMSE: 0.7228 | RÂ²: 0.4407\n",
      "Epoch 4/20 | Train Loss: 0.7084 | Val Loss: 0.6222 | MAE: 0.5184 | RMSE: 0.6222 | RÂ²: 0.5856\n",
      "Epoch 5/20 | Train Loss: 0.6564 | Val Loss: 0.5274 | MAE: 0.4225 | RMSE: 0.5274 | RÂ²: 0.7022\n",
      "Epoch 6/20 | Train Loss: 0.6213 | Val Loss: 0.4591 | MAE: 0.3513 | RMSE: 0.4591 | RÂ²: 0.7744\n",
      "Epoch 7/20 | Train Loss: 0.5933 | Val Loss: 0.4271 | MAE: 0.3080 | RMSE: 0.4271 | RÂ²: 0.8047\n",
      "Epoch 8/20 | Train Loss: 0.5714 | Val Loss: 0.4097 | MAE: 0.2818 | RMSE: 0.4097 | RÂ²: 0.8203\n",
      "Epoch 9/20 | Train Loss: 0.5599 | Val Loss: 0.3820 | MAE: 0.2643 | RMSE: 0.3820 | RÂ²: 0.8438\n",
      "Epoch 10/20 | Train Loss: 0.5384 | Val Loss: 0.3481 | MAE: 0.2533 | RMSE: 0.3481 | RÂ²: 0.8703\n",
      "Epoch 11/20 | Train Loss: 0.5226 | Val Loss: 0.3417 | MAE: 0.2515 | RMSE: 0.3417 | RÂ²: 0.8750\n",
      "Epoch 12/20 | Train Loss: 0.5145 | Val Loss: 0.3502 | MAE: 0.2520 | RMSE: 0.3502 | RÂ²: 0.8687\n",
      "Epoch 13/20 | Train Loss: 0.5021 | Val Loss: 0.3618 | MAE: 0.2554 | RMSE: 0.3618 | RÂ²: 0.8599\n",
      "Epoch 14/20 | Train Loss: 0.4918 | Val Loss: 0.3541 | MAE: 0.2555 | RMSE: 0.3541 | RÂ²: 0.8658\n",
      "Epoch 15/20 | Train Loss: 0.4827 | Val Loss: 0.3532 | MAE: 0.2565 | RMSE: 0.3532 | RÂ²: 0.8665\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3417 | MAE = 0.2515\n",
      "\n",
      "ðŸ”§ [60/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8091 | Val Loss: 0.8101 | MAE: 0.6745 | RMSE: 0.8101 | RÂ²: 0.2974\n",
      "Epoch 2/20 | Train Loss: 0.3579 | Val Loss: 0.4650 | MAE: 0.3856 | RMSE: 0.4650 | RÂ²: 0.7686\n",
      "Epoch 3/20 | Train Loss: 0.3112 | Val Loss: 0.3537 | MAE: 0.2474 | RMSE: 0.3537 | RÂ²: 0.8661\n",
      "Epoch 4/20 | Train Loss: 0.2922 | Val Loss: 0.3266 | MAE: 0.2357 | RMSE: 0.3266 | RÂ²: 0.8858\n",
      "Epoch 5/20 | Train Loss: 0.2801 | Val Loss: 0.2703 | MAE: 0.1913 | RMSE: 0.2703 | RÂ²: 0.9218\n",
      "Epoch 6/20 | Train Loss: 0.2588 | Val Loss: 0.3340 | MAE: 0.2521 | RMSE: 0.3340 | RÂ²: 0.8805\n",
      "Epoch 7/20 | Train Loss: 0.2572 | Val Loss: 0.2826 | MAE: 0.2027 | RMSE: 0.2826 | RÂ²: 0.9145\n",
      "Epoch 8/20 | Train Loss: 0.2582 | Val Loss: 0.2638 | MAE: 0.1843 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 9/20 | Train Loss: 0.2500 | Val Loss: 0.2512 | MAE: 0.1720 | RMSE: 0.2512 | RÂ²: 0.9325\n",
      "Epoch 10/20 | Train Loss: 0.2464 | Val Loss: 0.2811 | MAE: 0.1992 | RMSE: 0.2811 | RÂ²: 0.9154\n",
      "Epoch 11/20 | Train Loss: 0.2450 | Val Loss: 0.2745 | MAE: 0.1892 | RMSE: 0.2745 | RÂ²: 0.9193\n",
      "Epoch 12/20 | Train Loss: 0.2350 | Val Loss: 0.2795 | MAE: 0.1846 | RMSE: 0.2795 | RÂ²: 0.9164\n",
      "Epoch 13/20 | Train Loss: 0.2350 | Val Loss: 0.2647 | MAE: 0.1811 | RMSE: 0.2647 | RÂ²: 0.9250\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2512 | MAE = 0.1720\n",
      "\n",
      "ðŸ”§ [61/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8183 | Val Loss: 0.8413 | MAE: 0.7029 | RMSE: 0.8413 | RÂ²: 0.2423\n",
      "Epoch 2/20 | Train Loss: 0.4455 | Val Loss: 0.6240 | MAE: 0.5100 | RMSE: 0.6240 | RÂ²: 0.5831\n",
      "Epoch 3/20 | Train Loss: 0.3653 | Val Loss: 0.3493 | MAE: 0.2796 | RMSE: 0.3493 | RÂ²: 0.8694\n",
      "Epoch 4/20 | Train Loss: 0.3336 | Val Loss: 0.2936 | MAE: 0.2197 | RMSE: 0.2936 | RÂ²: 0.9077\n",
      "Epoch 5/20 | Train Loss: 0.3072 | Val Loss: 0.2580 | MAE: 0.1782 | RMSE: 0.2580 | RÂ²: 0.9287\n",
      "Epoch 6/20 | Train Loss: 0.2989 | Val Loss: 0.2837 | MAE: 0.2008 | RMSE: 0.2837 | RÂ²: 0.9138\n",
      "Epoch 7/20 | Train Loss: 0.2880 | Val Loss: 0.2544 | MAE: 0.1698 | RMSE: 0.2544 | RÂ²: 0.9307\n",
      "Epoch 8/20 | Train Loss: 0.2822 | Val Loss: 0.2760 | MAE: 0.1969 | RMSE: 0.2760 | RÂ²: 0.9185\n",
      "Epoch 9/20 | Train Loss: 0.2795 | Val Loss: 0.2578 | MAE: 0.1750 | RMSE: 0.2578 | RÂ²: 0.9289\n",
      "Epoch 10/20 | Train Loss: 0.2723 | Val Loss: 0.2647 | MAE: 0.1851 | RMSE: 0.2647 | RÂ²: 0.9250\n",
      "Epoch 11/20 | Train Loss: 0.2678 | Val Loss: 0.2638 | MAE: 0.1808 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2544 | MAE = 0.1698\n",
      "\n",
      "ðŸ”§ [62/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.9142 | Val Loss: 0.5555 | MAE: 0.4576 | RMSE: 0.5555 | RÂ²: 0.6697\n",
      "Epoch 2/20 | Train Loss: 0.3785 | Val Loss: 0.5669 | MAE: 0.3853 | RMSE: 0.5669 | RÂ²: 0.6559\n",
      "Epoch 3/20 | Train Loss: 0.3162 | Val Loss: 0.5675 | MAE: 0.4848 | RMSE: 0.5675 | RÂ²: 0.6552\n",
      "Epoch 4/20 | Train Loss: 0.2882 | Val Loss: 0.6386 | MAE: 0.5252 | RMSE: 0.6386 | RÂ²: 0.5634\n",
      "Epoch 5/20 | Train Loss: 0.2793 | Val Loss: 0.5359 | MAE: 0.4070 | RMSE: 0.5359 | RÂ²: 0.6926\n",
      "Epoch 6/20 | Train Loss: 0.2576 | Val Loss: 0.4413 | MAE: 0.3400 | RMSE: 0.4413 | RÂ²: 0.7915\n",
      "Epoch 7/20 | Train Loss: 0.2537 | Val Loss: 0.3877 | MAE: 0.2978 | RMSE: 0.3877 | RÂ²: 0.8391\n",
      "Epoch 8/20 | Train Loss: 0.2514 | Val Loss: 0.2996 | MAE: 0.2172 | RMSE: 0.2996 | RÂ²: 0.9039\n",
      "Epoch 9/20 | Train Loss: 0.2495 | Val Loss: 0.2637 | MAE: 0.1787 | RMSE: 0.2637 | RÂ²: 0.9256\n",
      "Epoch 10/20 | Train Loss: 0.2424 | Val Loss: 0.2813 | MAE: 0.1969 | RMSE: 0.2813 | RÂ²: 0.9153\n",
      "Epoch 11/20 | Train Loss: 0.2415 | Val Loss: 0.2867 | MAE: 0.2041 | RMSE: 0.2867 | RÂ²: 0.9120\n",
      "Epoch 12/20 | Train Loss: 0.2363 | Val Loss: 0.2798 | MAE: 0.1864 | RMSE: 0.2798 | RÂ²: 0.9162\n",
      "Epoch 13/20 | Train Loss: 0.2333 | Val Loss: 0.2638 | MAE: 0.1776 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2637 | MAE = 0.1787\n",
      "\n",
      "ðŸ”§ [63/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[512, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8826 | Val Loss: 0.9032 | MAE: 0.7673 | RMSE: 0.9032 | RÂ²: 0.1266\n",
      "Epoch 2/20 | Train Loss: 0.6544 | Val Loss: 0.7783 | MAE: 0.6552 | RMSE: 0.7783 | RÂ²: 0.3515\n",
      "Epoch 3/20 | Train Loss: 0.5440 | Val Loss: 0.5783 | MAE: 0.4771 | RMSE: 0.5783 | RÂ²: 0.6420\n",
      "Epoch 4/20 | Train Loss: 0.5026 | Val Loss: 0.4521 | MAE: 0.3528 | RMSE: 0.4521 | RÂ²: 0.7812\n",
      "Epoch 5/20 | Train Loss: 0.4678 | Val Loss: 0.3412 | MAE: 0.2618 | RMSE: 0.3412 | RÂ²: 0.8754\n",
      "Epoch 6/20 | Train Loss: 0.4466 | Val Loss: 0.3093 | MAE: 0.2309 | RMSE: 0.3093 | RÂ²: 0.8976\n",
      "Epoch 7/20 | Train Loss: 0.4231 | Val Loss: 0.3036 | MAE: 0.2249 | RMSE: 0.3036 | RÂ²: 0.9013\n",
      "Epoch 8/20 | Train Loss: 0.4138 | Val Loss: 0.3112 | MAE: 0.2275 | RMSE: 0.3112 | RÂ²: 0.8963\n",
      "Epoch 9/20 | Train Loss: 0.4002 | Val Loss: 0.3024 | MAE: 0.2247 | RMSE: 0.3024 | RÂ²: 0.9021\n",
      "Epoch 10/20 | Train Loss: 0.3886 | Val Loss: 0.2973 | MAE: 0.2192 | RMSE: 0.2973 | RÂ²: 0.9054\n",
      "Epoch 11/20 | Train Loss: 0.3780 | Val Loss: 0.2923 | MAE: 0.2164 | RMSE: 0.2923 | RÂ²: 0.9085\n",
      "Epoch 12/20 | Train Loss: 0.3741 | Val Loss: 0.2883 | MAE: 0.2125 | RMSE: 0.2883 | RÂ²: 0.9110\n",
      "Epoch 13/20 | Train Loss: 0.3621 | Val Loss: 0.2904 | MAE: 0.2109 | RMSE: 0.2904 | RÂ²: 0.9097\n",
      "Epoch 14/20 | Train Loss: 0.3616 | Val Loss: 0.2903 | MAE: 0.2131 | RMSE: 0.2903 | RÂ²: 0.9098\n",
      "Epoch 15/20 | Train Loss: 0.3558 | Val Loss: 0.2815 | MAE: 0.1996 | RMSE: 0.2815 | RÂ²: 0.9152\n",
      "Epoch 16/20 | Train Loss: 0.3544 | Val Loss: 0.2903 | MAE: 0.2022 | RMSE: 0.2903 | RÂ²: 0.9098\n",
      "Epoch 17/20 | Train Loss: 0.3420 | Val Loss: 0.2893 | MAE: 0.2080 | RMSE: 0.2893 | RÂ²: 0.9104\n",
      "Epoch 18/20 | Train Loss: 0.3360 | Val Loss: 0.2777 | MAE: 0.1964 | RMSE: 0.2777 | RÂ²: 0.9174\n",
      "Epoch 19/20 | Train Loss: 0.3341 | Val Loss: 0.2749 | MAE: 0.1914 | RMSE: 0.2749 | RÂ²: 0.9191\n",
      "Epoch 20/20 | Train Loss: 0.3295 | Val Loss: 0.2729 | MAE: 0.1928 | RMSE: 0.2729 | RÂ²: 0.9202\n",
      "âœ… RMSE = 0.2729 | MAE = 0.1928\n",
      "\n",
      "ðŸ”§ [64/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.7447 | Val Loss: 0.8768 | MAE: 0.7340 | RMSE: 0.8768 | RÂ²: 0.1769\n",
      "Epoch 2/20 | Train Loss: 0.4066 | Val Loss: 0.6997 | MAE: 0.5858 | RMSE: 0.6997 | RÂ²: 0.4759\n",
      "Epoch 3/20 | Train Loss: 0.3376 | Val Loss: 0.4859 | MAE: 0.4050 | RMSE: 0.4859 | RÂ²: 0.7473\n",
      "Epoch 4/20 | Train Loss: 0.3029 | Val Loss: 0.3490 | MAE: 0.2797 | RMSE: 0.3490 | RÂ²: 0.8696\n",
      "Epoch 5/20 | Train Loss: 0.2797 | Val Loss: 0.2989 | MAE: 0.2283 | RMSE: 0.2989 | RÂ²: 0.9044\n",
      "Epoch 6/20 | Train Loss: 0.2666 | Val Loss: 0.2526 | MAE: 0.1778 | RMSE: 0.2526 | RÂ²: 0.9317\n",
      "Epoch 7/20 | Train Loss: 0.2581 | Val Loss: 0.2627 | MAE: 0.1794 | RMSE: 0.2627 | RÂ²: 0.9261\n",
      "Epoch 8/20 | Train Loss: 0.2547 | Val Loss: 0.2544 | MAE: 0.1748 | RMSE: 0.2544 | RÂ²: 0.9307\n",
      "Epoch 9/20 | Train Loss: 0.2532 | Val Loss: 0.2720 | MAE: 0.1869 | RMSE: 0.2720 | RÂ²: 0.9208\n",
      "Epoch 10/20 | Train Loss: 0.2525 | Val Loss: 0.2672 | MAE: 0.1839 | RMSE: 0.2672 | RÂ²: 0.9236\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2526 | MAE = 0.1778\n",
      "\n",
      "ðŸ”§ [65/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[2048, 1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9822 | Val Loss: 0.8960 | MAE: 0.7571 | RMSE: 0.8960 | RÂ²: 0.1406\n",
      "Epoch 2/20 | Train Loss: 0.7693 | Val Loss: 0.7609 | MAE: 0.6386 | RMSE: 0.7609 | RÂ²: 0.3802\n",
      "Epoch 3/20 | Train Loss: 0.6770 | Val Loss: 0.5507 | MAE: 0.4508 | RMSE: 0.5507 | RÂ²: 0.6754\n",
      "Epoch 4/20 | Train Loss: 0.6017 | Val Loss: 0.4543 | MAE: 0.3300 | RMSE: 0.4543 | RÂ²: 0.7791\n",
      "Epoch 5/20 | Train Loss: 0.5922 | Val Loss: 0.4030 | MAE: 0.2719 | RMSE: 0.4030 | RÂ²: 0.8262\n",
      "Epoch 6/20 | Train Loss: 0.5390 | Val Loss: 0.3842 | MAE: 0.2778 | RMSE: 0.3842 | RÂ²: 0.8420\n",
      "Epoch 7/20 | Train Loss: 0.5186 | Val Loss: 0.3969 | MAE: 0.2915 | RMSE: 0.3969 | RÂ²: 0.8314\n",
      "Epoch 8/20 | Train Loss: 0.4944 | Val Loss: 0.3988 | MAE: 0.2889 | RMSE: 0.3988 | RÂ²: 0.8298\n",
      "Epoch 9/20 | Train Loss: 0.4729 | Val Loss: 0.4018 | MAE: 0.2992 | RMSE: 0.4018 | RÂ²: 0.8272\n",
      "Epoch 10/20 | Train Loss: 0.4536 | Val Loss: 0.4243 | MAE: 0.3209 | RMSE: 0.4243 | RÂ²: 0.8073\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3842 | MAE = 0.2778\n",
      "\n",
      "ðŸ”§ [66/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9558 | Val Loss: 0.8679 | MAE: 0.7368 | RMSE: 0.8679 | RÂ²: 0.1937\n",
      "Epoch 2/20 | Train Loss: 0.7549 | Val Loss: 0.7451 | MAE: 0.6328 | RMSE: 0.7451 | RÂ²: 0.4057\n",
      "Epoch 3/20 | Train Loss: 0.6401 | Val Loss: 0.6338 | MAE: 0.5364 | RMSE: 0.6338 | RÂ²: 0.5700\n",
      "Epoch 4/20 | Train Loss: 0.5650 | Val Loss: 0.5421 | MAE: 0.4413 | RMSE: 0.5421 | RÂ²: 0.6854\n",
      "Epoch 5/20 | Train Loss: 0.5112 | Val Loss: 0.4399 | MAE: 0.3444 | RMSE: 0.4399 | RÂ²: 0.7929\n",
      "Epoch 6/20 | Train Loss: 0.4791 | Val Loss: 0.3723 | MAE: 0.2847 | RMSE: 0.3723 | RÂ²: 0.8516\n",
      "Epoch 7/20 | Train Loss: 0.4571 | Val Loss: 0.3529 | MAE: 0.2652 | RMSE: 0.3529 | RÂ²: 0.8667\n",
      "Epoch 8/20 | Train Loss: 0.4349 | Val Loss: 0.3381 | MAE: 0.2527 | RMSE: 0.3381 | RÂ²: 0.8776\n",
      "Epoch 9/20 | Train Loss: 0.4144 | Val Loss: 0.3228 | MAE: 0.2420 | RMSE: 0.3228 | RÂ²: 0.8885\n",
      "Epoch 10/20 | Train Loss: 0.4043 | Val Loss: 0.3193 | MAE: 0.2384 | RMSE: 0.3193 | RÂ²: 0.8909\n",
      "Epoch 11/20 | Train Loss: 0.3898 | Val Loss: 0.3112 | MAE: 0.2315 | RMSE: 0.3112 | RÂ²: 0.8963\n",
      "Epoch 12/20 | Train Loss: 0.3906 | Val Loss: 0.3047 | MAE: 0.2236 | RMSE: 0.3047 | RÂ²: 0.9006\n",
      "Epoch 13/20 | Train Loss: 0.3789 | Val Loss: 0.3019 | MAE: 0.2222 | RMSE: 0.3019 | RÂ²: 0.9024\n",
      "Epoch 14/20 | Train Loss: 0.3689 | Val Loss: 0.2992 | MAE: 0.2184 | RMSE: 0.2992 | RÂ²: 0.9042\n",
      "Epoch 15/20 | Train Loss: 0.3670 | Val Loss: 0.2960 | MAE: 0.2154 | RMSE: 0.2960 | RÂ²: 0.9062\n",
      "Epoch 16/20 | Train Loss: 0.3556 | Val Loss: 0.2947 | MAE: 0.2138 | RMSE: 0.2947 | RÂ²: 0.9071\n",
      "Epoch 17/20 | Train Loss: 0.3526 | Val Loss: 0.2904 | MAE: 0.2095 | RMSE: 0.2904 | RÂ²: 0.9097\n",
      "Epoch 18/20 | Train Loss: 0.3471 | Val Loss: 0.2887 | MAE: 0.2075 | RMSE: 0.2887 | RÂ²: 0.9108\n",
      "Epoch 19/20 | Train Loss: 0.3483 | Val Loss: 0.2878 | MAE: 0.2058 | RMSE: 0.2878 | RÂ²: 0.9113\n",
      "Epoch 20/20 | Train Loss: 0.3367 | Val Loss: 0.2899 | MAE: 0.2058 | RMSE: 0.2899 | RÂ²: 0.9100\n",
      "âœ… RMSE = 0.2878 | MAE = 0.2058\n",
      "\n",
      "ðŸ”§ [67/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[256, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 1.0341 | Val Loss: 0.8727 | MAE: 0.7393 | RMSE: 0.8727 | RÂ²: 0.1846\n",
      "Epoch 2/20 | Train Loss: 0.8592 | Val Loss: 0.7424 | MAE: 0.6211 | RMSE: 0.7424 | RÂ²: 0.4099\n",
      "Epoch 3/20 | Train Loss: 0.7930 | Val Loss: 0.6349 | MAE: 0.5274 | RMSE: 0.6349 | RÂ²: 0.5685\n",
      "Epoch 4/20 | Train Loss: 0.7380 | Val Loss: 0.5554 | MAE: 0.4591 | RMSE: 0.5554 | RÂ²: 0.6698\n",
      "Epoch 5/20 | Train Loss: 0.6942 | Val Loss: 0.5064 | MAE: 0.4082 | RMSE: 0.5064 | RÂ²: 0.7254\n",
      "Epoch 6/20 | Train Loss: 0.6517 | Val Loss: 0.4589 | MAE: 0.3581 | RMSE: 0.4589 | RÂ²: 0.7746\n",
      "Epoch 7/20 | Train Loss: 0.6375 | Val Loss: 0.4056 | MAE: 0.3099 | RMSE: 0.4056 | RÂ²: 0.8239\n",
      "Epoch 8/20 | Train Loss: 0.6038 | Val Loss: 0.3688 | MAE: 0.2780 | RMSE: 0.3688 | RÂ²: 0.8544\n",
      "Epoch 9/20 | Train Loss: 0.5827 | Val Loss: 0.3491 | MAE: 0.2605 | RMSE: 0.3491 | RÂ²: 0.8695\n",
      "Epoch 10/20 | Train Loss: 0.5642 | Val Loss: 0.3351 | MAE: 0.2482 | RMSE: 0.3351 | RÂ²: 0.8798\n",
      "Epoch 11/20 | Train Loss: 0.5413 | Val Loss: 0.3259 | MAE: 0.2403 | RMSE: 0.3259 | RÂ²: 0.8863\n",
      "Epoch 12/20 | Train Loss: 0.5389 | Val Loss: 0.3224 | MAE: 0.2368 | RMSE: 0.3224 | RÂ²: 0.8887\n",
      "Epoch 13/20 | Train Loss: 0.5231 | Val Loss: 0.3235 | MAE: 0.2360 | RMSE: 0.3235 | RÂ²: 0.8880\n",
      "Epoch 14/20 | Train Loss: 0.5160 | Val Loss: 0.3249 | MAE: 0.2348 | RMSE: 0.3249 | RÂ²: 0.8870\n",
      "Epoch 15/20 | Train Loss: 0.5042 | Val Loss: 0.3234 | MAE: 0.2328 | RMSE: 0.3234 | RÂ²: 0.8880\n",
      "Epoch 16/20 | Train Loss: 0.4914 | Val Loss: 0.3214 | MAE: 0.2301 | RMSE: 0.3214 | RÂ²: 0.8894\n",
      "Epoch 17/20 | Train Loss: 0.4874 | Val Loss: 0.3188 | MAE: 0.2274 | RMSE: 0.3188 | RÂ²: 0.8912\n",
      "Epoch 18/20 | Train Loss: 0.4759 | Val Loss: 0.3150 | MAE: 0.2250 | RMSE: 0.3150 | RÂ²: 0.8938\n",
      "Epoch 19/20 | Train Loss: 0.4690 | Val Loss: 0.3127 | MAE: 0.2230 | RMSE: 0.3127 | RÂ²: 0.8953\n",
      "Epoch 20/20 | Train Loss: 0.4607 | Val Loss: 0.3154 | MAE: 0.2230 | RMSE: 0.3154 | RÂ²: 0.8935\n",
      "âœ… RMSE = 0.3127 | MAE = 0.2230\n",
      "\n",
      "ðŸ”§ [68/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[1024, 256, 1024], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.7686 | Val Loss: 0.8800 | MAE: 0.7400 | RMSE: 0.8800 | RÂ²: 0.1710\n",
      "Epoch 2/20 | Train Loss: 0.4493 | Val Loss: 0.7261 | MAE: 0.6102 | RMSE: 0.7261 | RÂ²: 0.4355\n",
      "Epoch 3/20 | Train Loss: 0.3692 | Val Loss: 0.5304 | MAE: 0.4364 | RMSE: 0.5304 | RÂ²: 0.6989\n",
      "Epoch 4/20 | Train Loss: 0.3375 | Val Loss: 0.4051 | MAE: 0.3260 | RMSE: 0.4051 | RÂ²: 0.8243\n",
      "Epoch 5/20 | Train Loss: 0.3194 | Val Loss: 0.3061 | MAE: 0.2310 | RMSE: 0.3061 | RÂ²: 0.8997\n",
      "Epoch 6/20 | Train Loss: 0.3022 | Val Loss: 0.2805 | MAE: 0.2071 | RMSE: 0.2805 | RÂ²: 0.9158\n",
      "Epoch 7/20 | Train Loss: 0.2859 | Val Loss: 0.2677 | MAE: 0.1893 | RMSE: 0.2677 | RÂ²: 0.9233\n",
      "Epoch 8/20 | Train Loss: 0.2793 | Val Loss: 0.2656 | MAE: 0.1841 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "Epoch 9/20 | Train Loss: 0.2708 | Val Loss: 0.2727 | MAE: 0.1921 | RMSE: 0.2727 | RÂ²: 0.9204\n",
      "Epoch 10/20 | Train Loss: 0.2670 | Val Loss: 0.2644 | MAE: 0.1815 | RMSE: 0.2644 | RÂ²: 0.9251\n",
      "Epoch 11/20 | Train Loss: 0.2573 | Val Loss: 0.2630 | MAE: 0.1802 | RMSE: 0.2630 | RÂ²: 0.9260\n",
      "Epoch 12/20 | Train Loss: 0.2563 | Val Loss: 0.2583 | MAE: 0.1764 | RMSE: 0.2583 | RÂ²: 0.9286\n",
      "Epoch 13/20 | Train Loss: 0.2512 | Val Loss: 0.2718 | MAE: 0.1904 | RMSE: 0.2718 | RÂ²: 0.9209\n",
      "Epoch 14/20 | Train Loss: 0.2505 | Val Loss: 0.2531 | MAE: 0.1713 | RMSE: 0.2531 | RÂ²: 0.9314\n",
      "Epoch 15/20 | Train Loss: 0.2513 | Val Loss: 0.2608 | MAE: 0.1781 | RMSE: 0.2608 | RÂ²: 0.9272\n",
      "Epoch 16/20 | Train Loss: 0.2499 | Val Loss: 0.2569 | MAE: 0.1740 | RMSE: 0.2569 | RÂ²: 0.9294\n",
      "Epoch 17/20 | Train Loss: 0.2428 | Val Loss: 0.2801 | MAE: 0.2013 | RMSE: 0.2801 | RÂ²: 0.9160\n",
      "Epoch 18/20 | Train Loss: 0.2416 | Val Loss: 0.2747 | MAE: 0.1918 | RMSE: 0.2747 | RÂ²: 0.9192\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2531 | MAE = 0.1713\n",
      "\n",
      "ðŸ”§ [69/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[2048, 1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.7472 | Val Loss: 0.9471 | MAE: 0.7959 | RMSE: 0.9471 | RÂ²: 0.0396\n",
      "Epoch 2/20 | Train Loss: 0.4165 | Val Loss: 0.8050 | MAE: 0.6803 | RMSE: 0.8050 | RÂ²: 0.3062\n",
      "Epoch 3/20 | Train Loss: 0.3498 | Val Loss: 0.6296 | MAE: 0.5304 | RMSE: 0.6296 | RÂ²: 0.5757\n",
      "Epoch 4/20 | Train Loss: 0.3170 | Val Loss: 0.4469 | MAE: 0.3683 | RMSE: 0.4469 | RÂ²: 0.7862\n",
      "Epoch 5/20 | Train Loss: 0.2918 | Val Loss: 0.3074 | MAE: 0.2384 | RMSE: 0.3074 | RÂ²: 0.8988\n",
      "Epoch 6/20 | Train Loss: 0.2806 | Val Loss: 0.2646 | MAE: 0.1872 | RMSE: 0.2646 | RÂ²: 0.9250\n",
      "Epoch 7/20 | Train Loss: 0.2714 | Val Loss: 0.2446 | MAE: 0.1710 | RMSE: 0.2446 | RÂ²: 0.9359\n",
      "Epoch 8/20 | Train Loss: 0.2689 | Val Loss: 0.2694 | MAE: 0.1801 | RMSE: 0.2694 | RÂ²: 0.9223\n",
      "Epoch 9/20 | Train Loss: 0.2662 | Val Loss: 0.2549 | MAE: 0.1785 | RMSE: 0.2549 | RÂ²: 0.9304\n",
      "Epoch 10/20 | Train Loss: 0.2586 | Val Loss: 0.2615 | MAE: 0.1816 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 11/20 | Train Loss: 0.2585 | Val Loss: 0.2547 | MAE: 0.1753 | RMSE: 0.2547 | RÂ²: 0.9306\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2446 | MAE = 0.1710\n",
      "\n",
      "ðŸ”§ [70/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[1024, 256, 1024], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8712 | Val Loss: 0.8965 | MAE: 0.7585 | RMSE: 0.8965 | RÂ²: 0.1396\n",
      "Epoch 2/20 | Train Loss: 0.6437 | Val Loss: 0.8036 | MAE: 0.6754 | RMSE: 0.8036 | RÂ²: 0.3086\n",
      "Epoch 3/20 | Train Loss: 0.5383 | Val Loss: 0.6606 | MAE: 0.5471 | RMSE: 0.6606 | RÂ²: 0.5328\n",
      "Epoch 4/20 | Train Loss: 0.4945 | Val Loss: 0.5363 | MAE: 0.4245 | RMSE: 0.5363 | RÂ²: 0.6921\n",
      "Epoch 5/20 | Train Loss: 0.4617 | Val Loss: 0.3973 | MAE: 0.3137 | RMSE: 0.3973 | RÂ²: 0.8310\n",
      "Epoch 6/20 | Train Loss: 0.4373 | Val Loss: 0.3509 | MAE: 0.2612 | RMSE: 0.3509 | RÂ²: 0.8682\n",
      "Epoch 7/20 | Train Loss: 0.4157 | Val Loss: 0.3170 | MAE: 0.2320 | RMSE: 0.3170 | RÂ²: 0.8925\n",
      "Epoch 8/20 | Train Loss: 0.4014 | Val Loss: 0.2901 | MAE: 0.2088 | RMSE: 0.2901 | RÂ²: 0.9099\n",
      "Epoch 9/20 | Train Loss: 0.3895 | Val Loss: 0.2944 | MAE: 0.2052 | RMSE: 0.2944 | RÂ²: 0.9072\n",
      "Epoch 10/20 | Train Loss: 0.3793 | Val Loss: 0.2820 | MAE: 0.1987 | RMSE: 0.2820 | RÂ²: 0.9149\n",
      "Epoch 11/20 | Train Loss: 0.3787 | Val Loss: 0.2809 | MAE: 0.1979 | RMSE: 0.2809 | RÂ²: 0.9155\n",
      "Epoch 12/20 | Train Loss: 0.3655 | Val Loss: 0.2766 | MAE: 0.1953 | RMSE: 0.2766 | RÂ²: 0.9181\n",
      "Epoch 13/20 | Train Loss: 0.3562 | Val Loss: 0.2772 | MAE: 0.1964 | RMSE: 0.2772 | RÂ²: 0.9177\n",
      "Epoch 14/20 | Train Loss: 0.3587 | Val Loss: 0.2743 | MAE: 0.1907 | RMSE: 0.2743 | RÂ²: 0.9194\n",
      "Epoch 15/20 | Train Loss: 0.3502 | Val Loss: 0.2750 | MAE: 0.1898 | RMSE: 0.2750 | RÂ²: 0.9191\n",
      "Epoch 16/20 | Train Loss: 0.3512 | Val Loss: 0.2791 | MAE: 0.1947 | RMSE: 0.2791 | RÂ²: 0.9166\n",
      "Epoch 17/20 | Train Loss: 0.3434 | Val Loss: 0.2743 | MAE: 0.1881 | RMSE: 0.2743 | RÂ²: 0.9195\n",
      "Epoch 18/20 | Train Loss: 0.3364 | Val Loss: 0.2957 | MAE: 0.1942 | RMSE: 0.2957 | RÂ²: 0.9064\n",
      "Epoch 19/20 | Train Loss: 0.3388 | Val Loss: 0.2823 | MAE: 0.1937 | RMSE: 0.2823 | RÂ²: 0.9147\n",
      "Epoch 20/20 | Train Loss: 0.3336 | Val Loss: 0.2828 | MAE: 0.1916 | RMSE: 0.2828 | RÂ²: 0.9144\n",
      "âœ… RMSE = 0.2743 | MAE = 0.1881\n",
      "\n",
      "ðŸ”§ [71/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9806 | Val Loss: 0.8609 | MAE: 0.7302 | RMSE: 0.8609 | RÂ²: 0.2065\n",
      "Epoch 2/20 | Train Loss: 0.7879 | Val Loss: 0.7284 | MAE: 0.6171 | RMSE: 0.7284 | RÂ²: 0.4320\n",
      "Epoch 3/20 | Train Loss: 0.6996 | Val Loss: 0.5801 | MAE: 0.4780 | RMSE: 0.5801 | RÂ²: 0.6398\n",
      "Epoch 4/20 | Train Loss: 0.6325 | Val Loss: 0.4479 | MAE: 0.3458 | RMSE: 0.4479 | RÂ²: 0.7852\n",
      "Epoch 5/20 | Train Loss: 0.5914 | Val Loss: 0.4017 | MAE: 0.2842 | RMSE: 0.4017 | RÂ²: 0.8273\n",
      "Epoch 6/20 | Train Loss: 0.5638 | Val Loss: 0.3851 | MAE: 0.2671 | RMSE: 0.3851 | RÂ²: 0.8413\n",
      "Epoch 7/20 | Train Loss: 0.5426 | Val Loss: 0.3847 | MAE: 0.2719 | RMSE: 0.3847 | RÂ²: 0.8416\n",
      "Epoch 8/20 | Train Loss: 0.5162 | Val Loss: 0.3797 | MAE: 0.2758 | RMSE: 0.3797 | RÂ²: 0.8457\n",
      "Epoch 9/20 | Train Loss: 0.4945 | Val Loss: 0.3926 | MAE: 0.2808 | RMSE: 0.3926 | RÂ²: 0.8350\n",
      "Epoch 10/20 | Train Loss: 0.4808 | Val Loss: 0.3983 | MAE: 0.2935 | RMSE: 0.3983 | RÂ²: 0.8302\n",
      "Epoch 11/20 | Train Loss: 0.4668 | Val Loss: 0.3992 | MAE: 0.3031 | RMSE: 0.3992 | RÂ²: 0.8294\n",
      "Epoch 12/20 | Train Loss: 0.4587 | Val Loss: 0.3983 | MAE: 0.2974 | RMSE: 0.3983 | RÂ²: 0.8302\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3797 | MAE = 0.2758\n",
      "\n",
      "ðŸ”§ [72/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[256, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8214 | Val Loss: 0.7619 | MAE: 0.6283 | RMSE: 0.7619 | RÂ²: 0.3785\n",
      "Epoch 2/20 | Train Loss: 0.5450 | Val Loss: 0.5130 | MAE: 0.4003 | RMSE: 0.5130 | RÂ²: 0.7182\n",
      "Epoch 3/20 | Train Loss: 0.4617 | Val Loss: 0.3466 | MAE: 0.2655 | RMSE: 0.3466 | RÂ²: 0.8714\n",
      "Epoch 4/20 | Train Loss: 0.4202 | Val Loss: 0.3420 | MAE: 0.2568 | RMSE: 0.3420 | RÂ²: 0.8748\n",
      "Epoch 5/20 | Train Loss: 0.3860 | Val Loss: 0.3151 | MAE: 0.2330 | RMSE: 0.3151 | RÂ²: 0.8937\n",
      "Epoch 6/20 | Train Loss: 0.3653 | Val Loss: 0.3000 | MAE: 0.2188 | RMSE: 0.3000 | RÂ²: 0.9037\n",
      "Epoch 7/20 | Train Loss: 0.3492 | Val Loss: 0.2773 | MAE: 0.1974 | RMSE: 0.2773 | RÂ²: 0.9177\n",
      "Epoch 8/20 | Train Loss: 0.3349 | Val Loss: 0.2717 | MAE: 0.1914 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "Epoch 9/20 | Train Loss: 0.3215 | Val Loss: 0.2614 | MAE: 0.1830 | RMSE: 0.2614 | RÂ²: 0.9269\n",
      "Epoch 10/20 | Train Loss: 0.3156 | Val Loss: 0.2615 | MAE: 0.1808 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 11/20 | Train Loss: 0.3069 | Val Loss: 0.2660 | MAE: 0.1840 | RMSE: 0.2660 | RÂ²: 0.9242\n",
      "Epoch 12/20 | Train Loss: 0.2981 | Val Loss: 0.2589 | MAE: 0.1771 | RMSE: 0.2589 | RÂ²: 0.9283\n",
      "Epoch 13/20 | Train Loss: 0.2941 | Val Loss: 0.2651 | MAE: 0.1821 | RMSE: 0.2651 | RÂ²: 0.9247\n",
      "Epoch 14/20 | Train Loss: 0.2861 | Val Loss: 0.2638 | MAE: 0.1811 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 15/20 | Train Loss: 0.2837 | Val Loss: 0.2666 | MAE: 0.1828 | RMSE: 0.2666 | RÂ²: 0.9239\n",
      "Epoch 16/20 | Train Loss: 0.2787 | Val Loss: 0.2635 | MAE: 0.1806 | RMSE: 0.2635 | RÂ²: 0.9257\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2589 | MAE = 0.1771\n",
      "\n",
      "ðŸ”§ [73/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8495 | Val Loss: 0.7489 | MAE: 0.6170 | RMSE: 0.7489 | RÂ²: 0.3996\n",
      "Epoch 2/20 | Train Loss: 0.4162 | Val Loss: 0.4808 | MAE: 0.3898 | RMSE: 0.4808 | RÂ²: 0.7525\n",
      "Epoch 3/20 | Train Loss: 0.3563 | Val Loss: 0.3074 | MAE: 0.2292 | RMSE: 0.3074 | RÂ²: 0.8988\n",
      "Epoch 4/20 | Train Loss: 0.3304 | Val Loss: 0.2810 | MAE: 0.2048 | RMSE: 0.2810 | RÂ²: 0.9155\n",
      "Epoch 5/20 | Train Loss: 0.3086 | Val Loss: 0.3014 | MAE: 0.2266 | RMSE: 0.3014 | RÂ²: 0.9027\n",
      "Epoch 6/20 | Train Loss: 0.2970 | Val Loss: 0.3020 | MAE: 0.2191 | RMSE: 0.3020 | RÂ²: 0.9023\n",
      "Epoch 7/20 | Train Loss: 0.2871 | Val Loss: 0.2857 | MAE: 0.1912 | RMSE: 0.2857 | RÂ²: 0.9126\n",
      "Epoch 8/20 | Train Loss: 0.2915 | Val Loss: 0.2974 | MAE: 0.2183 | RMSE: 0.2974 | RÂ²: 0.9053\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2810 | MAE = 0.2048\n",
      "\n",
      "ðŸ”§ [74/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[2048, 1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.0301 | Val Loss: 0.7373 | MAE: 0.6057 | RMSE: 0.7373 | RÂ²: 0.4180\n",
      "Epoch 2/20 | Train Loss: 0.5146 | Val Loss: 0.4377 | MAE: 0.3464 | RMSE: 0.4377 | RÂ²: 0.7949\n",
      "Epoch 3/20 | Train Loss: 0.4374 | Val Loss: 0.5337 | MAE: 0.4318 | RMSE: 0.5337 | RÂ²: 0.6950\n",
      "Epoch 4/20 | Train Loss: 0.3926 | Val Loss: 0.4703 | MAE: 0.3887 | RMSE: 0.4703 | RÂ²: 0.7633\n",
      "Epoch 5/20 | Train Loss: 0.3661 | Val Loss: 0.4573 | MAE: 0.3531 | RMSE: 0.4573 | RÂ²: 0.7761\n",
      "Epoch 6/20 | Train Loss: 0.3498 | Val Loss: 0.3461 | MAE: 0.2727 | RMSE: 0.3461 | RÂ²: 0.8718\n",
      "Epoch 7/20 | Train Loss: 0.3317 | Val Loss: 0.3402 | MAE: 0.2325 | RMSE: 0.3402 | RÂ²: 0.8761\n",
      "Epoch 8/20 | Train Loss: 0.3258 | Val Loss: 0.2699 | MAE: 0.1903 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 9/20 | Train Loss: 0.3121 | Val Loss: 0.2666 | MAE: 0.1832 | RMSE: 0.2666 | RÂ²: 0.9239\n",
      "Epoch 10/20 | Train Loss: 0.2982 | Val Loss: 0.2566 | MAE: 0.1733 | RMSE: 0.2566 | RÂ²: 0.9295\n",
      "Epoch 11/20 | Train Loss: 0.2916 | Val Loss: 0.2676 | MAE: 0.1849 | RMSE: 0.2676 | RÂ²: 0.9233\n",
      "Epoch 12/20 | Train Loss: 0.2849 | Val Loss: 0.2744 | MAE: 0.1855 | RMSE: 0.2744 | RÂ²: 0.9194\n",
      "Epoch 13/20 | Train Loss: 0.2769 | Val Loss: 0.2598 | MAE: 0.1758 | RMSE: 0.2598 | RÂ²: 0.9277\n",
      "Epoch 14/20 | Train Loss: 0.2749 | Val Loss: 0.2606 | MAE: 0.1769 | RMSE: 0.2606 | RÂ²: 0.9273\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2566 | MAE = 0.1733\n",
      "\n",
      "ðŸ”§ [75/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.7839 | Val Loss: 0.7116 | MAE: 0.5960 | RMSE: 0.7116 | RÂ²: 0.4579\n",
      "Epoch 2/20 | Train Loss: 0.5303 | Val Loss: 0.4187 | MAE: 0.3453 | RMSE: 0.4187 | RÂ²: 0.8123\n",
      "Epoch 3/20 | Train Loss: 0.4524 | Val Loss: 0.3778 | MAE: 0.2881 | RMSE: 0.3778 | RÂ²: 0.8472\n",
      "Epoch 4/20 | Train Loss: 0.4141 | Val Loss: 0.3287 | MAE: 0.2413 | RMSE: 0.3287 | RÂ²: 0.8843\n",
      "Epoch 5/20 | Train Loss: 0.3882 | Val Loss: 0.3025 | MAE: 0.2234 | RMSE: 0.3025 | RÂ²: 0.9020\n",
      "Epoch 6/20 | Train Loss: 0.3665 | Val Loss: 0.2917 | MAE: 0.2140 | RMSE: 0.2917 | RÂ²: 0.9089\n",
      "Epoch 7/20 | Train Loss: 0.3573 | Val Loss: 0.2732 | MAE: 0.1958 | RMSE: 0.2732 | RÂ²: 0.9201\n",
      "Epoch 8/20 | Train Loss: 0.3419 | Val Loss: 0.2734 | MAE: 0.1947 | RMSE: 0.2734 | RÂ²: 0.9200\n",
      "Epoch 9/20 | Train Loss: 0.3281 | Val Loss: 0.2649 | MAE: 0.1867 | RMSE: 0.2649 | RÂ²: 0.9249\n",
      "Epoch 10/20 | Train Loss: 0.3207 | Val Loss: 0.2741 | MAE: 0.1959 | RMSE: 0.2741 | RÂ²: 0.9196\n",
      "Epoch 11/20 | Train Loss: 0.3143 | Val Loss: 0.2700 | MAE: 0.1928 | RMSE: 0.2700 | RÂ²: 0.9219\n",
      "Epoch 12/20 | Train Loss: 0.3070 | Val Loss: 0.2716 | MAE: 0.1928 | RMSE: 0.2716 | RÂ²: 0.9210\n",
      "Epoch 13/20 | Train Loss: 0.2973 | Val Loss: 0.2755 | MAE: 0.1974 | RMSE: 0.2755 | RÂ²: 0.9188\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2649 | MAE = 0.1867\n",
      "\n",
      "ðŸ”§ [76/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[256, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8630 | Val Loss: 0.7143 | MAE: 0.6011 | RMSE: 0.7143 | RÂ²: 0.4538\n",
      "Epoch 2/20 | Train Loss: 0.5916 | Val Loss: 0.4938 | MAE: 0.3965 | RMSE: 0.4938 | RÂ²: 0.7390\n",
      "Epoch 3/20 | Train Loss: 0.5056 | Val Loss: 0.3755 | MAE: 0.2928 | RMSE: 0.3755 | RÂ²: 0.8491\n",
      "Epoch 4/20 | Train Loss: 0.4550 | Val Loss: 0.3313 | MAE: 0.2487 | RMSE: 0.3313 | RÂ²: 0.8825\n",
      "Epoch 5/20 | Train Loss: 0.4312 | Val Loss: 0.3094 | MAE: 0.2309 | RMSE: 0.3094 | RÂ²: 0.8975\n",
      "Epoch 6/20 | Train Loss: 0.4012 | Val Loss: 0.3251 | MAE: 0.2313 | RMSE: 0.3251 | RÂ²: 0.8869\n",
      "Epoch 7/20 | Train Loss: 0.3837 | Val Loss: 0.2811 | MAE: 0.2009 | RMSE: 0.2811 | RÂ²: 0.9154\n",
      "Epoch 8/20 | Train Loss: 0.3659 | Val Loss: 0.2803 | MAE: 0.1985 | RMSE: 0.2803 | RÂ²: 0.9159\n",
      "Epoch 9/20 | Train Loss: 0.3506 | Val Loss: 0.2717 | MAE: 0.1916 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "Epoch 10/20 | Train Loss: 0.3434 | Val Loss: 0.2734 | MAE: 0.1923 | RMSE: 0.2734 | RÂ²: 0.9200\n",
      "Epoch 11/20 | Train Loss: 0.3315 | Val Loss: 0.2703 | MAE: 0.1886 | RMSE: 0.2703 | RÂ²: 0.9218\n",
      "Epoch 12/20 | Train Loss: 0.3260 | Val Loss: 0.2734 | MAE: 0.1906 | RMSE: 0.2734 | RÂ²: 0.9200\n",
      "Epoch 13/20 | Train Loss: 0.3180 | Val Loss: 0.2686 | MAE: 0.1849 | RMSE: 0.2686 | RÂ²: 0.9228\n",
      "Epoch 14/20 | Train Loss: 0.3114 | Val Loss: 0.2805 | MAE: 0.1966 | RMSE: 0.2805 | RÂ²: 0.9158\n",
      "Epoch 15/20 | Train Loss: 0.3069 | Val Loss: 0.2741 | MAE: 0.1900 | RMSE: 0.2741 | RÂ²: 0.9196\n",
      "Epoch 16/20 | Train Loss: 0.2975 | Val Loss: 0.2720 | MAE: 0.1888 | RMSE: 0.2720 | RÂ²: 0.9208\n",
      "Epoch 17/20 | Train Loss: 0.3011 | Val Loss: 0.2740 | MAE: 0.1886 | RMSE: 0.2740 | RÂ²: 0.9196\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2686 | MAE = 0.1849\n",
      "\n",
      "ðŸ”§ [77/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[512, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8207 | Val Loss: 0.8542 | MAE: 0.7173 | RMSE: 0.8542 | RÂ²: 0.2189\n",
      "Epoch 2/20 | Train Loss: 0.5591 | Val Loss: 0.6738 | MAE: 0.5496 | RMSE: 0.6738 | RÂ²: 0.5140\n",
      "Epoch 3/20 | Train Loss: 0.4683 | Val Loss: 0.4582 | MAE: 0.3766 | RMSE: 0.4582 | RÂ²: 0.7753\n",
      "Epoch 4/20 | Train Loss: 0.4209 | Val Loss: 0.3683 | MAE: 0.2855 | RMSE: 0.3683 | RÂ²: 0.8548\n",
      "Epoch 5/20 | Train Loss: 0.3920 | Val Loss: 0.3373 | MAE: 0.2332 | RMSE: 0.3373 | RÂ²: 0.8782\n",
      "Epoch 6/20 | Train Loss: 0.3743 | Val Loss: 0.3029 | MAE: 0.2174 | RMSE: 0.3029 | RÂ²: 0.9018\n",
      "Epoch 7/20 | Train Loss: 0.3563 | Val Loss: 0.2888 | MAE: 0.2095 | RMSE: 0.2888 | RÂ²: 0.9107\n",
      "Epoch 8/20 | Train Loss: 0.3497 | Val Loss: 0.2895 | MAE: 0.2108 | RMSE: 0.2895 | RÂ²: 0.9103\n",
      "Epoch 9/20 | Train Loss: 0.3401 | Val Loss: 0.2904 | MAE: 0.2167 | RMSE: 0.2904 | RÂ²: 0.9097\n",
      "Epoch 10/20 | Train Loss: 0.3306 | Val Loss: 0.2738 | MAE: 0.1950 | RMSE: 0.2738 | RÂ²: 0.9197\n",
      "Epoch 11/20 | Train Loss: 0.3198 | Val Loss: 0.2819 | MAE: 0.2055 | RMSE: 0.2819 | RÂ²: 0.9149\n",
      "Epoch 12/20 | Train Loss: 0.3078 | Val Loss: 0.2772 | MAE: 0.1935 | RMSE: 0.2772 | RÂ²: 0.9178\n",
      "Epoch 13/20 | Train Loss: 0.3073 | Val Loss: 0.2681 | MAE: 0.1879 | RMSE: 0.2681 | RÂ²: 0.9231\n",
      "Epoch 14/20 | Train Loss: 0.2973 | Val Loss: 0.2654 | MAE: 0.1836 | RMSE: 0.2654 | RÂ²: 0.9246\n",
      "Epoch 15/20 | Train Loss: 0.2907 | Val Loss: 0.2680 | MAE: 0.1893 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 16/20 | Train Loss: 0.2859 | Val Loss: 0.2631 | MAE: 0.1821 | RMSE: 0.2631 | RÂ²: 0.9259\n",
      "Epoch 17/20 | Train Loss: 0.2839 | Val Loss: 0.2682 | MAE: 0.1870 | RMSE: 0.2682 | RÂ²: 0.9230\n",
      "Epoch 18/20 | Train Loss: 0.2838 | Val Loss: 0.2761 | MAE: 0.1893 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 19/20 | Train Loss: 0.2799 | Val Loss: 0.2728 | MAE: 0.1885 | RMSE: 0.2728 | RÂ²: 0.9204\n",
      "Epoch 20/20 | Train Loss: 0.2772 | Val Loss: 0.2733 | MAE: 0.1897 | RMSE: 0.2733 | RÂ²: 0.9200\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2631 | MAE = 0.1821\n",
      "\n",
      "ðŸ”§ [78/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9712 | Val Loss: 0.8527 | MAE: 0.7124 | RMSE: 0.8527 | RÂ²: 0.2215\n",
      "Epoch 2/20 | Train Loss: 0.6511 | Val Loss: 0.4946 | MAE: 0.3751 | RMSE: 0.4946 | RÂ²: 0.7381\n",
      "Epoch 3/20 | Train Loss: 0.5487 | Val Loss: 0.4269 | MAE: 0.3229 | RMSE: 0.4269 | RÂ²: 0.8049\n",
      "Epoch 4/20 | Train Loss: 0.4896 | Val Loss: 0.4845 | MAE: 0.3921 | RMSE: 0.4845 | RÂ²: 0.7487\n",
      "Epoch 5/20 | Train Loss: 0.4746 | Val Loss: 0.4253 | MAE: 0.3198 | RMSE: 0.4253 | RÂ²: 0.8063\n",
      "Epoch 6/20 | Train Loss: 0.4483 | Val Loss: 0.3977 | MAE: 0.3113 | RMSE: 0.3977 | RÂ²: 0.8307\n",
      "Epoch 7/20 | Train Loss: 0.4140 | Val Loss: 0.3401 | MAE: 0.2625 | RMSE: 0.3401 | RÂ²: 0.8762\n",
      "Epoch 8/20 | Train Loss: 0.3913 | Val Loss: 0.2842 | MAE: 0.1927 | RMSE: 0.2842 | RÂ²: 0.9135\n",
      "Epoch 9/20 | Train Loss: 0.3832 | Val Loss: 0.3465 | MAE: 0.2651 | RMSE: 0.3465 | RÂ²: 0.8714\n",
      "Epoch 10/20 | Train Loss: 0.3767 | Val Loss: 0.3044 | MAE: 0.1960 | RMSE: 0.3044 | RÂ²: 0.9008\n",
      "Epoch 11/20 | Train Loss: 0.3529 | Val Loss: 0.3010 | MAE: 0.2166 | RMSE: 0.3010 | RÂ²: 0.9030\n",
      "Epoch 12/20 | Train Loss: 0.3494 | Val Loss: 0.2795 | MAE: 0.1920 | RMSE: 0.2795 | RÂ²: 0.9164\n",
      "Epoch 13/20 | Train Loss: 0.3405 | Val Loss: 0.2998 | MAE: 0.1989 | RMSE: 0.2998 | RÂ²: 0.9038\n",
      "Epoch 14/20 | Train Loss: 0.3344 | Val Loss: 0.2679 | MAE: 0.1820 | RMSE: 0.2679 | RÂ²: 0.9231\n",
      "Epoch 15/20 | Train Loss: 0.3263 | Val Loss: 0.2665 | MAE: 0.1835 | RMSE: 0.2665 | RÂ²: 0.9240\n",
      "Epoch 16/20 | Train Loss: 0.3201 | Val Loss: 0.2799 | MAE: 0.1931 | RMSE: 0.2799 | RÂ²: 0.9161\n",
      "Epoch 17/20 | Train Loss: 0.3158 | Val Loss: 0.2663 | MAE: 0.1805 | RMSE: 0.2663 | RÂ²: 0.9241\n",
      "Epoch 18/20 | Train Loss: 0.3086 | Val Loss: 0.2727 | MAE: 0.1834 | RMSE: 0.2727 | RÂ²: 0.9204\n",
      "Epoch 19/20 | Train Loss: 0.3108 | Val Loss: 0.2715 | MAE: 0.1809 | RMSE: 0.2715 | RÂ²: 0.9211\n",
      "Epoch 20/20 | Train Loss: 0.2974 | Val Loss: 0.2716 | MAE: 0.1843 | RMSE: 0.2716 | RÂ²: 0.9210\n",
      "âœ… RMSE = 0.2663 | MAE = 0.1805\n",
      "\n",
      "ðŸ”§ [79/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[512, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9641 | Val Loss: 0.8903 | MAE: 0.7534 | RMSE: 0.8903 | RÂ²: 0.1515\n",
      "Epoch 2/20 | Train Loss: 0.7532 | Val Loss: 0.7703 | MAE: 0.6521 | RMSE: 0.7703 | RÂ²: 0.3648\n",
      "Epoch 3/20 | Train Loss: 0.6648 | Val Loss: 0.6500 | MAE: 0.5344 | RMSE: 0.6500 | RÂ²: 0.5476\n",
      "Epoch 4/20 | Train Loss: 0.5985 | Val Loss: 0.5075 | MAE: 0.3941 | RMSE: 0.5075 | RÂ²: 0.7243\n",
      "Epoch 5/20 | Train Loss: 0.5579 | Val Loss: 0.4035 | MAE: 0.2900 | RMSE: 0.4035 | RÂ²: 0.8257\n",
      "Epoch 6/20 | Train Loss: 0.5272 | Val Loss: 0.3385 | MAE: 0.2459 | RMSE: 0.3385 | RÂ²: 0.8773\n",
      "Epoch 7/20 | Train Loss: 0.5053 | Val Loss: 0.3301 | MAE: 0.2390 | RMSE: 0.3301 | RÂ²: 0.8833\n",
      "Epoch 8/20 | Train Loss: 0.4828 | Val Loss: 0.3257 | MAE: 0.2342 | RMSE: 0.3257 | RÂ²: 0.8864\n",
      "Epoch 9/20 | Train Loss: 0.4647 | Val Loss: 0.3278 | MAE: 0.2387 | RMSE: 0.3278 | RÂ²: 0.8850\n",
      "Epoch 10/20 | Train Loss: 0.4491 | Val Loss: 0.3395 | MAE: 0.2523 | RMSE: 0.3395 | RÂ²: 0.8766\n",
      "Epoch 11/20 | Train Loss: 0.4380 | Val Loss: 0.3371 | MAE: 0.2533 | RMSE: 0.3371 | RÂ²: 0.8784\n",
      "Epoch 12/20 | Train Loss: 0.4252 | Val Loss: 0.3284 | MAE: 0.2435 | RMSE: 0.3284 | RÂ²: 0.8846\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3257 | MAE = 0.2342\n",
      "\n",
      "ðŸ”§ [80/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[512, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9051 | Val Loss: 0.7543 | MAE: 0.6363 | RMSE: 0.7543 | RÂ²: 0.3909\n",
      "Epoch 2/20 | Train Loss: 0.6370 | Val Loss: 0.5591 | MAE: 0.4402 | RMSE: 0.5591 | RÂ²: 0.6654\n",
      "Epoch 3/20 | Train Loss: 0.5312 | Val Loss: 0.3745 | MAE: 0.2803 | RMSE: 0.3745 | RÂ²: 0.8498\n",
      "Epoch 4/20 | Train Loss: 0.4671 | Val Loss: 0.3899 | MAE: 0.2870 | RMSE: 0.3899 | RÂ²: 0.8373\n",
      "Epoch 5/20 | Train Loss: 0.4300 | Val Loss: 0.4505 | MAE: 0.3606 | RMSE: 0.4505 | RÂ²: 0.7828\n",
      "Epoch 6/20 | Train Loss: 0.4038 | Val Loss: 0.4146 | MAE: 0.3302 | RMSE: 0.4146 | RÂ²: 0.8159\n",
      "Epoch 7/20 | Train Loss: 0.3847 | Val Loss: 0.3716 | MAE: 0.2945 | RMSE: 0.3716 | RÂ²: 0.8522\n",
      "Epoch 8/20 | Train Loss: 0.3709 | Val Loss: 0.3293 | MAE: 0.2501 | RMSE: 0.3293 | RÂ²: 0.8839\n",
      "Epoch 9/20 | Train Loss: 0.3559 | Val Loss: 0.3134 | MAE: 0.2353 | RMSE: 0.3134 | RÂ²: 0.8948\n",
      "Epoch 10/20 | Train Loss: 0.3452 | Val Loss: 0.2908 | MAE: 0.2111 | RMSE: 0.2908 | RÂ²: 0.9095\n",
      "Epoch 11/20 | Train Loss: 0.3359 | Val Loss: 0.2871 | MAE: 0.2064 | RMSE: 0.2871 | RÂ²: 0.9118\n",
      "Epoch 12/20 | Train Loss: 0.3292 | Val Loss: 0.2720 | MAE: 0.1901 | RMSE: 0.2720 | RÂ²: 0.9208\n",
      "Epoch 13/20 | Train Loss: 0.3228 | Val Loss: 0.2717 | MAE: 0.1881 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "Epoch 14/20 | Train Loss: 0.3187 | Val Loss: 0.2750 | MAE: 0.1902 | RMSE: 0.2750 | RÂ²: 0.9190\n",
      "Epoch 15/20 | Train Loss: 0.3088 | Val Loss: 0.2689 | MAE: 0.1862 | RMSE: 0.2689 | RÂ²: 0.9226\n",
      "Epoch 16/20 | Train Loss: 0.3047 | Val Loss: 0.2730 | MAE: 0.1899 | RMSE: 0.2730 | RÂ²: 0.9202\n",
      "Epoch 17/20 | Train Loss: 0.2998 | Val Loss: 0.2730 | MAE: 0.1880 | RMSE: 0.2730 | RÂ²: 0.9202\n",
      "Epoch 18/20 | Train Loss: 0.2963 | Val Loss: 0.2710 | MAE: 0.1855 | RMSE: 0.2710 | RÂ²: 0.9214\n",
      "Epoch 19/20 | Train Loss: 0.2919 | Val Loss: 0.2707 | MAE: 0.1848 | RMSE: 0.2707 | RÂ²: 0.9216\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2689 | MAE = 0.1862\n",
      "\n",
      "ðŸ”§ [81/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[512, 256, 128], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8716 | Val Loss: 0.9078 | MAE: 0.7680 | RMSE: 0.9078 | RÂ²: 0.1178\n",
      "Epoch 2/20 | Train Loss: 0.6677 | Val Loss: 0.8243 | MAE: 0.6957 | RMSE: 0.8243 | RÂ²: 0.2726\n",
      "Epoch 3/20 | Train Loss: 0.5405 | Val Loss: 0.6774 | MAE: 0.5618 | RMSE: 0.6774 | RÂ²: 0.5088\n",
      "Epoch 4/20 | Train Loss: 0.4675 | Val Loss: 0.4943 | MAE: 0.4024 | RMSE: 0.4943 | RÂ²: 0.7384\n",
      "Epoch 5/20 | Train Loss: 0.4275 | Val Loss: 0.3842 | MAE: 0.3056 | RMSE: 0.3842 | RÂ²: 0.8420\n",
      "Epoch 6/20 | Train Loss: 0.4040 | Val Loss: 0.3406 | MAE: 0.2656 | RMSE: 0.3406 | RÂ²: 0.8758\n",
      "Epoch 7/20 | Train Loss: 0.3816 | Val Loss: 0.3070 | MAE: 0.2326 | RMSE: 0.3070 | RÂ²: 0.8991\n",
      "Epoch 8/20 | Train Loss: 0.3724 | Val Loss: 0.2819 | MAE: 0.2046 | RMSE: 0.2819 | RÂ²: 0.9149\n",
      "Epoch 9/20 | Train Loss: 0.3614 | Val Loss: 0.2705 | MAE: 0.1898 | RMSE: 0.2705 | RÂ²: 0.9217\n",
      "Epoch 10/20 | Train Loss: 0.3532 | Val Loss: 0.2752 | MAE: 0.1887 | RMSE: 0.2752 | RÂ²: 0.9189\n",
      "Epoch 11/20 | Train Loss: 0.3410 | Val Loss: 0.2655 | MAE: 0.1862 | RMSE: 0.2655 | RÂ²: 0.9245\n",
      "Epoch 12/20 | Train Loss: 0.3345 | Val Loss: 0.2709 | MAE: 0.1895 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 13/20 | Train Loss: 0.3290 | Val Loss: 0.2707 | MAE: 0.1884 | RMSE: 0.2707 | RÂ²: 0.9216\n",
      "Epoch 14/20 | Train Loss: 0.3204 | Val Loss: 0.2697 | MAE: 0.1884 | RMSE: 0.2697 | RÂ²: 0.9221\n",
      "Epoch 15/20 | Train Loss: 0.3121 | Val Loss: 0.2710 | MAE: 0.1894 | RMSE: 0.2710 | RÂ²: 0.9214\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2655 | MAE = 0.1862\n",
      "\n",
      "ðŸ”§ [82/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8442 | Val Loss: 0.9121 | MAE: 0.7721 | RMSE: 0.9121 | RÂ²: 0.1094\n",
      "Epoch 2/20 | Train Loss: 0.6337 | Val Loss: 0.8280 | MAE: 0.6969 | RMSE: 0.8280 | RÂ²: 0.2661\n",
      "Epoch 3/20 | Train Loss: 0.5127 | Val Loss: 0.6760 | MAE: 0.5665 | RMSE: 0.6760 | RÂ²: 0.5107\n",
      "Epoch 4/20 | Train Loss: 0.4624 | Val Loss: 0.5274 | MAE: 0.4335 | RMSE: 0.5274 | RÂ²: 0.7022\n",
      "Epoch 5/20 | Train Loss: 0.4323 | Val Loss: 0.3981 | MAE: 0.3250 | RMSE: 0.3981 | RÂ²: 0.8304\n",
      "Epoch 6/20 | Train Loss: 0.4045 | Val Loss: 0.3340 | MAE: 0.2643 | RMSE: 0.3340 | RÂ²: 0.8806\n",
      "Epoch 7/20 | Train Loss: 0.3833 | Val Loss: 0.3089 | MAE: 0.2411 | RMSE: 0.3089 | RÂ²: 0.8979\n",
      "Epoch 8/20 | Train Loss: 0.3771 | Val Loss: 0.2899 | MAE: 0.2177 | RMSE: 0.2899 | RÂ²: 0.9101\n",
      "Epoch 9/20 | Train Loss: 0.3726 | Val Loss: 0.2785 | MAE: 0.2020 | RMSE: 0.2785 | RÂ²: 0.9170\n",
      "Epoch 10/20 | Train Loss: 0.3558 | Val Loss: 0.2706 | MAE: 0.1955 | RMSE: 0.2706 | RÂ²: 0.9216\n",
      "Epoch 11/20 | Train Loss: 0.3503 | Val Loss: 0.2765 | MAE: 0.1991 | RMSE: 0.2765 | RÂ²: 0.9182\n",
      "Epoch 12/20 | Train Loss: 0.3422 | Val Loss: 0.2867 | MAE: 0.2017 | RMSE: 0.2867 | RÂ²: 0.9120\n",
      "Epoch 13/20 | Train Loss: 0.3332 | Val Loss: 0.2843 | MAE: 0.2019 | RMSE: 0.2843 | RÂ²: 0.9135\n",
      "Epoch 14/20 | Train Loss: 0.3259 | Val Loss: 0.2801 | MAE: 0.2004 | RMSE: 0.2801 | RÂ²: 0.9160\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2706 | MAE = 0.1955\n",
      "\n",
      "ðŸ”§ [83/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 256, 1024], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.7644 | Val Loss: 0.8625 | MAE: 0.7226 | RMSE: 0.8625 | RÂ²: 0.2037\n",
      "Epoch 2/20 | Train Loss: 0.4896 | Val Loss: 0.6615 | MAE: 0.5545 | RMSE: 0.6615 | RÂ²: 0.5316\n",
      "Epoch 3/20 | Train Loss: 0.4108 | Val Loss: 0.5297 | MAE: 0.4408 | RMSE: 0.5297 | RÂ²: 0.6996\n",
      "Epoch 4/20 | Train Loss: 0.3789 | Val Loss: 0.3573 | MAE: 0.2837 | RMSE: 0.3573 | RÂ²: 0.8633\n",
      "Epoch 5/20 | Train Loss: 0.3555 | Val Loss: 0.2772 | MAE: 0.2030 | RMSE: 0.2772 | RÂ²: 0.9178\n",
      "Epoch 6/20 | Train Loss: 0.3401 | Val Loss: 0.3282 | MAE: 0.2235 | RMSE: 0.3282 | RÂ²: 0.8847\n",
      "Epoch 7/20 | Train Loss: 0.3300 | Val Loss: 0.2642 | MAE: 0.1859 | RMSE: 0.2642 | RÂ²: 0.9253\n",
      "Epoch 8/20 | Train Loss: 0.3179 | Val Loss: 0.2564 | MAE: 0.1767 | RMSE: 0.2564 | RÂ²: 0.9296\n",
      "Epoch 9/20 | Train Loss: 0.3112 | Val Loss: 0.2600 | MAE: 0.1819 | RMSE: 0.2600 | RÂ²: 0.9276\n",
      "Epoch 10/20 | Train Loss: 0.3049 | Val Loss: 0.2777 | MAE: 0.1987 | RMSE: 0.2777 | RÂ²: 0.9174\n",
      "Epoch 11/20 | Train Loss: 0.2923 | Val Loss: 0.2566 | MAE: 0.1778 | RMSE: 0.2566 | RÂ²: 0.9295\n",
      "Epoch 12/20 | Train Loss: 0.2870 | Val Loss: 0.2693 | MAE: 0.1891 | RMSE: 0.2693 | RÂ²: 0.9224\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2564 | MAE = 0.1767\n",
      "\n",
      "ðŸ”§ [84/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[512, 512, 512], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0415 | Val Loss: 0.8536 | MAE: 0.7084 | RMSE: 0.8536 | RÂ²: 0.2200\n",
      "Epoch 2/20 | Train Loss: 0.6189 | Val Loss: 0.5704 | MAE: 0.4686 | RMSE: 0.5704 | RÂ²: 0.6517\n",
      "Epoch 3/20 | Train Loss: 0.5063 | Val Loss: 0.3907 | MAE: 0.3152 | RMSE: 0.3907 | RÂ²: 0.8366\n",
      "Epoch 4/20 | Train Loss: 0.4580 | Val Loss: 0.3462 | MAE: 0.2628 | RMSE: 0.3462 | RÂ²: 0.8717\n",
      "Epoch 5/20 | Train Loss: 0.4221 | Val Loss: 0.3440 | MAE: 0.2605 | RMSE: 0.3440 | RÂ²: 0.8733\n",
      "Epoch 6/20 | Train Loss: 0.4015 | Val Loss: 0.3324 | MAE: 0.2277 | RMSE: 0.3324 | RÂ²: 0.8817\n",
      "Epoch 7/20 | Train Loss: 0.3850 | Val Loss: 0.3473 | MAE: 0.2636 | RMSE: 0.3473 | RÂ²: 0.8709\n",
      "Epoch 8/20 | Train Loss: 0.3602 | Val Loss: 0.3505 | MAE: 0.2602 | RMSE: 0.3505 | RÂ²: 0.8685\n",
      "Epoch 9/20 | Train Loss: 0.3396 | Val Loss: 0.2952 | MAE: 0.2193 | RMSE: 0.2952 | RÂ²: 0.9067\n",
      "Epoch 10/20 | Train Loss: 0.3326 | Val Loss: 0.2868 | MAE: 0.2108 | RMSE: 0.2868 | RÂ²: 0.9120\n",
      "Epoch 11/20 | Train Loss: 0.3215 | Val Loss: 0.2762 | MAE: 0.2005 | RMSE: 0.2762 | RÂ²: 0.9184\n",
      "Epoch 12/20 | Train Loss: 0.3161 | Val Loss: 0.2889 | MAE: 0.2070 | RMSE: 0.2889 | RÂ²: 0.9106\n",
      "Epoch 13/20 | Train Loss: 0.3047 | Val Loss: 0.2667 | MAE: 0.1846 | RMSE: 0.2667 | RÂ²: 0.9239\n",
      "Epoch 14/20 | Train Loss: 0.2985 | Val Loss: 0.2688 | MAE: 0.1908 | RMSE: 0.2688 | RÂ²: 0.9226\n",
      "Epoch 15/20 | Train Loss: 0.2963 | Val Loss: 0.2624 | MAE: 0.1804 | RMSE: 0.2624 | RÂ²: 0.9263\n",
      "Epoch 16/20 | Train Loss: 0.2915 | Val Loss: 0.2719 | MAE: 0.1895 | RMSE: 0.2719 | RÂ²: 0.9208\n",
      "Epoch 17/20 | Train Loss: 0.2843 | Val Loss: 0.2589 | MAE: 0.1797 | RMSE: 0.2589 | RÂ²: 0.9282\n",
      "Epoch 18/20 | Train Loss: 0.2786 | Val Loss: 0.2736 | MAE: 0.1889 | RMSE: 0.2736 | RÂ²: 0.9199\n",
      "Epoch 19/20 | Train Loss: 0.2768 | Val Loss: 0.2680 | MAE: 0.1861 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 20/20 | Train Loss: 0.2740 | Val Loss: 0.2560 | MAE: 0.1736 | RMSE: 0.2560 | RÂ²: 0.9298\n",
      "âœ… RMSE = 0.2560 | MAE = 0.1736\n",
      "\n",
      "ðŸ”§ [85/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9492 | Val Loss: 0.9099 | MAE: 0.7704 | RMSE: 0.9099 | RÂ²: 0.1137\n",
      "Epoch 2/20 | Train Loss: 0.7641 | Val Loss: 0.8273 | MAE: 0.6989 | RMSE: 0.8273 | RÂ²: 0.2672\n",
      "Epoch 3/20 | Train Loss: 0.6557 | Val Loss: 0.7208 | MAE: 0.5995 | RMSE: 0.7208 | RÂ²: 0.4437\n",
      "Epoch 4/20 | Train Loss: 0.5794 | Val Loss: 0.6000 | MAE: 0.4736 | RMSE: 0.6000 | RÂ²: 0.6145\n",
      "Epoch 5/20 | Train Loss: 0.5232 | Val Loss: 0.4708 | MAE: 0.3506 | RMSE: 0.4708 | RÂ²: 0.7627\n",
      "Epoch 6/20 | Train Loss: 0.4973 | Val Loss: 0.3822 | MAE: 0.2793 | RMSE: 0.3822 | RÂ²: 0.8436\n",
      "Epoch 7/20 | Train Loss: 0.4749 | Val Loss: 0.3589 | MAE: 0.2572 | RMSE: 0.3589 | RÂ²: 0.8621\n",
      "Epoch 8/20 | Train Loss: 0.4486 | Val Loss: 0.3313 | MAE: 0.2387 | RMSE: 0.3313 | RÂ²: 0.8825\n",
      "Epoch 9/20 | Train Loss: 0.4348 | Val Loss: 0.3136 | MAE: 0.2264 | RMSE: 0.3136 | RÂ²: 0.8947\n",
      "Epoch 10/20 | Train Loss: 0.4268 | Val Loss: 0.3267 | MAE: 0.2271 | RMSE: 0.3267 | RÂ²: 0.8857\n",
      "Epoch 11/20 | Train Loss: 0.4160 | Val Loss: 0.3343 | MAE: 0.2259 | RMSE: 0.3343 | RÂ²: 0.8804\n",
      "Epoch 12/20 | Train Loss: 0.4026 | Val Loss: 0.3107 | MAE: 0.2157 | RMSE: 0.3107 | RÂ²: 0.8966\n",
      "Epoch 13/20 | Train Loss: 0.3952 | Val Loss: 0.3005 | MAE: 0.2073 | RMSE: 0.3005 | RÂ²: 0.9033\n",
      "Epoch 14/20 | Train Loss: 0.3884 | Val Loss: 0.2998 | MAE: 0.2047 | RMSE: 0.2998 | RÂ²: 0.9037\n",
      "Epoch 15/20 | Train Loss: 0.3744 | Val Loss: 0.2911 | MAE: 0.2037 | RMSE: 0.2911 | RÂ²: 0.9093\n",
      "Epoch 16/20 | Train Loss: 0.3739 | Val Loss: 0.2887 | MAE: 0.1995 | RMSE: 0.2887 | RÂ²: 0.9108\n",
      "Epoch 17/20 | Train Loss: 0.3711 | Val Loss: 0.2946 | MAE: 0.2003 | RMSE: 0.2946 | RÂ²: 0.9071\n",
      "Epoch 18/20 | Train Loss: 0.3572 | Val Loss: 0.2933 | MAE: 0.1995 | RMSE: 0.2933 | RÂ²: 0.9079\n",
      "Epoch 19/20 | Train Loss: 0.3536 | Val Loss: 0.2844 | MAE: 0.1960 | RMSE: 0.2844 | RÂ²: 0.9134\n",
      "Epoch 20/20 | Train Loss: 0.3482 | Val Loss: 0.2751 | MAE: 0.1911 | RMSE: 0.2751 | RÂ²: 0.9190\n",
      "âœ… RMSE = 0.2751 | MAE = 0.1911\n",
      "\n",
      "ðŸ”§ [86/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.7878 | Val Loss: 0.8513 | MAE: 0.7074 | RMSE: 0.8513 | RÂ²: 0.2242\n",
      "Epoch 2/20 | Train Loss: 0.5115 | Val Loss: 0.6341 | MAE: 0.5282 | RMSE: 0.6341 | RÂ²: 0.5695\n",
      "Epoch 3/20 | Train Loss: 0.4463 | Val Loss: 0.4528 | MAE: 0.3754 | RMSE: 0.4528 | RÂ²: 0.7805\n",
      "Epoch 4/20 | Train Loss: 0.3977 | Val Loss: 0.3338 | MAE: 0.2565 | RMSE: 0.3338 | RÂ²: 0.8807\n",
      "Epoch 5/20 | Train Loss: 0.3772 | Val Loss: 0.2728 | MAE: 0.1944 | RMSE: 0.2728 | RÂ²: 0.9203\n",
      "Epoch 6/20 | Train Loss: 0.3568 | Val Loss: 0.2546 | MAE: 0.1809 | RMSE: 0.2546 | RÂ²: 0.9306\n",
      "Epoch 7/20 | Train Loss: 0.3411 | Val Loss: 0.2777 | MAE: 0.2018 | RMSE: 0.2777 | RÂ²: 0.9174\n",
      "Epoch 8/20 | Train Loss: 0.3271 | Val Loss: 0.2785 | MAE: 0.1965 | RMSE: 0.2785 | RÂ²: 0.9170\n",
      "Epoch 9/20 | Train Loss: 0.3248 | Val Loss: 0.2705 | MAE: 0.1902 | RMSE: 0.2705 | RÂ²: 0.9217\n",
      "Epoch 10/20 | Train Loss: 0.3078 | Val Loss: 0.2854 | MAE: 0.2013 | RMSE: 0.2854 | RÂ²: 0.9128\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2546 | MAE = 0.1809\n",
      "\n",
      "ðŸ”§ [87/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.7849 | Val Loss: 0.9408 | MAE: 0.7888 | RMSE: 0.9408 | RÂ²: 0.0524\n",
      "Epoch 2/20 | Train Loss: 0.5118 | Val Loss: 0.8270 | MAE: 0.6935 | RMSE: 0.8270 | RÂ²: 0.2678\n",
      "Epoch 3/20 | Train Loss: 0.4504 | Val Loss: 0.6711 | MAE: 0.5642 | RMSE: 0.6711 | RÂ²: 0.5178\n",
      "Epoch 4/20 | Train Loss: 0.4218 | Val Loss: 0.5187 | MAE: 0.4342 | RMSE: 0.5187 | RÂ²: 0.7119\n",
      "Epoch 5/20 | Train Loss: 0.3949 | Val Loss: 0.4065 | MAE: 0.3237 | RMSE: 0.4065 | RÂ²: 0.8231\n",
      "Epoch 6/20 | Train Loss: 0.3759 | Val Loss: 0.3031 | MAE: 0.2215 | RMSE: 0.3031 | RÂ²: 0.9017\n",
      "Epoch 7/20 | Train Loss: 0.3564 | Val Loss: 0.2739 | MAE: 0.1935 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 8/20 | Train Loss: 0.3474 | Val Loss: 0.2703 | MAE: 0.1904 | RMSE: 0.2703 | RÂ²: 0.9218\n",
      "Epoch 9/20 | Train Loss: 0.3373 | Val Loss: 0.2688 | MAE: 0.1904 | RMSE: 0.2688 | RÂ²: 0.9227\n",
      "Epoch 10/20 | Train Loss: 0.3227 | Val Loss: 0.2761 | MAE: 0.1982 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 11/20 | Train Loss: 0.3224 | Val Loss: 0.2774 | MAE: 0.1984 | RMSE: 0.2774 | RÂ²: 0.9176\n",
      "Epoch 12/20 | Train Loss: 0.3191 | Val Loss: 0.2805 | MAE: 0.2027 | RMSE: 0.2805 | RÂ²: 0.9158\n",
      "Epoch 13/20 | Train Loss: 0.3163 | Val Loss: 0.2932 | MAE: 0.2096 | RMSE: 0.2932 | RÂ²: 0.9080\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2688 | MAE = 0.1904\n",
      "\n",
      "ðŸ”§ [88/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8289 | Val Loss: 0.8304 | MAE: 0.6970 | RMSE: 0.8304 | RÂ²: 0.2618\n",
      "Epoch 2/20 | Train Loss: 0.4891 | Val Loss: 0.4629 | MAE: 0.3821 | RMSE: 0.4629 | RÂ²: 0.7706\n",
      "Epoch 3/20 | Train Loss: 0.4271 | Val Loss: 0.3818 | MAE: 0.3018 | RMSE: 0.3818 | RÂ²: 0.8439\n",
      "Epoch 4/20 | Train Loss: 0.3808 | Val Loss: 0.3625 | MAE: 0.2803 | RMSE: 0.3625 | RÂ²: 0.8593\n",
      "Epoch 5/20 | Train Loss: 0.3465 | Val Loss: 0.3717 | MAE: 0.2896 | RMSE: 0.3717 | RÂ²: 0.8521\n",
      "Epoch 6/20 | Train Loss: 0.3368 | Val Loss: 0.3732 | MAE: 0.2559 | RMSE: 0.3732 | RÂ²: 0.8509\n",
      "Epoch 7/20 | Train Loss: 0.3226 | Val Loss: 0.3097 | MAE: 0.2248 | RMSE: 0.3097 | RÂ²: 0.8973\n",
      "Epoch 8/20 | Train Loss: 0.3150 | Val Loss: 0.2840 | MAE: 0.2023 | RMSE: 0.2840 | RÂ²: 0.9137\n",
      "Epoch 9/20 | Train Loss: 0.3045 | Val Loss: 0.2622 | MAE: 0.1837 | RMSE: 0.2622 | RÂ²: 0.9264\n",
      "Epoch 10/20 | Train Loss: 0.2910 | Val Loss: 0.2579 | MAE: 0.1757 | RMSE: 0.2579 | RÂ²: 0.9288\n",
      "Epoch 11/20 | Train Loss: 0.2887 | Val Loss: 0.2591 | MAE: 0.1809 | RMSE: 0.2591 | RÂ²: 0.9281\n",
      "Epoch 12/20 | Train Loss: 0.2798 | Val Loss: 0.2628 | MAE: 0.1796 | RMSE: 0.2628 | RÂ²: 0.9261\n",
      "Epoch 13/20 | Train Loss: 0.2724 | Val Loss: 0.2585 | MAE: 0.1760 | RMSE: 0.2585 | RÂ²: 0.9285\n",
      "Epoch 14/20 | Train Loss: 0.2725 | Val Loss: 0.2711 | MAE: 0.1799 | RMSE: 0.2711 | RÂ²: 0.9213\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2579 | MAE = 0.1757\n",
      "\n",
      "ðŸ”§ [89/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 1.0425 | Val Loss: 0.9462 | MAE: 0.8037 | RMSE: 0.9462 | RÂ²: 0.0416\n",
      "Epoch 2/20 | Train Loss: 0.8172 | Val Loss: 0.8058 | MAE: 0.6857 | RMSE: 0.8058 | RÂ²: 0.3049\n",
      "Epoch 3/20 | Train Loss: 0.7261 | Val Loss: 0.6449 | MAE: 0.5487 | RMSE: 0.6449 | RÂ²: 0.5547\n",
      "Epoch 4/20 | Train Loss: 0.6510 | Val Loss: 0.5083 | MAE: 0.4235 | RMSE: 0.5083 | RÂ²: 0.7234\n",
      "Epoch 5/20 | Train Loss: 0.5856 | Val Loss: 0.4175 | MAE: 0.3303 | RMSE: 0.4175 | RÂ²: 0.8134\n",
      "Epoch 6/20 | Train Loss: 0.5547 | Val Loss: 0.3655 | MAE: 0.2771 | RMSE: 0.3655 | RÂ²: 0.8570\n",
      "Epoch 7/20 | Train Loss: 0.5216 | Val Loss: 0.3400 | MAE: 0.2526 | RMSE: 0.3400 | RÂ²: 0.8762\n",
      "Epoch 8/20 | Train Loss: 0.5023 | Val Loss: 0.3260 | MAE: 0.2399 | RMSE: 0.3260 | RÂ²: 0.8862\n",
      "Epoch 9/20 | Train Loss: 0.4855 | Val Loss: 0.3163 | MAE: 0.2329 | RMSE: 0.3163 | RÂ²: 0.8929\n",
      "Epoch 10/20 | Train Loss: 0.4760 | Val Loss: 0.3120 | MAE: 0.2306 | RMSE: 0.3120 | RÂ²: 0.8958\n",
      "Epoch 11/20 | Train Loss: 0.4521 | Val Loss: 0.3120 | MAE: 0.2317 | RMSE: 0.3120 | RÂ²: 0.8958\n",
      "Epoch 12/20 | Train Loss: 0.4452 | Val Loss: 0.3128 | MAE: 0.2309 | RMSE: 0.3128 | RÂ²: 0.8952\n",
      "Epoch 13/20 | Train Loss: 0.4308 | Val Loss: 0.3116 | MAE: 0.2294 | RMSE: 0.3116 | RÂ²: 0.8960\n",
      "Epoch 14/20 | Train Loss: 0.4261 | Val Loss: 0.3093 | MAE: 0.2284 | RMSE: 0.3093 | RÂ²: 0.8976\n",
      "Epoch 15/20 | Train Loss: 0.4219 | Val Loss: 0.3058 | MAE: 0.2240 | RMSE: 0.3058 | RÂ²: 0.8999\n",
      "Epoch 16/20 | Train Loss: 0.4137 | Val Loss: 0.2993 | MAE: 0.2168 | RMSE: 0.2993 | RÂ²: 0.9041\n",
      "Epoch 17/20 | Train Loss: 0.4045 | Val Loss: 0.2982 | MAE: 0.2161 | RMSE: 0.2982 | RÂ²: 0.9048\n",
      "Epoch 18/20 | Train Loss: 0.3986 | Val Loss: 0.2981 | MAE: 0.2165 | RMSE: 0.2981 | RÂ²: 0.9049\n",
      "Epoch 19/20 | Train Loss: 0.3947 | Val Loss: 0.2990 | MAE: 0.2151 | RMSE: 0.2990 | RÂ²: 0.9043\n",
      "Epoch 20/20 | Train Loss: 0.3844 | Val Loss: 0.3019 | MAE: 0.2153 | RMSE: 0.3019 | RÂ²: 0.9024\n",
      "âœ… RMSE = 0.2981 | MAE = 0.2165\n",
      "\n",
      "ðŸ”§ [90/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[512, 512, 512], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8577 | Val Loss: 0.9474 | MAE: 0.8052 | RMSE: 0.9474 | RÂ²: 0.0390\n",
      "Epoch 2/20 | Train Loss: 0.6295 | Val Loss: 0.8690 | MAE: 0.7366 | RMSE: 0.8690 | RÂ²: 0.1915\n",
      "Epoch 3/20 | Train Loss: 0.5089 | Val Loss: 0.7238 | MAE: 0.6087 | RMSE: 0.7238 | RÂ²: 0.4391\n",
      "Epoch 4/20 | Train Loss: 0.4539 | Val Loss: 0.5583 | MAE: 0.4622 | RMSE: 0.5583 | RÂ²: 0.6663\n",
      "Epoch 5/20 | Train Loss: 0.4101 | Val Loss: 0.4295 | MAE: 0.3481 | RMSE: 0.4295 | RÂ²: 0.8025\n",
      "Epoch 6/20 | Train Loss: 0.3836 | Val Loss: 0.3699 | MAE: 0.2888 | RMSE: 0.3699 | RÂ²: 0.8535\n",
      "Epoch 7/20 | Train Loss: 0.3635 | Val Loss: 0.3126 | MAE: 0.2347 | RMSE: 0.3126 | RÂ²: 0.8954\n",
      "Epoch 8/20 | Train Loss: 0.3495 | Val Loss: 0.2840 | MAE: 0.2083 | RMSE: 0.2840 | RÂ²: 0.9137\n",
      "Epoch 9/20 | Train Loss: 0.3381 | Val Loss: 0.2998 | MAE: 0.2144 | RMSE: 0.2998 | RÂ²: 0.9038\n",
      "Epoch 10/20 | Train Loss: 0.3286 | Val Loss: 0.2826 | MAE: 0.2024 | RMSE: 0.2826 | RÂ²: 0.9145\n",
      "Epoch 11/20 | Train Loss: 0.3195 | Val Loss: 0.2690 | MAE: 0.1931 | RMSE: 0.2690 | RÂ²: 0.9226\n",
      "Epoch 12/20 | Train Loss: 0.3156 | Val Loss: 0.2797 | MAE: 0.1963 | RMSE: 0.2797 | RÂ²: 0.9163\n",
      "Epoch 13/20 | Train Loss: 0.3104 | Val Loss: 0.2778 | MAE: 0.1971 | RMSE: 0.2778 | RÂ²: 0.9174\n",
      "Epoch 14/20 | Train Loss: 0.3092 | Val Loss: 0.2719 | MAE: 0.1930 | RMSE: 0.2719 | RÂ²: 0.9208\n",
      "Epoch 15/20 | Train Loss: 0.3011 | Val Loss: 0.2745 | MAE: 0.1922 | RMSE: 0.2745 | RÂ²: 0.9193\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2690 | MAE = 0.1931\n",
      "\n",
      "ðŸ”§ [91/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[512, 512, 512], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0022 | Val Loss: 0.9106 | MAE: 0.7709 | RMSE: 0.9106 | RÂ²: 0.1122\n",
      "Epoch 2/20 | Train Loss: 0.8787 | Val Loss: 0.8042 | MAE: 0.6823 | RMSE: 0.8042 | RÂ²: 0.3076\n",
      "Epoch 3/20 | Train Loss: 0.7656 | Val Loss: 0.7156 | MAE: 0.6056 | RMSE: 0.7156 | RÂ²: 0.4518\n",
      "Epoch 4/20 | Train Loss: 0.7141 | Val Loss: 0.6208 | MAE: 0.5177 | RMSE: 0.6208 | RÂ²: 0.5874\n",
      "Epoch 5/20 | Train Loss: 0.6513 | Val Loss: 0.5175 | MAE: 0.4202 | RMSE: 0.5175 | RÂ²: 0.7133\n",
      "Epoch 6/20 | Train Loss: 0.6246 | Val Loss: 0.4421 | MAE: 0.3444 | RMSE: 0.4421 | RÂ²: 0.7908\n",
      "Epoch 7/20 | Train Loss: 0.6081 | Val Loss: 0.4196 | MAE: 0.3005 | RMSE: 0.4196 | RÂ²: 0.8116\n",
      "Epoch 8/20 | Train Loss: 0.5732 | Val Loss: 0.3987 | MAE: 0.2743 | RMSE: 0.3987 | RÂ²: 0.8298\n",
      "Epoch 9/20 | Train Loss: 0.5501 | Val Loss: 0.3673 | MAE: 0.2557 | RMSE: 0.3673 | RÂ²: 0.8556\n",
      "Epoch 10/20 | Train Loss: 0.5348 | Val Loss: 0.3565 | MAE: 0.2490 | RMSE: 0.3565 | RÂ²: 0.8639\n",
      "Epoch 11/20 | Train Loss: 0.5260 | Val Loss: 0.3583 | MAE: 0.2459 | RMSE: 0.3583 | RÂ²: 0.8626\n",
      "Epoch 12/20 | Train Loss: 0.5125 | Val Loss: 0.3417 | MAE: 0.2404 | RMSE: 0.3417 | RÂ²: 0.8750\n",
      "Epoch 13/20 | Train Loss: 0.5015 | Val Loss: 0.3374 | MAE: 0.2396 | RMSE: 0.3374 | RÂ²: 0.8781\n",
      "Epoch 14/20 | Train Loss: 0.4931 | Val Loss: 0.3402 | MAE: 0.2388 | RMSE: 0.3402 | RÂ²: 0.8761\n",
      "Epoch 15/20 | Train Loss: 0.4829 | Val Loss: 0.3412 | MAE: 0.2387 | RMSE: 0.3412 | RÂ²: 0.8754\n",
      "Epoch 16/20 | Train Loss: 0.4724 | Val Loss: 0.3390 | MAE: 0.2389 | RMSE: 0.3390 | RÂ²: 0.8770\n",
      "Epoch 17/20 | Train Loss: 0.4710 | Val Loss: 0.3394 | MAE: 0.2374 | RMSE: 0.3394 | RÂ²: 0.8767\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3374 | MAE = 0.2396\n",
      "\n",
      "ðŸ”§ [92/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.7714 | Val Loss: 0.7523 | MAE: 0.6201 | RMSE: 0.7523 | RÂ²: 0.3941\n",
      "Epoch 2/20 | Train Loss: 0.4214 | Val Loss: 0.4473 | MAE: 0.3718 | RMSE: 0.4473 | RÂ²: 0.7859\n",
      "Epoch 3/20 | Train Loss: 0.3514 | Val Loss: 0.3626 | MAE: 0.2886 | RMSE: 0.3626 | RÂ²: 0.8592\n",
      "Epoch 4/20 | Train Loss: 0.3158 | Val Loss: 0.2777 | MAE: 0.2070 | RMSE: 0.2777 | RÂ²: 0.9174\n",
      "Epoch 5/20 | Train Loss: 0.2929 | Val Loss: 0.2772 | MAE: 0.1910 | RMSE: 0.2772 | RÂ²: 0.9177\n",
      "Epoch 6/20 | Train Loss: 0.2752 | Val Loss: 0.2636 | MAE: 0.1820 | RMSE: 0.2636 | RÂ²: 0.9256\n",
      "Epoch 7/20 | Train Loss: 0.2631 | Val Loss: 0.2785 | MAE: 0.1894 | RMSE: 0.2785 | RÂ²: 0.9170\n",
      "Epoch 8/20 | Train Loss: 0.2601 | Val Loss: 0.2526 | MAE: 0.1734 | RMSE: 0.2526 | RÂ²: 0.9317\n",
      "Epoch 9/20 | Train Loss: 0.2522 | Val Loss: 0.2675 | MAE: 0.1837 | RMSE: 0.2675 | RÂ²: 0.9234\n",
      "Epoch 10/20 | Train Loss: 0.2430 | Val Loss: 0.2550 | MAE: 0.1741 | RMSE: 0.2550 | RÂ²: 0.9304\n",
      "Epoch 11/20 | Train Loss: 0.2395 | Val Loss: 0.2680 | MAE: 0.1818 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 12/20 | Train Loss: 0.2361 | Val Loss: 0.2656 | MAE: 0.1808 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2526 | MAE = 0.1734\n",
      "\n",
      "ðŸ”§ [93/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[2048, 1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0810 | Val Loss: 0.7798 | MAE: 0.6237 | RMSE: 0.7798 | RÂ²: 0.3491\n",
      "Epoch 2/20 | Train Loss: 0.4139 | Val Loss: 0.3781 | MAE: 0.2500 | RMSE: 0.3781 | RÂ²: 0.8470\n",
      "Epoch 3/20 | Train Loss: 0.3272 | Val Loss: 0.7107 | MAE: 0.4596 | RMSE: 0.7107 | RÂ²: 0.4593\n",
      "Epoch 4/20 | Train Loss: 0.2983 | Val Loss: 0.5590 | MAE: 0.4488 | RMSE: 0.5590 | RÂ²: 0.6655\n",
      "Epoch 5/20 | Train Loss: 0.2730 | Val Loss: 0.4383 | MAE: 0.3339 | RMSE: 0.4383 | RÂ²: 0.7943\n",
      "Epoch 6/20 | Train Loss: 0.2663 | Val Loss: 0.4230 | MAE: 0.2973 | RMSE: 0.4230 | RÂ²: 0.8084\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3781 | MAE = 0.2500\n",
      "\n",
      "ðŸ”§ [94/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8846 | Val Loss: 0.7010 | MAE: 0.5897 | RMSE: 0.7010 | RÂ²: 0.4739\n",
      "Epoch 2/20 | Train Loss: 0.5472 | Val Loss: 0.5824 | MAE: 0.4608 | RMSE: 0.5824 | RÂ²: 0.6369\n",
      "Epoch 3/20 | Train Loss: 0.4762 | Val Loss: 0.3772 | MAE: 0.3006 | RMSE: 0.3772 | RÂ²: 0.8477\n",
      "Epoch 4/20 | Train Loss: 0.4253 | Val Loss: 0.3567 | MAE: 0.2724 | RMSE: 0.3567 | RÂ²: 0.8638\n",
      "Epoch 5/20 | Train Loss: 0.3942 | Val Loss: 0.2855 | MAE: 0.2088 | RMSE: 0.2855 | RÂ²: 0.9127\n",
      "Epoch 6/20 | Train Loss: 0.3788 | Val Loss: 0.2770 | MAE: 0.1954 | RMSE: 0.2770 | RÂ²: 0.9179\n",
      "Epoch 7/20 | Train Loss: 0.3642 | Val Loss: 0.2693 | MAE: 0.1905 | RMSE: 0.2693 | RÂ²: 0.9223\n",
      "Epoch 8/20 | Train Loss: 0.3487 | Val Loss: 0.2650 | MAE: 0.1858 | RMSE: 0.2650 | RÂ²: 0.9248\n",
      "Epoch 9/20 | Train Loss: 0.3392 | Val Loss: 0.2693 | MAE: 0.1876 | RMSE: 0.2693 | RÂ²: 0.9223\n",
      "Epoch 10/20 | Train Loss: 0.3295 | Val Loss: 0.2638 | MAE: 0.1840 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 11/20 | Train Loss: 0.3201 | Val Loss: 0.2635 | MAE: 0.1830 | RMSE: 0.2635 | RÂ²: 0.9257\n",
      "Epoch 12/20 | Train Loss: 0.3178 | Val Loss: 0.2571 | MAE: 0.1782 | RMSE: 0.2571 | RÂ²: 0.9292\n",
      "Epoch 13/20 | Train Loss: 0.3113 | Val Loss: 0.2656 | MAE: 0.1853 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "Epoch 14/20 | Train Loss: 0.3052 | Val Loss: 0.2710 | MAE: 0.1884 | RMSE: 0.2710 | RÂ²: 0.9214\n",
      "Epoch 15/20 | Train Loss: 0.3056 | Val Loss: 0.2773 | MAE: 0.1935 | RMSE: 0.2773 | RÂ²: 0.9177\n",
      "Epoch 16/20 | Train Loss: 0.2979 | Val Loss: 0.2632 | MAE: 0.1831 | RMSE: 0.2632 | RÂ²: 0.9258\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2571 | MAE = 0.1782\n",
      "\n",
      "ðŸ”§ [95/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[2048, 1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8748 | Val Loss: 0.6716 | MAE: 0.5625 | RMSE: 0.6716 | RÂ²: 0.5172\n",
      "Epoch 2/20 | Train Loss: 0.5836 | Val Loss: 0.4005 | MAE: 0.3050 | RMSE: 0.4005 | RÂ²: 0.8283\n",
      "Epoch 3/20 | Train Loss: 0.4874 | Val Loss: 0.4558 | MAE: 0.3787 | RMSE: 0.4558 | RÂ²: 0.7776\n",
      "Epoch 4/20 | Train Loss: 0.4481 | Val Loss: 0.5628 | MAE: 0.4838 | RMSE: 0.5628 | RÂ²: 0.6609\n",
      "Epoch 5/20 | Train Loss: 0.4115 | Val Loss: 0.4582 | MAE: 0.3660 | RMSE: 0.4582 | RÂ²: 0.7753\n",
      "Epoch 6/20 | Train Loss: 0.3961 | Val Loss: 0.3929 | MAE: 0.3193 | RMSE: 0.3929 | RÂ²: 0.8348\n",
      "Epoch 7/20 | Train Loss: 0.3655 | Val Loss: 0.3367 | MAE: 0.2456 | RMSE: 0.3367 | RÂ²: 0.8786\n",
      "Epoch 8/20 | Train Loss: 0.3561 | Val Loss: 0.3124 | MAE: 0.2293 | RMSE: 0.3124 | RÂ²: 0.8955\n",
      "Epoch 9/20 | Train Loss: 0.3338 | Val Loss: 0.3067 | MAE: 0.2063 | RMSE: 0.3067 | RÂ²: 0.8993\n",
      "Epoch 10/20 | Train Loss: 0.3242 | Val Loss: 0.2742 | MAE: 0.1917 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "Epoch 11/20 | Train Loss: 0.3196 | Val Loss: 0.2727 | MAE: 0.1872 | RMSE: 0.2727 | RÂ²: 0.9204\n",
      "Epoch 12/20 | Train Loss: 0.3047 | Val Loss: 0.2729 | MAE: 0.1881 | RMSE: 0.2729 | RÂ²: 0.9203\n",
      "Epoch 13/20 | Train Loss: 0.3002 | Val Loss: 0.2591 | MAE: 0.1761 | RMSE: 0.2591 | RÂ²: 0.9281\n",
      "Epoch 14/20 | Train Loss: 0.2938 | Val Loss: 0.2715 | MAE: 0.1874 | RMSE: 0.2715 | RÂ²: 0.9211\n",
      "Epoch 15/20 | Train Loss: 0.2888 | Val Loss: 0.2633 | MAE: 0.1803 | RMSE: 0.2633 | RÂ²: 0.9258\n",
      "Epoch 16/20 | Train Loss: 0.2878 | Val Loss: 0.2670 | MAE: 0.1849 | RMSE: 0.2670 | RÂ²: 0.9237\n",
      "Epoch 17/20 | Train Loss: 0.2835 | Val Loss: 0.2711 | MAE: 0.1870 | RMSE: 0.2711 | RÂ²: 0.9213\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2591 | MAE = 0.1761\n",
      "\n",
      "ðŸ”§ [96/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[1024, 256, 1024], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9090 | Val Loss: 0.8477 | MAE: 0.7191 | RMSE: 0.8477 | RÂ²: 0.2307\n",
      "Epoch 2/20 | Train Loss: 0.7109 | Val Loss: 0.7336 | MAE: 0.6160 | RMSE: 0.7336 | RÂ²: 0.4238\n",
      "Epoch 3/20 | Train Loss: 0.6221 | Val Loss: 0.5700 | MAE: 0.4682 | RMSE: 0.5700 | RÂ²: 0.6522\n",
      "Epoch 4/20 | Train Loss: 0.5848 | Val Loss: 0.4792 | MAE: 0.3612 | RMSE: 0.4792 | RÂ²: 0.7542\n",
      "Epoch 5/20 | Train Loss: 0.5456 | Val Loss: 0.3277 | MAE: 0.2472 | RMSE: 0.3277 | RÂ²: 0.8851\n",
      "Epoch 6/20 | Train Loss: 0.5137 | Val Loss: 0.3071 | MAE: 0.2252 | RMSE: 0.3071 | RÂ²: 0.8990\n",
      "Epoch 7/20 | Train Loss: 0.4967 | Val Loss: 0.3056 | MAE: 0.2238 | RMSE: 0.3056 | RÂ²: 0.9000\n",
      "Epoch 8/20 | Train Loss: 0.4761 | Val Loss: 0.3105 | MAE: 0.2299 | RMSE: 0.3105 | RÂ²: 0.8968\n",
      "Epoch 9/20 | Train Loss: 0.4626 | Val Loss: 0.3289 | MAE: 0.2433 | RMSE: 0.3289 | RÂ²: 0.8842\n",
      "Epoch 10/20 | Train Loss: 0.4568 | Val Loss: 0.3170 | MAE: 0.2391 | RMSE: 0.3170 | RÂ²: 0.8924\n",
      "Epoch 11/20 | Train Loss: 0.4476 | Val Loss: 0.2979 | MAE: 0.2166 | RMSE: 0.2979 | RÂ²: 0.9050\n",
      "Epoch 12/20 | Train Loss: 0.4311 | Val Loss: 0.3027 | MAE: 0.2226 | RMSE: 0.3027 | RÂ²: 0.9019\n",
      "Epoch 13/20 | Train Loss: 0.4233 | Val Loss: 0.3051 | MAE: 0.2252 | RMSE: 0.3051 | RÂ²: 0.9003\n",
      "Epoch 14/20 | Train Loss: 0.4138 | Val Loss: 0.2999 | MAE: 0.2076 | RMSE: 0.2999 | RÂ²: 0.9037\n",
      "Epoch 15/20 | Train Loss: 0.4139 | Val Loss: 0.2971 | MAE: 0.2097 | RMSE: 0.2971 | RÂ²: 0.9055\n",
      "Epoch 16/20 | Train Loss: 0.3994 | Val Loss: 0.3145 | MAE: 0.2236 | RMSE: 0.3145 | RÂ²: 0.8941\n",
      "Epoch 17/20 | Train Loss: 0.3940 | Val Loss: 0.2974 | MAE: 0.2066 | RMSE: 0.2974 | RÂ²: 0.9053\n",
      "Epoch 18/20 | Train Loss: 0.3957 | Val Loss: 0.3119 | MAE: 0.2131 | RMSE: 0.3119 | RÂ²: 0.8959\n",
      "Epoch 19/20 | Train Loss: 0.3884 | Val Loss: 0.2976 | MAE: 0.2059 | RMSE: 0.2976 | RÂ²: 0.9052\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2971 | MAE = 0.2097\n",
      "\n",
      "ðŸ”§ [97/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[512, 512, 512], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9206 | Val Loss: 0.9084 | MAE: 0.7714 | RMSE: 0.9084 | RÂ²: 0.1166\n",
      "Epoch 2/20 | Train Loss: 0.5571 | Val Loss: 0.8161 | MAE: 0.6847 | RMSE: 0.8161 | RÂ²: 0.2869\n",
      "Epoch 3/20 | Train Loss: 0.4502 | Val Loss: 0.6297 | MAE: 0.5309 | RMSE: 0.6297 | RÂ²: 0.5755\n",
      "Epoch 4/20 | Train Loss: 0.3917 | Val Loss: 0.5157 | MAE: 0.4309 | RMSE: 0.5157 | RÂ²: 0.7153\n",
      "Epoch 5/20 | Train Loss: 0.3704 | Val Loss: 0.4066 | MAE: 0.3283 | RMSE: 0.4066 | RÂ²: 0.8230\n",
      "Epoch 6/20 | Train Loss: 0.3445 | Val Loss: 0.3204 | MAE: 0.2422 | RMSE: 0.3204 | RÂ²: 0.8901\n",
      "Epoch 7/20 | Train Loss: 0.3304 | Val Loss: 0.2733 | MAE: 0.2000 | RMSE: 0.2733 | RÂ²: 0.9201\n",
      "Epoch 8/20 | Train Loss: 0.3188 | Val Loss: 0.2833 | MAE: 0.1975 | RMSE: 0.2833 | RÂ²: 0.9141\n",
      "Epoch 9/20 | Train Loss: 0.3077 | Val Loss: 0.2715 | MAE: 0.1913 | RMSE: 0.2715 | RÂ²: 0.9211\n",
      "Epoch 10/20 | Train Loss: 0.3008 | Val Loss: 0.2636 | MAE: 0.1812 | RMSE: 0.2636 | RÂ²: 0.9256\n",
      "Epoch 11/20 | Train Loss: 0.3023 | Val Loss: 0.2585 | MAE: 0.1797 | RMSE: 0.2585 | RÂ²: 0.9285\n",
      "Epoch 12/20 | Train Loss: 0.2871 | Val Loss: 0.2686 | MAE: 0.1868 | RMSE: 0.2686 | RÂ²: 0.9228\n",
      "Epoch 13/20 | Train Loss: 0.2835 | Val Loss: 0.2663 | MAE: 0.1853 | RMSE: 0.2663 | RÂ²: 0.9241\n",
      "Epoch 14/20 | Train Loss: 0.2811 | Val Loss: 0.2697 | MAE: 0.1886 | RMSE: 0.2697 | RÂ²: 0.9221\n",
      "Epoch 15/20 | Train Loss: 0.2750 | Val Loss: 0.2762 | MAE: 0.1879 | RMSE: 0.2762 | RÂ²: 0.9183\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2585 | MAE = 0.1797\n",
      "\n",
      "ðŸ”§ [98/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[2048, 1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.7746 | Val Loss: 0.8107 | MAE: 0.6788 | RMSE: 0.8107 | RÂ²: 0.2964\n",
      "Epoch 2/20 | Train Loss: 0.5247 | Val Loss: 0.5318 | MAE: 0.4416 | RMSE: 0.5318 | RÂ²: 0.6972\n",
      "Epoch 3/20 | Train Loss: 0.4477 | Val Loss: 0.3924 | MAE: 0.2848 | RMSE: 0.3924 | RÂ²: 0.8352\n",
      "Epoch 4/20 | Train Loss: 0.4094 | Val Loss: 0.3615 | MAE: 0.2871 | RMSE: 0.3615 | RÂ²: 0.8601\n",
      "Epoch 5/20 | Train Loss: 0.3839 | Val Loss: 0.3648 | MAE: 0.2923 | RMSE: 0.3648 | RÂ²: 0.8575\n",
      "Epoch 6/20 | Train Loss: 0.3659 | Val Loss: 0.3475 | MAE: 0.2784 | RMSE: 0.3475 | RÂ²: 0.8707\n",
      "Epoch 7/20 | Train Loss: 0.3494 | Val Loss: 0.3187 | MAE: 0.2472 | RMSE: 0.3187 | RÂ²: 0.8912\n",
      "Epoch 8/20 | Train Loss: 0.3346 | Val Loss: 0.2963 | MAE: 0.2259 | RMSE: 0.2963 | RÂ²: 0.9060\n",
      "Epoch 9/20 | Train Loss: 0.3275 | Val Loss: 0.2838 | MAE: 0.2106 | RMSE: 0.2838 | RÂ²: 0.9138\n",
      "Epoch 10/20 | Train Loss: 0.3137 | Val Loss: 0.2786 | MAE: 0.2004 | RMSE: 0.2786 | RÂ²: 0.9169\n",
      "Epoch 11/20 | Train Loss: 0.3077 | Val Loss: 0.2684 | MAE: 0.1861 | RMSE: 0.2684 | RÂ²: 0.9229\n",
      "Epoch 12/20 | Train Loss: 0.2998 | Val Loss: 0.2724 | MAE: 0.1876 | RMSE: 0.2724 | RÂ²: 0.9206\n",
      "Epoch 13/20 | Train Loss: 0.2974 | Val Loss: 0.2827 | MAE: 0.1919 | RMSE: 0.2827 | RÂ²: 0.9144\n",
      "Epoch 14/20 | Train Loss: 0.2921 | Val Loss: 0.2632 | MAE: 0.1821 | RMSE: 0.2632 | RÂ²: 0.9258\n",
      "Epoch 15/20 | Train Loss: 0.2860 | Val Loss: 0.2650 | MAE: 0.1817 | RMSE: 0.2650 | RÂ²: 0.9248\n",
      "Epoch 16/20 | Train Loss: 0.2805 | Val Loss: 0.2688 | MAE: 0.1832 | RMSE: 0.2688 | RÂ²: 0.9227\n",
      "Epoch 17/20 | Train Loss: 0.2796 | Val Loss: 0.2688 | MAE: 0.1825 | RMSE: 0.2688 | RÂ²: 0.9226\n",
      "Epoch 18/20 | Train Loss: 0.2786 | Val Loss: 0.2720 | MAE: 0.1863 | RMSE: 0.2720 | RÂ²: 0.9208\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2632 | MAE = 0.1821\n",
      "\n",
      "ðŸ”§ [99/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[512, 512, 512], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8638 | Val Loss: 0.8598 | MAE: 0.7211 | RMSE: 0.8598 | RÂ²: 0.2085\n",
      "Epoch 2/20 | Train Loss: 0.5590 | Val Loss: 0.7030 | MAE: 0.5727 | RMSE: 0.7030 | RÂ²: 0.4710\n",
      "Epoch 3/20 | Train Loss: 0.4815 | Val Loss: 0.4466 | MAE: 0.3681 | RMSE: 0.4466 | RÂ²: 0.7864\n",
      "Epoch 4/20 | Train Loss: 0.4470 | Val Loss: 0.4021 | MAE: 0.3194 | RMSE: 0.4021 | RÂ²: 0.8269\n",
      "Epoch 5/20 | Train Loss: 0.4042 | Val Loss: 0.2946 | MAE: 0.2102 | RMSE: 0.2946 | RÂ²: 0.9071\n",
      "Epoch 6/20 | Train Loss: 0.3920 | Val Loss: 0.2829 | MAE: 0.1967 | RMSE: 0.2829 | RÂ²: 0.9143\n",
      "Epoch 7/20 | Train Loss: 0.3771 | Val Loss: 0.2781 | MAE: 0.1941 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 8/20 | Train Loss: 0.3580 | Val Loss: 0.2954 | MAE: 0.2131 | RMSE: 0.2954 | RÂ²: 0.9066\n",
      "Epoch 9/20 | Train Loss: 0.3504 | Val Loss: 0.2871 | MAE: 0.2038 | RMSE: 0.2871 | RÂ²: 0.9117\n",
      "Epoch 10/20 | Train Loss: 0.3348 | Val Loss: 0.2978 | MAE: 0.2157 | RMSE: 0.2978 | RÂ²: 0.9050\n",
      "Epoch 11/20 | Train Loss: 0.3320 | Val Loss: 0.2699 | MAE: 0.1882 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 12/20 | Train Loss: 0.3223 | Val Loss: 0.2918 | MAE: 0.2089 | RMSE: 0.2918 | RÂ²: 0.9089\n",
      "Epoch 13/20 | Train Loss: 0.3127 | Val Loss: 0.2619 | MAE: 0.1794 | RMSE: 0.2619 | RÂ²: 0.9266\n",
      "Epoch 14/20 | Train Loss: 0.3070 | Val Loss: 0.2919 | MAE: 0.2125 | RMSE: 0.2919 | RÂ²: 0.9088\n",
      "Epoch 15/20 | Train Loss: 0.3013 | Val Loss: 0.2701 | MAE: 0.1842 | RMSE: 0.2701 | RÂ²: 0.9219\n",
      "Epoch 16/20 | Train Loss: 0.2978 | Val Loss: 0.2747 | MAE: 0.1918 | RMSE: 0.2747 | RÂ²: 0.9192\n",
      "Epoch 17/20 | Train Loss: 0.2912 | Val Loss: 0.2797 | MAE: 0.1910 | RMSE: 0.2797 | RÂ²: 0.9163\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2619 | MAE = 0.1794\n",
      "\n",
      "ðŸ”§ [100/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[1024, 512, 256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9769 | Val Loss: 0.9731 | MAE: 0.8245 | RMSE: 0.9731 | RÂ²: -0.0138\n",
      "Epoch 2/20 | Train Loss: 0.7637 | Val Loss: 0.8538 | MAE: 0.7229 | RMSE: 0.8538 | RÂ²: 0.2195\n",
      "Epoch 3/20 | Train Loss: 0.6452 | Val Loss: 0.6735 | MAE: 0.5619 | RMSE: 0.6735 | RÂ²: 0.5144\n",
      "Epoch 4/20 | Train Loss: 0.5715 | Val Loss: 0.4682 | MAE: 0.3743 | RMSE: 0.4682 | RÂ²: 0.7654\n",
      "Epoch 5/20 | Train Loss: 0.5247 | Val Loss: 0.3773 | MAE: 0.2798 | RMSE: 0.3773 | RÂ²: 0.8476\n",
      "Epoch 6/20 | Train Loss: 0.5004 | Val Loss: 0.3438 | MAE: 0.2466 | RMSE: 0.3438 | RÂ²: 0.8734\n",
      "Epoch 7/20 | Train Loss: 0.4663 | Val Loss: 0.3194 | MAE: 0.2307 | RMSE: 0.3194 | RÂ²: 0.8908\n",
      "Epoch 8/20 | Train Loss: 0.4477 | Val Loss: 0.3173 | MAE: 0.2324 | RMSE: 0.3173 | RÂ²: 0.8922\n",
      "Epoch 9/20 | Train Loss: 0.4413 | Val Loss: 0.3249 | MAE: 0.2407 | RMSE: 0.3249 | RÂ²: 0.8870\n",
      "Epoch 10/20 | Train Loss: 0.4232 | Val Loss: 0.3306 | MAE: 0.2441 | RMSE: 0.3306 | RÂ²: 0.8830\n",
      "Epoch 11/20 | Train Loss: 0.4092 | Val Loss: 0.3340 | MAE: 0.2445 | RMSE: 0.3340 | RÂ²: 0.8806\n",
      "Epoch 12/20 | Train Loss: 0.4008 | Val Loss: 0.3292 | MAE: 0.2409 | RMSE: 0.3292 | RÂ²: 0.8840\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3173 | MAE = 0.2324\n",
      "\n",
      "ðŸ”§ [101/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[1024, 256, 1024], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8808 | Val Loss: 0.8412 | MAE: 0.7084 | RMSE: 0.8412 | RÂ²: 0.2425\n",
      "Epoch 2/20 | Train Loss: 0.6183 | Val Loss: 0.6731 | MAE: 0.5441 | RMSE: 0.6731 | RÂ²: 0.5149\n",
      "Epoch 3/20 | Train Loss: 0.5446 | Val Loss: 0.4147 | MAE: 0.3323 | RMSE: 0.4147 | RÂ²: 0.8159\n",
      "Epoch 4/20 | Train Loss: 0.4963 | Val Loss: 0.4095 | MAE: 0.3005 | RMSE: 0.4095 | RÂ²: 0.8205\n",
      "Epoch 5/20 | Train Loss: 0.4804 | Val Loss: 0.3732 | MAE: 0.2767 | RMSE: 0.3732 | RÂ²: 0.8509\n",
      "Epoch 6/20 | Train Loss: 0.4458 | Val Loss: 0.3226 | MAE: 0.2362 | RMSE: 0.3226 | RÂ²: 0.8886\n",
      "Epoch 7/20 | Train Loss: 0.4321 | Val Loss: 0.2858 | MAE: 0.2050 | RMSE: 0.2858 | RÂ²: 0.9126\n",
      "Epoch 8/20 | Train Loss: 0.4045 | Val Loss: 0.3075 | MAE: 0.2274 | RMSE: 0.3075 | RÂ²: 0.8988\n",
      "Epoch 9/20 | Train Loss: 0.3940 | Val Loss: 0.3254 | MAE: 0.2205 | RMSE: 0.3254 | RÂ²: 0.8866\n",
      "Epoch 10/20 | Train Loss: 0.3867 | Val Loss: 0.2952 | MAE: 0.2163 | RMSE: 0.2952 | RÂ²: 0.9067\n",
      "Epoch 11/20 | Train Loss: 0.3824 | Val Loss: 0.3038 | MAE: 0.1970 | RMSE: 0.3038 | RÂ²: 0.9012\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2858 | MAE = 0.2050\n",
      "\n",
      "ðŸ”§ [102/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8636 | Val Loss: 0.7755 | MAE: 0.6595 | RMSE: 0.7755 | RÂ²: 0.3561\n",
      "Epoch 2/20 | Train Loss: 0.5642 | Val Loss: 0.5011 | MAE: 0.4196 | RMSE: 0.5011 | RÂ²: 0.7312\n",
      "Epoch 3/20 | Train Loss: 0.4742 | Val Loss: 0.3409 | MAE: 0.2691 | RMSE: 0.3409 | RÂ²: 0.8756\n",
      "Epoch 4/20 | Train Loss: 0.4354 | Val Loss: 0.3315 | MAE: 0.2510 | RMSE: 0.3315 | RÂ²: 0.8823\n",
      "Epoch 5/20 | Train Loss: 0.4019 | Val Loss: 0.3640 | MAE: 0.2849 | RMSE: 0.3640 | RÂ²: 0.8582\n",
      "Epoch 6/20 | Train Loss: 0.3821 | Val Loss: 0.3331 | MAE: 0.2545 | RMSE: 0.3331 | RÂ²: 0.8812\n",
      "Epoch 7/20 | Train Loss: 0.3632 | Val Loss: 0.3184 | MAE: 0.2410 | RMSE: 0.3184 | RÂ²: 0.8915\n",
      "Epoch 8/20 | Train Loss: 0.3436 | Val Loss: 0.2978 | MAE: 0.2170 | RMSE: 0.2978 | RÂ²: 0.9051\n",
      "Epoch 9/20 | Train Loss: 0.3351 | Val Loss: 0.2715 | MAE: 0.1927 | RMSE: 0.2715 | RÂ²: 0.9211\n",
      "Epoch 10/20 | Train Loss: 0.3257 | Val Loss: 0.2812 | MAE: 0.1986 | RMSE: 0.2812 | RÂ²: 0.9154\n",
      "Epoch 11/20 | Train Loss: 0.3197 | Val Loss: 0.2709 | MAE: 0.1882 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 12/20 | Train Loss: 0.3104 | Val Loss: 0.2739 | MAE: 0.1905 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 13/20 | Train Loss: 0.3019 | Val Loss: 0.2793 | MAE: 0.1976 | RMSE: 0.2793 | RÂ²: 0.9165\n",
      "Epoch 14/20 | Train Loss: 0.3008 | Val Loss: 0.2758 | MAE: 0.1909 | RMSE: 0.2758 | RÂ²: 0.9186\n",
      "Epoch 15/20 | Train Loss: 0.3015 | Val Loss: 0.2819 | MAE: 0.1994 | RMSE: 0.2819 | RÂ²: 0.9149\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2709 | MAE = 0.1882\n",
      "\n",
      "ðŸ”§ [103/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[1024, 512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8785 | Val Loss: 0.9212 | MAE: 0.7764 | RMSE: 0.9212 | RÂ²: 0.0915\n",
      "Epoch 2/20 | Train Loss: 0.5890 | Val Loss: 0.8217 | MAE: 0.6796 | RMSE: 0.8217 | RÂ²: 0.2771\n",
      "Epoch 3/20 | Train Loss: 0.4942 | Val Loss: 0.5971 | MAE: 0.4985 | RMSE: 0.5971 | RÂ²: 0.6183\n",
      "Epoch 4/20 | Train Loss: 0.4332 | Val Loss: 0.4374 | MAE: 0.3624 | RMSE: 0.4374 | RÂ²: 0.7952\n",
      "Epoch 5/20 | Train Loss: 0.4011 | Val Loss: 0.3693 | MAE: 0.2839 | RMSE: 0.3693 | RÂ²: 0.8540\n",
      "Epoch 6/20 | Train Loss: 0.3714 | Val Loss: 0.3029 | MAE: 0.2237 | RMSE: 0.3029 | RÂ²: 0.9018\n",
      "Epoch 7/20 | Train Loss: 0.3584 | Val Loss: 0.3007 | MAE: 0.2170 | RMSE: 0.3007 | RÂ²: 0.9032\n",
      "Epoch 8/20 | Train Loss: 0.3433 | Val Loss: 0.2885 | MAE: 0.2083 | RMSE: 0.2885 | RÂ²: 0.9109\n",
      "Epoch 9/20 | Train Loss: 0.3378 | Val Loss: 0.2708 | MAE: 0.1935 | RMSE: 0.2708 | RÂ²: 0.9215\n",
      "Epoch 10/20 | Train Loss: 0.3251 | Val Loss: 0.2794 | MAE: 0.2051 | RMSE: 0.2794 | RÂ²: 0.9164\n",
      "Epoch 11/20 | Train Loss: 0.3202 | Val Loss: 0.2664 | MAE: 0.1890 | RMSE: 0.2664 | RÂ²: 0.9240\n",
      "Epoch 12/20 | Train Loss: 0.3109 | Val Loss: 0.2689 | MAE: 0.1883 | RMSE: 0.2689 | RÂ²: 0.9226\n",
      "Epoch 13/20 | Train Loss: 0.3130 | Val Loss: 0.2684 | MAE: 0.1861 | RMSE: 0.2684 | RÂ²: 0.9229\n",
      "Epoch 14/20 | Train Loss: 0.3047 | Val Loss: 0.2627 | MAE: 0.1828 | RMSE: 0.2627 | RÂ²: 0.9261\n",
      "Epoch 15/20 | Train Loss: 0.3044 | Val Loss: 0.2611 | MAE: 0.1803 | RMSE: 0.2611 | RÂ²: 0.9270\n",
      "Epoch 16/20 | Train Loss: 0.2960 | Val Loss: 0.2641 | MAE: 0.1822 | RMSE: 0.2641 | RÂ²: 0.9253\n",
      "Epoch 17/20 | Train Loss: 0.2935 | Val Loss: 0.2618 | MAE: 0.1812 | RMSE: 0.2618 | RÂ²: 0.9266\n",
      "Epoch 18/20 | Train Loss: 0.2880 | Val Loss: 0.2661 | MAE: 0.1825 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 19/20 | Train Loss: 0.2869 | Val Loss: 0.2667 | MAE: 0.1819 | RMSE: 0.2667 | RÂ²: 0.9239\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2611 | MAE = 0.1803\n",
      "\n",
      "ðŸ”§ [104/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8168 | Val Loss: 0.8155 | MAE: 0.6923 | RMSE: 0.8155 | RÂ²: 0.2880\n",
      "Epoch 2/20 | Train Loss: 0.5506 | Val Loss: 0.6235 | MAE: 0.5248 | RMSE: 0.6235 | RÂ²: 0.5838\n",
      "Epoch 3/20 | Train Loss: 0.4388 | Val Loss: 0.4613 | MAE: 0.3808 | RMSE: 0.4613 | RÂ²: 0.7722\n",
      "Epoch 4/20 | Train Loss: 0.3716 | Val Loss: 0.3593 | MAE: 0.2829 | RMSE: 0.3593 | RÂ²: 0.8618\n",
      "Epoch 5/20 | Train Loss: 0.3464 | Val Loss: 0.3174 | MAE: 0.2433 | RMSE: 0.3174 | RÂ²: 0.8922\n",
      "Epoch 6/20 | Train Loss: 0.3223 | Val Loss: 0.2884 | MAE: 0.2126 | RMSE: 0.2884 | RÂ²: 0.9109\n",
      "Epoch 7/20 | Train Loss: 0.3064 | Val Loss: 0.2809 | MAE: 0.2031 | RMSE: 0.2809 | RÂ²: 0.9155\n",
      "Epoch 8/20 | Train Loss: 0.2922 | Val Loss: 0.2748 | MAE: 0.2003 | RMSE: 0.2748 | RÂ²: 0.9191\n",
      "Epoch 9/20 | Train Loss: 0.2865 | Val Loss: 0.2680 | MAE: 0.1899 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 10/20 | Train Loss: 0.2740 | Val Loss: 0.2704 | MAE: 0.1926 | RMSE: 0.2704 | RÂ²: 0.9217\n",
      "Epoch 11/20 | Train Loss: 0.2690 | Val Loss: 0.2600 | MAE: 0.1823 | RMSE: 0.2600 | RÂ²: 0.9276\n",
      "Epoch 12/20 | Train Loss: 0.2657 | Val Loss: 0.2635 | MAE: 0.1860 | RMSE: 0.2635 | RÂ²: 0.9257\n",
      "Epoch 13/20 | Train Loss: 0.2619 | Val Loss: 0.2692 | MAE: 0.1887 | RMSE: 0.2692 | RÂ²: 0.9224\n",
      "Epoch 14/20 | Train Loss: 0.2555 | Val Loss: 0.2709 | MAE: 0.1909 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 15/20 | Train Loss: 0.2545 | Val Loss: 0.2613 | MAE: 0.1828 | RMSE: 0.2613 | RÂ²: 0.9269\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2600 | MAE = 0.1823\n",
      "\n",
      "ðŸ”§ [105/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8240 | Val Loss: 0.6506 | MAE: 0.5484 | RMSE: 0.6506 | RÂ²: 0.5468\n",
      "Epoch 2/20 | Train Loss: 0.5351 | Val Loss: 0.4786 | MAE: 0.3869 | RMSE: 0.4786 | RÂ²: 0.7548\n",
      "Epoch 3/20 | Train Loss: 0.4354 | Val Loss: 0.3593 | MAE: 0.2801 | RMSE: 0.3593 | RÂ²: 0.8618\n",
      "Epoch 4/20 | Train Loss: 0.3981 | Val Loss: 0.3066 | MAE: 0.2292 | RMSE: 0.3066 | RÂ²: 0.8994\n",
      "Epoch 5/20 | Train Loss: 0.3625 | Val Loss: 0.2838 | MAE: 0.1986 | RMSE: 0.2838 | RÂ²: 0.9138\n",
      "Epoch 6/20 | Train Loss: 0.3489 | Val Loss: 0.2761 | MAE: 0.1945 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 7/20 | Train Loss: 0.3342 | Val Loss: 0.2778 | MAE: 0.1937 | RMSE: 0.2778 | RÂ²: 0.9174\n",
      "Epoch 8/20 | Train Loss: 0.3208 | Val Loss: 0.2625 | MAE: 0.1844 | RMSE: 0.2625 | RÂ²: 0.9262\n",
      "Epoch 9/20 | Train Loss: 0.3137 | Val Loss: 0.2685 | MAE: 0.1889 | RMSE: 0.2685 | RÂ²: 0.9228\n",
      "Epoch 10/20 | Train Loss: 0.3038 | Val Loss: 0.2709 | MAE: 0.1870 | RMSE: 0.2709 | RÂ²: 0.9215\n",
      "Epoch 11/20 | Train Loss: 0.2925 | Val Loss: 0.2652 | MAE: 0.1854 | RMSE: 0.2652 | RÂ²: 0.9247\n",
      "Epoch 12/20 | Train Loss: 0.2900 | Val Loss: 0.2705 | MAE: 0.1909 | RMSE: 0.2705 | RÂ²: 0.9217\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2625 | MAE = 0.1844\n",
      "\n",
      "ðŸ”§ [106/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8875 | Val Loss: 0.8663 | MAE: 0.7340 | RMSE: 0.8663 | RÂ²: 0.1965\n",
      "Epoch 2/20 | Train Loss: 0.6732 | Val Loss: 0.7434 | MAE: 0.6241 | RMSE: 0.7434 | RÂ²: 0.4084\n",
      "Epoch 3/20 | Train Loss: 0.5801 | Val Loss: 0.5788 | MAE: 0.4766 | RMSE: 0.5788 | RÂ²: 0.6413\n",
      "Epoch 4/20 | Train Loss: 0.5287 | Val Loss: 0.4447 | MAE: 0.3507 | RMSE: 0.4447 | RÂ²: 0.7882\n",
      "Epoch 5/20 | Train Loss: 0.4990 | Val Loss: 0.3264 | MAE: 0.2480 | RMSE: 0.3264 | RÂ²: 0.8860\n",
      "Epoch 6/20 | Train Loss: 0.4733 | Val Loss: 0.3070 | MAE: 0.2247 | RMSE: 0.3070 | RÂ²: 0.8991\n",
      "Epoch 7/20 | Train Loss: 0.4476 | Val Loss: 0.3052 | MAE: 0.2195 | RMSE: 0.3052 | RÂ²: 0.9003\n",
      "Epoch 8/20 | Train Loss: 0.4321 | Val Loss: 0.3054 | MAE: 0.2192 | RMSE: 0.3054 | RÂ²: 0.9002\n",
      "Epoch 9/20 | Train Loss: 0.4196 | Val Loss: 0.3240 | MAE: 0.2385 | RMSE: 0.3240 | RÂ²: 0.8876\n",
      "Epoch 10/20 | Train Loss: 0.4079 | Val Loss: 0.3100 | MAE: 0.2254 | RMSE: 0.3100 | RÂ²: 0.8971\n",
      "Epoch 11/20 | Train Loss: 0.3996 | Val Loss: 0.3038 | MAE: 0.2194 | RMSE: 0.3038 | RÂ²: 0.9012\n",
      "Epoch 12/20 | Train Loss: 0.3925 | Val Loss: 0.3130 | MAE: 0.2264 | RMSE: 0.3130 | RÂ²: 0.8951\n",
      "Epoch 13/20 | Train Loss: 0.3776 | Val Loss: 0.3231 | MAE: 0.2376 | RMSE: 0.3231 | RÂ²: 0.8882\n",
      "Epoch 14/20 | Train Loss: 0.3664 | Val Loss: 0.3098 | MAE: 0.2250 | RMSE: 0.3098 | RÂ²: 0.8973\n",
      "Epoch 15/20 | Train Loss: 0.3672 | Val Loss: 0.3049 | MAE: 0.2180 | RMSE: 0.3049 | RÂ²: 0.9005\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3038 | MAE = 0.2194\n",
      "\n",
      "ðŸ”§ [107/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[512, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.7769 | Val Loss: 0.9057 | MAE: 0.7635 | RMSE: 0.9057 | RÂ²: 0.1219\n",
      "Epoch 2/20 | Train Loss: 0.5128 | Val Loss: 0.7901 | MAE: 0.6628 | RMSE: 0.7901 | RÂ²: 0.3316\n",
      "Epoch 3/20 | Train Loss: 0.4272 | Val Loss: 0.6321 | MAE: 0.5305 | RMSE: 0.6321 | RÂ²: 0.5722\n",
      "Epoch 4/20 | Train Loss: 0.3808 | Val Loss: 0.5082 | MAE: 0.4232 | RMSE: 0.5082 | RÂ²: 0.7235\n",
      "Epoch 5/20 | Train Loss: 0.3460 | Val Loss: 0.4246 | MAE: 0.3339 | RMSE: 0.4246 | RÂ²: 0.8070\n",
      "Epoch 6/20 | Train Loss: 0.3273 | Val Loss: 0.3050 | MAE: 0.2331 | RMSE: 0.3050 | RÂ²: 0.9004\n",
      "Epoch 7/20 | Train Loss: 0.3159 | Val Loss: 0.2816 | MAE: 0.2078 | RMSE: 0.2816 | RÂ²: 0.9151\n",
      "Epoch 8/20 | Train Loss: 0.3032 | Val Loss: 0.2735 | MAE: 0.1942 | RMSE: 0.2735 | RÂ²: 0.9199\n",
      "Epoch 9/20 | Train Loss: 0.2987 | Val Loss: 0.2608 | MAE: 0.1839 | RMSE: 0.2608 | RÂ²: 0.9272\n",
      "Epoch 10/20 | Train Loss: 0.2872 | Val Loss: 0.2557 | MAE: 0.1806 | RMSE: 0.2557 | RÂ²: 0.9300\n",
      "Epoch 11/20 | Train Loss: 0.2808 | Val Loss: 0.2583 | MAE: 0.1826 | RMSE: 0.2583 | RÂ²: 0.9286\n",
      "Epoch 12/20 | Train Loss: 0.2774 | Val Loss: 0.2601 | MAE: 0.1833 | RMSE: 0.2601 | RÂ²: 0.9276\n",
      "Epoch 13/20 | Train Loss: 0.2722 | Val Loss: 0.2662 | MAE: 0.1901 | RMSE: 0.2662 | RÂ²: 0.9241\n",
      "Epoch 14/20 | Train Loss: 0.2717 | Val Loss: 0.2615 | MAE: 0.1841 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2557 | MAE = 0.1806\n",
      "\n",
      "ðŸ”§ [108/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8440 | Val Loss: 0.6981 | MAE: 0.5589 | RMSE: 0.6981 | RÂ²: 0.4783\n",
      "Epoch 2/20 | Train Loss: 0.5415 | Val Loss: 0.4035 | MAE: 0.3286 | RMSE: 0.4035 | RÂ²: 0.8257\n",
      "Epoch 3/20 | Train Loss: 0.4721 | Val Loss: 0.3751 | MAE: 0.2785 | RMSE: 0.3751 | RÂ²: 0.8494\n",
      "Epoch 4/20 | Train Loss: 0.4184 | Val Loss: 0.3423 | MAE: 0.2570 | RMSE: 0.3423 | RÂ²: 0.8746\n",
      "Epoch 5/20 | Train Loss: 0.3906 | Val Loss: 0.2892 | MAE: 0.2064 | RMSE: 0.2892 | RÂ²: 0.9105\n",
      "Epoch 6/20 | Train Loss: 0.3721 | Val Loss: 0.2786 | MAE: 0.2009 | RMSE: 0.2786 | RÂ²: 0.9169\n",
      "Epoch 7/20 | Train Loss: 0.3528 | Val Loss: 0.2815 | MAE: 0.1987 | RMSE: 0.2815 | RÂ²: 0.9152\n",
      "Epoch 8/20 | Train Loss: 0.3352 | Val Loss: 0.2661 | MAE: 0.1874 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 9/20 | Train Loss: 0.3236 | Val Loss: 0.2687 | MAE: 0.1890 | RMSE: 0.2687 | RÂ²: 0.9227\n",
      "Epoch 10/20 | Train Loss: 0.3181 | Val Loss: 0.2715 | MAE: 0.1885 | RMSE: 0.2715 | RÂ²: 0.9211\n",
      "Epoch 11/20 | Train Loss: 0.3042 | Val Loss: 0.2759 | MAE: 0.1918 | RMSE: 0.2759 | RÂ²: 0.9185\n",
      "Epoch 12/20 | Train Loss: 0.3010 | Val Loss: 0.2720 | MAE: 0.1894 | RMSE: 0.2720 | RÂ²: 0.9208\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2661 | MAE = 0.1874\n",
      "\n",
      "ðŸ”§ [109/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.7809 | Val Loss: 0.7499 | MAE: 0.6102 | RMSE: 0.7499 | RÂ²: 0.3979\n",
      "Epoch 2/20 | Train Loss: 0.4676 | Val Loss: 0.4402 | MAE: 0.3668 | RMSE: 0.4402 | RÂ²: 0.7925\n",
      "Epoch 3/20 | Train Loss: 0.3967 | Val Loss: 0.3689 | MAE: 0.2909 | RMSE: 0.3689 | RÂ²: 0.8543\n",
      "Epoch 4/20 | Train Loss: 0.3582 | Val Loss: 0.3261 | MAE: 0.2255 | RMSE: 0.3261 | RÂ²: 0.8861\n",
      "Epoch 5/20 | Train Loss: 0.3258 | Val Loss: 0.2788 | MAE: 0.2018 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "Epoch 6/20 | Train Loss: 0.3132 | Val Loss: 0.2653 | MAE: 0.1885 | RMSE: 0.2653 | RÂ²: 0.9246\n",
      "Epoch 7/20 | Train Loss: 0.2974 | Val Loss: 0.2656 | MAE: 0.1914 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "Epoch 8/20 | Train Loss: 0.2883 | Val Loss: 0.2556 | MAE: 0.1747 | RMSE: 0.2556 | RÂ²: 0.9300\n",
      "Epoch 9/20 | Train Loss: 0.2816 | Val Loss: 0.2655 | MAE: 0.1817 | RMSE: 0.2655 | RÂ²: 0.9245\n",
      "Epoch 10/20 | Train Loss: 0.2754 | Val Loss: 0.2594 | MAE: 0.1788 | RMSE: 0.2594 | RÂ²: 0.9280\n",
      "Epoch 11/20 | Train Loss: 0.2680 | Val Loss: 0.2781 | MAE: 0.1902 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 12/20 | Train Loss: 0.2644 | Val Loss: 0.2763 | MAE: 0.1908 | RMSE: 0.2763 | RÂ²: 0.9183\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2556 | MAE = 0.1747\n",
      "\n",
      "ðŸ”§ [110/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 1.0230 | Val Loss: 0.9807 | MAE: 0.8255 | RMSE: 0.9807 | RÂ²: -0.0297\n",
      "Epoch 2/20 | Train Loss: 0.8760 | Val Loss: 0.8972 | MAE: 0.7592 | RMSE: 0.8972 | RÂ²: 0.1382\n",
      "Epoch 3/20 | Train Loss: 0.7740 | Val Loss: 0.7541 | MAE: 0.6353 | RMSE: 0.7541 | RÂ²: 0.3913\n",
      "Epoch 4/20 | Train Loss: 0.6927 | Val Loss: 0.5654 | MAE: 0.4603 | RMSE: 0.5654 | RÂ²: 0.6578\n",
      "Epoch 5/20 | Train Loss: 0.6308 | Val Loss: 0.4216 | MAE: 0.3099 | RMSE: 0.4216 | RÂ²: 0.8097\n",
      "Epoch 6/20 | Train Loss: 0.6135 | Val Loss: 0.3789 | MAE: 0.2620 | RMSE: 0.3789 | RÂ²: 0.8463\n",
      "Epoch 7/20 | Train Loss: 0.5684 | Val Loss: 0.3944 | MAE: 0.2694 | RMSE: 0.3944 | RÂ²: 0.8334\n",
      "Epoch 8/20 | Train Loss: 0.5491 | Val Loss: 0.3985 | MAE: 0.2805 | RMSE: 0.3985 | RÂ²: 0.8300\n",
      "Epoch 9/20 | Train Loss: 0.5393 | Val Loss: 0.4009 | MAE: 0.2979 | RMSE: 0.4009 | RÂ²: 0.8279\n",
      "Epoch 10/20 | Train Loss: 0.5133 | Val Loss: 0.4219 | MAE: 0.3203 | RMSE: 0.4219 | RÂ²: 0.8094\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3789 | MAE = 0.2620\n",
      "\n",
      "ðŸ”§ [111/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[512, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9559 | Val Loss: 0.9288 | MAE: 0.7843 | RMSE: 0.9288 | RÂ²: 0.0765\n",
      "Epoch 2/20 | Train Loss: 0.7349 | Val Loss: 0.8438 | MAE: 0.7137 | RMSE: 0.8438 | RÂ²: 0.2377\n",
      "Epoch 3/20 | Train Loss: 0.6182 | Val Loss: 0.7324 | MAE: 0.6139 | RMSE: 0.7324 | RÂ²: 0.4258\n",
      "Epoch 4/20 | Train Loss: 0.5352 | Val Loss: 0.5802 | MAE: 0.4762 | RMSE: 0.5802 | RÂ²: 0.6396\n",
      "Epoch 5/20 | Train Loss: 0.4970 | Val Loss: 0.4631 | MAE: 0.3593 | RMSE: 0.4631 | RÂ²: 0.7704\n",
      "Epoch 6/20 | Train Loss: 0.4663 | Val Loss: 0.3598 | MAE: 0.2731 | RMSE: 0.3598 | RÂ²: 0.8614\n",
      "Epoch 7/20 | Train Loss: 0.4417 | Val Loss: 0.3342 | MAE: 0.2496 | RMSE: 0.3342 | RÂ²: 0.8804\n",
      "Epoch 8/20 | Train Loss: 0.4256 | Val Loss: 0.3300 | MAE: 0.2416 | RMSE: 0.3300 | RÂ²: 0.8834\n",
      "Epoch 9/20 | Train Loss: 0.4097 | Val Loss: 0.3087 | MAE: 0.2253 | RMSE: 0.3087 | RÂ²: 0.8980\n",
      "Epoch 10/20 | Train Loss: 0.3970 | Val Loss: 0.2972 | MAE: 0.2141 | RMSE: 0.2972 | RÂ²: 0.9055\n",
      "Epoch 11/20 | Train Loss: 0.3869 | Val Loss: 0.2939 | MAE: 0.2125 | RMSE: 0.2939 | RÂ²: 0.9075\n",
      "Epoch 12/20 | Train Loss: 0.3780 | Val Loss: 0.2933 | MAE: 0.2123 | RMSE: 0.2933 | RÂ²: 0.9079\n",
      "Epoch 13/20 | Train Loss: 0.3704 | Val Loss: 0.2900 | MAE: 0.2078 | RMSE: 0.2900 | RÂ²: 0.9099\n",
      "Epoch 14/20 | Train Loss: 0.3685 | Val Loss: 0.2882 | MAE: 0.2060 | RMSE: 0.2882 | RÂ²: 0.9111\n",
      "Epoch 15/20 | Train Loss: 0.3572 | Val Loss: 0.2889 | MAE: 0.2076 | RMSE: 0.2889 | RÂ²: 0.9106\n",
      "Epoch 16/20 | Train Loss: 0.3546 | Val Loss: 0.2916 | MAE: 0.2064 | RMSE: 0.2916 | RÂ²: 0.9089\n",
      "Epoch 17/20 | Train Loss: 0.3496 | Val Loss: 0.2872 | MAE: 0.2055 | RMSE: 0.2872 | RÂ²: 0.9117\n",
      "Epoch 18/20 | Train Loss: 0.3474 | Val Loss: 0.2836 | MAE: 0.2023 | RMSE: 0.2836 | RÂ²: 0.9139\n",
      "Epoch 19/20 | Train Loss: 0.3407 | Val Loss: 0.2814 | MAE: 0.1996 | RMSE: 0.2814 | RÂ²: 0.9153\n",
      "Epoch 20/20 | Train Loss: 0.3354 | Val Loss: 0.2809 | MAE: 0.2007 | RMSE: 0.2809 | RÂ²: 0.9156\n",
      "âœ… RMSE = 0.2809 | MAE = 0.2007\n",
      "\n",
      "ðŸ”§ [112/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.7422 | Val Loss: 0.7608 | MAE: 0.6351 | RMSE: 0.7608 | RÂ²: 0.3804\n",
      "Epoch 2/20 | Train Loss: 0.4345 | Val Loss: 0.4787 | MAE: 0.3988 | RMSE: 0.4787 | RÂ²: 0.7547\n",
      "Epoch 3/20 | Train Loss: 0.3566 | Val Loss: 0.3748 | MAE: 0.2984 | RMSE: 0.3748 | RÂ²: 0.8496\n",
      "Epoch 4/20 | Train Loss: 0.3223 | Val Loss: 0.2967 | MAE: 0.2205 | RMSE: 0.2967 | RÂ²: 0.9058\n",
      "Epoch 5/20 | Train Loss: 0.2976 | Val Loss: 0.2781 | MAE: 0.1941 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 6/20 | Train Loss: 0.2848 | Val Loss: 0.2734 | MAE: 0.1886 | RMSE: 0.2734 | RÂ²: 0.9200\n",
      "Epoch 7/20 | Train Loss: 0.2731 | Val Loss: 0.2680 | MAE: 0.1860 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 8/20 | Train Loss: 0.2654 | Val Loss: 0.2642 | MAE: 0.1801 | RMSE: 0.2642 | RÂ²: 0.9253\n",
      "Epoch 9/20 | Train Loss: 0.2638 | Val Loss: 0.2556 | MAE: 0.1748 | RMSE: 0.2556 | RÂ²: 0.9300\n",
      "Epoch 10/20 | Train Loss: 0.2586 | Val Loss: 0.2611 | MAE: 0.1783 | RMSE: 0.2611 | RÂ²: 0.9270\n",
      "Epoch 11/20 | Train Loss: 0.2505 | Val Loss: 0.2651 | MAE: 0.1813 | RMSE: 0.2651 | RÂ²: 0.9248\n",
      "Epoch 12/20 | Train Loss: 0.2470 | Val Loss: 0.2727 | MAE: 0.1865 | RMSE: 0.2727 | RÂ²: 0.9204\n",
      "Epoch 13/20 | Train Loss: 0.2439 | Val Loss: 0.2713 | MAE: 0.1893 | RMSE: 0.2713 | RÂ²: 0.9212\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2556 | MAE = 0.1748\n",
      "\n",
      "ðŸ”§ [113/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 1.0983 | Val Loss: 0.5449 | MAE: 0.4566 | RMSE: 0.5449 | RÂ²: 0.6821\n",
      "Epoch 2/20 | Train Loss: 0.4902 | Val Loss: 0.7295 | MAE: 0.6580 | RMSE: 0.7295 | RÂ²: 0.4303\n",
      "Epoch 3/20 | Train Loss: 0.3990 | Val Loss: 1.0600 | MAE: 0.8830 | RMSE: 1.0600 | RÂ²: -0.2029\n",
      "Epoch 4/20 | Train Loss: 0.3673 | Val Loss: 0.6054 | MAE: 0.5214 | RMSE: 0.6054 | RÂ²: 0.6077\n",
      "Epoch 5/20 | Train Loss: 0.3458 | Val Loss: 0.7784 | MAE: 0.6676 | RMSE: 0.7784 | RÂ²: 0.3513\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.5449 | MAE = 0.4566\n",
      "\n",
      "ðŸ”§ [114/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[1024, 512, 256, 128], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8791 | Val Loss: 0.9362 | MAE: 0.7883 | RMSE: 0.9362 | RÂ²: 0.0618\n",
      "Epoch 2/20 | Train Loss: 0.6421 | Val Loss: 0.8160 | MAE: 0.6833 | RMSE: 0.8160 | RÂ²: 0.2871\n",
      "Epoch 3/20 | Train Loss: 0.5261 | Val Loss: 0.6215 | MAE: 0.5040 | RMSE: 0.6215 | RÂ²: 0.5865\n",
      "Epoch 4/20 | Train Loss: 0.4910 | Val Loss: 0.4361 | MAE: 0.3385 | RMSE: 0.4361 | RÂ²: 0.7964\n",
      "Epoch 5/20 | Train Loss: 0.4527 | Val Loss: 0.3427 | MAE: 0.2556 | RMSE: 0.3427 | RÂ²: 0.8742\n",
      "Epoch 6/20 | Train Loss: 0.4329 | Val Loss: 0.3036 | MAE: 0.2224 | RMSE: 0.3036 | RÂ²: 0.9013\n",
      "Epoch 7/20 | Train Loss: 0.4159 | Val Loss: 0.3047 | MAE: 0.2210 | RMSE: 0.3047 | RÂ²: 0.9006\n",
      "Epoch 8/20 | Train Loss: 0.3964 | Val Loss: 0.3000 | MAE: 0.2169 | RMSE: 0.3000 | RÂ²: 0.9037\n",
      "Epoch 9/20 | Train Loss: 0.3826 | Val Loss: 0.3040 | MAE: 0.2201 | RMSE: 0.3040 | RÂ²: 0.9010\n",
      "Epoch 10/20 | Train Loss: 0.3725 | Val Loss: 0.2911 | MAE: 0.2111 | RMSE: 0.2911 | RÂ²: 0.9093\n",
      "Epoch 11/20 | Train Loss: 0.3653 | Val Loss: 0.2914 | MAE: 0.2125 | RMSE: 0.2914 | RÂ²: 0.9091\n",
      "Epoch 12/20 | Train Loss: 0.3560 | Val Loss: 0.2952 | MAE: 0.2118 | RMSE: 0.2952 | RÂ²: 0.9067\n",
      "Epoch 13/20 | Train Loss: 0.3525 | Val Loss: 0.2878 | MAE: 0.2041 | RMSE: 0.2878 | RÂ²: 0.9113\n",
      "Epoch 14/20 | Train Loss: 0.3428 | Val Loss: 0.2830 | MAE: 0.2010 | RMSE: 0.2830 | RÂ²: 0.9142\n",
      "Epoch 15/20 | Train Loss: 0.3400 | Val Loss: 0.2870 | MAE: 0.2032 | RMSE: 0.2870 | RÂ²: 0.9118\n",
      "Epoch 16/20 | Train Loss: 0.3280 | Val Loss: 0.2843 | MAE: 0.1995 | RMSE: 0.2843 | RÂ²: 0.9135\n",
      "Epoch 17/20 | Train Loss: 0.3259 | Val Loss: 0.2797 | MAE: 0.1953 | RMSE: 0.2797 | RÂ²: 0.9163\n",
      "Epoch 18/20 | Train Loss: 0.3195 | Val Loss: 0.2816 | MAE: 0.1974 | RMSE: 0.2816 | RÂ²: 0.9151\n",
      "Epoch 19/20 | Train Loss: 0.3211 | Val Loss: 0.2845 | MAE: 0.1980 | RMSE: 0.2845 | RÂ²: 0.9134\n",
      "Epoch 20/20 | Train Loss: 0.3149 | Val Loss: 0.2801 | MAE: 0.1925 | RMSE: 0.2801 | RÂ²: 0.9160\n",
      "âœ… RMSE = 0.2797 | MAE = 0.1953\n",
      "\n",
      "ðŸ”§ [115/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[2048, 1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 1.0243 | Val Loss: 0.9622 | MAE: 0.8133 | RMSE: 0.9622 | RÂ²: 0.0089\n",
      "Epoch 2/20 | Train Loss: 0.8963 | Val Loss: 0.8783 | MAE: 0.7443 | RMSE: 0.8783 | RÂ²: 0.1741\n",
      "Epoch 3/20 | Train Loss: 0.7995 | Val Loss: 0.7599 | MAE: 0.6436 | RMSE: 0.7599 | RÂ²: 0.3818\n",
      "Epoch 4/20 | Train Loss: 0.7231 | Val Loss: 0.6008 | MAE: 0.5002 | RMSE: 0.6008 | RÂ²: 0.6136\n",
      "Epoch 5/20 | Train Loss: 0.6746 | Val Loss: 0.4653 | MAE: 0.3692 | RMSE: 0.4653 | RÂ²: 0.7683\n",
      "Epoch 6/20 | Train Loss: 0.6374 | Val Loss: 0.4085 | MAE: 0.2971 | RMSE: 0.4085 | RÂ²: 0.8213\n",
      "Epoch 7/20 | Train Loss: 0.5928 | Val Loss: 0.3974 | MAE: 0.2767 | RMSE: 0.3974 | RÂ²: 0.8309\n",
      "Epoch 8/20 | Train Loss: 0.5771 | Val Loss: 0.3980 | MAE: 0.2888 | RMSE: 0.3980 | RÂ²: 0.8304\n",
      "Epoch 9/20 | Train Loss: 0.5565 | Val Loss: 0.4069 | MAE: 0.3125 | RMSE: 0.4069 | RÂ²: 0.8228\n",
      "Epoch 10/20 | Train Loss: 0.5405 | Val Loss: 0.4191 | MAE: 0.3316 | RMSE: 0.4191 | RÂ²: 0.8120\n",
      "Epoch 11/20 | Train Loss: 0.5264 | Val Loss: 0.4208 | MAE: 0.3341 | RMSE: 0.4208 | RÂ²: 0.8104\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3974 | MAE = 0.2767\n",
      "\n",
      "ðŸ”§ [116/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9292 | Val Loss: 0.9718 | MAE: 0.8182 | RMSE: 0.9718 | RÂ²: -0.0110\n",
      "Epoch 2/20 | Train Loss: 0.7302 | Val Loss: 0.8811 | MAE: 0.7386 | RMSE: 0.8811 | RÂ²: 0.1689\n",
      "Epoch 3/20 | Train Loss: 0.6120 | Val Loss: 0.7000 | MAE: 0.5730 | RMSE: 0.7000 | RÂ²: 0.4755\n",
      "Epoch 4/20 | Train Loss: 0.5603 | Val Loss: 0.5025 | MAE: 0.3957 | RMSE: 0.5025 | RÂ²: 0.7297\n",
      "Epoch 5/20 | Train Loss: 0.5175 | Val Loss: 0.3548 | MAE: 0.2674 | RMSE: 0.3548 | RÂ²: 0.8653\n",
      "Epoch 6/20 | Train Loss: 0.5075 | Val Loss: 0.3124 | MAE: 0.2241 | RMSE: 0.3124 | RÂ²: 0.8956\n",
      "Epoch 7/20 | Train Loss: 0.4790 | Val Loss: 0.3195 | MAE: 0.2299 | RMSE: 0.3195 | RÂ²: 0.8907\n",
      "Epoch 8/20 | Train Loss: 0.4396 | Val Loss: 0.3415 | MAE: 0.2522 | RMSE: 0.3415 | RÂ²: 0.8751\n",
      "Epoch 9/20 | Train Loss: 0.4447 | Val Loss: 0.3515 | MAE: 0.2673 | RMSE: 0.3515 | RÂ²: 0.8678\n",
      "Epoch 10/20 | Train Loss: 0.4312 | Val Loss: 0.3586 | MAE: 0.2727 | RMSE: 0.3586 | RÂ²: 0.8623\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3124 | MAE = 0.2241\n",
      "\n",
      "ðŸ”§ [117/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[1024, 512, 256, 128], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.6606 | Val Loss: 0.7811 | MAE: 0.6556 | RMSE: 0.7811 | RÂ²: 0.3469\n",
      "Epoch 2/20 | Train Loss: 0.3459 | Val Loss: 0.4940 | MAE: 0.4081 | RMSE: 0.4940 | RÂ²: 0.7388\n",
      "Epoch 3/20 | Train Loss: 0.3068 | Val Loss: 0.2824 | MAE: 0.2122 | RMSE: 0.2824 | RÂ²: 0.9146\n",
      "Epoch 4/20 | Train Loss: 0.2841 | Val Loss: 0.2760 | MAE: 0.1961 | RMSE: 0.2760 | RÂ²: 0.9185\n",
      "Epoch 5/20 | Train Loss: 0.2732 | Val Loss: 0.3110 | MAE: 0.2121 | RMSE: 0.3110 | RÂ²: 0.8964\n",
      "Epoch 6/20 | Train Loss: 0.2619 | Val Loss: 0.2889 | MAE: 0.2079 | RMSE: 0.2889 | RÂ²: 0.9107\n",
      "Epoch 7/20 | Train Loss: 0.2529 | Val Loss: 0.2853 | MAE: 0.1966 | RMSE: 0.2853 | RÂ²: 0.9128\n",
      "Epoch 8/20 | Train Loss: 0.2457 | Val Loss: 0.2595 | MAE: 0.1801 | RMSE: 0.2595 | RÂ²: 0.9279\n",
      "Epoch 9/20 | Train Loss: 0.2438 | Val Loss: 0.2648 | MAE: 0.1803 | RMSE: 0.2648 | RÂ²: 0.9250\n",
      "Epoch 10/20 | Train Loss: 0.2384 | Val Loss: 0.2595 | MAE: 0.1759 | RMSE: 0.2595 | RÂ²: 0.9279\n",
      "Epoch 11/20 | Train Loss: 0.2391 | Val Loss: 0.2674 | MAE: 0.1827 | RMSE: 0.2674 | RÂ²: 0.9234\n",
      "Epoch 12/20 | Train Loss: 0.2351 | Val Loss: 0.2697 | MAE: 0.1845 | RMSE: 0.2697 | RÂ²: 0.9222\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2595 | MAE = 0.1801\n",
      "\n",
      "ðŸ”§ [118/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 256, 1024], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.0425 | Val Loss: 0.8043 | MAE: 0.6697 | RMSE: 0.8043 | RÂ²: 0.3075\n",
      "Epoch 2/20 | Train Loss: 0.5530 | Val Loss: 0.5832 | MAE: 0.4642 | RMSE: 0.5832 | RÂ²: 0.6359\n",
      "Epoch 3/20 | Train Loss: 0.4782 | Val Loss: 0.3602 | MAE: 0.2762 | RMSE: 0.3602 | RÂ²: 0.8611\n",
      "Epoch 4/20 | Train Loss: 0.4373 | Val Loss: 0.3487 | MAE: 0.2576 | RMSE: 0.3487 | RÂ²: 0.8698\n",
      "Epoch 5/20 | Train Loss: 0.3917 | Val Loss: 0.3067 | MAE: 0.2277 | RMSE: 0.3067 | RÂ²: 0.8993\n",
      "Epoch 6/20 | Train Loss: 0.3637 | Val Loss: 0.2828 | MAE: 0.1971 | RMSE: 0.2828 | RÂ²: 0.9144\n",
      "Epoch 7/20 | Train Loss: 0.3417 | Val Loss: 0.2821 | MAE: 0.2056 | RMSE: 0.2821 | RÂ²: 0.9148\n",
      "Epoch 8/20 | Train Loss: 0.3244 | Val Loss: 0.2820 | MAE: 0.1853 | RMSE: 0.2820 | RÂ²: 0.9149\n",
      "Epoch 9/20 | Train Loss: 0.3241 | Val Loss: 0.2893 | MAE: 0.2026 | RMSE: 0.2893 | RÂ²: 0.9104\n",
      "Epoch 10/20 | Train Loss: 0.3175 | Val Loss: 0.2685 | MAE: 0.1895 | RMSE: 0.2685 | RÂ²: 0.9228\n",
      "Epoch 11/20 | Train Loss: 0.3031 | Val Loss: 0.2648 | MAE: 0.1836 | RMSE: 0.2648 | RÂ²: 0.9249\n",
      "Epoch 12/20 | Train Loss: 0.2992 | Val Loss: 0.2866 | MAE: 0.1937 | RMSE: 0.2866 | RÂ²: 0.9121\n",
      "Epoch 13/20 | Train Loss: 0.2978 | Val Loss: 0.2695 | MAE: 0.1893 | RMSE: 0.2695 | RÂ²: 0.9223\n",
      "Epoch 14/20 | Train Loss: 0.2879 | Val Loss: 0.2674 | MAE: 0.1874 | RMSE: 0.2674 | RÂ²: 0.9234\n",
      "Epoch 15/20 | Train Loss: 0.2771 | Val Loss: 0.2765 | MAE: 0.1888 | RMSE: 0.2765 | RÂ²: 0.9181\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2648 | MAE = 0.1836\n",
      "\n",
      "ðŸ”§ [119/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[256, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9307 | Val Loss: 0.8892 | MAE: 0.7544 | RMSE: 0.8892 | RÂ²: 0.1535\n",
      "Epoch 2/20 | Train Loss: 0.6903 | Val Loss: 0.7497 | MAE: 0.6325 | RMSE: 0.7497 | RÂ²: 0.3983\n",
      "Epoch 3/20 | Train Loss: 0.5617 | Val Loss: 0.6329 | MAE: 0.5262 | RMSE: 0.6329 | RÂ²: 0.5711\n",
      "Epoch 4/20 | Train Loss: 0.4795 | Val Loss: 0.4881 | MAE: 0.3971 | RMSE: 0.4881 | RÂ²: 0.7450\n",
      "Epoch 5/20 | Train Loss: 0.4337 | Val Loss: 0.3833 | MAE: 0.3035 | RMSE: 0.3833 | RÂ²: 0.8427\n",
      "Epoch 6/20 | Train Loss: 0.4020 | Val Loss: 0.3452 | MAE: 0.2656 | RMSE: 0.3452 | RÂ²: 0.8724\n",
      "Epoch 7/20 | Train Loss: 0.3761 | Val Loss: 0.3292 | MAE: 0.2473 | RMSE: 0.3292 | RÂ²: 0.8840\n",
      "Epoch 8/20 | Train Loss: 0.3645 | Val Loss: 0.3189 | MAE: 0.2396 | RMSE: 0.3189 | RÂ²: 0.8911\n",
      "Epoch 9/20 | Train Loss: 0.3486 | Val Loss: 0.3057 | MAE: 0.2275 | RMSE: 0.3057 | RÂ²: 0.8999\n",
      "Epoch 10/20 | Train Loss: 0.3419 | Val Loss: 0.3026 | MAE: 0.2225 | RMSE: 0.3026 | RÂ²: 0.9020\n",
      "Epoch 11/20 | Train Loss: 0.3291 | Val Loss: 0.2967 | MAE: 0.2178 | RMSE: 0.2967 | RÂ²: 0.9058\n",
      "Epoch 12/20 | Train Loss: 0.3165 | Val Loss: 0.2956 | MAE: 0.2147 | RMSE: 0.2956 | RÂ²: 0.9065\n",
      "Epoch 13/20 | Train Loss: 0.3121 | Val Loss: 0.2896 | MAE: 0.2090 | RMSE: 0.2896 | RÂ²: 0.9102\n",
      "Epoch 14/20 | Train Loss: 0.3032 | Val Loss: 0.2840 | MAE: 0.2040 | RMSE: 0.2840 | RÂ²: 0.9137\n",
      "Epoch 15/20 | Train Loss: 0.3013 | Val Loss: 0.2829 | MAE: 0.2025 | RMSE: 0.2829 | RÂ²: 0.9143\n",
      "Epoch 16/20 | Train Loss: 0.2978 | Val Loss: 0.2837 | MAE: 0.2025 | RMSE: 0.2837 | RÂ²: 0.9138\n",
      "Epoch 17/20 | Train Loss: 0.2933 | Val Loss: 0.2847 | MAE: 0.2024 | RMSE: 0.2847 | RÂ²: 0.9133\n",
      "Epoch 18/20 | Train Loss: 0.2902 | Val Loss: 0.2834 | MAE: 0.2009 | RMSE: 0.2834 | RÂ²: 0.9140\n",
      "Epoch 19/20 | Train Loss: 0.2891 | Val Loss: 0.2825 | MAE: 0.1984 | RMSE: 0.2825 | RÂ²: 0.9146\n",
      "Epoch 20/20 | Train Loss: 0.2785 | Val Loss: 0.2754 | MAE: 0.1940 | RMSE: 0.2754 | RÂ²: 0.9188\n",
      "âœ… RMSE = 0.2754 | MAE = 0.1940\n",
      "\n",
      "ðŸ”§ [120/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9910 | Val Loss: 0.9310 | MAE: 0.7813 | RMSE: 0.9310 | RÂ²: 0.0721\n",
      "Epoch 2/20 | Train Loss: 0.7577 | Val Loss: 0.7602 | MAE: 0.6351 | RMSE: 0.7602 | RÂ²: 0.3813\n",
      "Epoch 3/20 | Train Loss: 0.6399 | Val Loss: 0.5019 | MAE: 0.3978 | RMSE: 0.5019 | RÂ²: 0.7303\n",
      "Epoch 4/20 | Train Loss: 0.5695 | Val Loss: 0.4163 | MAE: 0.3020 | RMSE: 0.4163 | RÂ²: 0.8144\n",
      "Epoch 5/20 | Train Loss: 0.5389 | Val Loss: 0.3911 | MAE: 0.2970 | RMSE: 0.3911 | RÂ²: 0.8363\n",
      "Epoch 6/20 | Train Loss: 0.4962 | Val Loss: 0.4137 | MAE: 0.3307 | RMSE: 0.4137 | RÂ²: 0.8168\n",
      "Epoch 7/20 | Train Loss: 0.4868 | Val Loss: 0.4348 | MAE: 0.3487 | RMSE: 0.4348 | RÂ²: 0.7976\n",
      "Epoch 8/20 | Train Loss: 0.4500 | Val Loss: 0.4076 | MAE: 0.3218 | RMSE: 0.4076 | RÂ²: 0.8221\n",
      "Epoch 9/20 | Train Loss: 0.4322 | Val Loss: 0.4032 | MAE: 0.3092 | RMSE: 0.4032 | RÂ²: 0.8260\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3911 | MAE = 0.2970\n",
      "\n",
      "ðŸ”§ [121/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.7707 | Val Loss: 0.9349 | MAE: 0.7810 | RMSE: 0.9349 | RÂ²: 0.0643\n",
      "Epoch 2/20 | Train Loss: 0.4337 | Val Loss: 0.7628 | MAE: 0.6396 | RMSE: 0.7628 | RÂ²: 0.3771\n",
      "Epoch 3/20 | Train Loss: 0.3846 | Val Loss: 0.5297 | MAE: 0.4319 | RMSE: 0.5297 | RÂ²: 0.6996\n",
      "Epoch 4/20 | Train Loss: 0.3528 | Val Loss: 0.3548 | MAE: 0.2800 | RMSE: 0.3548 | RÂ²: 0.8652\n",
      "Epoch 5/20 | Train Loss: 0.3422 | Val Loss: 0.2774 | MAE: 0.2031 | RMSE: 0.2774 | RÂ²: 0.9176\n",
      "Epoch 6/20 | Train Loss: 0.3204 | Val Loss: 0.2583 | MAE: 0.1725 | RMSE: 0.2583 | RÂ²: 0.9286\n",
      "Epoch 7/20 | Train Loss: 0.3073 | Val Loss: 0.2627 | MAE: 0.1689 | RMSE: 0.2627 | RÂ²: 0.9261\n",
      "Epoch 8/20 | Train Loss: 0.3048 | Val Loss: 0.2663 | MAE: 0.1777 | RMSE: 0.2663 | RÂ²: 0.9241\n",
      "Epoch 9/20 | Train Loss: 0.3033 | Val Loss: 0.2661 | MAE: 0.1768 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 10/20 | Train Loss: 0.2922 | Val Loss: 0.2688 | MAE: 0.1840 | RMSE: 0.2688 | RÂ²: 0.9226\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2583 | MAE = 0.1725\n",
      "\n",
      "ðŸ”§ [122/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[1024, 512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0657 | Val Loss: 0.9288 | MAE: 0.7837 | RMSE: 0.9288 | RÂ²: 0.0765\n",
      "Epoch 2/20 | Train Loss: 0.9275 | Val Loss: 0.8185 | MAE: 0.6942 | RMSE: 0.8185 | RÂ²: 0.2828\n",
      "Epoch 3/20 | Train Loss: 0.8255 | Val Loss: 0.6917 | MAE: 0.5888 | RMSE: 0.6917 | RÂ²: 0.4878\n",
      "Epoch 4/20 | Train Loss: 0.7562 | Val Loss: 0.5813 | MAE: 0.4831 | RMSE: 0.5813 | RÂ²: 0.6383\n",
      "Epoch 5/20 | Train Loss: 0.6898 | Val Loss: 0.4966 | MAE: 0.3871 | RMSE: 0.4966 | RÂ²: 0.7360\n",
      "Epoch 6/20 | Train Loss: 0.6352 | Val Loss: 0.4585 | MAE: 0.3416 | RMSE: 0.4585 | RÂ²: 0.7749\n",
      "Epoch 7/20 | Train Loss: 0.5826 | Val Loss: 0.4757 | MAE: 0.3552 | RMSE: 0.4757 | RÂ²: 0.7577\n",
      "Epoch 8/20 | Train Loss: 0.5639 | Val Loss: 0.4984 | MAE: 0.3731 | RMSE: 0.4984 | RÂ²: 0.7341\n",
      "Epoch 9/20 | Train Loss: 0.5460 | Val Loss: 0.4953 | MAE: 0.3522 | RMSE: 0.4953 | RÂ²: 0.7374\n",
      "Epoch 10/20 | Train Loss: 0.5302 | Val Loss: 0.4743 | MAE: 0.3300 | RMSE: 0.4743 | RÂ²: 0.7592\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.4585 | MAE = 0.3416\n",
      "\n",
      "ðŸ”§ [123/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[1024, 512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8115 | Val Loss: 0.9298 | MAE: 0.7812 | RMSE: 0.9298 | RÂ²: 0.0744\n",
      "Epoch 2/20 | Train Loss: 0.4988 | Val Loss: 0.8100 | MAE: 0.6765 | RMSE: 0.8100 | RÂ²: 0.2976\n",
      "Epoch 3/20 | Train Loss: 0.4053 | Val Loss: 0.6284 | MAE: 0.5316 | RMSE: 0.6284 | RÂ²: 0.5772\n",
      "Epoch 4/20 | Train Loss: 0.3552 | Val Loss: 0.5102 | MAE: 0.4161 | RMSE: 0.5102 | RÂ²: 0.7213\n",
      "Epoch 5/20 | Train Loss: 0.3296 | Val Loss: 0.3416 | MAE: 0.2715 | RMSE: 0.3416 | RÂ²: 0.8751\n",
      "Epoch 6/20 | Train Loss: 0.3115 | Val Loss: 0.2729 | MAE: 0.2026 | RMSE: 0.2729 | RÂ²: 0.9203\n",
      "Epoch 7/20 | Train Loss: 0.2949 | Val Loss: 0.2550 | MAE: 0.1814 | RMSE: 0.2550 | RÂ²: 0.9304\n",
      "Epoch 8/20 | Train Loss: 0.2835 | Val Loss: 0.2522 | MAE: 0.1773 | RMSE: 0.2522 | RÂ²: 0.9319\n",
      "Epoch 9/20 | Train Loss: 0.2804 | Val Loss: 0.2495 | MAE: 0.1753 | RMSE: 0.2495 | RÂ²: 0.9333\n",
      "Epoch 10/20 | Train Loss: 0.2745 | Val Loss: 0.2589 | MAE: 0.1814 | RMSE: 0.2589 | RÂ²: 0.9283\n",
      "Epoch 11/20 | Train Loss: 0.2717 | Val Loss: 0.2509 | MAE: 0.1727 | RMSE: 0.2509 | RÂ²: 0.9326\n",
      "Epoch 12/20 | Train Loss: 0.2711 | Val Loss: 0.2584 | MAE: 0.1791 | RMSE: 0.2584 | RÂ²: 0.9285\n",
      "Epoch 13/20 | Train Loss: 0.2649 | Val Loss: 0.2565 | MAE: 0.1732 | RMSE: 0.2565 | RÂ²: 0.9296\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2495 | MAE = 0.1753\n",
      "\n",
      "ðŸ”§ [124/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[1024, 256, 1024], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9725 | Val Loss: 0.7072 | MAE: 0.6027 | RMSE: 0.7072 | RÂ²: 0.4646\n",
      "Epoch 2/20 | Train Loss: 0.4618 | Val Loss: 0.5398 | MAE: 0.4525 | RMSE: 0.5398 | RÂ²: 0.6881\n",
      "Epoch 3/20 | Train Loss: 0.3628 | Val Loss: 0.4390 | MAE: 0.3451 | RMSE: 0.4390 | RÂ²: 0.7937\n",
      "Epoch 4/20 | Train Loss: 0.3275 | Val Loss: 0.2823 | MAE: 0.2083 | RMSE: 0.2823 | RÂ²: 0.9147\n",
      "Epoch 5/20 | Train Loss: 0.3005 | Val Loss: 0.2744 | MAE: 0.2035 | RMSE: 0.2744 | RÂ²: 0.9194\n",
      "Epoch 6/20 | Train Loss: 0.2886 | Val Loss: 0.2790 | MAE: 0.2052 | RMSE: 0.2790 | RÂ²: 0.9167\n",
      "Epoch 7/20 | Train Loss: 0.2879 | Val Loss: 0.2642 | MAE: 0.1836 | RMSE: 0.2642 | RÂ²: 0.9252\n",
      "Epoch 8/20 | Train Loss: 0.2640 | Val Loss: 0.2617 | MAE: 0.1834 | RMSE: 0.2617 | RÂ²: 0.9267\n",
      "Epoch 9/20 | Train Loss: 0.2558 | Val Loss: 0.2524 | MAE: 0.1764 | RMSE: 0.2524 | RÂ²: 0.9318\n",
      "Epoch 10/20 | Train Loss: 0.2582 | Val Loss: 0.2639 | MAE: 0.1784 | RMSE: 0.2639 | RÂ²: 0.9254\n",
      "Epoch 11/20 | Train Loss: 0.2441 | Val Loss: 0.2760 | MAE: 0.1882 | RMSE: 0.2760 | RÂ²: 0.9185\n",
      "Epoch 12/20 | Train Loss: 0.2425 | Val Loss: 0.2828 | MAE: 0.2039 | RMSE: 0.2828 | RÂ²: 0.9144\n",
      "Epoch 13/20 | Train Loss: 0.2373 | Val Loss: 0.2696 | MAE: 0.1893 | RMSE: 0.2696 | RÂ²: 0.9222\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2524 | MAE = 0.1764\n",
      "\n",
      "ðŸ”§ [125/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[512, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.7392 | Val Loss: 0.8615 | MAE: 0.7204 | RMSE: 0.8615 | RÂ²: 0.2054\n",
      "Epoch 2/20 | Train Loss: 0.4399 | Val Loss: 0.6944 | MAE: 0.5849 | RMSE: 0.6944 | RÂ²: 0.4838\n",
      "Epoch 3/20 | Train Loss: 0.3743 | Val Loss: 0.5333 | MAE: 0.4434 | RMSE: 0.5333 | RÂ²: 0.6955\n",
      "Epoch 4/20 | Train Loss: 0.3234 | Val Loss: 0.3508 | MAE: 0.2749 | RMSE: 0.3508 | RÂ²: 0.8683\n",
      "Epoch 5/20 | Train Loss: 0.3005 | Val Loss: 0.3006 | MAE: 0.2266 | RMSE: 0.3006 | RÂ²: 0.9032\n",
      "Epoch 6/20 | Train Loss: 0.2901 | Val Loss: 0.2653 | MAE: 0.1880 | RMSE: 0.2653 | RÂ²: 0.9246\n",
      "Epoch 7/20 | Train Loss: 0.2767 | Val Loss: 0.2579 | MAE: 0.1813 | RMSE: 0.2579 | RÂ²: 0.9288\n",
      "Epoch 8/20 | Train Loss: 0.2687 | Val Loss: 0.2583 | MAE: 0.1838 | RMSE: 0.2583 | RÂ²: 0.9286\n",
      "Epoch 9/20 | Train Loss: 0.2624 | Val Loss: 0.2565 | MAE: 0.1805 | RMSE: 0.2565 | RÂ²: 0.9296\n",
      "Epoch 10/20 | Train Loss: 0.2553 | Val Loss: 0.2522 | MAE: 0.1747 | RMSE: 0.2522 | RÂ²: 0.9319\n",
      "Epoch 11/20 | Train Loss: 0.2498 | Val Loss: 0.2722 | MAE: 0.1875 | RMSE: 0.2722 | RÂ²: 0.9207\n",
      "Epoch 12/20 | Train Loss: 0.2497 | Val Loss: 0.2602 | MAE: 0.1789 | RMSE: 0.2602 | RÂ²: 0.9275\n",
      "Epoch 13/20 | Train Loss: 0.2524 | Val Loss: 0.2660 | MAE: 0.1841 | RMSE: 0.2660 | RÂ²: 0.9243\n",
      "Epoch 14/20 | Train Loss: 0.2513 | Val Loss: 0.2699 | MAE: 0.1834 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2522 | MAE = 0.1747\n",
      "\n",
      "ðŸ”§ [126/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[1024, 256, 1024], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9803 | Val Loss: 0.8790 | MAE: 0.7473 | RMSE: 0.8790 | RÂ²: 0.1729\n",
      "Epoch 2/20 | Train Loss: 0.8129 | Val Loss: 0.7513 | MAE: 0.6388 | RMSE: 0.7513 | RÂ²: 0.3956\n",
      "Epoch 3/20 | Train Loss: 0.7363 | Val Loss: 0.6102 | MAE: 0.5111 | RMSE: 0.6102 | RÂ²: 0.6014\n",
      "Epoch 4/20 | Train Loss: 0.6850 | Val Loss: 0.4958 | MAE: 0.3994 | RMSE: 0.4958 | RÂ²: 0.7369\n",
      "Epoch 5/20 | Train Loss: 0.6397 | Val Loss: 0.4263 | MAE: 0.3158 | RMSE: 0.4263 | RÂ²: 0.8054\n",
      "Epoch 6/20 | Train Loss: 0.6057 | Val Loss: 0.3728 | MAE: 0.2686 | RMSE: 0.3728 | RÂ²: 0.8512\n",
      "Epoch 7/20 | Train Loss: 0.5861 | Val Loss: 0.3476 | MAE: 0.2587 | RMSE: 0.3476 | RÂ²: 0.8706\n",
      "Epoch 8/20 | Train Loss: 0.5550 | Val Loss: 0.3406 | MAE: 0.2541 | RMSE: 0.3406 | RÂ²: 0.8758\n",
      "Epoch 9/20 | Train Loss: 0.5456 | Val Loss: 0.3467 | MAE: 0.2531 | RMSE: 0.3467 | RÂ²: 0.8713\n",
      "Epoch 10/20 | Train Loss: 0.5361 | Val Loss: 0.3565 | MAE: 0.2595 | RMSE: 0.3565 | RÂ²: 0.8640\n",
      "Epoch 11/20 | Train Loss: 0.5132 | Val Loss: 0.3721 | MAE: 0.2803 | RMSE: 0.3721 | RÂ²: 0.8517\n",
      "Epoch 12/20 | Train Loss: 0.4996 | Val Loss: 0.3630 | MAE: 0.2676 | RMSE: 0.3630 | RÂ²: 0.8589\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3406 | MAE = 0.2541\n",
      "\n",
      "ðŸ”§ [127/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[512, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9470 | Val Loss: 0.9157 | MAE: 0.7733 | RMSE: 0.9157 | RÂ²: 0.1023\n",
      "Epoch 2/20 | Train Loss: 0.7493 | Val Loss: 0.8060 | MAE: 0.6826 | RMSE: 0.8060 | RÂ²: 0.3045\n",
      "Epoch 3/20 | Train Loss: 0.6643 | Val Loss: 0.6760 | MAE: 0.5687 | RMSE: 0.6760 | RÂ²: 0.5107\n",
      "Epoch 4/20 | Train Loss: 0.5825 | Val Loss: 0.5330 | MAE: 0.4369 | RMSE: 0.5330 | RÂ²: 0.6959\n",
      "Epoch 5/20 | Train Loss: 0.5412 | Val Loss: 0.4238 | MAE: 0.3286 | RMSE: 0.4238 | RÂ²: 0.8077\n",
      "Epoch 6/20 | Train Loss: 0.5135 | Val Loss: 0.3637 | MAE: 0.2684 | RMSE: 0.3637 | RÂ²: 0.8584\n",
      "Epoch 7/20 | Train Loss: 0.4943 | Val Loss: 0.3487 | MAE: 0.2490 | RMSE: 0.3487 | RÂ²: 0.8698\n",
      "Epoch 8/20 | Train Loss: 0.4742 | Val Loss: 0.3190 | MAE: 0.2335 | RMSE: 0.3190 | RÂ²: 0.8910\n",
      "Epoch 9/20 | Train Loss: 0.4565 | Val Loss: 0.3061 | MAE: 0.2250 | RMSE: 0.3061 | RÂ²: 0.8997\n",
      "Epoch 10/20 | Train Loss: 0.4456 | Val Loss: 0.2979 | MAE: 0.2188 | RMSE: 0.2979 | RÂ²: 0.9050\n",
      "Epoch 11/20 | Train Loss: 0.4325 | Val Loss: 0.3094 | MAE: 0.2222 | RMSE: 0.3094 | RÂ²: 0.8975\n",
      "Epoch 12/20 | Train Loss: 0.4243 | Val Loss: 0.3193 | MAE: 0.2258 | RMSE: 0.3193 | RÂ²: 0.8909\n",
      "Epoch 13/20 | Train Loss: 0.4198 | Val Loss: 0.3128 | MAE: 0.2266 | RMSE: 0.3128 | RÂ²: 0.8953\n",
      "Epoch 14/20 | Train Loss: 0.4146 | Val Loss: 0.3072 | MAE: 0.2228 | RMSE: 0.3072 | RÂ²: 0.8990\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2979 | MAE = 0.2188\n",
      "\n",
      "ðŸ”§ [128/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8540 | Val Loss: 0.9337 | MAE: 0.7829 | RMSE: 0.9337 | RÂ²: 0.0667\n",
      "Epoch 2/20 | Train Loss: 0.5295 | Val Loss: 0.8016 | MAE: 0.6704 | RMSE: 0.8016 | RÂ²: 0.3120\n",
      "Epoch 3/20 | Train Loss: 0.4410 | Val Loss: 0.6290 | MAE: 0.5229 | RMSE: 0.6290 | RÂ²: 0.5764\n",
      "Epoch 4/20 | Train Loss: 0.3996 | Val Loss: 0.5064 | MAE: 0.3875 | RMSE: 0.5064 | RÂ²: 0.7255\n",
      "Epoch 5/20 | Train Loss: 0.3662 | Val Loss: 0.3982 | MAE: 0.2631 | RMSE: 0.3982 | RÂ²: 0.8303\n",
      "Epoch 6/20 | Train Loss: 0.3455 | Val Loss: 0.3102 | MAE: 0.2095 | RMSE: 0.3102 | RÂ²: 0.8970\n",
      "Epoch 7/20 | Train Loss: 0.3390 | Val Loss: 0.3106 | MAE: 0.2059 | RMSE: 0.3106 | RÂ²: 0.8967\n",
      "Epoch 8/20 | Train Loss: 0.3275 | Val Loss: 0.2656 | MAE: 0.1852 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "Epoch 9/20 | Train Loss: 0.3191 | Val Loss: 0.3041 | MAE: 0.2015 | RMSE: 0.3041 | RÂ²: 0.9010\n",
      "Epoch 10/20 | Train Loss: 0.3253 | Val Loss: 0.2897 | MAE: 0.1919 | RMSE: 0.2897 | RÂ²: 0.9102\n",
      "Epoch 11/20 | Train Loss: 0.3117 | Val Loss: 0.2736 | MAE: 0.1876 | RMSE: 0.2736 | RÂ²: 0.9199\n",
      "Epoch 12/20 | Train Loss: 0.3009 | Val Loss: 0.2714 | MAE: 0.1837 | RMSE: 0.2714 | RÂ²: 0.9212\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2656 | MAE = 0.1852\n",
      "\n",
      "ðŸ”§ [129/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[512, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8412 | Val Loss: 0.9058 | MAE: 0.7671 | RMSE: 0.9058 | RÂ²: 0.1216\n",
      "Epoch 2/20 | Train Loss: 0.5715 | Val Loss: 0.7925 | MAE: 0.6627 | RMSE: 0.7925 | RÂ²: 0.3277\n",
      "Epoch 3/20 | Train Loss: 0.4877 | Val Loss: 0.6209 | MAE: 0.5075 | RMSE: 0.6209 | RÂ²: 0.5873\n",
      "Epoch 4/20 | Train Loss: 0.4424 | Val Loss: 0.4827 | MAE: 0.3936 | RMSE: 0.4827 | RÂ²: 0.7505\n",
      "Epoch 5/20 | Train Loss: 0.4096 | Val Loss: 0.3849 | MAE: 0.3067 | RMSE: 0.3849 | RÂ²: 0.8414\n",
      "Epoch 6/20 | Train Loss: 0.3870 | Val Loss: 0.3282 | MAE: 0.2493 | RMSE: 0.3282 | RÂ²: 0.8847\n",
      "Epoch 7/20 | Train Loss: 0.3690 | Val Loss: 0.2965 | MAE: 0.2108 | RMSE: 0.2965 | RÂ²: 0.9059\n",
      "Epoch 8/20 | Train Loss: 0.3559 | Val Loss: 0.2719 | MAE: 0.1942 | RMSE: 0.2719 | RÂ²: 0.9208\n",
      "Epoch 9/20 | Train Loss: 0.3471 | Val Loss: 0.2717 | MAE: 0.1942 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "Epoch 10/20 | Train Loss: 0.3423 | Val Loss: 0.2730 | MAE: 0.1940 | RMSE: 0.2730 | RÂ²: 0.9202\n",
      "Epoch 11/20 | Train Loss: 0.3294 | Val Loss: 0.2905 | MAE: 0.2020 | RMSE: 0.2905 | RÂ²: 0.9097\n",
      "Epoch 12/20 | Train Loss: 0.3292 | Val Loss: 0.2787 | MAE: 0.1979 | RMSE: 0.2787 | RÂ²: 0.9168\n",
      "Epoch 13/20 | Train Loss: 0.3211 | Val Loss: 0.2672 | MAE: 0.1873 | RMSE: 0.2672 | RÂ²: 0.9235\n",
      "Epoch 14/20 | Train Loss: 0.3143 | Val Loss: 0.2690 | MAE: 0.1904 | RMSE: 0.2690 | RÂ²: 0.9225\n",
      "Epoch 15/20 | Train Loss: 0.3073 | Val Loss: 0.2728 | MAE: 0.1917 | RMSE: 0.2728 | RÂ²: 0.9203\n",
      "Epoch 16/20 | Train Loss: 0.3005 | Val Loss: 0.2741 | MAE: 0.1908 | RMSE: 0.2741 | RÂ²: 0.9196\n",
      "Epoch 17/20 | Train Loss: 0.3009 | Val Loss: 0.2726 | MAE: 0.1908 | RMSE: 0.2726 | RÂ²: 0.9204\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2672 | MAE = 0.1873\n",
      "\n",
      "ðŸ”§ [130/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9700 | Val Loss: 0.8910 | MAE: 0.7534 | RMSE: 0.8910 | RÂ²: 0.1500\n",
      "Epoch 2/20 | Train Loss: 0.8173 | Val Loss: 0.7641 | MAE: 0.6392 | RMSE: 0.7641 | RÂ²: 0.3749\n",
      "Epoch 3/20 | Train Loss: 0.7356 | Val Loss: 0.6396 | MAE: 0.5320 | RMSE: 0.6396 | RÂ²: 0.5620\n",
      "Epoch 4/20 | Train Loss: 0.6795 | Val Loss: 0.5448 | MAE: 0.4486 | RMSE: 0.5448 | RÂ²: 0.6822\n",
      "Epoch 5/20 | Train Loss: 0.6336 | Val Loss: 0.4732 | MAE: 0.3807 | RMSE: 0.4732 | RÂ²: 0.7603\n",
      "Epoch 6/20 | Train Loss: 0.5964 | Val Loss: 0.4133 | MAE: 0.3222 | RMSE: 0.4133 | RÂ²: 0.8171\n",
      "Epoch 7/20 | Train Loss: 0.5715 | Val Loss: 0.3687 | MAE: 0.2819 | RMSE: 0.3687 | RÂ²: 0.8545\n",
      "Epoch 8/20 | Train Loss: 0.5511 | Val Loss: 0.3457 | MAE: 0.2600 | RMSE: 0.3457 | RÂ²: 0.8721\n",
      "Epoch 9/20 | Train Loss: 0.5309 | Val Loss: 0.3324 | MAE: 0.2476 | RMSE: 0.3324 | RÂ²: 0.8817\n",
      "Epoch 10/20 | Train Loss: 0.5168 | Val Loss: 0.3264 | MAE: 0.2413 | RMSE: 0.3264 | RÂ²: 0.8859\n",
      "Epoch 11/20 | Train Loss: 0.5096 | Val Loss: 0.3201 | MAE: 0.2367 | RMSE: 0.3201 | RÂ²: 0.8903\n",
      "Epoch 12/20 | Train Loss: 0.4934 | Val Loss: 0.3167 | MAE: 0.2339 | RMSE: 0.3167 | RÂ²: 0.8926\n",
      "Epoch 13/20 | Train Loss: 0.4860 | Val Loss: 0.3157 | MAE: 0.2319 | RMSE: 0.3157 | RÂ²: 0.8933\n",
      "Epoch 14/20 | Train Loss: 0.4721 | Val Loss: 0.3162 | MAE: 0.2311 | RMSE: 0.3162 | RÂ²: 0.8930\n",
      "Epoch 15/20 | Train Loss: 0.4619 | Val Loss: 0.3156 | MAE: 0.2281 | RMSE: 0.3156 | RÂ²: 0.8934\n",
      "Epoch 16/20 | Train Loss: 0.4571 | Val Loss: 0.3055 | MAE: 0.2212 | RMSE: 0.3055 | RÂ²: 0.9001\n",
      "Epoch 17/20 | Train Loss: 0.4506 | Val Loss: 0.3009 | MAE: 0.2187 | RMSE: 0.3009 | RÂ²: 0.9031\n",
      "Epoch 18/20 | Train Loss: 0.4412 | Val Loss: 0.3006 | MAE: 0.2182 | RMSE: 0.3006 | RÂ²: 0.9033\n",
      "Epoch 19/20 | Train Loss: 0.4369 | Val Loss: 0.2982 | MAE: 0.2153 | RMSE: 0.2982 | RÂ²: 0.9048\n",
      "Epoch 20/20 | Train Loss: 0.4358 | Val Loss: 0.2992 | MAE: 0.2145 | RMSE: 0.2992 | RÂ²: 0.9042\n",
      "âœ… RMSE = 0.2982 | MAE = 0.2153\n",
      "\n",
      "ðŸ”§ [131/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[2048, 1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.7769 | Val Loss: 0.9511 | MAE: 0.7973 | RMSE: 0.9511 | RÂ²: 0.0316\n",
      "Epoch 2/20 | Train Loss: 0.4614 | Val Loss: 0.8396 | MAE: 0.7054 | RMSE: 0.8396 | RÂ²: 0.2454\n",
      "Epoch 3/20 | Train Loss: 0.3956 | Val Loss: 0.7129 | MAE: 0.6005 | RMSE: 0.7129 | RÂ²: 0.4560\n",
      "Epoch 4/20 | Train Loss: 0.3552 | Val Loss: 0.5794 | MAE: 0.4779 | RMSE: 0.5794 | RÂ²: 0.6406\n",
      "Epoch 5/20 | Train Loss: 0.3266 | Val Loss: 0.3987 | MAE: 0.3256 | RMSE: 0.3987 | RÂ²: 0.8298\n",
      "Epoch 6/20 | Train Loss: 0.3147 | Val Loss: 0.3070 | MAE: 0.2352 | RMSE: 0.3070 | RÂ²: 0.8991\n",
      "Epoch 7/20 | Train Loss: 0.3021 | Val Loss: 0.2999 | MAE: 0.2112 | RMSE: 0.2999 | RÂ²: 0.9037\n",
      "Epoch 8/20 | Train Loss: 0.2993 | Val Loss: 0.2746 | MAE: 0.1897 | RMSE: 0.2746 | RÂ²: 0.9193\n",
      "Epoch 9/20 | Train Loss: 0.2926 | Val Loss: 0.2741 | MAE: 0.1878 | RMSE: 0.2741 | RÂ²: 0.9196\n",
      "Epoch 10/20 | Train Loss: 0.2770 | Val Loss: 0.2727 | MAE: 0.1865 | RMSE: 0.2727 | RÂ²: 0.9204\n",
      "Epoch 11/20 | Train Loss: 0.2811 | Val Loss: 0.2689 | MAE: 0.1874 | RMSE: 0.2689 | RÂ²: 0.9226\n",
      "Epoch 12/20 | Train Loss: 0.2756 | Val Loss: 0.2750 | MAE: 0.1896 | RMSE: 0.2750 | RÂ²: 0.9191\n",
      "Epoch 13/20 | Train Loss: 0.2712 | Val Loss: 0.2783 | MAE: 0.1915 | RMSE: 0.2783 | RÂ²: 0.9171\n",
      "Epoch 14/20 | Train Loss: 0.2652 | Val Loss: 0.2732 | MAE: 0.1875 | RMSE: 0.2732 | RÂ²: 0.9201\n",
      "Epoch 15/20 | Train Loss: 0.2694 | Val Loss: 0.2782 | MAE: 0.1933 | RMSE: 0.2782 | RÂ²: 0.9171\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2689 | MAE = 0.1874\n",
      "\n",
      "ðŸ”§ [132/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7935 | Val Loss: 0.9068 | MAE: 0.7654 | RMSE: 0.9068 | RÂ²: 0.1197\n",
      "Epoch 2/20 | Train Loss: 0.4757 | Val Loss: 0.6952 | MAE: 0.5861 | RMSE: 0.6952 | RÂ²: 0.4826\n",
      "Epoch 3/20 | Train Loss: 0.4117 | Val Loss: 0.4602 | MAE: 0.3818 | RMSE: 0.4602 | RÂ²: 0.7732\n",
      "Epoch 4/20 | Train Loss: 0.3738 | Val Loss: 0.3013 | MAE: 0.2289 | RMSE: 0.3013 | RÂ²: 0.9028\n",
      "Epoch 5/20 | Train Loss: 0.3527 | Val Loss: 0.2753 | MAE: 0.1965 | RMSE: 0.2753 | RÂ²: 0.9189\n",
      "Epoch 6/20 | Train Loss: 0.3359 | Val Loss: 0.3154 | MAE: 0.2008 | RMSE: 0.3154 | RÂ²: 0.8935\n",
      "Epoch 7/20 | Train Loss: 0.3258 | Val Loss: 0.2668 | MAE: 0.1867 | RMSE: 0.2668 | RÂ²: 0.9238\n",
      "Epoch 8/20 | Train Loss: 0.3075 | Val Loss: 0.2845 | MAE: 0.1864 | RMSE: 0.2845 | RÂ²: 0.9134\n",
      "Epoch 9/20 | Train Loss: 0.3020 | Val Loss: 0.2585 | MAE: 0.1777 | RMSE: 0.2585 | RÂ²: 0.9285\n",
      "Epoch 10/20 | Train Loss: 0.2938 | Val Loss: 0.2653 | MAE: 0.1787 | RMSE: 0.2653 | RÂ²: 0.9246\n",
      "Epoch 11/20 | Train Loss: 0.2945 | Val Loss: 0.2589 | MAE: 0.1760 | RMSE: 0.2589 | RÂ²: 0.9282\n",
      "Epoch 12/20 | Train Loss: 0.2837 | Val Loss: 0.2605 | MAE: 0.1814 | RMSE: 0.2605 | RÂ²: 0.9274\n",
      "Epoch 13/20 | Train Loss: 0.2859 | Val Loss: 0.2572 | MAE: 0.1755 | RMSE: 0.2572 | RÂ²: 0.9292\n",
      "Epoch 14/20 | Train Loss: 0.2791 | Val Loss: 0.2640 | MAE: 0.1808 | RMSE: 0.2640 | RÂ²: 0.9254\n",
      "Epoch 15/20 | Train Loss: 0.2747 | Val Loss: 0.2661 | MAE: 0.1823 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 16/20 | Train Loss: 0.2672 | Val Loss: 0.2633 | MAE: 0.1800 | RMSE: 0.2633 | RÂ²: 0.9258\n",
      "Epoch 17/20 | Train Loss: 0.2644 | Val Loss: 0.2711 | MAE: 0.1864 | RMSE: 0.2711 | RÂ²: 0.9213\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2572 | MAE = 0.1755\n",
      "\n",
      "ðŸ”§ [133/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.7262 | Val Loss: 0.9577 | MAE: 0.8001 | RMSE: 0.9577 | RÂ²: 0.0180\n",
      "Epoch 2/20 | Train Loss: 0.4039 | Val Loss: 0.8668 | MAE: 0.7265 | RMSE: 0.8668 | RÂ²: 0.1956\n",
      "Epoch 3/20 | Train Loss: 0.3572 | Val Loss: 0.7140 | MAE: 0.6001 | RMSE: 0.7140 | RÂ²: 0.4543\n",
      "Epoch 4/20 | Train Loss: 0.3313 | Val Loss: 0.5247 | MAE: 0.4376 | RMSE: 0.5247 | RÂ²: 0.7052\n",
      "Epoch 5/20 | Train Loss: 0.3107 | Val Loss: 0.3604 | MAE: 0.2946 | RMSE: 0.3604 | RÂ²: 0.8610\n",
      "Epoch 6/20 | Train Loss: 0.3018 | Val Loss: 0.2845 | MAE: 0.2174 | RMSE: 0.2845 | RÂ²: 0.9134\n",
      "Epoch 7/20 | Train Loss: 0.2893 | Val Loss: 0.2454 | MAE: 0.1719 | RMSE: 0.2454 | RÂ²: 0.9355\n",
      "Epoch 8/20 | Train Loss: 0.2812 | Val Loss: 0.2528 | MAE: 0.1737 | RMSE: 0.2528 | RÂ²: 0.9316\n",
      "Epoch 9/20 | Train Loss: 0.2785 | Val Loss: 0.2617 | MAE: 0.1744 | RMSE: 0.2617 | RÂ²: 0.9267\n",
      "Epoch 10/20 | Train Loss: 0.2720 | Val Loss: 0.2739 | MAE: 0.1743 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 11/20 | Train Loss: 0.2695 | Val Loss: 0.2588 | MAE: 0.1773 | RMSE: 0.2588 | RÂ²: 0.9283\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2454 | MAE = 0.1719\n",
      "\n",
      "ðŸ”§ [134/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[256, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8342 | Val Loss: 0.7950 | MAE: 0.6729 | RMSE: 0.7950 | RÂ²: 0.3233\n",
      "Epoch 2/20 | Train Loss: 0.5132 | Val Loss: 0.6483 | MAE: 0.5328 | RMSE: 0.6483 | RÂ²: 0.5501\n",
      "Epoch 3/20 | Train Loss: 0.4225 | Val Loss: 0.4545 | MAE: 0.3764 | RMSE: 0.4545 | RÂ²: 0.7789\n",
      "Epoch 4/20 | Train Loss: 0.3766 | Val Loss: 0.3687 | MAE: 0.2951 | RMSE: 0.3687 | RÂ²: 0.8544\n",
      "Epoch 5/20 | Train Loss: 0.3479 | Val Loss: 0.3379 | MAE: 0.2583 | RMSE: 0.3379 | RÂ²: 0.8778\n",
      "Epoch 6/20 | Train Loss: 0.3262 | Val Loss: 0.2952 | MAE: 0.2106 | RMSE: 0.2952 | RÂ²: 0.9067\n",
      "Epoch 7/20 | Train Loss: 0.3136 | Val Loss: 0.2840 | MAE: 0.2060 | RMSE: 0.2840 | RÂ²: 0.9137\n",
      "Epoch 8/20 | Train Loss: 0.3015 | Val Loss: 0.2781 | MAE: 0.1984 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 9/20 | Train Loss: 0.2908 | Val Loss: 0.2698 | MAE: 0.1896 | RMSE: 0.2698 | RÂ²: 0.9221\n",
      "Epoch 10/20 | Train Loss: 0.2880 | Val Loss: 0.2723 | MAE: 0.1910 | RMSE: 0.2723 | RÂ²: 0.9206\n",
      "Epoch 11/20 | Train Loss: 0.2770 | Val Loss: 0.2709 | MAE: 0.1869 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 12/20 | Train Loss: 0.2687 | Val Loss: 0.2682 | MAE: 0.1860 | RMSE: 0.2682 | RÂ²: 0.9230\n",
      "Epoch 13/20 | Train Loss: 0.2651 | Val Loss: 0.2702 | MAE: 0.1867 | RMSE: 0.2702 | RÂ²: 0.9218\n",
      "Epoch 14/20 | Train Loss: 0.2617 | Val Loss: 0.2618 | MAE: 0.1803 | RMSE: 0.2618 | RÂ²: 0.9266\n",
      "Epoch 15/20 | Train Loss: 0.2600 | Val Loss: 0.2701 | MAE: 0.1857 | RMSE: 0.2701 | RÂ²: 0.9219\n",
      "Epoch 16/20 | Train Loss: 0.2567 | Val Loss: 0.2702 | MAE: 0.1860 | RMSE: 0.2702 | RÂ²: 0.9219\n",
      "Epoch 17/20 | Train Loss: 0.2536 | Val Loss: 0.2682 | MAE: 0.1836 | RMSE: 0.2682 | RÂ²: 0.9230\n",
      "Epoch 18/20 | Train Loss: 0.2519 | Val Loss: 0.2683 | MAE: 0.1832 | RMSE: 0.2683 | RÂ²: 0.9229\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2618 | MAE = 0.1803\n",
      "\n",
      "ðŸ”§ [135/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8600 | Val Loss: 0.6692 | MAE: 0.5430 | RMSE: 0.6692 | RÂ²: 0.5205\n",
      "Epoch 2/20 | Train Loss: 0.6037 | Val Loss: 0.4394 | MAE: 0.3530 | RMSE: 0.4394 | RÂ²: 0.7933\n",
      "Epoch 3/20 | Train Loss: 0.5146 | Val Loss: 0.3856 | MAE: 0.2937 | RMSE: 0.3856 | RÂ²: 0.8408\n",
      "Epoch 4/20 | Train Loss: 0.4698 | Val Loss: 0.3624 | MAE: 0.2819 | RMSE: 0.3624 | RÂ²: 0.8594\n",
      "Epoch 5/20 | Train Loss: 0.4344 | Val Loss: 0.3539 | MAE: 0.2686 | RMSE: 0.3539 | RÂ²: 0.8659\n",
      "Epoch 6/20 | Train Loss: 0.4066 | Val Loss: 0.3180 | MAE: 0.2428 | RMSE: 0.3180 | RÂ²: 0.8918\n",
      "Epoch 7/20 | Train Loss: 0.3918 | Val Loss: 0.3062 | MAE: 0.2265 | RMSE: 0.3062 | RÂ²: 0.8996\n",
      "Epoch 8/20 | Train Loss: 0.3688 | Val Loss: 0.2949 | MAE: 0.2152 | RMSE: 0.2949 | RÂ²: 0.9069\n",
      "Epoch 9/20 | Train Loss: 0.3544 | Val Loss: 0.2894 | MAE: 0.2074 | RMSE: 0.2894 | RÂ²: 0.9104\n",
      "Epoch 10/20 | Train Loss: 0.3415 | Val Loss: 0.2841 | MAE: 0.2028 | RMSE: 0.2841 | RÂ²: 0.9136\n",
      "Epoch 11/20 | Train Loss: 0.3317 | Val Loss: 0.2786 | MAE: 0.1973 | RMSE: 0.2786 | RÂ²: 0.9169\n",
      "Epoch 12/20 | Train Loss: 0.3267 | Val Loss: 0.2869 | MAE: 0.2013 | RMSE: 0.2869 | RÂ²: 0.9119\n",
      "Epoch 13/20 | Train Loss: 0.3183 | Val Loss: 0.2814 | MAE: 0.1973 | RMSE: 0.2814 | RÂ²: 0.9152\n",
      "Epoch 14/20 | Train Loss: 0.3097 | Val Loss: 0.2876 | MAE: 0.2028 | RMSE: 0.2876 | RÂ²: 0.9114\n",
      "Epoch 15/20 | Train Loss: 0.3039 | Val Loss: 0.2855 | MAE: 0.2005 | RMSE: 0.2855 | RÂ²: 0.9127\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2786 | MAE = 0.1973\n",
      "\n",
      "ðŸ”§ [136/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9734 | Val Loss: 0.8028 | MAE: 0.6776 | RMSE: 0.8028 | RÂ²: 0.3101\n",
      "Epoch 2/20 | Train Loss: 0.7771 | Val Loss: 0.6508 | MAE: 0.5494 | RMSE: 0.6508 | RÂ²: 0.5466\n",
      "Epoch 3/20 | Train Loss: 0.6579 | Val Loss: 0.5681 | MAE: 0.4677 | RMSE: 0.5681 | RÂ²: 0.6544\n",
      "Epoch 4/20 | Train Loss: 0.5829 | Val Loss: 0.4628 | MAE: 0.3601 | RMSE: 0.4628 | RÂ²: 0.7707\n",
      "Epoch 5/20 | Train Loss: 0.5542 | Val Loss: 0.3954 | MAE: 0.2898 | RMSE: 0.3954 | RÂ²: 0.8326\n",
      "Epoch 6/20 | Train Loss: 0.5206 | Val Loss: 0.3819 | MAE: 0.2736 | RMSE: 0.3819 | RÂ²: 0.8438\n",
      "Epoch 7/20 | Train Loss: 0.4961 | Val Loss: 0.3516 | MAE: 0.2535 | RMSE: 0.3516 | RÂ²: 0.8676\n",
      "Epoch 8/20 | Train Loss: 0.4752 | Val Loss: 0.3195 | MAE: 0.2368 | RMSE: 0.3195 | RÂ²: 0.8907\n",
      "Epoch 9/20 | Train Loss: 0.4601 | Val Loss: 0.3216 | MAE: 0.2333 | RMSE: 0.3216 | RÂ²: 0.8893\n",
      "Epoch 10/20 | Train Loss: 0.4499 | Val Loss: 0.3301 | MAE: 0.2311 | RMSE: 0.3301 | RÂ²: 0.8833\n",
      "Epoch 11/20 | Train Loss: 0.4350 | Val Loss: 0.3060 | MAE: 0.2190 | RMSE: 0.3060 | RÂ²: 0.8998\n",
      "Epoch 12/20 | Train Loss: 0.4289 | Val Loss: 0.3087 | MAE: 0.2179 | RMSE: 0.3087 | RÂ²: 0.8980\n",
      "Epoch 13/20 | Train Loss: 0.4166 | Val Loss: 0.3033 | MAE: 0.2162 | RMSE: 0.3033 | RÂ²: 0.9015\n",
      "Epoch 14/20 | Train Loss: 0.4079 | Val Loss: 0.2945 | MAE: 0.2119 | RMSE: 0.2945 | RÂ²: 0.9072\n",
      "Epoch 15/20 | Train Loss: 0.3987 | Val Loss: 0.2977 | MAE: 0.2098 | RMSE: 0.2977 | RÂ²: 0.9051\n",
      "Epoch 16/20 | Train Loss: 0.3970 | Val Loss: 0.3011 | MAE: 0.2100 | RMSE: 0.3011 | RÂ²: 0.9029\n",
      "Epoch 17/20 | Train Loss: 0.3888 | Val Loss: 0.2928 | MAE: 0.2065 | RMSE: 0.2928 | RÂ²: 0.9082\n",
      "Epoch 18/20 | Train Loss: 0.3820 | Val Loss: 0.2944 | MAE: 0.2057 | RMSE: 0.2944 | RÂ²: 0.9072\n",
      "Epoch 19/20 | Train Loss: 0.3791 | Val Loss: 0.3017 | MAE: 0.2067 | RMSE: 0.3017 | RÂ²: 0.9026\n",
      "Epoch 20/20 | Train Loss: 0.3712 | Val Loss: 0.2967 | MAE: 0.2027 | RMSE: 0.2967 | RÂ²: 0.9058\n",
      "âœ… RMSE = 0.2928 | MAE = 0.2065\n",
      "\n",
      "ðŸ”§ [137/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[512, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8287 | Val Loss: 0.7337 | MAE: 0.6155 | RMSE: 0.7337 | RÂ²: 0.4237\n",
      "Epoch 2/20 | Train Loss: 0.5425 | Val Loss: 0.5407 | MAE: 0.4256 | RMSE: 0.5407 | RÂ²: 0.6870\n",
      "Epoch 3/20 | Train Loss: 0.4697 | Val Loss: 0.3481 | MAE: 0.2661 | RMSE: 0.3481 | RÂ²: 0.8702\n",
      "Epoch 4/20 | Train Loss: 0.4156 | Val Loss: 0.3330 | MAE: 0.2433 | RMSE: 0.3330 | RÂ²: 0.8813\n",
      "Epoch 5/20 | Train Loss: 0.3850 | Val Loss: 0.3592 | MAE: 0.2753 | RMSE: 0.3592 | RÂ²: 0.8619\n",
      "Epoch 6/20 | Train Loss: 0.3599 | Val Loss: 0.3183 | MAE: 0.2394 | RMSE: 0.3183 | RÂ²: 0.8916\n",
      "Epoch 7/20 | Train Loss: 0.3428 | Val Loss: 0.3105 | MAE: 0.2311 | RMSE: 0.3105 | RÂ²: 0.8968\n",
      "Epoch 8/20 | Train Loss: 0.3349 | Val Loss: 0.2881 | MAE: 0.2108 | RMSE: 0.2881 | RÂ²: 0.9111\n",
      "Epoch 9/20 | Train Loss: 0.3195 | Val Loss: 0.2760 | MAE: 0.1981 | RMSE: 0.2760 | RÂ²: 0.9184\n",
      "Epoch 10/20 | Train Loss: 0.3118 | Val Loss: 0.2717 | MAE: 0.1873 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "Epoch 11/20 | Train Loss: 0.3076 | Val Loss: 0.2769 | MAE: 0.1936 | RMSE: 0.2769 | RÂ²: 0.9179\n",
      "Epoch 12/20 | Train Loss: 0.2989 | Val Loss: 0.2627 | MAE: 0.1821 | RMSE: 0.2627 | RÂ²: 0.9261\n",
      "Epoch 13/20 | Train Loss: 0.2947 | Val Loss: 0.2673 | MAE: 0.1863 | RMSE: 0.2673 | RÂ²: 0.9235\n",
      "Epoch 14/20 | Train Loss: 0.2890 | Val Loss: 0.2612 | MAE: 0.1796 | RMSE: 0.2612 | RÂ²: 0.9269\n",
      "Epoch 15/20 | Train Loss: 0.2843 | Val Loss: 0.2665 | MAE: 0.1833 | RMSE: 0.2665 | RÂ²: 0.9240\n",
      "Epoch 16/20 | Train Loss: 0.2811 | Val Loss: 0.2699 | MAE: 0.1857 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 17/20 | Train Loss: 0.2794 | Val Loss: 0.2739 | MAE: 0.1889 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 18/20 | Train Loss: 0.2798 | Val Loss: 0.2628 | MAE: 0.1810 | RMSE: 0.2628 | RÂ²: 0.9260\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2612 | MAE = 0.1796\n",
      "\n",
      "ðŸ”§ [138/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.7465 | Val Loss: 0.9284 | MAE: 0.7827 | RMSE: 0.9284 | RÂ²: 0.0773\n",
      "Epoch 2/20 | Train Loss: 0.4520 | Val Loss: 0.7457 | MAE: 0.6298 | RMSE: 0.7457 | RÂ²: 0.4047\n",
      "Epoch 3/20 | Train Loss: 0.3936 | Val Loss: 0.5967 | MAE: 0.4998 | RMSE: 0.5967 | RÂ²: 0.6188\n",
      "Epoch 4/20 | Train Loss: 0.3588 | Val Loss: 0.4058 | MAE: 0.3213 | RMSE: 0.4058 | RÂ²: 0.8237\n",
      "Epoch 5/20 | Train Loss: 0.3436 | Val Loss: 0.2807 | MAE: 0.2067 | RMSE: 0.2807 | RÂ²: 0.9157\n",
      "Epoch 6/20 | Train Loss: 0.3242 | Val Loss: 0.2630 | MAE: 0.1820 | RMSE: 0.2630 | RÂ²: 0.9259\n",
      "Epoch 7/20 | Train Loss: 0.3145 | Val Loss: 0.2732 | MAE: 0.1872 | RMSE: 0.2732 | RÂ²: 0.9201\n",
      "Epoch 8/20 | Train Loss: 0.3000 | Val Loss: 0.2561 | MAE: 0.1750 | RMSE: 0.2561 | RÂ²: 0.9298\n",
      "Epoch 9/20 | Train Loss: 0.3019 | Val Loss: 0.2703 | MAE: 0.1910 | RMSE: 0.2703 | RÂ²: 0.9218\n",
      "Epoch 10/20 | Train Loss: 0.2869 | Val Loss: 0.2645 | MAE: 0.1804 | RMSE: 0.2645 | RÂ²: 0.9251\n",
      "Epoch 11/20 | Train Loss: 0.2870 | Val Loss: 0.2755 | MAE: 0.1926 | RMSE: 0.2755 | RÂ²: 0.9188\n",
      "Epoch 12/20 | Train Loss: 0.2763 | Val Loss: 0.2678 | MAE: 0.1849 | RMSE: 0.2678 | RÂ²: 0.9232\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2561 | MAE = 0.1750\n",
      "\n",
      "ðŸ”§ [139/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9286 | Val Loss: 0.7873 | MAE: 0.6582 | RMSE: 0.7873 | RÂ²: 0.3364\n",
      "Epoch 2/20 | Train Loss: 0.4912 | Val Loss: 0.4899 | MAE: 0.3975 | RMSE: 0.4899 | RÂ²: 0.7430\n",
      "Epoch 3/20 | Train Loss: 0.4208 | Val Loss: 0.3073 | MAE: 0.2351 | RMSE: 0.3073 | RÂ²: 0.8989\n",
      "Epoch 4/20 | Train Loss: 0.3729 | Val Loss: 0.3056 | MAE: 0.2324 | RMSE: 0.3056 | RÂ²: 0.9000\n",
      "Epoch 5/20 | Train Loss: 0.3464 | Val Loss: 0.2981 | MAE: 0.2209 | RMSE: 0.2981 | RÂ²: 0.9048\n",
      "Epoch 6/20 | Train Loss: 0.3251 | Val Loss: 0.3024 | MAE: 0.2275 | RMSE: 0.3024 | RÂ²: 0.9021\n",
      "Epoch 7/20 | Train Loss: 0.3207 | Val Loss: 0.2648 | MAE: 0.1876 | RMSE: 0.2648 | RÂ²: 0.9249\n",
      "Epoch 8/20 | Train Loss: 0.3055 | Val Loss: 0.2981 | MAE: 0.2022 | RMSE: 0.2981 | RÂ²: 0.9049\n",
      "Epoch 9/20 | Train Loss: 0.3034 | Val Loss: 0.2621 | MAE: 0.1831 | RMSE: 0.2621 | RÂ²: 0.9265\n",
      "Epoch 10/20 | Train Loss: 0.2906 | Val Loss: 0.2621 | MAE: 0.1773 | RMSE: 0.2621 | RÂ²: 0.9265\n",
      "Epoch 11/20 | Train Loss: 0.2829 | Val Loss: 0.2682 | MAE: 0.1884 | RMSE: 0.2682 | RÂ²: 0.9230\n",
      "Epoch 12/20 | Train Loss: 0.2808 | Val Loss: 0.2586 | MAE: 0.1760 | RMSE: 0.2586 | RÂ²: 0.9284\n",
      "Epoch 13/20 | Train Loss: 0.2761 | Val Loss: 0.2848 | MAE: 0.1989 | RMSE: 0.2848 | RÂ²: 0.9132\n",
      "Epoch 14/20 | Train Loss: 0.2700 | Val Loss: 0.2626 | MAE: 0.1795 | RMSE: 0.2626 | RÂ²: 0.9262\n",
      "Epoch 15/20 | Train Loss: 0.2612 | Val Loss: 0.2591 | MAE: 0.1769 | RMSE: 0.2591 | RÂ²: 0.9281\n",
      "Epoch 16/20 | Train Loss: 0.2613 | Val Loss: 0.2751 | MAE: 0.1950 | RMSE: 0.2751 | RÂ²: 0.9190\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2586 | MAE = 0.1760\n",
      "\n",
      "ðŸ”§ [140/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.0351 | Val Loss: 0.8782 | MAE: 0.7421 | RMSE: 0.8782 | RÂ²: 0.1744\n",
      "Epoch 2/20 | Train Loss: 0.7399 | Val Loss: 0.7375 | MAE: 0.6175 | RMSE: 0.7375 | RÂ²: 0.4177\n",
      "Epoch 3/20 | Train Loss: 0.6352 | Val Loss: 0.6100 | MAE: 0.5089 | RMSE: 0.6100 | RÂ²: 0.6016\n",
      "Epoch 4/20 | Train Loss: 0.5159 | Val Loss: 0.5463 | MAE: 0.4370 | RMSE: 0.5463 | RÂ²: 0.6805\n",
      "Epoch 5/20 | Train Loss: 0.4698 | Val Loss: 0.4433 | MAE: 0.3430 | RMSE: 0.4433 | RÂ²: 0.7897\n",
      "Epoch 6/20 | Train Loss: 0.4113 | Val Loss: 0.3444 | MAE: 0.2629 | RMSE: 0.3444 | RÂ²: 0.8730\n",
      "Epoch 7/20 | Train Loss: 0.3886 | Val Loss: 0.3207 | MAE: 0.2412 | RMSE: 0.3207 | RÂ²: 0.8899\n",
      "Epoch 8/20 | Train Loss: 0.3658 | Val Loss: 0.3221 | MAE: 0.2407 | RMSE: 0.3221 | RÂ²: 0.8889\n",
      "Epoch 9/20 | Train Loss: 0.3507 | Val Loss: 0.3161 | MAE: 0.2335 | RMSE: 0.3161 | RÂ²: 0.8930\n",
      "Epoch 10/20 | Train Loss: 0.3387 | Val Loss: 0.3024 | MAE: 0.2215 | RMSE: 0.3024 | RÂ²: 0.9021\n",
      "Epoch 11/20 | Train Loss: 0.3300 | Val Loss: 0.2993 | MAE: 0.2166 | RMSE: 0.2993 | RÂ²: 0.9041\n",
      "Epoch 12/20 | Train Loss: 0.3225 | Val Loss: 0.3061 | MAE: 0.2207 | RMSE: 0.3061 | RÂ²: 0.8997\n",
      "Epoch 13/20 | Train Loss: 0.3149 | Val Loss: 0.2984 | MAE: 0.2140 | RMSE: 0.2984 | RÂ²: 0.9047\n",
      "Epoch 14/20 | Train Loss: 0.3139 | Val Loss: 0.3022 | MAE: 0.2138 | RMSE: 0.3022 | RÂ²: 0.9022\n",
      "Epoch 15/20 | Train Loss: 0.3071 | Val Loss: 0.3041 | MAE: 0.2148 | RMSE: 0.3041 | RÂ²: 0.9010\n",
      "Epoch 16/20 | Train Loss: 0.3020 | Val Loss: 0.2891 | MAE: 0.2065 | RMSE: 0.2891 | RÂ²: 0.9105\n",
      "Epoch 17/20 | Train Loss: 0.2957 | Val Loss: 0.2872 | MAE: 0.2045 | RMSE: 0.2872 | RÂ²: 0.9117\n",
      "Epoch 18/20 | Train Loss: 0.2917 | Val Loss: 0.2961 | MAE: 0.2110 | RMSE: 0.2961 | RÂ²: 0.9061\n",
      "Epoch 19/20 | Train Loss: 0.2937 | Val Loss: 0.3005 | MAE: 0.2139 | RMSE: 0.3005 | RÂ²: 0.9034\n",
      "Epoch 20/20 | Train Loss: 0.2895 | Val Loss: 0.2881 | MAE: 0.2072 | RMSE: 0.2881 | RÂ²: 0.9112\n",
      "âœ… RMSE = 0.2872 | MAE = 0.2045\n",
      "\n",
      "ðŸ”§ [141/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[512, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9938 | Val Loss: 0.8738 | MAE: 0.7414 | RMSE: 0.8738 | RÂ²: 0.1825\n",
      "Epoch 2/20 | Train Loss: 0.8364 | Val Loss: 0.7547 | MAE: 0.6409 | RMSE: 0.7547 | RÂ²: 0.3902\n",
      "Epoch 3/20 | Train Loss: 0.7545 | Val Loss: 0.6338 | MAE: 0.5315 | RMSE: 0.6338 | RÂ²: 0.5699\n",
      "Epoch 4/20 | Train Loss: 0.6893 | Val Loss: 0.4953 | MAE: 0.4077 | RMSE: 0.4953 | RÂ²: 0.7374\n",
      "Epoch 5/20 | Train Loss: 0.6579 | Val Loss: 0.4273 | MAE: 0.3368 | RMSE: 0.4273 | RÂ²: 0.8046\n",
      "Epoch 6/20 | Train Loss: 0.6196 | Val Loss: 0.4173 | MAE: 0.3044 | RMSE: 0.4173 | RÂ²: 0.8136\n",
      "Epoch 7/20 | Train Loss: 0.5933 | Val Loss: 0.4029 | MAE: 0.2820 | RMSE: 0.4029 | RÂ²: 0.8262\n",
      "Epoch 8/20 | Train Loss: 0.5642 | Val Loss: 0.3863 | MAE: 0.2789 | RMSE: 0.3863 | RÂ²: 0.8402\n",
      "Epoch 9/20 | Train Loss: 0.5430 | Val Loss: 0.3863 | MAE: 0.2902 | RMSE: 0.3863 | RÂ²: 0.8403\n",
      "Epoch 10/20 | Train Loss: 0.5259 | Val Loss: 0.3910 | MAE: 0.2952 | RMSE: 0.3910 | RÂ²: 0.8363\n",
      "Epoch 11/20 | Train Loss: 0.5077 | Val Loss: 0.3880 | MAE: 0.2902 | RMSE: 0.3880 | RÂ²: 0.8388\n",
      "Epoch 12/20 | Train Loss: 0.4960 | Val Loss: 0.3850 | MAE: 0.2885 | RMSE: 0.3850 | RÂ²: 0.8414\n",
      "Epoch 13/20 | Train Loss: 0.4907 | Val Loss: 0.3807 | MAE: 0.2848 | RMSE: 0.3807 | RÂ²: 0.8448\n",
      "Epoch 14/20 | Train Loss: 0.4737 | Val Loss: 0.3916 | MAE: 0.2867 | RMSE: 0.3916 | RÂ²: 0.8358\n",
      "Epoch 15/20 | Train Loss: 0.4609 | Val Loss: 0.3944 | MAE: 0.2884 | RMSE: 0.3944 | RÂ²: 0.8335\n",
      "Epoch 16/20 | Train Loss: 0.4496 | Val Loss: 0.3890 | MAE: 0.2885 | RMSE: 0.3890 | RÂ²: 0.8380\n",
      "Epoch 17/20 | Train Loss: 0.4470 | Val Loss: 0.3833 | MAE: 0.2806 | RMSE: 0.3833 | RÂ²: 0.8427\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3807 | MAE = 0.2848\n",
      "\n",
      "ðŸ”§ [142/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8563 | Val Loss: 0.9152 | MAE: 0.7714 | RMSE: 0.9152 | RÂ²: 0.1033\n",
      "Epoch 2/20 | Train Loss: 0.5481 | Val Loss: 0.6513 | MAE: 0.5420 | RMSE: 0.6513 | RÂ²: 0.5459\n",
      "Epoch 3/20 | Train Loss: 0.4680 | Val Loss: 0.4442 | MAE: 0.3606 | RMSE: 0.4442 | RÂ²: 0.7888\n",
      "Epoch 4/20 | Train Loss: 0.4187 | Val Loss: 0.3481 | MAE: 0.2675 | RMSE: 0.3481 | RÂ²: 0.8703\n",
      "Epoch 5/20 | Train Loss: 0.3793 | Val Loss: 0.3223 | MAE: 0.2473 | RMSE: 0.3223 | RÂ²: 0.8888\n",
      "Epoch 6/20 | Train Loss: 0.3661 | Val Loss: 0.3242 | MAE: 0.2505 | RMSE: 0.3242 | RÂ²: 0.8875\n",
      "Epoch 7/20 | Train Loss: 0.3465 | Val Loss: 0.3228 | MAE: 0.2496 | RMSE: 0.3228 | RÂ²: 0.8885\n",
      "Epoch 8/20 | Train Loss: 0.3317 | Val Loss: 0.2853 | MAE: 0.2126 | RMSE: 0.2853 | RÂ²: 0.9128\n",
      "Epoch 9/20 | Train Loss: 0.3234 | Val Loss: 0.2915 | MAE: 0.2174 | RMSE: 0.2915 | RÂ²: 0.9090\n",
      "Epoch 10/20 | Train Loss: 0.3105 | Val Loss: 0.2739 | MAE: 0.1956 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 11/20 | Train Loss: 0.3077 | Val Loss: 0.2626 | MAE: 0.1855 | RMSE: 0.2626 | RÂ²: 0.9262\n",
      "Epoch 12/20 | Train Loss: 0.3003 | Val Loss: 0.2680 | MAE: 0.1877 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 13/20 | Train Loss: 0.2927 | Val Loss: 0.2639 | MAE: 0.1850 | RMSE: 0.2639 | RÂ²: 0.9255\n",
      "Epoch 14/20 | Train Loss: 0.2916 | Val Loss: 0.2607 | MAE: 0.1803 | RMSE: 0.2607 | RÂ²: 0.9273\n",
      "Epoch 15/20 | Train Loss: 0.2887 | Val Loss: 0.2658 | MAE: 0.1835 | RMSE: 0.2658 | RÂ²: 0.9244\n",
      "Epoch 16/20 | Train Loss: 0.2801 | Val Loss: 0.2656 | MAE: 0.1791 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "Epoch 17/20 | Train Loss: 0.2826 | Val Loss: 0.2715 | MAE: 0.1884 | RMSE: 0.2715 | RÂ²: 0.9211\n",
      "Epoch 18/20 | Train Loss: 0.2788 | Val Loss: 0.2687 | MAE: 0.1843 | RMSE: 0.2687 | RÂ²: 0.9227\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2607 | MAE = 0.1803\n",
      "\n",
      "ðŸ”§ [143/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8537 | Val Loss: 0.7054 | MAE: 0.5952 | RMSE: 0.7054 | RÂ²: 0.4673\n",
      "Epoch 2/20 | Train Loss: 0.5965 | Val Loss: 0.4961 | MAE: 0.4038 | RMSE: 0.4961 | RÂ²: 0.7365\n",
      "Epoch 3/20 | Train Loss: 0.5055 | Val Loss: 0.3698 | MAE: 0.2924 | RMSE: 0.3698 | RÂ²: 0.8536\n",
      "Epoch 4/20 | Train Loss: 0.4534 | Val Loss: 0.3374 | MAE: 0.2562 | RMSE: 0.3374 | RÂ²: 0.8782\n",
      "Epoch 5/20 | Train Loss: 0.4186 | Val Loss: 0.3222 | MAE: 0.2439 | RMSE: 0.3222 | RÂ²: 0.8889\n",
      "Epoch 6/20 | Train Loss: 0.3988 | Val Loss: 0.3246 | MAE: 0.2374 | RMSE: 0.3246 | RÂ²: 0.8872\n",
      "Epoch 7/20 | Train Loss: 0.3801 | Val Loss: 0.2973 | MAE: 0.2140 | RMSE: 0.2973 | RÂ²: 0.9054\n",
      "Epoch 8/20 | Train Loss: 0.3652 | Val Loss: 0.2798 | MAE: 0.1988 | RMSE: 0.2798 | RÂ²: 0.9162\n",
      "Epoch 9/20 | Train Loss: 0.3531 | Val Loss: 0.2763 | MAE: 0.1944 | RMSE: 0.2763 | RÂ²: 0.9183\n",
      "Epoch 10/20 | Train Loss: 0.3441 | Val Loss: 0.2716 | MAE: 0.1916 | RMSE: 0.2716 | RÂ²: 0.9210\n",
      "Epoch 11/20 | Train Loss: 0.3349 | Val Loss: 0.2748 | MAE: 0.1935 | RMSE: 0.2748 | RÂ²: 0.9191\n",
      "Epoch 12/20 | Train Loss: 0.3278 | Val Loss: 0.2767 | MAE: 0.1953 | RMSE: 0.2767 | RÂ²: 0.9181\n",
      "Epoch 13/20 | Train Loss: 0.3208 | Val Loss: 0.2811 | MAE: 0.1998 | RMSE: 0.2811 | RÂ²: 0.9154\n",
      "Epoch 14/20 | Train Loss: 0.3160 | Val Loss: 0.2775 | MAE: 0.1957 | RMSE: 0.2775 | RÂ²: 0.9175\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2716 | MAE = 0.1916\n",
      "\n",
      "ðŸ”§ [144/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[512, 512, 512], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9569 | Val Loss: 0.9360 | MAE: 0.7941 | RMSE: 0.9360 | RÂ²: 0.0621\n",
      "Epoch 2/20 | Train Loss: 0.7328 | Val Loss: 0.8159 | MAE: 0.6927 | RMSE: 0.8159 | RÂ²: 0.2873\n",
      "Epoch 3/20 | Train Loss: 0.5805 | Val Loss: 0.6962 | MAE: 0.5800 | RMSE: 0.6962 | RÂ²: 0.4812\n",
      "Epoch 4/20 | Train Loss: 0.5092 | Val Loss: 0.5357 | MAE: 0.4375 | RMSE: 0.5357 | RÂ²: 0.6928\n",
      "Epoch 5/20 | Train Loss: 0.4766 | Val Loss: 0.4251 | MAE: 0.3321 | RMSE: 0.4251 | RÂ²: 0.8065\n",
      "Epoch 6/20 | Train Loss: 0.4524 | Val Loss: 0.3772 | MAE: 0.2728 | RMSE: 0.3772 | RÂ²: 0.8477\n",
      "Epoch 7/20 | Train Loss: 0.4332 | Val Loss: 0.3149 | MAE: 0.2378 | RMSE: 0.3149 | RÂ²: 0.8938\n",
      "Epoch 8/20 | Train Loss: 0.4149 | Val Loss: 0.3021 | MAE: 0.2262 | RMSE: 0.3021 | RÂ²: 0.9023\n",
      "Epoch 9/20 | Train Loss: 0.4003 | Val Loss: 0.2938 | MAE: 0.2170 | RMSE: 0.2938 | RÂ²: 0.9076\n",
      "Epoch 10/20 | Train Loss: 0.3877 | Val Loss: 0.2806 | MAE: 0.2049 | RMSE: 0.2806 | RÂ²: 0.9157\n",
      "Epoch 11/20 | Train Loss: 0.3873 | Val Loss: 0.2760 | MAE: 0.1995 | RMSE: 0.2760 | RÂ²: 0.9184\n",
      "Epoch 12/20 | Train Loss: 0.3782 | Val Loss: 0.2768 | MAE: 0.2006 | RMSE: 0.2768 | RÂ²: 0.9180\n",
      "Epoch 13/20 | Train Loss: 0.3665 | Val Loss: 0.2803 | MAE: 0.2019 | RMSE: 0.2803 | RÂ²: 0.9159\n",
      "Epoch 14/20 | Train Loss: 0.3653 | Val Loss: 0.2784 | MAE: 0.2000 | RMSE: 0.2784 | RÂ²: 0.9170\n",
      "Epoch 15/20 | Train Loss: 0.3588 | Val Loss: 0.2804 | MAE: 0.2019 | RMSE: 0.2804 | RÂ²: 0.9158\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2760 | MAE = 0.1995\n",
      "\n",
      "ðŸ”§ [145/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9248 | Val Loss: 0.7801 | MAE: 0.6522 | RMSE: 0.7801 | RÂ²: 0.3486\n",
      "Epoch 2/20 | Train Loss: 0.6649 | Val Loss: 0.5011 | MAE: 0.4029 | RMSE: 0.5011 | RÂ²: 0.7311\n",
      "Epoch 3/20 | Train Loss: 0.5701 | Val Loss: 0.4198 | MAE: 0.3187 | RMSE: 0.4198 | RÂ²: 0.8114\n",
      "Epoch 4/20 | Train Loss: 0.5218 | Val Loss: 0.3875 | MAE: 0.2979 | RMSE: 0.3875 | RÂ²: 0.8393\n",
      "Epoch 5/20 | Train Loss: 0.4700 | Val Loss: 0.4078 | MAE: 0.3180 | RMSE: 0.4078 | RÂ²: 0.8220\n",
      "Epoch 6/20 | Train Loss: 0.4448 | Val Loss: 0.3949 | MAE: 0.3023 | RMSE: 0.3949 | RÂ²: 0.8330\n",
      "Epoch 7/20 | Train Loss: 0.4193 | Val Loss: 0.3595 | MAE: 0.2674 | RMSE: 0.3595 | RÂ²: 0.8616\n",
      "Epoch 8/20 | Train Loss: 0.3920 | Val Loss: 0.3263 | MAE: 0.2430 | RMSE: 0.3263 | RÂ²: 0.8860\n",
      "Epoch 9/20 | Train Loss: 0.3769 | Val Loss: 0.2973 | MAE: 0.2168 | RMSE: 0.2973 | RÂ²: 0.9054\n",
      "Epoch 10/20 | Train Loss: 0.3679 | Val Loss: 0.2973 | MAE: 0.2141 | RMSE: 0.2973 | RÂ²: 0.9054\n",
      "Epoch 11/20 | Train Loss: 0.3545 | Val Loss: 0.2861 | MAE: 0.2029 | RMSE: 0.2861 | RÂ²: 0.9124\n",
      "Epoch 12/20 | Train Loss: 0.3445 | Val Loss: 0.2788 | MAE: 0.1970 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "Epoch 13/20 | Train Loss: 0.3446 | Val Loss: 0.2912 | MAE: 0.2049 | RMSE: 0.2912 | RÂ²: 0.9092\n",
      "Epoch 14/20 | Train Loss: 0.3348 | Val Loss: 0.2865 | MAE: 0.2024 | RMSE: 0.2865 | RÂ²: 0.9121\n",
      "Epoch 15/20 | Train Loss: 0.3296 | Val Loss: 0.2933 | MAE: 0.2077 | RMSE: 0.2933 | RÂ²: 0.9079\n",
      "Epoch 16/20 | Train Loss: 0.3256 | Val Loss: 0.2821 | MAE: 0.1970 | RMSE: 0.2821 | RÂ²: 0.9148\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2788 | MAE = 0.1970\n",
      "\n",
      "ðŸ”§ [146/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9191 | Val Loss: 0.7720 | MAE: 0.6497 | RMSE: 0.7720 | RÂ²: 0.3619\n",
      "Epoch 2/20 | Train Loss: 0.6983 | Val Loss: 0.6309 | MAE: 0.5263 | RMSE: 0.6309 | RÂ²: 0.5739\n",
      "Epoch 3/20 | Train Loss: 0.6050 | Val Loss: 0.5163 | MAE: 0.4088 | RMSE: 0.5163 | RÂ²: 0.7147\n",
      "Epoch 4/20 | Train Loss: 0.5439 | Val Loss: 0.3749 | MAE: 0.2925 | RMSE: 0.3749 | RÂ²: 0.8495\n",
      "Epoch 5/20 | Train Loss: 0.4997 | Val Loss: 0.3537 | MAE: 0.2698 | RMSE: 0.3537 | RÂ²: 0.8661\n",
      "Epoch 6/20 | Train Loss: 0.4754 | Val Loss: 0.3309 | MAE: 0.2447 | RMSE: 0.3309 | RÂ²: 0.8828\n",
      "Epoch 7/20 | Train Loss: 0.4506 | Val Loss: 0.3102 | MAE: 0.2285 | RMSE: 0.3102 | RÂ²: 0.8970\n",
      "Epoch 8/20 | Train Loss: 0.4274 | Val Loss: 0.3040 | MAE: 0.2235 | RMSE: 0.3040 | RÂ²: 0.9011\n",
      "Epoch 9/20 | Train Loss: 0.4166 | Val Loss: 0.3030 | MAE: 0.2201 | RMSE: 0.3030 | RÂ²: 0.9017\n",
      "Epoch 10/20 | Train Loss: 0.4048 | Val Loss: 0.2929 | MAE: 0.2131 | RMSE: 0.2929 | RÂ²: 0.9082\n",
      "Epoch 11/20 | Train Loss: 0.3910 | Val Loss: 0.2861 | MAE: 0.2056 | RMSE: 0.2861 | RÂ²: 0.9124\n",
      "Epoch 12/20 | Train Loss: 0.3805 | Val Loss: 0.2869 | MAE: 0.2037 | RMSE: 0.2869 | RÂ²: 0.9119\n",
      "Epoch 13/20 | Train Loss: 0.3714 | Val Loss: 0.2849 | MAE: 0.2013 | RMSE: 0.2849 | RÂ²: 0.9131\n",
      "Epoch 14/20 | Train Loss: 0.3637 | Val Loss: 0.2830 | MAE: 0.2006 | RMSE: 0.2830 | RÂ²: 0.9143\n",
      "Epoch 15/20 | Train Loss: 0.3559 | Val Loss: 0.2816 | MAE: 0.1991 | RMSE: 0.2816 | RÂ²: 0.9151\n",
      "Epoch 16/20 | Train Loss: 0.3506 | Val Loss: 0.2819 | MAE: 0.1979 | RMSE: 0.2819 | RÂ²: 0.9149\n",
      "Epoch 17/20 | Train Loss: 0.3503 | Val Loss: 0.2835 | MAE: 0.1992 | RMSE: 0.2835 | RÂ²: 0.9139\n",
      "Epoch 18/20 | Train Loss: 0.3406 | Val Loss: 0.2770 | MAE: 0.1944 | RMSE: 0.2770 | RÂ²: 0.9179\n",
      "Epoch 19/20 | Train Loss: 0.3362 | Val Loss: 0.2821 | MAE: 0.1972 | RMSE: 0.2821 | RÂ²: 0.9148\n",
      "Epoch 20/20 | Train Loss: 0.3332 | Val Loss: 0.2816 | MAE: 0.1968 | RMSE: 0.2816 | RÂ²: 0.9151\n",
      "âœ… RMSE = 0.2770 | MAE = 0.1944\n",
      "\n",
      "ðŸ”§ [147/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[512, 256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9243 | Val Loss: 0.9892 | MAE: 0.8405 | RMSE: 0.9892 | RÂ²: -0.0476\n",
      "Epoch 2/20 | Train Loss: 0.7371 | Val Loss: 0.9175 | MAE: 0.7811 | RMSE: 0.9175 | RÂ²: 0.0988\n",
      "Epoch 3/20 | Train Loss: 0.6202 | Val Loss: 0.8204 | MAE: 0.6964 | RMSE: 0.8204 | RÂ²: 0.2795\n",
      "Epoch 4/20 | Train Loss: 0.5399 | Val Loss: 0.6741 | MAE: 0.5675 | RMSE: 0.6741 | RÂ²: 0.5135\n",
      "Epoch 5/20 | Train Loss: 0.4796 | Val Loss: 0.5151 | MAE: 0.4237 | RMSE: 0.5151 | RÂ²: 0.7160\n",
      "Epoch 6/20 | Train Loss: 0.4382 | Val Loss: 0.4103 | MAE: 0.3263 | RMSE: 0.4103 | RÂ²: 0.8198\n",
      "Epoch 7/20 | Train Loss: 0.4040 | Val Loss: 0.3493 | MAE: 0.2672 | RMSE: 0.3493 | RÂ²: 0.8693\n",
      "Epoch 8/20 | Train Loss: 0.3781 | Val Loss: 0.3129 | MAE: 0.2317 | RMSE: 0.3129 | RÂ²: 0.8952\n",
      "Epoch 9/20 | Train Loss: 0.3594 | Val Loss: 0.2945 | MAE: 0.2144 | RMSE: 0.2945 | RÂ²: 0.9071\n",
      "Epoch 10/20 | Train Loss: 0.3462 | Val Loss: 0.2868 | MAE: 0.2078 | RMSE: 0.2868 | RÂ²: 0.9119\n",
      "Epoch 11/20 | Train Loss: 0.3387 | Val Loss: 0.2842 | MAE: 0.2051 | RMSE: 0.2842 | RÂ²: 0.9135\n",
      "Epoch 12/20 | Train Loss: 0.3305 | Val Loss: 0.2801 | MAE: 0.2010 | RMSE: 0.2801 | RÂ²: 0.9160\n",
      "Epoch 13/20 | Train Loss: 0.3219 | Val Loss: 0.2779 | MAE: 0.1981 | RMSE: 0.2779 | RÂ²: 0.9173\n",
      "Epoch 14/20 | Train Loss: 0.3155 | Val Loss: 0.2781 | MAE: 0.1962 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 15/20 | Train Loss: 0.3108 | Val Loss: 0.2798 | MAE: 0.1962 | RMSE: 0.2798 | RÂ²: 0.9162\n",
      "Epoch 16/20 | Train Loss: 0.3063 | Val Loss: 0.2756 | MAE: 0.1960 | RMSE: 0.2756 | RÂ²: 0.9187\n",
      "Epoch 17/20 | Train Loss: 0.2985 | Val Loss: 0.2686 | MAE: 0.1912 | RMSE: 0.2686 | RÂ²: 0.9228\n",
      "Epoch 18/20 | Train Loss: 0.2959 | Val Loss: 0.2717 | MAE: 0.1925 | RMSE: 0.2717 | RÂ²: 0.9209\n",
      "Epoch 19/20 | Train Loss: 0.2923 | Val Loss: 0.2769 | MAE: 0.1954 | RMSE: 0.2769 | RÂ²: 0.9179\n",
      "Epoch 20/20 | Train Loss: 0.2917 | Val Loss: 0.2741 | MAE: 0.1940 | RMSE: 0.2741 | RÂ²: 0.9196\n",
      "âœ… RMSE = 0.2686 | MAE = 0.1912\n",
      "\n",
      "ðŸ”§ [148/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[1024, 256, 1024], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.0051 | Val Loss: 0.7904 | MAE: 0.6694 | RMSE: 0.7904 | RÂ²: 0.3312\n",
      "Epoch 2/20 | Train Loss: 0.6054 | Val Loss: 0.6116 | MAE: 0.4828 | RMSE: 0.6116 | RÂ²: 0.5996\n",
      "Epoch 3/20 | Train Loss: 0.5311 | Val Loss: 0.3585 | MAE: 0.2910 | RMSE: 0.3585 | RÂ²: 0.8624\n",
      "Epoch 4/20 | Train Loss: 0.4648 | Val Loss: 0.3477 | MAE: 0.2574 | RMSE: 0.3477 | RÂ²: 0.8705\n",
      "Epoch 5/20 | Train Loss: 0.4373 | Val Loss: 0.3168 | MAE: 0.2459 | RMSE: 0.3168 | RÂ²: 0.8925\n",
      "Epoch 6/20 | Train Loss: 0.4076 | Val Loss: 0.2979 | MAE: 0.2128 | RMSE: 0.2979 | RÂ²: 0.9050\n",
      "Epoch 7/20 | Train Loss: 0.3867 | Val Loss: 0.2753 | MAE: 0.1952 | RMSE: 0.2753 | RÂ²: 0.9188\n",
      "Epoch 8/20 | Train Loss: 0.3650 | Val Loss: 0.2661 | MAE: 0.1865 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 9/20 | Train Loss: 0.3552 | Val Loss: 0.2804 | MAE: 0.2079 | RMSE: 0.2804 | RÂ²: 0.9159\n",
      "Epoch 10/20 | Train Loss: 0.3422 | Val Loss: 0.2688 | MAE: 0.1955 | RMSE: 0.2688 | RÂ²: 0.9226\n",
      "Epoch 11/20 | Train Loss: 0.3255 | Val Loss: 0.2780 | MAE: 0.1902 | RMSE: 0.2780 | RÂ²: 0.9172\n",
      "Epoch 12/20 | Train Loss: 0.3203 | Val Loss: 0.2628 | MAE: 0.1891 | RMSE: 0.2628 | RÂ²: 0.9260\n",
      "Epoch 13/20 | Train Loss: 0.3097 | Val Loss: 0.2620 | MAE: 0.1839 | RMSE: 0.2620 | RÂ²: 0.9265\n",
      "Epoch 14/20 | Train Loss: 0.3049 | Val Loss: 0.2659 | MAE: 0.1873 | RMSE: 0.2659 | RÂ²: 0.9243\n",
      "Epoch 15/20 | Train Loss: 0.2979 | Val Loss: 0.2535 | MAE: 0.1737 | RMSE: 0.2535 | RÂ²: 0.9312\n",
      "Epoch 16/20 | Train Loss: 0.2899 | Val Loss: 0.2646 | MAE: 0.1840 | RMSE: 0.2646 | RÂ²: 0.9250\n",
      "Epoch 17/20 | Train Loss: 0.2799 | Val Loss: 0.2616 | MAE: 0.1799 | RMSE: 0.2616 | RÂ²: 0.9267\n",
      "Epoch 18/20 | Train Loss: 0.2765 | Val Loss: 0.2698 | MAE: 0.1927 | RMSE: 0.2698 | RÂ²: 0.9221\n",
      "Epoch 19/20 | Train Loss: 0.2724 | Val Loss: 0.2684 | MAE: 0.1899 | RMSE: 0.2684 | RÂ²: 0.9229\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2535 | MAE = 0.1737\n",
      "\n",
      "ðŸ”§ [149/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[2048, 1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9074 | Val Loss: 0.9290 | MAE: 0.7834 | RMSE: 0.9290 | RÂ²: 0.0760\n",
      "Epoch 2/20 | Train Loss: 0.6647 | Val Loss: 0.7693 | MAE: 0.6418 | RMSE: 0.7693 | RÂ²: 0.3665\n",
      "Epoch 3/20 | Train Loss: 0.5645 | Val Loss: 0.5579 | MAE: 0.4495 | RMSE: 0.5579 | RÂ²: 0.6668\n",
      "Epoch 4/20 | Train Loss: 0.5027 | Val Loss: 0.3835 | MAE: 0.2900 | RMSE: 0.3835 | RÂ²: 0.8426\n",
      "Epoch 5/20 | Train Loss: 0.4814 | Val Loss: 0.3336 | MAE: 0.2420 | RMSE: 0.3336 | RÂ²: 0.8809\n",
      "Epoch 6/20 | Train Loss: 0.4490 | Val Loss: 0.3658 | MAE: 0.2772 | RMSE: 0.3658 | RÂ²: 0.8568\n",
      "Epoch 7/20 | Train Loss: 0.4217 | Val Loss: 0.3797 | MAE: 0.2917 | RMSE: 0.3797 | RÂ²: 0.8457\n",
      "Epoch 8/20 | Train Loss: 0.4047 | Val Loss: 0.3518 | MAE: 0.2701 | RMSE: 0.3518 | RÂ²: 0.8675\n",
      "Epoch 9/20 | Train Loss: 0.3993 | Val Loss: 0.3548 | MAE: 0.2714 | RMSE: 0.3548 | RÂ²: 0.8652\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3336 | MAE = 0.2420\n",
      "\n",
      "ðŸ”§ [150/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[256, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9624 | Val Loss: 0.8690 | MAE: 0.7369 | RMSE: 0.8690 | RÂ²: 0.1916\n",
      "Epoch 2/20 | Train Loss: 0.8058 | Val Loss: 0.7332 | MAE: 0.6163 | RMSE: 0.7332 | RÂ²: 0.4245\n",
      "Epoch 3/20 | Train Loss: 0.7234 | Val Loss: 0.6197 | MAE: 0.5191 | RMSE: 0.6197 | RÂ²: 0.5889\n",
      "Epoch 4/20 | Train Loss: 0.6577 | Val Loss: 0.5343 | MAE: 0.4388 | RMSE: 0.5343 | RÂ²: 0.6944\n",
      "Epoch 5/20 | Train Loss: 0.6179 | Val Loss: 0.4451 | MAE: 0.3561 | RMSE: 0.4451 | RÂ²: 0.7879\n",
      "Epoch 6/20 | Train Loss: 0.5792 | Val Loss: 0.3811 | MAE: 0.2961 | RMSE: 0.3811 | RÂ²: 0.8445\n",
      "Epoch 7/20 | Train Loss: 0.5556 | Val Loss: 0.3593 | MAE: 0.2732 | RMSE: 0.3593 | RÂ²: 0.8618\n",
      "Epoch 8/20 | Train Loss: 0.5339 | Val Loss: 0.3553 | MAE: 0.2626 | RMSE: 0.3553 | RÂ²: 0.8649\n",
      "Epoch 9/20 | Train Loss: 0.5179 | Val Loss: 0.3458 | MAE: 0.2511 | RMSE: 0.3458 | RÂ²: 0.8720\n",
      "Epoch 10/20 | Train Loss: 0.4943 | Val Loss: 0.3424 | MAE: 0.2470 | RMSE: 0.3424 | RÂ²: 0.8745\n",
      "Epoch 11/20 | Train Loss: 0.4956 | Val Loss: 0.3388 | MAE: 0.2445 | RMSE: 0.3388 | RÂ²: 0.8771\n",
      "Epoch 12/20 | Train Loss: 0.4734 | Val Loss: 0.3246 | MAE: 0.2378 | RMSE: 0.3246 | RÂ²: 0.8872\n",
      "Epoch 13/20 | Train Loss: 0.4621 | Val Loss: 0.3193 | MAE: 0.2336 | RMSE: 0.3193 | RÂ²: 0.8909\n",
      "Epoch 14/20 | Train Loss: 0.4532 | Val Loss: 0.3170 | MAE: 0.2303 | RMSE: 0.3170 | RÂ²: 0.8924\n",
      "Epoch 15/20 | Train Loss: 0.4411 | Val Loss: 0.3129 | MAE: 0.2260 | RMSE: 0.3129 | RÂ²: 0.8952\n",
      "Epoch 16/20 | Train Loss: 0.4366 | Val Loss: 0.3162 | MAE: 0.2243 | RMSE: 0.3162 | RÂ²: 0.8930\n",
      "Epoch 17/20 | Train Loss: 0.4298 | Val Loss: 0.3244 | MAE: 0.2251 | RMSE: 0.3244 | RÂ²: 0.8873\n",
      "Epoch 18/20 | Train Loss: 0.4245 | Val Loss: 0.3250 | MAE: 0.2232 | RMSE: 0.3250 | RÂ²: 0.8869\n",
      "Epoch 19/20 | Train Loss: 0.4185 | Val Loss: 0.3154 | MAE: 0.2193 | RMSE: 0.3154 | RÂ²: 0.8935\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3129 | MAE = 0.2260\n",
      "\n",
      "ðŸ”§ [151/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[512, 256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.7097 | Val Loss: 0.8403 | MAE: 0.7092 | RMSE: 0.8403 | RÂ²: 0.2441\n",
      "Epoch 2/20 | Train Loss: 0.3870 | Val Loss: 0.5505 | MAE: 0.4610 | RMSE: 0.5505 | RÂ²: 0.6756\n",
      "Epoch 3/20 | Train Loss: 0.3240 | Val Loss: 0.3202 | MAE: 0.2531 | RMSE: 0.3202 | RÂ²: 0.8902\n",
      "Epoch 4/20 | Train Loss: 0.3009 | Val Loss: 0.2733 | MAE: 0.1993 | RMSE: 0.2733 | RÂ²: 0.9200\n",
      "Epoch 5/20 | Train Loss: 0.2813 | Val Loss: 0.2659 | MAE: 0.1887 | RMSE: 0.2659 | RÂ²: 0.9243\n",
      "Epoch 6/20 | Train Loss: 0.2677 | Val Loss: 0.2619 | MAE: 0.1862 | RMSE: 0.2619 | RÂ²: 0.9266\n",
      "Epoch 7/20 | Train Loss: 0.2609 | Val Loss: 0.2783 | MAE: 0.2014 | RMSE: 0.2783 | RÂ²: 0.9171\n",
      "Epoch 8/20 | Train Loss: 0.2584 | Val Loss: 0.2567 | MAE: 0.1756 | RMSE: 0.2567 | RÂ²: 0.9295\n",
      "Epoch 9/20 | Train Loss: 0.2515 | Val Loss: 0.2717 | MAE: 0.1921 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "Epoch 10/20 | Train Loss: 0.2461 | Val Loss: 0.2612 | MAE: 0.1805 | RMSE: 0.2612 | RÂ²: 0.9269\n",
      "Epoch 11/20 | Train Loss: 0.2436 | Val Loss: 0.2596 | MAE: 0.1784 | RMSE: 0.2596 | RÂ²: 0.9279\n",
      "Epoch 12/20 | Train Loss: 0.2386 | Val Loss: 0.2542 | MAE: 0.1748 | RMSE: 0.2542 | RÂ²: 0.9308\n",
      "Epoch 13/20 | Train Loss: 0.2354 | Val Loss: 0.2625 | MAE: 0.1808 | RMSE: 0.2625 | RÂ²: 0.9262\n",
      "Epoch 14/20 | Train Loss: 0.2365 | Val Loss: 0.2667 | MAE: 0.1852 | RMSE: 0.2667 | RÂ²: 0.9238\n",
      "Epoch 15/20 | Train Loss: 0.2325 | Val Loss: 0.2634 | MAE: 0.1821 | RMSE: 0.2634 | RÂ²: 0.9257\n",
      "Epoch 16/20 | Train Loss: 0.2288 | Val Loss: 0.2730 | MAE: 0.1876 | RMSE: 0.2730 | RÂ²: 0.9202\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2542 | MAE = 0.1748\n",
      "\n",
      "ðŸ”§ [152/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9407 | Val Loss: 0.7867 | MAE: 0.6638 | RMSE: 0.7867 | RÂ²: 0.3375\n",
      "Epoch 2/20 | Train Loss: 0.7631 | Val Loss: 0.6664 | MAE: 0.5555 | RMSE: 0.6664 | RÂ²: 0.5246\n",
      "Epoch 3/20 | Train Loss: 0.6703 | Val Loss: 0.5614 | MAE: 0.4379 | RMSE: 0.5614 | RÂ²: 0.6625\n",
      "Epoch 4/20 | Train Loss: 0.6042 | Val Loss: 0.4615 | MAE: 0.3370 | RMSE: 0.4615 | RÂ²: 0.7720\n",
      "Epoch 5/20 | Train Loss: 0.5565 | Val Loss: 0.4070 | MAE: 0.2930 | RMSE: 0.4070 | RÂ²: 0.8226\n",
      "Epoch 6/20 | Train Loss: 0.5161 | Val Loss: 0.3727 | MAE: 0.2733 | RMSE: 0.3727 | RÂ²: 0.8513\n",
      "Epoch 7/20 | Train Loss: 0.4936 | Val Loss: 0.3533 | MAE: 0.2564 | RMSE: 0.3533 | RÂ²: 0.8664\n",
      "Epoch 8/20 | Train Loss: 0.4813 | Val Loss: 0.3344 | MAE: 0.2419 | RMSE: 0.3344 | RÂ²: 0.8803\n",
      "Epoch 9/20 | Train Loss: 0.4580 | Val Loss: 0.3216 | MAE: 0.2337 | RMSE: 0.3216 | RÂ²: 0.8893\n",
      "Epoch 10/20 | Train Loss: 0.4474 | Val Loss: 0.3258 | MAE: 0.2330 | RMSE: 0.3258 | RÂ²: 0.8864\n",
      "Epoch 11/20 | Train Loss: 0.4364 | Val Loss: 0.3253 | MAE: 0.2294 | RMSE: 0.3253 | RÂ²: 0.8867\n",
      "Epoch 12/20 | Train Loss: 0.4234 | Val Loss: 0.3151 | MAE: 0.2217 | RMSE: 0.3151 | RÂ²: 0.8937\n",
      "Epoch 13/20 | Train Loss: 0.4159 | Val Loss: 0.3126 | MAE: 0.2181 | RMSE: 0.3126 | RÂ²: 0.8954\n",
      "Epoch 14/20 | Train Loss: 0.4091 | Val Loss: 0.3101 | MAE: 0.2163 | RMSE: 0.3101 | RÂ²: 0.8971\n",
      "Epoch 15/20 | Train Loss: 0.3958 | Val Loss: 0.3037 | MAE: 0.2131 | RMSE: 0.3037 | RÂ²: 0.9013\n",
      "Epoch 16/20 | Train Loss: 0.3909 | Val Loss: 0.3042 | MAE: 0.2121 | RMSE: 0.3042 | RÂ²: 0.9009\n",
      "Epoch 17/20 | Train Loss: 0.3869 | Val Loss: 0.2970 | MAE: 0.2085 | RMSE: 0.2970 | RÂ²: 0.9055\n",
      "Epoch 18/20 | Train Loss: 0.3804 | Val Loss: 0.2978 | MAE: 0.2089 | RMSE: 0.2978 | RÂ²: 0.9051\n",
      "Epoch 19/20 | Train Loss: 0.3726 | Val Loss: 0.3008 | MAE: 0.2105 | RMSE: 0.3008 | RÂ²: 0.9031\n",
      "Epoch 20/20 | Train Loss: 0.3703 | Val Loss: 0.2977 | MAE: 0.2075 | RMSE: 0.2977 | RÂ²: 0.9051\n",
      "âœ… RMSE = 0.2970 | MAE = 0.2085\n",
      "\n",
      "ðŸ”§ [153/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[256, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8955 | Val Loss: 0.6922 | MAE: 0.5828 | RMSE: 0.6922 | RÂ²: 0.4871\n",
      "Epoch 2/20 | Train Loss: 0.6632 | Val Loss: 0.5179 | MAE: 0.4067 | RMSE: 0.5179 | RÂ²: 0.7129\n",
      "Epoch 3/20 | Train Loss: 0.5711 | Val Loss: 0.4071 | MAE: 0.3062 | RMSE: 0.4071 | RÂ²: 0.8226\n",
      "Epoch 4/20 | Train Loss: 0.5080 | Val Loss: 0.4014 | MAE: 0.2921 | RMSE: 0.4014 | RÂ²: 0.8275\n",
      "Epoch 5/20 | Train Loss: 0.4777 | Val Loss: 0.3831 | MAE: 0.2954 | RMSE: 0.3831 | RÂ²: 0.8429\n",
      "Epoch 6/20 | Train Loss: 0.4433 | Val Loss: 0.3889 | MAE: 0.2930 | RMSE: 0.3889 | RÂ²: 0.8381\n",
      "Epoch 7/20 | Train Loss: 0.4238 | Val Loss: 0.3640 | MAE: 0.2726 | RMSE: 0.3640 | RÂ²: 0.8581\n",
      "Epoch 8/20 | Train Loss: 0.4061 | Val Loss: 0.3517 | MAE: 0.2656 | RMSE: 0.3517 | RÂ²: 0.8676\n",
      "Epoch 9/20 | Train Loss: 0.3900 | Val Loss: 0.3296 | MAE: 0.2387 | RMSE: 0.3296 | RÂ²: 0.8837\n",
      "Epoch 10/20 | Train Loss: 0.3722 | Val Loss: 0.3213 | MAE: 0.2260 | RMSE: 0.3213 | RÂ²: 0.8895\n",
      "Epoch 11/20 | Train Loss: 0.3666 | Val Loss: 0.3035 | MAE: 0.2162 | RMSE: 0.3035 | RÂ²: 0.9014\n",
      "Epoch 12/20 | Train Loss: 0.3584 | Val Loss: 0.2864 | MAE: 0.2020 | RMSE: 0.2864 | RÂ²: 0.9122\n",
      "Epoch 13/20 | Train Loss: 0.3478 | Val Loss: 0.2852 | MAE: 0.1990 | RMSE: 0.2852 | RÂ²: 0.9129\n",
      "Epoch 14/20 | Train Loss: 0.3351 | Val Loss: 0.2786 | MAE: 0.1950 | RMSE: 0.2786 | RÂ²: 0.9169\n",
      "Epoch 15/20 | Train Loss: 0.3300 | Val Loss: 0.2901 | MAE: 0.2018 | RMSE: 0.2901 | RÂ²: 0.9099\n",
      "Epoch 16/20 | Train Loss: 0.3254 | Val Loss: 0.2857 | MAE: 0.1968 | RMSE: 0.2857 | RÂ²: 0.9126\n",
      "Epoch 17/20 | Train Loss: 0.3182 | Val Loss: 0.2768 | MAE: 0.1903 | RMSE: 0.2768 | RÂ²: 0.9180\n",
      "Epoch 18/20 | Train Loss: 0.3137 | Val Loss: 0.2874 | MAE: 0.1987 | RMSE: 0.2874 | RÂ²: 0.9115\n",
      "Epoch 19/20 | Train Loss: 0.3075 | Val Loss: 0.2803 | MAE: 0.1962 | RMSE: 0.2803 | RÂ²: 0.9159\n",
      "Epoch 20/20 | Train Loss: 0.3058 | Val Loss: 0.2831 | MAE: 0.1979 | RMSE: 0.2831 | RÂ²: 0.9142\n",
      "âœ… RMSE = 0.2768 | MAE = 0.1903\n",
      "\n",
      "ðŸ”§ [154/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8267 | Val Loss: 0.8794 | MAE: 0.7420 | RMSE: 0.8794 | RÂ²: 0.1721\n",
      "Epoch 2/20 | Train Loss: 0.5949 | Val Loss: 0.7523 | MAE: 0.6280 | RMSE: 0.7523 | RÂ²: 0.3942\n",
      "Epoch 3/20 | Train Loss: 0.5171 | Val Loss: 0.5919 | MAE: 0.4846 | RMSE: 0.5919 | RÂ²: 0.6249\n",
      "Epoch 4/20 | Train Loss: 0.4639 | Val Loss: 0.4335 | MAE: 0.3539 | RMSE: 0.4335 | RÂ²: 0.7988\n",
      "Epoch 5/20 | Train Loss: 0.4281 | Val Loss: 0.3407 | MAE: 0.2699 | RMSE: 0.3407 | RÂ²: 0.8758\n",
      "Epoch 6/20 | Train Loss: 0.4166 | Val Loss: 0.3175 | MAE: 0.2377 | RMSE: 0.3175 | RÂ²: 0.8921\n",
      "Epoch 7/20 | Train Loss: 0.3960 | Val Loss: 0.2971 | MAE: 0.2116 | RMSE: 0.2971 | RÂ²: 0.9055\n",
      "Epoch 8/20 | Train Loss: 0.3817 | Val Loss: 0.2876 | MAE: 0.2058 | RMSE: 0.2876 | RÂ²: 0.9114\n",
      "Epoch 9/20 | Train Loss: 0.3780 | Val Loss: 0.2907 | MAE: 0.2075 | RMSE: 0.2907 | RÂ²: 0.9095\n",
      "Epoch 10/20 | Train Loss: 0.3586 | Val Loss: 0.2817 | MAE: 0.2026 | RMSE: 0.2817 | RÂ²: 0.9150\n",
      "Epoch 11/20 | Train Loss: 0.3523 | Val Loss: 0.2847 | MAE: 0.2024 | RMSE: 0.2847 | RÂ²: 0.9132\n",
      "Epoch 12/20 | Train Loss: 0.3451 | Val Loss: 0.2938 | MAE: 0.2103 | RMSE: 0.2938 | RÂ²: 0.9076\n",
      "Epoch 13/20 | Train Loss: 0.3372 | Val Loss: 0.2949 | MAE: 0.2107 | RMSE: 0.2949 | RÂ²: 0.9069\n",
      "Epoch 14/20 | Train Loss: 0.3298 | Val Loss: 0.2887 | MAE: 0.2047 | RMSE: 0.2887 | RÂ²: 0.9108\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2817 | MAE = 0.2026\n",
      "\n",
      "ðŸ”§ [155/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.9730 | Val Loss: 0.9256 | MAE: 0.7863 | RMSE: 0.9256 | RÂ²: 0.0827\n",
      "Epoch 2/20 | Train Loss: 0.7993 | Val Loss: 0.7929 | MAE: 0.6721 | RMSE: 0.7929 | RÂ²: 0.3269\n",
      "Epoch 3/20 | Train Loss: 0.6851 | Val Loss: 0.6624 | MAE: 0.5578 | RMSE: 0.6624 | RÂ²: 0.5302\n",
      "Epoch 4/20 | Train Loss: 0.6118 | Val Loss: 0.5425 | MAE: 0.4441 | RMSE: 0.5425 | RÂ²: 0.6849\n",
      "Epoch 5/20 | Train Loss: 0.5693 | Val Loss: 0.4361 | MAE: 0.3443 | RMSE: 0.4361 | RÂ²: 0.7964\n",
      "Epoch 6/20 | Train Loss: 0.5398 | Val Loss: 0.3718 | MAE: 0.2795 | RMSE: 0.3718 | RÂ²: 0.8520\n",
      "Epoch 7/20 | Train Loss: 0.5133 | Val Loss: 0.3334 | MAE: 0.2462 | RMSE: 0.3334 | RÂ²: 0.8810\n",
      "Epoch 8/20 | Train Loss: 0.4929 | Val Loss: 0.3161 | MAE: 0.2335 | RMSE: 0.3161 | RÂ²: 0.8930\n",
      "Epoch 9/20 | Train Loss: 0.4813 | Val Loss: 0.3075 | MAE: 0.2272 | RMSE: 0.3075 | RÂ²: 0.8988\n",
      "Epoch 10/20 | Train Loss: 0.4733 | Val Loss: 0.3058 | MAE: 0.2245 | RMSE: 0.3058 | RÂ²: 0.8999\n",
      "Epoch 11/20 | Train Loss: 0.4567 | Val Loss: 0.3035 | MAE: 0.2214 | RMSE: 0.3035 | RÂ²: 0.9014\n",
      "Epoch 12/20 | Train Loss: 0.4452 | Val Loss: 0.3008 | MAE: 0.2192 | RMSE: 0.3008 | RÂ²: 0.9032\n",
      "Epoch 13/20 | Train Loss: 0.4464 | Val Loss: 0.3022 | MAE: 0.2199 | RMSE: 0.3022 | RÂ²: 0.9022\n",
      "Epoch 14/20 | Train Loss: 0.4391 | Val Loss: 0.3042 | MAE: 0.2213 | RMSE: 0.3042 | RÂ²: 0.9010\n",
      "Epoch 15/20 | Train Loss: 0.4260 | Val Loss: 0.3020 | MAE: 0.2197 | RMSE: 0.3020 | RÂ²: 0.9024\n",
      "Epoch 16/20 | Train Loss: 0.4167 | Val Loss: 0.3010 | MAE: 0.2183 | RMSE: 0.3010 | RÂ²: 0.9030\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3008 | MAE = 0.2192\n",
      "\n",
      "ðŸ”§ [156/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 1.0026 | Val Loss: 0.9166 | MAE: 0.7748 | RMSE: 0.9166 | RÂ²: 0.1005\n",
      "Epoch 2/20 | Train Loss: 0.8447 | Val Loss: 0.8214 | MAE: 0.6973 | RMSE: 0.8214 | RÂ²: 0.2777\n",
      "Epoch 3/20 | Train Loss: 0.7515 | Val Loss: 0.7260 | MAE: 0.6125 | RMSE: 0.7260 | RÂ²: 0.4357\n",
      "Epoch 4/20 | Train Loss: 0.6728 | Val Loss: 0.6325 | MAE: 0.5314 | RMSE: 0.6325 | RÂ²: 0.5718\n",
      "Epoch 5/20 | Train Loss: 0.6053 | Val Loss: 0.5505 | MAE: 0.4574 | RMSE: 0.5505 | RÂ²: 0.6756\n",
      "Epoch 6/20 | Train Loss: 0.5508 | Val Loss: 0.4786 | MAE: 0.3887 | RMSE: 0.4786 | RÂ²: 0.7548\n",
      "Epoch 7/20 | Train Loss: 0.5019 | Val Loss: 0.4145 | MAE: 0.3291 | RMSE: 0.4145 | RÂ²: 0.8161\n",
      "Epoch 8/20 | Train Loss: 0.4648 | Val Loss: 0.3755 | MAE: 0.2890 | RMSE: 0.3755 | RÂ²: 0.8491\n",
      "Epoch 9/20 | Train Loss: 0.4419 | Val Loss: 0.3576 | MAE: 0.2713 | RMSE: 0.3576 | RÂ²: 0.8631\n",
      "Epoch 10/20 | Train Loss: 0.4204 | Val Loss: 0.3461 | MAE: 0.2622 | RMSE: 0.3461 | RÂ²: 0.8717\n",
      "Epoch 11/20 | Train Loss: 0.4062 | Val Loss: 0.3378 | MAE: 0.2544 | RMSE: 0.3378 | RÂ²: 0.8779\n",
      "Epoch 12/20 | Train Loss: 0.3901 | Val Loss: 0.3330 | MAE: 0.2492 | RMSE: 0.3330 | RÂ²: 0.8813\n",
      "Epoch 13/20 | Train Loss: 0.3800 | Val Loss: 0.3277 | MAE: 0.2447 | RMSE: 0.3277 | RÂ²: 0.8851\n",
      "Epoch 14/20 | Train Loss: 0.3709 | Val Loss: 0.3223 | MAE: 0.2397 | RMSE: 0.3223 | RÂ²: 0.8888\n",
      "Epoch 15/20 | Train Loss: 0.3625 | Val Loss: 0.3190 | MAE: 0.2355 | RMSE: 0.3190 | RÂ²: 0.8910\n",
      "Epoch 16/20 | Train Loss: 0.3568 | Val Loss: 0.3136 | MAE: 0.2310 | RMSE: 0.3136 | RÂ²: 0.8947\n",
      "Epoch 17/20 | Train Loss: 0.3501 | Val Loss: 0.3089 | MAE: 0.2270 | RMSE: 0.3089 | RÂ²: 0.8978\n",
      "Epoch 18/20 | Train Loss: 0.3427 | Val Loss: 0.3067 | MAE: 0.2243 | RMSE: 0.3067 | RÂ²: 0.8993\n",
      "Epoch 19/20 | Train Loss: 0.3396 | Val Loss: 0.3053 | MAE: 0.2221 | RMSE: 0.3053 | RÂ²: 0.9002\n",
      "Epoch 20/20 | Train Loss: 0.3343 | Val Loss: 0.3016 | MAE: 0.2194 | RMSE: 0.3016 | RÂ²: 0.9026\n",
      "âœ… RMSE = 0.3016 | MAE = 0.2194\n",
      "\n",
      "ðŸ”§ [157/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.0428 | Val Loss: 0.9276 | MAE: 0.7833 | RMSE: 0.9276 | RÂ²: 0.0789\n",
      "Epoch 2/20 | Train Loss: 0.9077 | Val Loss: 0.8305 | MAE: 0.7036 | RMSE: 0.8305 | RÂ²: 0.2616\n",
      "Epoch 3/20 | Train Loss: 0.8482 | Val Loss: 0.7426 | MAE: 0.6246 | RMSE: 0.7426 | RÂ²: 0.4097\n",
      "Epoch 4/20 | Train Loss: 0.8050 | Val Loss: 0.6648 | MAE: 0.5552 | RMSE: 0.6648 | RÂ²: 0.5268\n",
      "Epoch 5/20 | Train Loss: 0.7499 | Val Loss: 0.5949 | MAE: 0.4990 | RMSE: 0.5949 | RÂ²: 0.6212\n",
      "Epoch 6/20 | Train Loss: 0.7208 | Val Loss: 0.5412 | MAE: 0.4551 | RMSE: 0.5412 | RÂ²: 0.6864\n",
      "Epoch 7/20 | Train Loss: 0.7034 | Val Loss: 0.5015 | MAE: 0.4219 | RMSE: 0.5015 | RÂ²: 0.7307\n",
      "Epoch 8/20 | Train Loss: 0.6697 | Val Loss: 0.4695 | MAE: 0.3920 | RMSE: 0.4695 | RÂ²: 0.7640\n",
      "Epoch 9/20 | Train Loss: 0.6539 | Val Loss: 0.4404 | MAE: 0.3637 | RMSE: 0.4404 | RÂ²: 0.7924\n",
      "Epoch 10/20 | Train Loss: 0.6282 | Val Loss: 0.4151 | MAE: 0.3383 | RMSE: 0.4151 | RÂ²: 0.8156\n",
      "Epoch 11/20 | Train Loss: 0.6136 | Val Loss: 0.3894 | MAE: 0.3136 | RMSE: 0.3894 | RÂ²: 0.8377\n",
      "Epoch 12/20 | Train Loss: 0.5895 | Val Loss: 0.3735 | MAE: 0.2984 | RMSE: 0.3735 | RÂ²: 0.8506\n",
      "Epoch 13/20 | Train Loss: 0.5808 | Val Loss: 0.3632 | MAE: 0.2873 | RMSE: 0.3632 | RÂ²: 0.8588\n",
      "Epoch 14/20 | Train Loss: 0.5647 | Val Loss: 0.3574 | MAE: 0.2797 | RMSE: 0.3574 | RÂ²: 0.8633\n",
      "Epoch 15/20 | Train Loss: 0.5503 | Val Loss: 0.3510 | MAE: 0.2722 | RMSE: 0.3510 | RÂ²: 0.8681\n",
      "Epoch 16/20 | Train Loss: 0.5427 | Val Loss: 0.3427 | MAE: 0.2636 | RMSE: 0.3427 | RÂ²: 0.8743\n",
      "Epoch 17/20 | Train Loss: 0.5330 | Val Loss: 0.3344 | MAE: 0.2554 | RMSE: 0.3344 | RÂ²: 0.8803\n",
      "Epoch 18/20 | Train Loss: 0.5228 | Val Loss: 0.3319 | MAE: 0.2514 | RMSE: 0.3319 | RÂ²: 0.8821\n",
      "Epoch 19/20 | Train Loss: 0.5138 | Val Loss: 0.3297 | MAE: 0.2478 | RMSE: 0.3297 | RÂ²: 0.8836\n",
      "Epoch 20/20 | Train Loss: 0.5045 | Val Loss: 0.3281 | MAE: 0.2445 | RMSE: 0.3281 | RÂ²: 0.8847\n",
      "âœ… RMSE = 0.3281 | MAE = 0.2445\n",
      "\n",
      "ðŸ”§ [158/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.0440 | Val Loss: 0.6526 | MAE: 0.5369 | RMSE: 0.6526 | RÂ²: 0.5440\n",
      "Epoch 2/20 | Train Loss: 0.5930 | Val Loss: 0.5171 | MAE: 0.4097 | RMSE: 0.5171 | RÂ²: 0.7137\n",
      "Epoch 3/20 | Train Loss: 0.5144 | Val Loss: 0.6826 | MAE: 0.5332 | RMSE: 0.6826 | RÂ²: 0.5012\n",
      "Epoch 4/20 | Train Loss: 0.4456 | Val Loss: 0.5339 | MAE: 0.4079 | RMSE: 0.5339 | RÂ²: 0.6948\n",
      "Epoch 5/20 | Train Loss: 0.4080 | Val Loss: 0.5842 | MAE: 0.4427 | RMSE: 0.5842 | RÂ²: 0.6346\n",
      "Epoch 6/20 | Train Loss: 0.3928 | Val Loss: 0.3888 | MAE: 0.2906 | RMSE: 0.3888 | RÂ²: 0.8382\n",
      "Epoch 7/20 | Train Loss: 0.3630 | Val Loss: 0.3791 | MAE: 0.2796 | RMSE: 0.3791 | RÂ²: 0.8461\n",
      "Epoch 8/20 | Train Loss: 0.3434 | Val Loss: 0.3284 | MAE: 0.2447 | RMSE: 0.3284 | RÂ²: 0.8845\n",
      "Epoch 9/20 | Train Loss: 0.3252 | Val Loss: 0.3074 | MAE: 0.2143 | RMSE: 0.3074 | RÂ²: 0.8988\n",
      "Epoch 10/20 | Train Loss: 0.3232 | Val Loss: 0.3064 | MAE: 0.2224 | RMSE: 0.3064 | RÂ²: 0.8995\n",
      "Epoch 11/20 | Train Loss: 0.3121 | Val Loss: 0.2732 | MAE: 0.1850 | RMSE: 0.2732 | RÂ²: 0.9201\n",
      "Epoch 12/20 | Train Loss: 0.3013 | Val Loss: 0.2756 | MAE: 0.1916 | RMSE: 0.2756 | RÂ²: 0.9187\n",
      "Epoch 13/20 | Train Loss: 0.2950 | Val Loss: 0.2671 | MAE: 0.1787 | RMSE: 0.2671 | RÂ²: 0.9236\n",
      "Epoch 14/20 | Train Loss: 0.2913 | Val Loss: 0.2742 | MAE: 0.1868 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "Epoch 15/20 | Train Loss: 0.2835 | Val Loss: 0.2700 | MAE: 0.1824 | RMSE: 0.2700 | RÂ²: 0.9219\n",
      "Epoch 16/20 | Train Loss: 0.2785 | Val Loss: 0.2775 | MAE: 0.1924 | RMSE: 0.2775 | RÂ²: 0.9176\n",
      "Epoch 17/20 | Train Loss: 0.2831 | Val Loss: 0.2819 | MAE: 0.1935 | RMSE: 0.2819 | RÂ²: 0.9149\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2671 | MAE = 0.1787\n",
      "\n",
      "ðŸ”§ [159/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8633 | Val Loss: 0.8411 | MAE: 0.7079 | RMSE: 0.8411 | RÂ²: 0.2425\n",
      "Epoch 2/20 | Train Loss: 0.5885 | Val Loss: 0.6666 | MAE: 0.5482 | RMSE: 0.6666 | RÂ²: 0.5243\n",
      "Epoch 3/20 | Train Loss: 0.5027 | Val Loss: 0.4859 | MAE: 0.3944 | RMSE: 0.4859 | RÂ²: 0.7472\n",
      "Epoch 4/20 | Train Loss: 0.4609 | Val Loss: 0.3731 | MAE: 0.2874 | RMSE: 0.3731 | RÂ²: 0.8510\n",
      "Epoch 5/20 | Train Loss: 0.4367 | Val Loss: 0.3174 | MAE: 0.2424 | RMSE: 0.3174 | RÂ²: 0.8921\n",
      "Epoch 6/20 | Train Loss: 0.4136 | Val Loss: 0.3158 | MAE: 0.2333 | RMSE: 0.3158 | RÂ²: 0.8932\n",
      "Epoch 7/20 | Train Loss: 0.3919 | Val Loss: 0.2959 | MAE: 0.2149 | RMSE: 0.2959 | RÂ²: 0.9063\n",
      "Epoch 8/20 | Train Loss: 0.3796 | Val Loss: 0.2866 | MAE: 0.2079 | RMSE: 0.2866 | RÂ²: 0.9121\n",
      "Epoch 9/20 | Train Loss: 0.3670 | Val Loss: 0.2732 | MAE: 0.1939 | RMSE: 0.2732 | RÂ²: 0.9201\n",
      "Epoch 10/20 | Train Loss: 0.3543 | Val Loss: 0.2812 | MAE: 0.2002 | RMSE: 0.2812 | RÂ²: 0.9153\n",
      "Epoch 11/20 | Train Loss: 0.3386 | Val Loss: 0.2663 | MAE: 0.1870 | RMSE: 0.2663 | RÂ²: 0.9241\n",
      "Epoch 12/20 | Train Loss: 0.3327 | Val Loss: 0.2688 | MAE: 0.1891 | RMSE: 0.2688 | RÂ²: 0.9226\n",
      "Epoch 13/20 | Train Loss: 0.3314 | Val Loss: 0.2839 | MAE: 0.1966 | RMSE: 0.2839 | RÂ²: 0.9137\n",
      "Epoch 14/20 | Train Loss: 0.3179 | Val Loss: 0.2745 | MAE: 0.1906 | RMSE: 0.2745 | RÂ²: 0.9193\n",
      "Epoch 15/20 | Train Loss: 0.3126 | Val Loss: 0.2802 | MAE: 0.1947 | RMSE: 0.2802 | RÂ²: 0.9160\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2663 | MAE = 0.1870\n",
      "\n",
      "ðŸ”§ [160/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8946 | Val Loss: 0.7384 | MAE: 0.6208 | RMSE: 0.7384 | RÂ²: 0.4163\n",
      "Epoch 2/20 | Train Loss: 0.5582 | Val Loss: 0.4773 | MAE: 0.3617 | RMSE: 0.4773 | RÂ²: 0.7562\n",
      "Epoch 3/20 | Train Loss: 0.4776 | Val Loss: 0.4004 | MAE: 0.2822 | RMSE: 0.4004 | RÂ²: 0.8284\n",
      "Epoch 4/20 | Train Loss: 0.4384 | Val Loss: 0.3656 | MAE: 0.2853 | RMSE: 0.3656 | RÂ²: 0.8569\n",
      "Epoch 5/20 | Train Loss: 0.4063 | Val Loss: 0.3978 | MAE: 0.3091 | RMSE: 0.3978 | RÂ²: 0.8306\n",
      "Epoch 6/20 | Train Loss: 0.3748 | Val Loss: 0.3588 | MAE: 0.2647 | RMSE: 0.3588 | RÂ²: 0.8622\n",
      "Epoch 7/20 | Train Loss: 0.3534 | Val Loss: 0.3205 | MAE: 0.2417 | RMSE: 0.3205 | RÂ²: 0.8900\n",
      "Epoch 8/20 | Train Loss: 0.3409 | Val Loss: 0.2908 | MAE: 0.2123 | RMSE: 0.2908 | RÂ²: 0.9095\n",
      "Epoch 9/20 | Train Loss: 0.3305 | Val Loss: 0.3080 | MAE: 0.2139 | RMSE: 0.3080 | RÂ²: 0.8984\n",
      "Epoch 10/20 | Train Loss: 0.3205 | Val Loss: 0.2651 | MAE: 0.1881 | RMSE: 0.2651 | RÂ²: 0.9248\n",
      "Epoch 11/20 | Train Loss: 0.3066 | Val Loss: 0.2639 | MAE: 0.1817 | RMSE: 0.2639 | RÂ²: 0.9254\n",
      "Epoch 12/20 | Train Loss: 0.2977 | Val Loss: 0.2709 | MAE: 0.1900 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 13/20 | Train Loss: 0.2875 | Val Loss: 0.2634 | MAE: 0.1841 | RMSE: 0.2634 | RÂ²: 0.9257\n",
      "Epoch 14/20 | Train Loss: 0.2875 | Val Loss: 0.2599 | MAE: 0.1795 | RMSE: 0.2599 | RÂ²: 0.9277\n",
      "Epoch 15/20 | Train Loss: 0.2849 | Val Loss: 0.2784 | MAE: 0.1950 | RMSE: 0.2784 | RÂ²: 0.9171\n",
      "Epoch 16/20 | Train Loss: 0.2801 | Val Loss: 0.2528 | MAE: 0.1724 | RMSE: 0.2528 | RÂ²: 0.9316\n",
      "Epoch 17/20 | Train Loss: 0.2768 | Val Loss: 0.2800 | MAE: 0.1935 | RMSE: 0.2800 | RÂ²: 0.9160\n",
      "Epoch 18/20 | Train Loss: 0.2751 | Val Loss: 0.2689 | MAE: 0.1854 | RMSE: 0.2689 | RÂ²: 0.9226\n",
      "Epoch 19/20 | Train Loss: 0.2707 | Val Loss: 0.2599 | MAE: 0.1768 | RMSE: 0.2599 | RÂ²: 0.9277\n",
      "Epoch 20/20 | Train Loss: 0.2633 | Val Loss: 0.2806 | MAE: 0.1963 | RMSE: 0.2806 | RÂ²: 0.9157\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2528 | MAE = 0.1724\n",
      "\n",
      "ðŸ”§ [161/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[512, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.7719 | Val Loss: 0.7905 | MAE: 0.6643 | RMSE: 0.7905 | RÂ²: 0.3310\n",
      "Epoch 2/20 | Train Loss: 0.3984 | Val Loss: 0.6153 | MAE: 0.5112 | RMSE: 0.6153 | RÂ²: 0.5947\n",
      "Epoch 3/20 | Train Loss: 0.3341 | Val Loss: 0.3672 | MAE: 0.2904 | RMSE: 0.3672 | RÂ²: 0.8556\n",
      "Epoch 4/20 | Train Loss: 0.3005 | Val Loss: 0.2777 | MAE: 0.2019 | RMSE: 0.2777 | RÂ²: 0.9174\n",
      "Epoch 5/20 | Train Loss: 0.2807 | Val Loss: 0.2558 | MAE: 0.1805 | RMSE: 0.2558 | RÂ²: 0.9299\n",
      "Epoch 6/20 | Train Loss: 0.2694 | Val Loss: 0.3046 | MAE: 0.2057 | RMSE: 0.3046 | RÂ²: 0.9007\n",
      "Epoch 7/20 | Train Loss: 0.2599 | Val Loss: 0.2704 | MAE: 0.1911 | RMSE: 0.2704 | RÂ²: 0.9217\n",
      "Epoch 8/20 | Train Loss: 0.2550 | Val Loss: 0.2809 | MAE: 0.1881 | RMSE: 0.2809 | RÂ²: 0.9155\n",
      "Epoch 9/20 | Train Loss: 0.2500 | Val Loss: 0.2569 | MAE: 0.1780 | RMSE: 0.2569 | RÂ²: 0.9294\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2558 | MAE = 0.1805\n",
      "\n",
      "ðŸ”§ [162/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8406 | Val Loss: 0.8367 | MAE: 0.7072 | RMSE: 0.8367 | RÂ²: 0.2506\n",
      "Epoch 2/20 | Train Loss: 0.5851 | Val Loss: 0.6898 | MAE: 0.5779 | RMSE: 0.6898 | RÂ²: 0.4906\n",
      "Epoch 3/20 | Train Loss: 0.4618 | Val Loss: 0.5196 | MAE: 0.4332 | RMSE: 0.5196 | RÂ²: 0.7110\n",
      "Epoch 4/20 | Train Loss: 0.3947 | Val Loss: 0.4067 | MAE: 0.3359 | RMSE: 0.4067 | RÂ²: 0.8230\n",
      "Epoch 5/20 | Train Loss: 0.3609 | Val Loss: 0.3422 | MAE: 0.2687 | RMSE: 0.3422 | RÂ²: 0.8746\n",
      "Epoch 6/20 | Train Loss: 0.3434 | Val Loss: 0.3104 | MAE: 0.2337 | RMSE: 0.3104 | RÂ²: 0.8968\n",
      "Epoch 7/20 | Train Loss: 0.3259 | Val Loss: 0.2813 | MAE: 0.2047 | RMSE: 0.2813 | RÂ²: 0.9153\n",
      "Epoch 8/20 | Train Loss: 0.3094 | Val Loss: 0.2753 | MAE: 0.1988 | RMSE: 0.2753 | RÂ²: 0.9189\n",
      "Epoch 9/20 | Train Loss: 0.2999 | Val Loss: 0.2788 | MAE: 0.2014 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "Epoch 10/20 | Train Loss: 0.2922 | Val Loss: 0.2809 | MAE: 0.2001 | RMSE: 0.2809 | RÂ²: 0.9155\n",
      "Epoch 11/20 | Train Loss: 0.2873 | Val Loss: 0.2772 | MAE: 0.1978 | RMSE: 0.2772 | RÂ²: 0.9178\n",
      "Epoch 12/20 | Train Loss: 0.2858 | Val Loss: 0.2722 | MAE: 0.1928 | RMSE: 0.2722 | RÂ²: 0.9207\n",
      "Epoch 13/20 | Train Loss: 0.2773 | Val Loss: 0.2761 | MAE: 0.1959 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 14/20 | Train Loss: 0.2728 | Val Loss: 0.2687 | MAE: 0.1900 | RMSE: 0.2687 | RÂ²: 0.9227\n",
      "Epoch 15/20 | Train Loss: 0.2688 | Val Loss: 0.2774 | MAE: 0.1953 | RMSE: 0.2774 | RÂ²: 0.9176\n",
      "Epoch 16/20 | Train Loss: 0.2675 | Val Loss: 0.2808 | MAE: 0.1969 | RMSE: 0.2808 | RÂ²: 0.9156\n",
      "Epoch 17/20 | Train Loss: 0.2641 | Val Loss: 0.2755 | MAE: 0.1935 | RMSE: 0.2755 | RÂ²: 0.9187\n",
      "Epoch 18/20 | Train Loss: 0.2606 | Val Loss: 0.2788 | MAE: 0.1949 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2687 | MAE = 0.1900\n",
      "\n",
      "ðŸ”§ [163/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[512, 256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9187 | Val Loss: 0.8703 | MAE: 0.7408 | RMSE: 0.8703 | RÂ²: 0.1890\n",
      "Epoch 2/20 | Train Loss: 0.6689 | Val Loss: 0.7138 | MAE: 0.6014 | RMSE: 0.7138 | RÂ²: 0.4545\n",
      "Epoch 3/20 | Train Loss: 0.5667 | Val Loss: 0.5153 | MAE: 0.4145 | RMSE: 0.5153 | RÂ²: 0.7157\n",
      "Epoch 4/20 | Train Loss: 0.5055 | Val Loss: 0.3831 | MAE: 0.2973 | RMSE: 0.3831 | RÂ²: 0.8429\n",
      "Epoch 5/20 | Train Loss: 0.4648 | Val Loss: 0.3497 | MAE: 0.2621 | RMSE: 0.3497 | RÂ²: 0.8691\n",
      "Epoch 6/20 | Train Loss: 0.4324 | Val Loss: 0.3442 | MAE: 0.2614 | RMSE: 0.3442 | RÂ²: 0.8731\n",
      "Epoch 7/20 | Train Loss: 0.4131 | Val Loss: 0.3558 | MAE: 0.2761 | RMSE: 0.3558 | RÂ²: 0.8645\n",
      "Epoch 8/20 | Train Loss: 0.3984 | Val Loss: 0.3226 | MAE: 0.2427 | RMSE: 0.3226 | RÂ²: 0.8886\n",
      "Epoch 9/20 | Train Loss: 0.3768 | Val Loss: 0.3230 | MAE: 0.2405 | RMSE: 0.3230 | RÂ²: 0.8883\n",
      "Epoch 10/20 | Train Loss: 0.3687 | Val Loss: 0.3208 | MAE: 0.2425 | RMSE: 0.3208 | RÂ²: 0.8898\n",
      "Epoch 11/20 | Train Loss: 0.3619 | Val Loss: 0.3023 | MAE: 0.2224 | RMSE: 0.3023 | RÂ²: 0.9022\n",
      "Epoch 12/20 | Train Loss: 0.3480 | Val Loss: 0.2830 | MAE: 0.2026 | RMSE: 0.2830 | RÂ²: 0.9142\n",
      "Epoch 13/20 | Train Loss: 0.3415 | Val Loss: 0.2809 | MAE: 0.2001 | RMSE: 0.2809 | RÂ²: 0.9155\n",
      "Epoch 14/20 | Train Loss: 0.3348 | Val Loss: 0.2835 | MAE: 0.2037 | RMSE: 0.2835 | RÂ²: 0.9139\n",
      "Epoch 15/20 | Train Loss: 0.3260 | Val Loss: 0.2767 | MAE: 0.1940 | RMSE: 0.2767 | RÂ²: 0.9181\n",
      "Epoch 16/20 | Train Loss: 0.3227 | Val Loss: 0.2773 | MAE: 0.1939 | RMSE: 0.2773 | RÂ²: 0.9177\n",
      "Epoch 17/20 | Train Loss: 0.3184 | Val Loss: 0.2748 | MAE: 0.1926 | RMSE: 0.2748 | RÂ²: 0.9192\n",
      "Epoch 18/20 | Train Loss: 0.3157 | Val Loss: 0.2769 | MAE: 0.1919 | RMSE: 0.2769 | RÂ²: 0.9179\n",
      "Epoch 19/20 | Train Loss: 0.3098 | Val Loss: 0.2758 | MAE: 0.1917 | RMSE: 0.2758 | RÂ²: 0.9186\n",
      "Epoch 20/20 | Train Loss: 0.3076 | Val Loss: 0.2704 | MAE: 0.1868 | RMSE: 0.2704 | RÂ²: 0.9217\n",
      "âœ… RMSE = 0.2704 | MAE = 0.1868\n",
      "\n",
      "ðŸ”§ [164/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8792 | Val Loss: 0.7671 | MAE: 0.6508 | RMSE: 0.7671 | RÂ²: 0.3700\n",
      "Epoch 2/20 | Train Loss: 0.6365 | Val Loss: 0.6513 | MAE: 0.5305 | RMSE: 0.6513 | RÂ²: 0.5459\n",
      "Epoch 3/20 | Train Loss: 0.5408 | Val Loss: 0.4446 | MAE: 0.3579 | RMSE: 0.4446 | RÂ²: 0.7884\n",
      "Epoch 4/20 | Train Loss: 0.4879 | Val Loss: 0.3616 | MAE: 0.2802 | RMSE: 0.3616 | RÂ²: 0.8600\n",
      "Epoch 5/20 | Train Loss: 0.4439 | Val Loss: 0.3167 | MAE: 0.2345 | RMSE: 0.3167 | RÂ²: 0.8926\n",
      "Epoch 6/20 | Train Loss: 0.4237 | Val Loss: 0.3053 | MAE: 0.2258 | RMSE: 0.3053 | RÂ²: 0.9002\n",
      "Epoch 7/20 | Train Loss: 0.4081 | Val Loss: 0.3263 | MAE: 0.2355 | RMSE: 0.3263 | RÂ²: 0.8860\n",
      "Epoch 8/20 | Train Loss: 0.3916 | Val Loss: 0.2966 | MAE: 0.2133 | RMSE: 0.2966 | RÂ²: 0.9058\n",
      "Epoch 9/20 | Train Loss: 0.3805 | Val Loss: 0.2792 | MAE: 0.1997 | RMSE: 0.2792 | RÂ²: 0.9166\n",
      "Epoch 10/20 | Train Loss: 0.3679 | Val Loss: 0.2695 | MAE: 0.1907 | RMSE: 0.2695 | RÂ²: 0.9223\n",
      "Epoch 11/20 | Train Loss: 0.3547 | Val Loss: 0.2678 | MAE: 0.1875 | RMSE: 0.2678 | RÂ²: 0.9232\n",
      "Epoch 12/20 | Train Loss: 0.3422 | Val Loss: 0.2655 | MAE: 0.1851 | RMSE: 0.2655 | RÂ²: 0.9245\n",
      "Epoch 13/20 | Train Loss: 0.3415 | Val Loss: 0.2687 | MAE: 0.1865 | RMSE: 0.2687 | RÂ²: 0.9227\n",
      "Epoch 14/20 | Train Loss: 0.3295 | Val Loss: 0.2635 | MAE: 0.1819 | RMSE: 0.2635 | RÂ²: 0.9257\n",
      "Epoch 15/20 | Train Loss: 0.3251 | Val Loss: 0.2675 | MAE: 0.1847 | RMSE: 0.2675 | RÂ²: 0.9234\n",
      "Epoch 16/20 | Train Loss: 0.3207 | Val Loss: 0.2658 | MAE: 0.1833 | RMSE: 0.2658 | RÂ²: 0.9244\n",
      "Epoch 17/20 | Train Loss: 0.3170 | Val Loss: 0.2670 | MAE: 0.1836 | RMSE: 0.2670 | RÂ²: 0.9237\n",
      "Epoch 18/20 | Train Loss: 0.3130 | Val Loss: 0.2707 | MAE: 0.1862 | RMSE: 0.2707 | RÂ²: 0.9216\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2635 | MAE = 0.1819\n",
      "\n",
      "ðŸ”§ [165/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[512, 512, 512], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.9471 | Val Loss: 0.9186 | MAE: 0.7778 | RMSE: 0.9186 | RÂ²: 0.0967\n",
      "Epoch 2/20 | Train Loss: 0.7651 | Val Loss: 0.8242 | MAE: 0.6974 | RMSE: 0.8242 | RÂ²: 0.2727\n",
      "Epoch 3/20 | Train Loss: 0.6599 | Val Loss: 0.7186 | MAE: 0.5990 | RMSE: 0.7186 | RÂ²: 0.4472\n",
      "Epoch 4/20 | Train Loss: 0.5902 | Val Loss: 0.5860 | MAE: 0.4710 | RMSE: 0.5860 | RÂ²: 0.6324\n",
      "Epoch 5/20 | Train Loss: 0.5476 | Val Loss: 0.4847 | MAE: 0.3612 | RMSE: 0.4847 | RÂ²: 0.7485\n",
      "Epoch 6/20 | Train Loss: 0.5154 | Val Loss: 0.4255 | MAE: 0.2980 | RMSE: 0.4255 | RÂ²: 0.8062\n",
      "Epoch 7/20 | Train Loss: 0.5005 | Val Loss: 0.3719 | MAE: 0.2697 | RMSE: 0.3719 | RÂ²: 0.8519\n",
      "Epoch 8/20 | Train Loss: 0.4772 | Val Loss: 0.3548 | MAE: 0.2559 | RMSE: 0.3548 | RÂ²: 0.8652\n",
      "Epoch 9/20 | Train Loss: 0.4640 | Val Loss: 0.3450 | MAE: 0.2474 | RMSE: 0.3450 | RÂ²: 0.8726\n",
      "Epoch 10/20 | Train Loss: 0.4562 | Val Loss: 0.3300 | MAE: 0.2369 | RMSE: 0.3300 | RÂ²: 0.8834\n",
      "Epoch 11/20 | Train Loss: 0.4485 | Val Loss: 0.3216 | MAE: 0.2283 | RMSE: 0.3216 | RÂ²: 0.8893\n",
      "Epoch 12/20 | Train Loss: 0.4382 | Val Loss: 0.3228 | MAE: 0.2261 | RMSE: 0.3228 | RÂ²: 0.8885\n",
      "Epoch 13/20 | Train Loss: 0.4210 | Val Loss: 0.3339 | MAE: 0.2266 | RMSE: 0.3339 | RÂ²: 0.8806\n",
      "Epoch 14/20 | Train Loss: 0.4212 | Val Loss: 0.3264 | MAE: 0.2224 | RMSE: 0.3264 | RÂ²: 0.8859\n",
      "Epoch 15/20 | Train Loss: 0.4173 | Val Loss: 0.3150 | MAE: 0.2193 | RMSE: 0.3150 | RÂ²: 0.8938\n",
      "Epoch 16/20 | Train Loss: 0.4109 | Val Loss: 0.3066 | MAE: 0.2148 | RMSE: 0.3066 | RÂ²: 0.8993\n",
      "Epoch 17/20 | Train Loss: 0.4015 | Val Loss: 0.3086 | MAE: 0.2152 | RMSE: 0.3086 | RÂ²: 0.8980\n",
      "Epoch 18/20 | Train Loss: 0.3959 | Val Loss: 0.3026 | MAE: 0.2129 | RMSE: 0.3026 | RÂ²: 0.9020\n",
      "Epoch 19/20 | Train Loss: 0.3983 | Val Loss: 0.3190 | MAE: 0.2144 | RMSE: 0.3190 | RÂ²: 0.8911\n",
      "Epoch 20/20 | Train Loss: 0.3859 | Val Loss: 0.3197 | MAE: 0.2124 | RMSE: 0.3197 | RÂ²: 0.8906\n",
      "âœ… RMSE = 0.3026 | MAE = 0.2129\n",
      "\n",
      "ðŸ”§ [166/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[2048, 1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9661 | Val Loss: 0.9256 | MAE: 0.7817 | RMSE: 0.9256 | RÂ²: 0.0828\n",
      "Epoch 2/20 | Train Loss: 0.7639 | Val Loss: 0.8400 | MAE: 0.7052 | RMSE: 0.8400 | RÂ²: 0.2446\n",
      "Epoch 3/20 | Train Loss: 0.6699 | Val Loss: 0.6894 | MAE: 0.5732 | RMSE: 0.6894 | RÂ²: 0.4912\n",
      "Epoch 4/20 | Train Loss: 0.5914 | Val Loss: 0.5268 | MAE: 0.4249 | RMSE: 0.5268 | RÂ²: 0.7029\n",
      "Epoch 5/20 | Train Loss: 0.5712 | Val Loss: 0.4456 | MAE: 0.3254 | RMSE: 0.4456 | RÂ²: 0.7875\n",
      "Epoch 6/20 | Train Loss: 0.5364 | Val Loss: 0.3851 | MAE: 0.2629 | RMSE: 0.3851 | RÂ²: 0.8413\n",
      "Epoch 7/20 | Train Loss: 0.5222 | Val Loss: 0.3254 | MAE: 0.2344 | RMSE: 0.3254 | RÂ²: 0.8866\n",
      "Epoch 8/20 | Train Loss: 0.5046 | Val Loss: 0.3263 | MAE: 0.2328 | RMSE: 0.3263 | RÂ²: 0.8860\n",
      "Epoch 9/20 | Train Loss: 0.5047 | Val Loss: 0.3540 | MAE: 0.2410 | RMSE: 0.3540 | RÂ²: 0.8659\n",
      "Epoch 10/20 | Train Loss: 0.4819 | Val Loss: 0.3530 | MAE: 0.2475 | RMSE: 0.3530 | RÂ²: 0.8666\n",
      "Epoch 11/20 | Train Loss: 0.4688 | Val Loss: 0.3523 | MAE: 0.2504 | RMSE: 0.3523 | RÂ²: 0.8671\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3254 | MAE = 0.2344\n",
      "\n",
      "ðŸ”§ [167/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8468 | Val Loss: 0.8313 | MAE: 0.6843 | RMSE: 0.8313 | RÂ²: 0.2601\n",
      "Epoch 2/20 | Train Loss: 0.5692 | Val Loss: 0.5887 | MAE: 0.4819 | RMSE: 0.5887 | RÂ²: 0.6290\n",
      "Epoch 3/20 | Train Loss: 0.4958 | Val Loss: 0.3837 | MAE: 0.3088 | RMSE: 0.3837 | RÂ²: 0.8424\n",
      "Epoch 4/20 | Train Loss: 0.4585 | Val Loss: 0.3897 | MAE: 0.2758 | RMSE: 0.3897 | RÂ²: 0.8374\n",
      "Epoch 5/20 | Train Loss: 0.4190 | Val Loss: 0.3531 | MAE: 0.2466 | RMSE: 0.3531 | RÂ²: 0.8665\n",
      "Epoch 6/20 | Train Loss: 0.3953 | Val Loss: 0.3414 | MAE: 0.2596 | RMSE: 0.3414 | RÂ²: 0.8752\n",
      "Epoch 7/20 | Train Loss: 0.3753 | Val Loss: 0.3203 | MAE: 0.2437 | RMSE: 0.3203 | RÂ²: 0.8902\n",
      "Epoch 8/20 | Train Loss: 0.3607 | Val Loss: 0.3152 | MAE: 0.2400 | RMSE: 0.3152 | RÂ²: 0.8937\n",
      "Epoch 9/20 | Train Loss: 0.3504 | Val Loss: 0.3094 | MAE: 0.2227 | RMSE: 0.3094 | RÂ²: 0.8975\n",
      "Epoch 10/20 | Train Loss: 0.3360 | Val Loss: 0.3017 | MAE: 0.2268 | RMSE: 0.3017 | RÂ²: 0.9025\n",
      "Epoch 11/20 | Train Loss: 0.3275 | Val Loss: 0.2829 | MAE: 0.2036 | RMSE: 0.2829 | RÂ²: 0.9143\n",
      "Epoch 12/20 | Train Loss: 0.3215 | Val Loss: 0.2869 | MAE: 0.2034 | RMSE: 0.2869 | RÂ²: 0.9119\n",
      "Epoch 13/20 | Train Loss: 0.3137 | Val Loss: 0.2788 | MAE: 0.1913 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "Epoch 14/20 | Train Loss: 0.3082 | Val Loss: 0.2743 | MAE: 0.1925 | RMSE: 0.2743 | RÂ²: 0.9195\n",
      "Epoch 15/20 | Train Loss: 0.3010 | Val Loss: 0.2678 | MAE: 0.1823 | RMSE: 0.2678 | RÂ²: 0.9233\n",
      "Epoch 16/20 | Train Loss: 0.2988 | Val Loss: 0.2904 | MAE: 0.2017 | RMSE: 0.2904 | RÂ²: 0.9097\n",
      "Epoch 17/20 | Train Loss: 0.2952 | Val Loss: 0.2665 | MAE: 0.1816 | RMSE: 0.2665 | RÂ²: 0.9240\n",
      "Epoch 18/20 | Train Loss: 0.2914 | Val Loss: 0.2943 | MAE: 0.2054 | RMSE: 0.2943 | RÂ²: 0.9073\n",
      "Epoch 19/20 | Train Loss: 0.2882 | Val Loss: 0.2679 | MAE: 0.1817 | RMSE: 0.2679 | RÂ²: 0.9231\n",
      "Epoch 20/20 | Train Loss: 0.2861 | Val Loss: 0.2915 | MAE: 0.2032 | RMSE: 0.2915 | RÂ²: 0.9090\n",
      "âœ… RMSE = 0.2665 | MAE = 0.1816\n",
      "\n",
      "ðŸ”§ [168/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[512, 512, 512], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9696 | Val Loss: 0.8298 | MAE: 0.7020 | RMSE: 0.8298 | RÂ²: 0.2628\n",
      "Epoch 2/20 | Train Loss: 0.4584 | Val Loss: 0.6628 | MAE: 0.5529 | RMSE: 0.6628 | RÂ²: 0.5297\n",
      "Epoch 3/20 | Train Loss: 0.3641 | Val Loss: 0.4182 | MAE: 0.3345 | RMSE: 0.4182 | RÂ²: 0.8128\n",
      "Epoch 4/20 | Train Loss: 0.3153 | Val Loss: 0.3066 | MAE: 0.2335 | RMSE: 0.3066 | RÂ²: 0.8994\n",
      "Epoch 5/20 | Train Loss: 0.2902 | Val Loss: 0.2699 | MAE: 0.1866 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 6/20 | Train Loss: 0.2756 | Val Loss: 0.2646 | MAE: 0.1862 | RMSE: 0.2646 | RÂ²: 0.9250\n",
      "Epoch 7/20 | Train Loss: 0.2663 | Val Loss: 0.3014 | MAE: 0.2040 | RMSE: 0.3014 | RÂ²: 0.9028\n",
      "Epoch 8/20 | Train Loss: 0.2601 | Val Loss: 0.2712 | MAE: 0.1874 | RMSE: 0.2712 | RÂ²: 0.9212\n",
      "Epoch 9/20 | Train Loss: 0.2536 | Val Loss: 0.2582 | MAE: 0.1737 | RMSE: 0.2582 | RÂ²: 0.9286\n",
      "Epoch 10/20 | Train Loss: 0.2464 | Val Loss: 0.2704 | MAE: 0.1800 | RMSE: 0.2704 | RÂ²: 0.9217\n",
      "Epoch 11/20 | Train Loss: 0.2457 | Val Loss: 0.2743 | MAE: 0.1900 | RMSE: 0.2743 | RÂ²: 0.9195\n",
      "Epoch 12/20 | Train Loss: 0.2389 | Val Loss: 0.2661 | MAE: 0.1813 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 13/20 | Train Loss: 0.2360 | Val Loss: 0.2663 | MAE: 0.1844 | RMSE: 0.2663 | RÂ²: 0.9241\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2582 | MAE = 0.1737\n",
      "\n",
      "ðŸ”§ [169/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[2048, 1024, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.9768 | Val Loss: 0.6656 | MAE: 0.5422 | RMSE: 0.6656 | RÂ²: 0.5257\n",
      "Epoch 2/20 | Train Loss: 0.4027 | Val Loss: 0.3774 | MAE: 0.2905 | RMSE: 0.3774 | RÂ²: 0.8475\n",
      "Epoch 3/20 | Train Loss: 0.3538 | Val Loss: 0.5810 | MAE: 0.4567 | RMSE: 0.5810 | RÂ²: 0.6386\n",
      "Epoch 4/20 | Train Loss: 0.3298 | Val Loss: 0.3672 | MAE: 0.2870 | RMSE: 0.3672 | RÂ²: 0.8556\n",
      "Epoch 5/20 | Train Loss: 0.3098 | Val Loss: 0.3994 | MAE: 0.3152 | RMSE: 0.3994 | RÂ²: 0.8292\n",
      "Epoch 6/20 | Train Loss: 0.2926 | Val Loss: 0.3201 | MAE: 0.2339 | RMSE: 0.3201 | RÂ²: 0.8903\n",
      "Epoch 7/20 | Train Loss: 0.2876 | Val Loss: 0.2697 | MAE: 0.1954 | RMSE: 0.2697 | RÂ²: 0.9221\n",
      "Epoch 8/20 | Train Loss: 0.2784 | Val Loss: 0.2642 | MAE: 0.1801 | RMSE: 0.2642 | RÂ²: 0.9253\n",
      "Epoch 9/20 | Train Loss: 0.2728 | Val Loss: 0.2814 | MAE: 0.1891 | RMSE: 0.2814 | RÂ²: 0.9152\n",
      "Epoch 10/20 | Train Loss: 0.2723 | Val Loss: 0.2565 | MAE: 0.1754 | RMSE: 0.2565 | RÂ²: 0.9295\n",
      "Epoch 11/20 | Train Loss: 0.2651 | Val Loss: 0.2567 | MAE: 0.1711 | RMSE: 0.2567 | RÂ²: 0.9294\n",
      "Epoch 12/20 | Train Loss: 0.2604 | Val Loss: 0.2641 | MAE: 0.1779 | RMSE: 0.2641 | RÂ²: 0.9253\n",
      "Epoch 13/20 | Train Loss: 0.2559 | Val Loss: 0.2871 | MAE: 0.1917 | RMSE: 0.2871 | RÂ²: 0.9117\n",
      "Epoch 14/20 | Train Loss: 0.2557 | Val Loss: 0.2634 | MAE: 0.1815 | RMSE: 0.2634 | RÂ²: 0.9257\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2565 | MAE = 0.1754\n",
      "\n",
      "ðŸ”§ [170/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[1024, 512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.6679 | Val Loss: 0.8944 | MAE: 0.7549 | RMSE: 0.8944 | RÂ²: 0.1436\n",
      "Epoch 2/20 | Train Loss: 0.3876 | Val Loss: 0.7101 | MAE: 0.6011 | RMSE: 0.7101 | RÂ²: 0.4602\n",
      "Epoch 3/20 | Train Loss: 0.3281 | Val Loss: 0.4589 | MAE: 0.3842 | RMSE: 0.4589 | RÂ²: 0.7746\n",
      "Epoch 4/20 | Train Loss: 0.3071 | Val Loss: 0.3158 | MAE: 0.2522 | RMSE: 0.3158 | RÂ²: 0.8932\n",
      "Epoch 5/20 | Train Loss: 0.2847 | Val Loss: 0.2720 | MAE: 0.1943 | RMSE: 0.2720 | RÂ²: 0.9208\n",
      "Epoch 6/20 | Train Loss: 0.2712 | Val Loss: 0.2576 | MAE: 0.1715 | RMSE: 0.2576 | RÂ²: 0.9289\n",
      "Epoch 7/20 | Train Loss: 0.2646 | Val Loss: 0.2407 | MAE: 0.1615 | RMSE: 0.2407 | RÂ²: 0.9380\n",
      "Epoch 8/20 | Train Loss: 0.2598 | Val Loss: 0.2527 | MAE: 0.1719 | RMSE: 0.2527 | RÂ²: 0.9316\n",
      "Epoch 9/20 | Train Loss: 0.2563 | Val Loss: 0.2498 | MAE: 0.1688 | RMSE: 0.2498 | RÂ²: 0.9332\n",
      "Epoch 10/20 | Train Loss: 0.2482 | Val Loss: 0.2676 | MAE: 0.1827 | RMSE: 0.2676 | RÂ²: 0.9233\n",
      "Epoch 11/20 | Train Loss: 0.2451 | Val Loss: 0.2708 | MAE: 0.1830 | RMSE: 0.2708 | RÂ²: 0.9215\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2407 | MAE = 0.1615\n",
      "ðŸ’¾ Modelo guardado (mejor hasta ahora)\n",
      "\n",
      "ðŸ”§ [171/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[1024, 512, 256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.1229 | Val Loss: 0.9884 | MAE: 0.8330 | RMSE: 0.9884 | RÂ²: -0.0458\n",
      "Epoch 2/20 | Train Loss: 0.9970 | Val Loss: 0.9318 | MAE: 0.7887 | RMSE: 0.9318 | RÂ²: 0.0704\n",
      "Epoch 3/20 | Train Loss: 0.9315 | Val Loss: 0.8511 | MAE: 0.7226 | RMSE: 0.8511 | RÂ²: 0.2245\n",
      "Epoch 4/20 | Train Loss: 0.8970 | Val Loss: 0.7612 | MAE: 0.6473 | RMSE: 0.7612 | RÂ²: 0.3798\n",
      "Epoch 5/20 | Train Loss: 0.8482 | Val Loss: 0.6699 | MAE: 0.5681 | RMSE: 0.6699 | RÂ²: 0.5195\n",
      "Epoch 6/20 | Train Loss: 0.8131 | Val Loss: 0.5878 | MAE: 0.4929 | RMSE: 0.5878 | RÂ²: 0.6301\n",
      "Epoch 7/20 | Train Loss: 0.7748 | Val Loss: 0.5179 | MAE: 0.4283 | RMSE: 0.5179 | RÂ²: 0.7128\n",
      "Epoch 8/20 | Train Loss: 0.7402 | Val Loss: 0.4625 | MAE: 0.3759 | RMSE: 0.4625 | RÂ²: 0.7710\n",
      "Epoch 9/20 | Train Loss: 0.7109 | Val Loss: 0.4216 | MAE: 0.3334 | RMSE: 0.4216 | RÂ²: 0.8097\n",
      "Epoch 10/20 | Train Loss: 0.6839 | Val Loss: 0.3949 | MAE: 0.3024 | RMSE: 0.3949 | RÂ²: 0.8331\n",
      "Epoch 11/20 | Train Loss: 0.6606 | Val Loss: 0.3799 | MAE: 0.2832 | RMSE: 0.3799 | RÂ²: 0.8455\n",
      "Epoch 12/20 | Train Loss: 0.6386 | Val Loss: 0.3728 | MAE: 0.2733 | RMSE: 0.3728 | RÂ²: 0.8512\n",
      "Epoch 13/20 | Train Loss: 0.6211 | Val Loss: 0.3764 | MAE: 0.2715 | RMSE: 0.3764 | RÂ²: 0.8483\n",
      "Epoch 14/20 | Train Loss: 0.6100 | Val Loss: 0.3841 | MAE: 0.2731 | RMSE: 0.3841 | RÂ²: 0.8421\n",
      "Epoch 15/20 | Train Loss: 0.5926 | Val Loss: 0.3881 | MAE: 0.2735 | RMSE: 0.3881 | RÂ²: 0.8387\n",
      "Epoch 16/20 | Train Loss: 0.5774 | Val Loss: 0.3854 | MAE: 0.2729 | RMSE: 0.3854 | RÂ²: 0.8410\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3728 | MAE = 0.2733\n",
      "\n",
      "ðŸ”§ [172/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[1024, 256, 1024], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8139 | Val Loss: 0.9052 | MAE: 0.7636 | RMSE: 0.9052 | RÂ²: 0.1229\n",
      "Epoch 2/20 | Train Loss: 0.5460 | Val Loss: 0.7555 | MAE: 0.6353 | RMSE: 0.7555 | RÂ²: 0.3890\n",
      "Epoch 3/20 | Train Loss: 0.4712 | Val Loss: 0.6131 | MAE: 0.5090 | RMSE: 0.6131 | RÂ²: 0.5975\n",
      "Epoch 4/20 | Train Loss: 0.4294 | Val Loss: 0.4727 | MAE: 0.3871 | RMSE: 0.4727 | RÂ²: 0.7608\n",
      "Epoch 5/20 | Train Loss: 0.4048 | Val Loss: 0.3956 | MAE: 0.3058 | RMSE: 0.3956 | RÂ²: 0.8324\n",
      "Epoch 6/20 | Train Loss: 0.3810 | Val Loss: 0.2961 | MAE: 0.2253 | RMSE: 0.2961 | RÂ²: 0.9061\n",
      "Epoch 7/20 | Train Loss: 0.3689 | Val Loss: 0.2771 | MAE: 0.1937 | RMSE: 0.2771 | RÂ²: 0.9178\n",
      "Epoch 8/20 | Train Loss: 0.3582 | Val Loss: 0.2767 | MAE: 0.1976 | RMSE: 0.2767 | RÂ²: 0.9180\n",
      "Epoch 9/20 | Train Loss: 0.3456 | Val Loss: 0.2748 | MAE: 0.1933 | RMSE: 0.2748 | RÂ²: 0.9192\n",
      "Epoch 10/20 | Train Loss: 0.3444 | Val Loss: 0.2749 | MAE: 0.1950 | RMSE: 0.2749 | RÂ²: 0.9191\n",
      "Epoch 11/20 | Train Loss: 0.3312 | Val Loss: 0.2740 | MAE: 0.1931 | RMSE: 0.2740 | RÂ²: 0.9196\n",
      "Epoch 12/20 | Train Loss: 0.3312 | Val Loss: 0.2708 | MAE: 0.1905 | RMSE: 0.2708 | RÂ²: 0.9215\n",
      "Epoch 13/20 | Train Loss: 0.3215 | Val Loss: 0.2766 | MAE: 0.1960 | RMSE: 0.2766 | RÂ²: 0.9181\n",
      "Epoch 14/20 | Train Loss: 0.3209 | Val Loss: 0.2749 | MAE: 0.1919 | RMSE: 0.2749 | RÂ²: 0.9191\n",
      "Epoch 15/20 | Train Loss: 0.3160 | Val Loss: 0.2808 | MAE: 0.1949 | RMSE: 0.2808 | RÂ²: 0.9156\n",
      "Epoch 16/20 | Train Loss: 0.3115 | Val Loss: 0.2725 | MAE: 0.1883 | RMSE: 0.2725 | RÂ²: 0.9205\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2708 | MAE = 0.1905\n",
      "\n",
      "ðŸ”§ [173/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.7843 | Val Loss: 0.7334 | MAE: 0.6171 | RMSE: 0.7334 | RÂ²: 0.4242\n",
      "Epoch 2/20 | Train Loss: 0.5059 | Val Loss: 0.4909 | MAE: 0.4056 | RMSE: 0.4909 | RÂ²: 0.7420\n",
      "Epoch 3/20 | Train Loss: 0.4114 | Val Loss: 0.3606 | MAE: 0.2829 | RMSE: 0.3606 | RÂ²: 0.8608\n",
      "Epoch 4/20 | Train Loss: 0.3724 | Val Loss: 0.3296 | MAE: 0.2527 | RMSE: 0.3296 | RÂ²: 0.8837\n",
      "Epoch 5/20 | Train Loss: 0.3454 | Val Loss: 0.2871 | MAE: 0.2129 | RMSE: 0.2871 | RÂ²: 0.9117\n",
      "Epoch 6/20 | Train Loss: 0.3267 | Val Loss: 0.2743 | MAE: 0.2002 | RMSE: 0.2743 | RÂ²: 0.9194\n",
      "Epoch 7/20 | Train Loss: 0.3139 | Val Loss: 0.2665 | MAE: 0.1904 | RMSE: 0.2665 | RÂ²: 0.9240\n",
      "Epoch 8/20 | Train Loss: 0.3002 | Val Loss: 0.2599 | MAE: 0.1882 | RMSE: 0.2599 | RÂ²: 0.9277\n",
      "Epoch 9/20 | Train Loss: 0.2968 | Val Loss: 0.2638 | MAE: 0.1855 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 10/20 | Train Loss: 0.2879 | Val Loss: 0.2630 | MAE: 0.1833 | RMSE: 0.2630 | RÂ²: 0.9259\n",
      "Epoch 11/20 | Train Loss: 0.2788 | Val Loss: 0.2585 | MAE: 0.1813 | RMSE: 0.2585 | RÂ²: 0.9284\n",
      "Epoch 12/20 | Train Loss: 0.2763 | Val Loss: 0.2584 | MAE: 0.1799 | RMSE: 0.2584 | RÂ²: 0.9285\n",
      "Epoch 13/20 | Train Loss: 0.2726 | Val Loss: 0.2723 | MAE: 0.1922 | RMSE: 0.2723 | RÂ²: 0.9206\n",
      "Epoch 14/20 | Train Loss: 0.2641 | Val Loss: 0.2815 | MAE: 0.2031 | RMSE: 0.2815 | RÂ²: 0.9151\n",
      "Epoch 15/20 | Train Loss: 0.2616 | Val Loss: 0.2626 | MAE: 0.1832 | RMSE: 0.2626 | RÂ²: 0.9262\n",
      "Epoch 16/20 | Train Loss: 0.2555 | Val Loss: 0.2675 | MAE: 0.1878 | RMSE: 0.2675 | RÂ²: 0.9234\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2584 | MAE = 0.1799\n",
      "\n",
      "ðŸ”§ [174/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8178 | Val Loss: 0.9154 | MAE: 0.7723 | RMSE: 0.9154 | RÂ²: 0.1029\n",
      "Epoch 2/20 | Train Loss: 0.5316 | Val Loss: 0.8182 | MAE: 0.6820 | RMSE: 0.8182 | RÂ²: 0.2834\n",
      "Epoch 3/20 | Train Loss: 0.4394 | Val Loss: 0.6308 | MAE: 0.5310 | RMSE: 0.6308 | RÂ²: 0.5741\n",
      "Epoch 4/20 | Train Loss: 0.3911 | Val Loss: 0.5015 | MAE: 0.4220 | RMSE: 0.5015 | RÂ²: 0.7307\n",
      "Epoch 5/20 | Train Loss: 0.3590 | Val Loss: 0.4176 | MAE: 0.3434 | RMSE: 0.4176 | RÂ²: 0.8133\n",
      "Epoch 6/20 | Train Loss: 0.3396 | Val Loss: 0.3278 | MAE: 0.2605 | RMSE: 0.3278 | RÂ²: 0.8850\n",
      "Epoch 7/20 | Train Loss: 0.3238 | Val Loss: 0.2876 | MAE: 0.2146 | RMSE: 0.2876 | RÂ²: 0.9115\n",
      "Epoch 8/20 | Train Loss: 0.3155 | Val Loss: 0.2738 | MAE: 0.1954 | RMSE: 0.2738 | RÂ²: 0.9198\n",
      "Epoch 9/20 | Train Loss: 0.3065 | Val Loss: 0.2693 | MAE: 0.1907 | RMSE: 0.2693 | RÂ²: 0.9224\n",
      "Epoch 10/20 | Train Loss: 0.2958 | Val Loss: 0.2722 | MAE: 0.1914 | RMSE: 0.2722 | RÂ²: 0.9207\n",
      "Epoch 11/20 | Train Loss: 0.2911 | Val Loss: 0.2668 | MAE: 0.1880 | RMSE: 0.2668 | RÂ²: 0.9238\n",
      "Epoch 12/20 | Train Loss: 0.2860 | Val Loss: 0.2698 | MAE: 0.1906 | RMSE: 0.2698 | RÂ²: 0.9221\n",
      "Epoch 13/20 | Train Loss: 0.2854 | Val Loss: 0.2655 | MAE: 0.1859 | RMSE: 0.2655 | RÂ²: 0.9245\n",
      "Epoch 14/20 | Train Loss: 0.2790 | Val Loss: 0.2691 | MAE: 0.1889 | RMSE: 0.2691 | RÂ²: 0.9225\n",
      "Epoch 15/20 | Train Loss: 0.2779 | Val Loss: 0.2671 | MAE: 0.1862 | RMSE: 0.2671 | RÂ²: 0.9236\n",
      "Epoch 16/20 | Train Loss: 0.2733 | Val Loss: 0.2702 | MAE: 0.1894 | RMSE: 0.2702 | RÂ²: 0.9219\n",
      "Epoch 17/20 | Train Loss: 0.2701 | Val Loss: 0.2756 | MAE: 0.1894 | RMSE: 0.2756 | RÂ²: 0.9187\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2655 | MAE = 0.1859\n",
      "\n",
      "ðŸ”§ [175/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8720 | Val Loss: 0.7770 | MAE: 0.6586 | RMSE: 0.7770 | RÂ²: 0.3536\n",
      "Epoch 2/20 | Train Loss: 0.5898 | Val Loss: 0.6295 | MAE: 0.5161 | RMSE: 0.6295 | RÂ²: 0.5758\n",
      "Epoch 3/20 | Train Loss: 0.4805 | Val Loss: 0.4187 | MAE: 0.3361 | RMSE: 0.4187 | RÂ²: 0.8124\n",
      "Epoch 4/20 | Train Loss: 0.4330 | Val Loss: 0.3639 | MAE: 0.2840 | RMSE: 0.3639 | RÂ²: 0.8583\n",
      "Epoch 5/20 | Train Loss: 0.4028 | Val Loss: 0.3244 | MAE: 0.2457 | RMSE: 0.3244 | RÂ²: 0.8874\n",
      "Epoch 6/20 | Train Loss: 0.3786 | Val Loss: 0.3009 | MAE: 0.2248 | RMSE: 0.3009 | RÂ²: 0.9031\n",
      "Epoch 7/20 | Train Loss: 0.3641 | Val Loss: 0.2918 | MAE: 0.2144 | RMSE: 0.2918 | RÂ²: 0.9088\n",
      "Epoch 8/20 | Train Loss: 0.3557 | Val Loss: 0.2830 | MAE: 0.2062 | RMSE: 0.2830 | RÂ²: 0.9142\n",
      "Epoch 9/20 | Train Loss: 0.3409 | Val Loss: 0.2752 | MAE: 0.1979 | RMSE: 0.2752 | RÂ²: 0.9189\n",
      "Epoch 10/20 | Train Loss: 0.3290 | Val Loss: 0.2730 | MAE: 0.1949 | RMSE: 0.2730 | RÂ²: 0.9202\n",
      "Epoch 11/20 | Train Loss: 0.3227 | Val Loss: 0.2677 | MAE: 0.1886 | RMSE: 0.2677 | RÂ²: 0.9233\n",
      "Epoch 12/20 | Train Loss: 0.3106 | Val Loss: 0.2671 | MAE: 0.1879 | RMSE: 0.2671 | RÂ²: 0.9236\n",
      "Epoch 13/20 | Train Loss: 0.3062 | Val Loss: 0.2661 | MAE: 0.1861 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 14/20 | Train Loss: 0.3009 | Val Loss: 0.2654 | MAE: 0.1855 | RMSE: 0.2654 | RÂ²: 0.9246\n",
      "Epoch 15/20 | Train Loss: 0.2985 | Val Loss: 0.2683 | MAE: 0.1870 | RMSE: 0.2683 | RÂ²: 0.9230\n",
      "Epoch 16/20 | Train Loss: 0.2911 | Val Loss: 0.2696 | MAE: 0.1880 | RMSE: 0.2696 | RÂ²: 0.9222\n",
      "Epoch 17/20 | Train Loss: 0.2901 | Val Loss: 0.2684 | MAE: 0.1869 | RMSE: 0.2684 | RÂ²: 0.9229\n",
      "Epoch 18/20 | Train Loss: 0.2838 | Val Loss: 0.2718 | MAE: 0.1877 | RMSE: 0.2718 | RÂ²: 0.9209\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2654 | MAE = 0.1855\n",
      "\n",
      "ðŸ”§ [176/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8606 | Val Loss: 0.9758 | MAE: 0.8192 | RMSE: 0.9758 | RÂ²: -0.0193\n",
      "Epoch 2/20 | Train Loss: 0.6036 | Val Loss: 0.8579 | MAE: 0.7183 | RMSE: 0.8579 | RÂ²: 0.2121\n",
      "Epoch 3/20 | Train Loss: 0.5121 | Val Loss: 0.6788 | MAE: 0.5617 | RMSE: 0.6788 | RÂ²: 0.5068\n",
      "Epoch 4/20 | Train Loss: 0.4709 | Val Loss: 0.4683 | MAE: 0.3832 | RMSE: 0.4683 | RÂ²: 0.7652\n",
      "Epoch 5/20 | Train Loss: 0.4411 | Val Loss: 0.3403 | MAE: 0.2553 | RMSE: 0.3403 | RÂ²: 0.8760\n",
      "Epoch 6/20 | Train Loss: 0.4142 | Val Loss: 0.3190 | MAE: 0.2299 | RMSE: 0.3190 | RÂ²: 0.8911\n",
      "Epoch 7/20 | Train Loss: 0.3872 | Val Loss: 0.3241 | MAE: 0.2479 | RMSE: 0.3241 | RÂ²: 0.8876\n",
      "Epoch 8/20 | Train Loss: 0.3799 | Val Loss: 0.3281 | MAE: 0.2531 | RMSE: 0.3281 | RÂ²: 0.8848\n",
      "Epoch 9/20 | Train Loss: 0.3676 | Val Loss: 0.3314 | MAE: 0.2526 | RMSE: 0.3314 | RÂ²: 0.8824\n",
      "Epoch 10/20 | Train Loss: 0.3638 | Val Loss: 0.3160 | MAE: 0.2428 | RMSE: 0.3160 | RÂ²: 0.8931\n",
      "Epoch 11/20 | Train Loss: 0.3503 | Val Loss: 0.2995 | MAE: 0.2259 | RMSE: 0.2995 | RÂ²: 0.9040\n",
      "Epoch 12/20 | Train Loss: 0.3483 | Val Loss: 0.3075 | MAE: 0.2348 | RMSE: 0.3075 | RÂ²: 0.8988\n",
      "Epoch 13/20 | Train Loss: 0.3403 | Val Loss: 0.3135 | MAE: 0.2413 | RMSE: 0.3135 | RÂ²: 0.8948\n",
      "Epoch 14/20 | Train Loss: 0.3353 | Val Loss: 0.2893 | MAE: 0.2131 | RMSE: 0.2893 | RÂ²: 0.9104\n",
      "Epoch 15/20 | Train Loss: 0.3300 | Val Loss: 0.2858 | MAE: 0.2124 | RMSE: 0.2858 | RÂ²: 0.9126\n",
      "Epoch 16/20 | Train Loss: 0.3309 | Val Loss: 0.2984 | MAE: 0.2176 | RMSE: 0.2984 | RÂ²: 0.9046\n",
      "Epoch 17/20 | Train Loss: 0.3217 | Val Loss: 0.2826 | MAE: 0.2066 | RMSE: 0.2826 | RÂ²: 0.9145\n",
      "Epoch 18/20 | Train Loss: 0.3155 | Val Loss: 0.2932 | MAE: 0.2121 | RMSE: 0.2932 | RÂ²: 0.9080\n",
      "Epoch 19/20 | Train Loss: 0.3094 | Val Loss: 0.3089 | MAE: 0.2161 | RMSE: 0.3089 | RÂ²: 0.8978\n",
      "Epoch 20/20 | Train Loss: 0.3072 | Val Loss: 0.2936 | MAE: 0.2103 | RMSE: 0.2936 | RÂ²: 0.9077\n",
      "âœ… RMSE = 0.2826 | MAE = 0.2066\n",
      "\n",
      "ðŸ”§ [177/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8093 | Val Loss: 0.9033 | MAE: 0.7537 | RMSE: 0.9033 | RÂ²: 0.1265\n",
      "Epoch 2/20 | Train Loss: 0.5356 | Val Loss: 0.7369 | MAE: 0.6133 | RMSE: 0.7369 | RÂ²: 0.4186\n",
      "Epoch 3/20 | Train Loss: 0.4705 | Val Loss: 0.5552 | MAE: 0.4607 | RMSE: 0.5552 | RÂ²: 0.6700\n",
      "Epoch 4/20 | Train Loss: 0.4291 | Val Loss: 0.3809 | MAE: 0.3087 | RMSE: 0.3809 | RÂ²: 0.8447\n",
      "Epoch 5/20 | Train Loss: 0.4123 | Val Loss: 0.3450 | MAE: 0.2358 | RMSE: 0.3450 | RÂ²: 0.8726\n",
      "Epoch 6/20 | Train Loss: 0.3885 | Val Loss: 0.2805 | MAE: 0.2021 | RMSE: 0.2805 | RÂ²: 0.9158\n",
      "Epoch 7/20 | Train Loss: 0.3743 | Val Loss: 0.2828 | MAE: 0.2094 | RMSE: 0.2828 | RÂ²: 0.9144\n",
      "Epoch 8/20 | Train Loss: 0.3682 | Val Loss: 0.3142 | MAE: 0.2238 | RMSE: 0.3142 | RÂ²: 0.8943\n",
      "Epoch 9/20 | Train Loss: 0.3491 | Val Loss: 0.3107 | MAE: 0.2241 | RMSE: 0.3107 | RÂ²: 0.8967\n",
      "Epoch 10/20 | Train Loss: 0.3377 | Val Loss: 0.2817 | MAE: 0.2074 | RMSE: 0.2817 | RÂ²: 0.9150\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2805 | MAE = 0.2021\n",
      "\n",
      "ðŸ”§ [178/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[256, 128], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.0419 | Val Loss: 0.9512 | MAE: 0.8062 | RMSE: 0.9512 | RÂ²: 0.0313\n",
      "Epoch 2/20 | Train Loss: 0.8698 | Val Loss: 0.8574 | MAE: 0.7271 | RMSE: 0.8574 | RÂ²: 0.2130\n",
      "Epoch 3/20 | Train Loss: 0.7762 | Val Loss: 0.7622 | MAE: 0.6379 | RMSE: 0.7622 | RÂ²: 0.3781\n",
      "Epoch 4/20 | Train Loss: 0.7114 | Val Loss: 0.6710 | MAE: 0.5544 | RMSE: 0.6710 | RÂ²: 0.5180\n",
      "Epoch 5/20 | Train Loss: 0.6516 | Val Loss: 0.5846 | MAE: 0.4813 | RMSE: 0.5846 | RÂ²: 0.6341\n",
      "Epoch 6/20 | Train Loss: 0.6012 | Val Loss: 0.5145 | MAE: 0.4218 | RMSE: 0.5145 | RÂ²: 0.7166\n",
      "Epoch 7/20 | Train Loss: 0.5599 | Val Loss: 0.4612 | MAE: 0.3708 | RMSE: 0.4612 | RÂ²: 0.7723\n",
      "Epoch 8/20 | Train Loss: 0.5236 | Val Loss: 0.4145 | MAE: 0.3271 | RMSE: 0.4145 | RÂ²: 0.8161\n",
      "Epoch 9/20 | Train Loss: 0.5066 | Val Loss: 0.3838 | MAE: 0.2980 | RMSE: 0.3838 | RÂ²: 0.8423\n",
      "Epoch 10/20 | Train Loss: 0.4795 | Val Loss: 0.3617 | MAE: 0.2770 | RMSE: 0.3617 | RÂ²: 0.8599\n",
      "Epoch 11/20 | Train Loss: 0.4614 | Val Loss: 0.3521 | MAE: 0.2663 | RMSE: 0.3521 | RÂ²: 0.8673\n",
      "Epoch 12/20 | Train Loss: 0.4499 | Val Loss: 0.3400 | MAE: 0.2560 | RMSE: 0.3400 | RÂ²: 0.8763\n",
      "Epoch 13/20 | Train Loss: 0.4359 | Val Loss: 0.3311 | MAE: 0.2466 | RMSE: 0.3311 | RÂ²: 0.8826\n",
      "Epoch 14/20 | Train Loss: 0.4284 | Val Loss: 0.3273 | MAE: 0.2416 | RMSE: 0.3273 | RÂ²: 0.8853\n",
      "Epoch 15/20 | Train Loss: 0.4137 | Val Loss: 0.3220 | MAE: 0.2371 | RMSE: 0.3220 | RÂ²: 0.8890\n",
      "Epoch 16/20 | Train Loss: 0.4082 | Val Loss: 0.3167 | MAE: 0.2339 | RMSE: 0.3167 | RÂ²: 0.8926\n",
      "Epoch 17/20 | Train Loss: 0.4033 | Val Loss: 0.3118 | MAE: 0.2305 | RMSE: 0.3118 | RÂ²: 0.8959\n",
      "Epoch 18/20 | Train Loss: 0.3926 | Val Loss: 0.3057 | MAE: 0.2252 | RMSE: 0.3057 | RÂ²: 0.8999\n",
      "Epoch 19/20 | Train Loss: 0.3906 | Val Loss: 0.3016 | MAE: 0.2215 | RMSE: 0.3016 | RÂ²: 0.9026\n",
      "Epoch 20/20 | Train Loss: 0.3793 | Val Loss: 0.3004 | MAE: 0.2206 | RMSE: 0.3004 | RÂ²: 0.9034\n",
      "âœ… RMSE = 0.3004 | MAE = 0.2206\n",
      "\n",
      "ðŸ”§ [179/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[1024, 256, 1024], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0099 | Val Loss: 0.9253 | MAE: 0.7824 | RMSE: 0.9253 | RÂ²: 0.0835\n",
      "Epoch 2/20 | Train Loss: 0.8302 | Val Loss: 0.8033 | MAE: 0.6813 | RMSE: 0.8033 | RÂ²: 0.3092\n",
      "Epoch 3/20 | Train Loss: 0.7232 | Val Loss: 0.6820 | MAE: 0.5763 | RMSE: 0.6820 | RÂ²: 0.5020\n",
      "Epoch 4/20 | Train Loss: 0.6640 | Val Loss: 0.5560 | MAE: 0.4608 | RMSE: 0.5560 | RÂ²: 0.6690\n",
      "Epoch 5/20 | Train Loss: 0.6126 | Val Loss: 0.4482 | MAE: 0.3569 | RMSE: 0.4482 | RÂ²: 0.7849\n",
      "Epoch 6/20 | Train Loss: 0.5827 | Val Loss: 0.4055 | MAE: 0.2987 | RMSE: 0.4055 | RÂ²: 0.8240\n",
      "Epoch 7/20 | Train Loss: 0.5715 | Val Loss: 0.3666 | MAE: 0.2601 | RMSE: 0.3666 | RÂ²: 0.8561\n",
      "Epoch 8/20 | Train Loss: 0.5522 | Val Loss: 0.3434 | MAE: 0.2444 | RMSE: 0.3434 | RÂ²: 0.8737\n",
      "Epoch 9/20 | Train Loss: 0.5454 | Val Loss: 0.3378 | MAE: 0.2376 | RMSE: 0.3378 | RÂ²: 0.8778\n",
      "Epoch 10/20 | Train Loss: 0.5205 | Val Loss: 0.3171 | MAE: 0.2287 | RMSE: 0.3171 | RÂ²: 0.8924\n",
      "Epoch 11/20 | Train Loss: 0.5074 | Val Loss: 0.3178 | MAE: 0.2280 | RMSE: 0.3178 | RÂ²: 0.8919\n",
      "Epoch 12/20 | Train Loss: 0.5016 | Val Loss: 0.3247 | MAE: 0.2273 | RMSE: 0.3247 | RÂ²: 0.8871\n",
      "Epoch 13/20 | Train Loss: 0.4890 | Val Loss: 0.3171 | MAE: 0.2251 | RMSE: 0.3171 | RÂ²: 0.8924\n",
      "Epoch 14/20 | Train Loss: 0.4846 | Val Loss: 0.3151 | MAE: 0.2257 | RMSE: 0.3151 | RÂ²: 0.8937\n",
      "Epoch 15/20 | Train Loss: 0.4731 | Val Loss: 0.3138 | MAE: 0.2235 | RMSE: 0.3138 | RÂ²: 0.8946\n",
      "Epoch 16/20 | Train Loss: 0.4809 | Val Loss: 0.3215 | MAE: 0.2225 | RMSE: 0.3215 | RÂ²: 0.8894\n",
      "Epoch 17/20 | Train Loss: 0.4568 | Val Loss: 0.3179 | MAE: 0.2210 | RMSE: 0.3179 | RÂ²: 0.8918\n",
      "Epoch 18/20 | Train Loss: 0.4580 | Val Loss: 0.3241 | MAE: 0.2216 | RMSE: 0.3241 | RÂ²: 0.8876\n",
      "Epoch 19/20 | Train Loss: 0.4483 | Val Loss: 0.3248 | MAE: 0.2239 | RMSE: 0.3248 | RÂ²: 0.8871\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3138 | MAE = 0.2235\n",
      "\n",
      "ðŸ”§ [180/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[512, 512, 512], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.7787 | Val Loss: 0.8838 | MAE: 0.7376 | RMSE: 0.8838 | RÂ²: 0.1637\n",
      "Epoch 2/20 | Train Loss: 0.4556 | Val Loss: 0.7239 | MAE: 0.6085 | RMSE: 0.7239 | RÂ²: 0.4391\n",
      "Epoch 3/20 | Train Loss: 0.3797 | Val Loss: 0.5923 | MAE: 0.4914 | RMSE: 0.5923 | RÂ²: 0.6244\n",
      "Epoch 4/20 | Train Loss: 0.3361 | Val Loss: 0.3713 | MAE: 0.3016 | RMSE: 0.3713 | RÂ²: 0.8524\n",
      "Epoch 5/20 | Train Loss: 0.3119 | Val Loss: 0.3569 | MAE: 0.2796 | RMSE: 0.3569 | RÂ²: 0.8637\n",
      "Epoch 6/20 | Train Loss: 0.2942 | Val Loss: 0.2573 | MAE: 0.1834 | RMSE: 0.2573 | RÂ²: 0.9291\n",
      "Epoch 7/20 | Train Loss: 0.2850 | Val Loss: 0.2534 | MAE: 0.1771 | RMSE: 0.2534 | RÂ²: 0.9313\n",
      "Epoch 8/20 | Train Loss: 0.2759 | Val Loss: 0.2961 | MAE: 0.2021 | RMSE: 0.2961 | RÂ²: 0.9061\n",
      "Epoch 9/20 | Train Loss: 0.2687 | Val Loss: 0.2766 | MAE: 0.1918 | RMSE: 0.2766 | RÂ²: 0.9181\n",
      "Epoch 10/20 | Train Loss: 0.2619 | Val Loss: 0.2583 | MAE: 0.1802 | RMSE: 0.2583 | RÂ²: 0.9286\n",
      "Epoch 11/20 | Train Loss: 0.2549 | Val Loss: 0.2578 | MAE: 0.1808 | RMSE: 0.2578 | RÂ²: 0.9289\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2534 | MAE = 0.1771\n",
      "\n",
      "ðŸ”§ [181/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[1024, 256, 1024], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0012 | Val Loss: 0.7325 | MAE: 0.6143 | RMSE: 0.7325 | RÂ²: 0.4257\n",
      "Epoch 2/20 | Train Loss: 0.6584 | Val Loss: 0.6156 | MAE: 0.4645 | RMSE: 0.6156 | RÂ²: 0.5943\n",
      "Epoch 3/20 | Train Loss: 0.5883 | Val Loss: 0.3311 | MAE: 0.2503 | RMSE: 0.3311 | RÂ²: 0.8826\n",
      "Epoch 4/20 | Train Loss: 0.5217 | Val Loss: 0.3564 | MAE: 0.2628 | RMSE: 0.3564 | RÂ²: 0.8640\n",
      "Epoch 5/20 | Train Loss: 0.4901 | Val Loss: 0.3137 | MAE: 0.2269 | RMSE: 0.3137 | RÂ²: 0.8946\n",
      "Epoch 6/20 | Train Loss: 0.4637 | Val Loss: 0.3203 | MAE: 0.2396 | RMSE: 0.3203 | RÂ²: 0.8902\n",
      "Epoch 7/20 | Train Loss: 0.4356 | Val Loss: 0.3299 | MAE: 0.2351 | RMSE: 0.3299 | RÂ²: 0.8835\n",
      "Epoch 8/20 | Train Loss: 0.4186 | Val Loss: 0.3152 | MAE: 0.2320 | RMSE: 0.3152 | RÂ²: 0.8937\n",
      "Epoch 9/20 | Train Loss: 0.3885 | Val Loss: 0.3182 | MAE: 0.2198 | RMSE: 0.3182 | RÂ²: 0.8916\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3137 | MAE = 0.2269\n",
      "\n",
      "ðŸ”§ [182/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[512, 512, 512], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9763 | Val Loss: 0.8343 | MAE: 0.7042 | RMSE: 0.8343 | RÂ²: 0.2549\n",
      "Epoch 2/20 | Train Loss: 0.7432 | Val Loss: 0.6569 | MAE: 0.5462 | RMSE: 0.6569 | RÂ²: 0.5380\n",
      "Epoch 3/20 | Train Loss: 0.6540 | Val Loss: 0.5514 | MAE: 0.4210 | RMSE: 0.5514 | RÂ²: 0.6745\n",
      "Epoch 4/20 | Train Loss: 0.5913 | Val Loss: 0.3644 | MAE: 0.2733 | RMSE: 0.3644 | RÂ²: 0.8578\n",
      "Epoch 5/20 | Train Loss: 0.5460 | Val Loss: 0.3699 | MAE: 0.2765 | RMSE: 0.3699 | RÂ²: 0.8535\n",
      "Epoch 6/20 | Train Loss: 0.5110 | Val Loss: 0.3727 | MAE: 0.2724 | RMSE: 0.3727 | RÂ²: 0.8513\n",
      "Epoch 7/20 | Train Loss: 0.4799 | Val Loss: 0.4125 | MAE: 0.3086 | RMSE: 0.4125 | RÂ²: 0.8178\n",
      "Epoch 8/20 | Train Loss: 0.4542 | Val Loss: 0.4181 | MAE: 0.3049 | RMSE: 0.4181 | RÂ²: 0.8129\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3644 | MAE = 0.2733\n",
      "\n",
      "ðŸ”§ [183/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9100 | Val Loss: 0.6939 | MAE: 0.5838 | RMSE: 0.6939 | RÂ²: 0.4846\n",
      "Epoch 2/20 | Train Loss: 0.6597 | Val Loss: 0.5383 | MAE: 0.4331 | RMSE: 0.5383 | RÂ²: 0.6898\n",
      "Epoch 3/20 | Train Loss: 0.5715 | Val Loss: 0.3816 | MAE: 0.2967 | RMSE: 0.3816 | RÂ²: 0.8441\n",
      "Epoch 4/20 | Train Loss: 0.5079 | Val Loss: 0.3850 | MAE: 0.2905 | RMSE: 0.3850 | RÂ²: 0.8413\n",
      "Epoch 5/20 | Train Loss: 0.4669 | Val Loss: 0.3685 | MAE: 0.2773 | RMSE: 0.3685 | RÂ²: 0.8547\n",
      "Epoch 6/20 | Train Loss: 0.4356 | Val Loss: 0.3492 | MAE: 0.2622 | RMSE: 0.3492 | RÂ²: 0.8695\n",
      "Epoch 7/20 | Train Loss: 0.4222 | Val Loss: 0.3373 | MAE: 0.2480 | RMSE: 0.3373 | RÂ²: 0.8782\n",
      "Epoch 8/20 | Train Loss: 0.4007 | Val Loss: 0.3113 | MAE: 0.2246 | RMSE: 0.3113 | RÂ²: 0.8962\n",
      "Epoch 9/20 | Train Loss: 0.3894 | Val Loss: 0.2925 | MAE: 0.2098 | RMSE: 0.2925 | RÂ²: 0.9084\n",
      "Epoch 10/20 | Train Loss: 0.3783 | Val Loss: 0.2849 | MAE: 0.2037 | RMSE: 0.2849 | RÂ²: 0.9131\n",
      "Epoch 11/20 | Train Loss: 0.3667 | Val Loss: 0.2859 | MAE: 0.2036 | RMSE: 0.2859 | RÂ²: 0.9125\n",
      "Epoch 12/20 | Train Loss: 0.3620 | Val Loss: 0.2868 | MAE: 0.2022 | RMSE: 0.2868 | RÂ²: 0.9120\n",
      "Epoch 13/20 | Train Loss: 0.3524 | Val Loss: 0.2867 | MAE: 0.2038 | RMSE: 0.2867 | RÂ²: 0.9120\n",
      "Epoch 14/20 | Train Loss: 0.3476 | Val Loss: 0.2884 | MAE: 0.2059 | RMSE: 0.2884 | RÂ²: 0.9109\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2849 | MAE = 0.2037\n",
      "\n",
      "ðŸ”§ [184/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[256, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.0516 | Val Loss: 0.9731 | MAE: 0.8209 | RMSE: 0.9731 | RÂ²: -0.0138\n",
      "Epoch 2/20 | Train Loss: 0.7970 | Val Loss: 0.8539 | MAE: 0.7205 | RMSE: 0.8539 | RÂ²: 0.2193\n",
      "Epoch 3/20 | Train Loss: 0.7058 | Val Loss: 0.7481 | MAE: 0.6197 | RMSE: 0.7481 | RÂ²: 0.4009\n",
      "Epoch 4/20 | Train Loss: 0.6321 | Val Loss: 0.6436 | MAE: 0.5275 | RMSE: 0.6436 | RÂ²: 0.5565\n",
      "Epoch 5/20 | Train Loss: 0.5593 | Val Loss: 0.5646 | MAE: 0.4594 | RMSE: 0.5646 | RÂ²: 0.6588\n",
      "Epoch 6/20 | Train Loss: 0.5110 | Val Loss: 0.5020 | MAE: 0.3941 | RMSE: 0.5020 | RÂ²: 0.7302\n",
      "Epoch 7/20 | Train Loss: 0.4798 | Val Loss: 0.4332 | MAE: 0.3339 | RMSE: 0.4332 | RÂ²: 0.7991\n",
      "Epoch 8/20 | Train Loss: 0.4452 | Val Loss: 0.3991 | MAE: 0.3065 | RMSE: 0.3991 | RÂ²: 0.8295\n",
      "Epoch 9/20 | Train Loss: 0.4241 | Val Loss: 0.3798 | MAE: 0.2899 | RMSE: 0.3798 | RÂ²: 0.8456\n",
      "Epoch 10/20 | Train Loss: 0.4071 | Val Loss: 0.3685 | MAE: 0.2808 | RMSE: 0.3685 | RÂ²: 0.8546\n",
      "Epoch 11/20 | Train Loss: 0.3936 | Val Loss: 0.3631 | MAE: 0.2788 | RMSE: 0.3631 | RÂ²: 0.8589\n",
      "Epoch 12/20 | Train Loss: 0.3830 | Val Loss: 0.3502 | MAE: 0.2669 | RMSE: 0.3502 | RÂ²: 0.8687\n",
      "Epoch 13/20 | Train Loss: 0.3743 | Val Loss: 0.3384 | MAE: 0.2558 | RMSE: 0.3384 | RÂ²: 0.8774\n",
      "Epoch 14/20 | Train Loss: 0.3572 | Val Loss: 0.3368 | MAE: 0.2538 | RMSE: 0.3368 | RÂ²: 0.8786\n",
      "Epoch 15/20 | Train Loss: 0.3541 | Val Loss: 0.3353 | MAE: 0.2531 | RMSE: 0.3353 | RÂ²: 0.8796\n",
      "Epoch 16/20 | Train Loss: 0.3494 | Val Loss: 0.3281 | MAE: 0.2473 | RMSE: 0.3281 | RÂ²: 0.8847\n",
      "Epoch 17/20 | Train Loss: 0.3400 | Val Loss: 0.3237 | MAE: 0.2430 | RMSE: 0.3237 | RÂ²: 0.8878\n",
      "Epoch 18/20 | Train Loss: 0.3355 | Val Loss: 0.3191 | MAE: 0.2391 | RMSE: 0.3191 | RÂ²: 0.8910\n",
      "Epoch 19/20 | Train Loss: 0.3297 | Val Loss: 0.3136 | MAE: 0.2330 | RMSE: 0.3136 | RÂ²: 0.8947\n",
      "Epoch 20/20 | Train Loss: 0.3267 | Val Loss: 0.3086 | MAE: 0.2275 | RMSE: 0.3086 | RÂ²: 0.8981\n",
      "âœ… RMSE = 0.3086 | MAE = 0.2275\n",
      "\n",
      "ðŸ”§ [185/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[512, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 1.0283 | Val Loss: 0.9346 | MAE: 0.7900 | RMSE: 0.9346 | RÂ²: 0.0649\n",
      "Epoch 2/20 | Train Loss: 0.9084 | Val Loss: 0.8353 | MAE: 0.7093 | RMSE: 0.8353 | RÂ²: 0.2530\n",
      "Epoch 3/20 | Train Loss: 0.8512 | Val Loss: 0.7375 | MAE: 0.6266 | RMSE: 0.7375 | RÂ²: 0.4178\n",
      "Epoch 4/20 | Train Loss: 0.7910 | Val Loss: 0.6471 | MAE: 0.5478 | RMSE: 0.6471 | RÂ²: 0.5517\n",
      "Epoch 5/20 | Train Loss: 0.7698 | Val Loss: 0.5701 | MAE: 0.4734 | RMSE: 0.5701 | RÂ²: 0.6520\n",
      "Epoch 6/20 | Train Loss: 0.7314 | Val Loss: 0.5024 | MAE: 0.4050 | RMSE: 0.5024 | RÂ²: 0.7298\n",
      "Epoch 7/20 | Train Loss: 0.6912 | Val Loss: 0.4500 | MAE: 0.3515 | RMSE: 0.4500 | RÂ²: 0.7832\n",
      "Epoch 8/20 | Train Loss: 0.6726 | Val Loss: 0.4251 | MAE: 0.3198 | RMSE: 0.4251 | RÂ²: 0.8065\n",
      "Epoch 9/20 | Train Loss: 0.6526 | Val Loss: 0.4223 | MAE: 0.3035 | RMSE: 0.4223 | RÂ²: 0.8091\n",
      "Epoch 10/20 | Train Loss: 0.6336 | Val Loss: 0.4105 | MAE: 0.2873 | RMSE: 0.4105 | RÂ²: 0.8196\n",
      "Epoch 11/20 | Train Loss: 0.6166 | Val Loss: 0.3978 | MAE: 0.2775 | RMSE: 0.3978 | RÂ²: 0.8306\n",
      "Epoch 12/20 | Train Loss: 0.6037 | Val Loss: 0.3987 | MAE: 0.2737 | RMSE: 0.3987 | RÂ²: 0.8298\n",
      "Epoch 13/20 | Train Loss: 0.5914 | Val Loss: 0.3951 | MAE: 0.2717 | RMSE: 0.3951 | RÂ²: 0.8329\n",
      "Epoch 14/20 | Train Loss: 0.5767 | Val Loss: 0.3854 | MAE: 0.2715 | RMSE: 0.3854 | RÂ²: 0.8410\n",
      "Epoch 15/20 | Train Loss: 0.5598 | Val Loss: 0.3838 | MAE: 0.2728 | RMSE: 0.3838 | RÂ²: 0.8423\n",
      "Epoch 16/20 | Train Loss: 0.5528 | Val Loss: 0.3910 | MAE: 0.2750 | RMSE: 0.3910 | RÂ²: 0.8364\n",
      "Epoch 17/20 | Train Loss: 0.5484 | Val Loss: 0.3969 | MAE: 0.2779 | RMSE: 0.3969 | RÂ²: 0.8313\n",
      "Epoch 18/20 | Train Loss: 0.5332 | Val Loss: 0.3958 | MAE: 0.2805 | RMSE: 0.3958 | RÂ²: 0.8323\n",
      "Epoch 19/20 | Train Loss: 0.5241 | Val Loss: 0.3931 | MAE: 0.2840 | RMSE: 0.3931 | RÂ²: 0.8345\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3838 | MAE = 0.2728\n",
      "\n",
      "ðŸ”§ [186/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[1024, 512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8453 | Val Loss: 1.0132 | MAE: 0.8576 | RMSE: 1.0132 | RÂ²: -0.0990\n",
      "Epoch 2/20 | Train Loss: 0.5863 | Val Loss: 0.9536 | MAE: 0.8060 | RMSE: 0.9536 | RÂ²: 0.0265\n",
      "Epoch 3/20 | Train Loss: 0.4693 | Val Loss: 0.8189 | MAE: 0.6911 | RMSE: 0.8189 | RÂ²: 0.2820\n",
      "Epoch 4/20 | Train Loss: 0.4152 | Val Loss: 0.6536 | MAE: 0.5489 | RMSE: 0.6536 | RÂ²: 0.5427\n",
      "Epoch 5/20 | Train Loss: 0.3700 | Val Loss: 0.5043 | MAE: 0.4181 | RMSE: 0.5043 | RÂ²: 0.7277\n",
      "Epoch 6/20 | Train Loss: 0.3555 | Val Loss: 0.3814 | MAE: 0.3012 | RMSE: 0.3814 | RÂ²: 0.8442\n",
      "Epoch 7/20 | Train Loss: 0.3355 | Val Loss: 0.3201 | MAE: 0.2357 | RMSE: 0.3201 | RÂ²: 0.8903\n",
      "Epoch 8/20 | Train Loss: 0.3178 | Val Loss: 0.2899 | MAE: 0.2078 | RMSE: 0.2899 | RÂ²: 0.9100\n",
      "Epoch 9/20 | Train Loss: 0.3133 | Val Loss: 0.2817 | MAE: 0.1987 | RMSE: 0.2817 | RÂ²: 0.9150\n",
      "Epoch 10/20 | Train Loss: 0.3055 | Val Loss: 0.2774 | MAE: 0.1959 | RMSE: 0.2774 | RÂ²: 0.9176\n",
      "Epoch 11/20 | Train Loss: 0.3001 | Val Loss: 0.2802 | MAE: 0.1969 | RMSE: 0.2802 | RÂ²: 0.9159\n",
      "Epoch 12/20 | Train Loss: 0.2984 | Val Loss: 0.2827 | MAE: 0.1976 | RMSE: 0.2827 | RÂ²: 0.9144\n",
      "Epoch 13/20 | Train Loss: 0.2933 | Val Loss: 0.2845 | MAE: 0.1986 | RMSE: 0.2845 | RÂ²: 0.9133\n",
      "Epoch 14/20 | Train Loss: 0.2873 | Val Loss: 0.2911 | MAE: 0.1985 | RMSE: 0.2911 | RÂ²: 0.9093\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2774 | MAE = 0.1959\n",
      "\n",
      "ðŸ”§ [187/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.7494 | Val Loss: 0.8637 | MAE: 0.7201 | RMSE: 0.8637 | RÂ²: 0.2013\n",
      "Epoch 2/20 | Train Loss: 0.4483 | Val Loss: 0.6569 | MAE: 0.5546 | RMSE: 0.6569 | RÂ²: 0.5380\n",
      "Epoch 3/20 | Train Loss: 0.3746 | Val Loss: 0.5152 | MAE: 0.4320 | RMSE: 0.5152 | RÂ²: 0.7158\n",
      "Epoch 4/20 | Train Loss: 0.3441 | Val Loss: 0.3777 | MAE: 0.2961 | RMSE: 0.3777 | RÂ²: 0.8473\n",
      "Epoch 5/20 | Train Loss: 0.3168 | Val Loss: 0.2912 | MAE: 0.2143 | RMSE: 0.2912 | RÂ²: 0.9092\n",
      "Epoch 6/20 | Train Loss: 0.3027 | Val Loss: 0.2700 | MAE: 0.1892 | RMSE: 0.2700 | RÂ²: 0.9220\n",
      "Epoch 7/20 | Train Loss: 0.2879 | Val Loss: 0.2655 | MAE: 0.1853 | RMSE: 0.2655 | RÂ²: 0.9246\n",
      "Epoch 8/20 | Train Loss: 0.2810 | Val Loss: 0.2614 | MAE: 0.1803 | RMSE: 0.2614 | RÂ²: 0.9268\n",
      "Epoch 9/20 | Train Loss: 0.2798 | Val Loss: 0.2671 | MAE: 0.1797 | RMSE: 0.2671 | RÂ²: 0.9236\n",
      "Epoch 10/20 | Train Loss: 0.2701 | Val Loss: 0.2630 | MAE: 0.1795 | RMSE: 0.2630 | RÂ²: 0.9260\n",
      "Epoch 11/20 | Train Loss: 0.2652 | Val Loss: 0.2600 | MAE: 0.1782 | RMSE: 0.2600 | RÂ²: 0.9276\n",
      "Epoch 12/20 | Train Loss: 0.2619 | Val Loss: 0.2714 | MAE: 0.1842 | RMSE: 0.2714 | RÂ²: 0.9212\n",
      "Epoch 13/20 | Train Loss: 0.2619 | Val Loss: 0.2668 | MAE: 0.1819 | RMSE: 0.2668 | RÂ²: 0.9238\n",
      "Epoch 14/20 | Train Loss: 0.2556 | Val Loss: 0.2706 | MAE: 0.1856 | RMSE: 0.2706 | RÂ²: 0.9216\n",
      "Epoch 15/20 | Train Loss: 0.2511 | Val Loss: 0.2753 | MAE: 0.1893 | RMSE: 0.2753 | RÂ²: 0.9189\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2600 | MAE = 0.1782\n",
      "\n",
      "ðŸ”§ [188/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[1024, 512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9054 | Val Loss: 0.8433 | MAE: 0.7109 | RMSE: 0.8433 | RÂ²: 0.2386\n",
      "Epoch 2/20 | Train Loss: 0.6138 | Val Loss: 0.4879 | MAE: 0.3909 | RMSE: 0.4879 | RÂ²: 0.7452\n",
      "Epoch 3/20 | Train Loss: 0.5262 | Val Loss: 0.3985 | MAE: 0.3042 | RMSE: 0.3985 | RÂ²: 0.8300\n",
      "Epoch 4/20 | Train Loss: 0.4715 | Val Loss: 0.4774 | MAE: 0.3736 | RMSE: 0.4774 | RÂ²: 0.7560\n",
      "Epoch 5/20 | Train Loss: 0.4235 | Val Loss: 0.4606 | MAE: 0.3679 | RMSE: 0.4606 | RÂ²: 0.7729\n",
      "Epoch 6/20 | Train Loss: 0.3943 | Val Loss: 0.4150 | MAE: 0.3341 | RMSE: 0.4150 | RÂ²: 0.8156\n",
      "Epoch 7/20 | Train Loss: 0.3776 | Val Loss: 0.3590 | MAE: 0.2831 | RMSE: 0.3590 | RÂ²: 0.8620\n",
      "Epoch 8/20 | Train Loss: 0.3573 | Val Loss: 0.3339 | MAE: 0.2522 | RMSE: 0.3339 | RÂ²: 0.8806\n",
      "Epoch 9/20 | Train Loss: 0.3484 | Val Loss: 0.3064 | MAE: 0.2253 | RMSE: 0.3064 | RÂ²: 0.8995\n",
      "Epoch 10/20 | Train Loss: 0.3388 | Val Loss: 0.2797 | MAE: 0.1977 | RMSE: 0.2797 | RÂ²: 0.9162\n",
      "Epoch 11/20 | Train Loss: 0.3296 | Val Loss: 0.2839 | MAE: 0.1996 | RMSE: 0.2839 | RÂ²: 0.9137\n",
      "Epoch 12/20 | Train Loss: 0.3244 | Val Loss: 0.2871 | MAE: 0.1996 | RMSE: 0.2871 | RÂ²: 0.9117\n",
      "Epoch 13/20 | Train Loss: 0.3249 | Val Loss: 0.2781 | MAE: 0.1932 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 14/20 | Train Loss: 0.3145 | Val Loss: 0.2793 | MAE: 0.1930 | RMSE: 0.2793 | RÂ²: 0.9165\n",
      "Epoch 15/20 | Train Loss: 0.3136 | Val Loss: 0.2766 | MAE: 0.1889 | RMSE: 0.2766 | RÂ²: 0.9181\n",
      "Epoch 16/20 | Train Loss: 0.3069 | Val Loss: 0.2762 | MAE: 0.1900 | RMSE: 0.2762 | RÂ²: 0.9183\n",
      "Epoch 17/20 | Train Loss: 0.3022 | Val Loss: 0.2733 | MAE: 0.1901 | RMSE: 0.2733 | RÂ²: 0.9200\n",
      "Epoch 18/20 | Train Loss: 0.3064 | Val Loss: 0.2785 | MAE: 0.1916 | RMSE: 0.2785 | RÂ²: 0.9170\n",
      "Epoch 19/20 | Train Loss: 0.3019 | Val Loss: 0.2806 | MAE: 0.2008 | RMSE: 0.2806 | RÂ²: 0.9157\n",
      "Epoch 20/20 | Train Loss: 0.2989 | Val Loss: 0.2821 | MAE: 0.1979 | RMSE: 0.2821 | RÂ²: 0.9148\n",
      "âœ… RMSE = 0.2733 | MAE = 0.1901\n",
      "\n",
      "ðŸ”§ [189/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.9196 | Val Loss: 0.6891 | MAE: 0.5788 | RMSE: 0.6891 | RÂ²: 0.4916\n",
      "Epoch 2/20 | Train Loss: 0.5935 | Val Loss: 0.5900 | MAE: 0.4635 | RMSE: 0.5900 | RÂ²: 0.6273\n",
      "Epoch 3/20 | Train Loss: 0.5281 | Val Loss: 0.3756 | MAE: 0.2962 | RMSE: 0.3756 | RÂ²: 0.8490\n",
      "Epoch 4/20 | Train Loss: 0.4669 | Val Loss: 0.3419 | MAE: 0.2554 | RMSE: 0.3419 | RÂ²: 0.8749\n",
      "Epoch 5/20 | Train Loss: 0.4465 | Val Loss: 0.3040 | MAE: 0.2214 | RMSE: 0.3040 | RÂ²: 0.9011\n",
      "Epoch 6/20 | Train Loss: 0.4205 | Val Loss: 0.2907 | MAE: 0.2091 | RMSE: 0.2907 | RÂ²: 0.9095\n",
      "Epoch 7/20 | Train Loss: 0.4035 | Val Loss: 0.2810 | MAE: 0.1994 | RMSE: 0.2810 | RÂ²: 0.9154\n",
      "Epoch 8/20 | Train Loss: 0.3836 | Val Loss: 0.2718 | MAE: 0.1920 | RMSE: 0.2718 | RÂ²: 0.9209\n",
      "Epoch 9/20 | Train Loss: 0.3745 | Val Loss: 0.2770 | MAE: 0.1961 | RMSE: 0.2770 | RÂ²: 0.9179\n",
      "Epoch 10/20 | Train Loss: 0.3654 | Val Loss: 0.2774 | MAE: 0.1952 | RMSE: 0.2774 | RÂ²: 0.9176\n",
      "Epoch 11/20 | Train Loss: 0.3524 | Val Loss: 0.2762 | MAE: 0.1928 | RMSE: 0.2762 | RÂ²: 0.9184\n",
      "Epoch 12/20 | Train Loss: 0.3453 | Val Loss: 0.2696 | MAE: 0.1886 | RMSE: 0.2696 | RÂ²: 0.9222\n",
      "Epoch 13/20 | Train Loss: 0.3367 | Val Loss: 0.2686 | MAE: 0.1884 | RMSE: 0.2686 | RÂ²: 0.9227\n",
      "Epoch 14/20 | Train Loss: 0.3334 | Val Loss: 0.2747 | MAE: 0.1930 | RMSE: 0.2747 | RÂ²: 0.9192\n",
      "Epoch 15/20 | Train Loss: 0.3288 | Val Loss: 0.2770 | MAE: 0.1948 | RMSE: 0.2770 | RÂ²: 0.9179\n",
      "Epoch 16/20 | Train Loss: 0.3186 | Val Loss: 0.2751 | MAE: 0.1939 | RMSE: 0.2751 | RÂ²: 0.9190\n",
      "Epoch 17/20 | Train Loss: 0.3148 | Val Loss: 0.2796 | MAE: 0.1948 | RMSE: 0.2796 | RÂ²: 0.9163\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2686 | MAE = 0.1884\n",
      "\n",
      "ðŸ”§ [190/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[1024, 256, 1024], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.0308 | Val Loss: 0.7683 | MAE: 0.6518 | RMSE: 0.7683 | RÂ²: 0.3680\n",
      "Epoch 2/20 | Train Loss: 0.5288 | Val Loss: 0.6434 | MAE: 0.5334 | RMSE: 0.6434 | RÂ²: 0.5569\n",
      "Epoch 3/20 | Train Loss: 0.4372 | Val Loss: 0.3230 | MAE: 0.2569 | RMSE: 0.3230 | RÂ²: 0.8883\n",
      "Epoch 4/20 | Train Loss: 0.3691 | Val Loss: 0.2959 | MAE: 0.2202 | RMSE: 0.2959 | RÂ²: 0.9063\n",
      "Epoch 5/20 | Train Loss: 0.3503 | Val Loss: 0.2950 | MAE: 0.1974 | RMSE: 0.2950 | RÂ²: 0.9069\n",
      "Epoch 6/20 | Train Loss: 0.3252 | Val Loss: 0.2698 | MAE: 0.1839 | RMSE: 0.2698 | RÂ²: 0.9221\n",
      "Epoch 7/20 | Train Loss: 0.3063 | Val Loss: 0.2557 | MAE: 0.1749 | RMSE: 0.2557 | RÂ²: 0.9300\n",
      "Epoch 8/20 | Train Loss: 0.2955 | Val Loss: 0.2549 | MAE: 0.1748 | RMSE: 0.2549 | RÂ²: 0.9304\n",
      "Epoch 9/20 | Train Loss: 0.2910 | Val Loss: 0.2573 | MAE: 0.1771 | RMSE: 0.2573 | RÂ²: 0.9291\n",
      "Epoch 10/20 | Train Loss: 0.2855 | Val Loss: 0.2677 | MAE: 0.1855 | RMSE: 0.2677 | RÂ²: 0.9233\n",
      "Epoch 11/20 | Train Loss: 0.2750 | Val Loss: 0.2562 | MAE: 0.1775 | RMSE: 0.2562 | RÂ²: 0.9297\n",
      "Epoch 12/20 | Train Loss: 0.2719 | Val Loss: 0.2582 | MAE: 0.1765 | RMSE: 0.2582 | RÂ²: 0.9286\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2549 | MAE = 0.1748\n",
      "\n",
      "ðŸ”§ [191/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[2048, 1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.7761 | Val Loss: 0.8423 | MAE: 0.7069 | RMSE: 0.8423 | RÂ²: 0.2404\n",
      "Epoch 2/20 | Train Loss: 0.3663 | Val Loss: 0.6069 | MAE: 0.5039 | RMSE: 0.6069 | RÂ²: 0.6057\n",
      "Epoch 3/20 | Train Loss: 0.3031 | Val Loss: 0.3713 | MAE: 0.3003 | RMSE: 0.3713 | RÂ²: 0.8524\n",
      "Epoch 4/20 | Train Loss: 0.2783 | Val Loss: 0.2596 | MAE: 0.1859 | RMSE: 0.2596 | RÂ²: 0.9279\n",
      "Epoch 5/20 | Train Loss: 0.2648 | Val Loss: 0.2507 | MAE: 0.1722 | RMSE: 0.2507 | RÂ²: 0.9327\n",
      "Epoch 6/20 | Train Loss: 0.2591 | Val Loss: 0.2562 | MAE: 0.1771 | RMSE: 0.2562 | RÂ²: 0.9297\n",
      "Epoch 7/20 | Train Loss: 0.2536 | Val Loss: 0.2731 | MAE: 0.1821 | RMSE: 0.2731 | RÂ²: 0.9201\n",
      "Epoch 8/20 | Train Loss: 0.2506 | Val Loss: 0.2643 | MAE: 0.1804 | RMSE: 0.2643 | RÂ²: 0.9252\n",
      "Epoch 9/20 | Train Loss: 0.2485 | Val Loss: 0.2664 | MAE: 0.1834 | RMSE: 0.2664 | RÂ²: 0.9240\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2507 | MAE = 0.1722\n",
      "\n",
      "ðŸ”§ [192/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.0313 | Val Loss: 0.8748 | MAE: 0.7421 | RMSE: 0.8748 | RÂ²: 0.1808\n",
      "Epoch 2/20 | Train Loss: 0.8868 | Val Loss: 0.7524 | MAE: 0.6340 | RMSE: 0.7524 | RÂ²: 0.3940\n",
      "Epoch 3/20 | Train Loss: 0.8292 | Val Loss: 0.6466 | MAE: 0.5437 | RMSE: 0.6466 | RÂ²: 0.5524\n",
      "Epoch 4/20 | Train Loss: 0.7677 | Val Loss: 0.5697 | MAE: 0.4827 | RMSE: 0.5697 | RÂ²: 0.6525\n",
      "Epoch 5/20 | Train Loss: 0.7162 | Val Loss: 0.5166 | MAE: 0.4325 | RMSE: 0.5166 | RÂ²: 0.7143\n",
      "Epoch 6/20 | Train Loss: 0.6833 | Val Loss: 0.4593 | MAE: 0.3793 | RMSE: 0.4593 | RÂ²: 0.7742\n",
      "Epoch 7/20 | Train Loss: 0.6667 | Val Loss: 0.4123 | MAE: 0.3344 | RMSE: 0.4123 | RÂ²: 0.8180\n",
      "Epoch 8/20 | Train Loss: 0.6296 | Val Loss: 0.3867 | MAE: 0.3080 | RMSE: 0.3867 | RÂ²: 0.8399\n",
      "Epoch 9/20 | Train Loss: 0.6138 | Val Loss: 0.3761 | MAE: 0.2939 | RMSE: 0.3761 | RÂ²: 0.8486\n",
      "Epoch 10/20 | Train Loss: 0.5935 | Val Loss: 0.3675 | MAE: 0.2819 | RMSE: 0.3675 | RÂ²: 0.8554\n",
      "Epoch 11/20 | Train Loss: 0.5768 | Val Loss: 0.3524 | MAE: 0.2685 | RMSE: 0.3524 | RÂ²: 0.8670\n",
      "Epoch 12/20 | Train Loss: 0.5592 | Val Loss: 0.3434 | MAE: 0.2604 | RMSE: 0.3434 | RÂ²: 0.8737\n",
      "Epoch 13/20 | Train Loss: 0.5480 | Val Loss: 0.3394 | MAE: 0.2548 | RMSE: 0.3394 | RÂ²: 0.8767\n",
      "Epoch 14/20 | Train Loss: 0.5349 | Val Loss: 0.3394 | MAE: 0.2528 | RMSE: 0.3394 | RÂ²: 0.8767\n",
      "Epoch 15/20 | Train Loss: 0.5211 | Val Loss: 0.3381 | MAE: 0.2500 | RMSE: 0.3381 | RÂ²: 0.8776\n",
      "Epoch 16/20 | Train Loss: 0.5084 | Val Loss: 0.3352 | MAE: 0.2472 | RMSE: 0.3352 | RÂ²: 0.8797\n",
      "Epoch 17/20 | Train Loss: 0.5033 | Val Loss: 0.3322 | MAE: 0.2442 | RMSE: 0.3322 | RÂ²: 0.8818\n",
      "Epoch 18/20 | Train Loss: 0.4882 | Val Loss: 0.3325 | MAE: 0.2415 | RMSE: 0.3325 | RÂ²: 0.8816\n",
      "Epoch 19/20 | Train Loss: 0.4820 | Val Loss: 0.3306 | MAE: 0.2381 | RMSE: 0.3306 | RÂ²: 0.8830\n",
      "Epoch 20/20 | Train Loss: 0.4719 | Val Loss: 0.3274 | MAE: 0.2339 | RMSE: 0.3274 | RÂ²: 0.8852\n",
      "âœ… RMSE = 0.3274 | MAE = 0.2339\n",
      "\n",
      "ðŸ”§ [193/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[512, 256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.0795 | Val Loss: 0.9983 | MAE: 0.8479 | RMSE: 0.9983 | RÂ²: -0.0670\n",
      "Epoch 2/20 | Train Loss: 0.9903 | Val Loss: 0.9350 | MAE: 0.7952 | RMSE: 0.9350 | RÂ²: 0.0641\n",
      "Epoch 3/20 | Train Loss: 0.9313 | Val Loss: 0.8623 | MAE: 0.7333 | RMSE: 0.8623 | RÂ²: 0.2040\n",
      "Epoch 4/20 | Train Loss: 0.8899 | Val Loss: 0.7890 | MAE: 0.6680 | RMSE: 0.7890 | RÂ²: 0.3336\n",
      "Epoch 5/20 | Train Loss: 0.8425 | Val Loss: 0.7181 | MAE: 0.6046 | RMSE: 0.7181 | RÂ²: 0.4479\n",
      "Epoch 6/20 | Train Loss: 0.8107 | Val Loss: 0.6563 | MAE: 0.5493 | RMSE: 0.6563 | RÂ²: 0.5388\n",
      "Epoch 7/20 | Train Loss: 0.7803 | Val Loss: 0.6019 | MAE: 0.4987 | RMSE: 0.6019 | RÂ²: 0.6122\n",
      "Epoch 8/20 | Train Loss: 0.7590 | Val Loss: 0.5537 | MAE: 0.4527 | RMSE: 0.5537 | RÂ²: 0.6718\n",
      "Epoch 9/20 | Train Loss: 0.7286 | Val Loss: 0.5091 | MAE: 0.4096 | RMSE: 0.5091 | RÂ²: 0.7225\n",
      "Epoch 10/20 | Train Loss: 0.7033 | Val Loss: 0.4700 | MAE: 0.3738 | RMSE: 0.4700 | RÂ²: 0.7635\n",
      "Epoch 11/20 | Train Loss: 0.6867 | Val Loss: 0.4335 | MAE: 0.3402 | RMSE: 0.4335 | RÂ²: 0.7988\n",
      "Epoch 12/20 | Train Loss: 0.6625 | Val Loss: 0.4019 | MAE: 0.3118 | RMSE: 0.4019 | RÂ²: 0.8271\n",
      "Epoch 13/20 | Train Loss: 0.6402 | Val Loss: 0.3782 | MAE: 0.2907 | RMSE: 0.3782 | RÂ²: 0.8469\n",
      "Epoch 14/20 | Train Loss: 0.6236 | Val Loss: 0.3615 | MAE: 0.2748 | RMSE: 0.3615 | RÂ²: 0.8601\n",
      "Epoch 15/20 | Train Loss: 0.6106 | Val Loss: 0.3516 | MAE: 0.2641 | RMSE: 0.3516 | RÂ²: 0.8677\n",
      "Epoch 16/20 | Train Loss: 0.6061 | Val Loss: 0.3450 | MAE: 0.2565 | RMSE: 0.3450 | RÂ²: 0.8726\n",
      "Epoch 17/20 | Train Loss: 0.5903 | Val Loss: 0.3395 | MAE: 0.2507 | RMSE: 0.3395 | RÂ²: 0.8766\n",
      "Epoch 18/20 | Train Loss: 0.5731 | Val Loss: 0.3344 | MAE: 0.2467 | RMSE: 0.3344 | RÂ²: 0.8803\n",
      "Epoch 19/20 | Train Loss: 0.5623 | Val Loss: 0.3321 | MAE: 0.2443 | RMSE: 0.3321 | RÂ²: 0.8819\n",
      "Epoch 20/20 | Train Loss: 0.5538 | Val Loss: 0.3312 | MAE: 0.2423 | RMSE: 0.3312 | RÂ²: 0.8825\n",
      "âœ… RMSE = 0.3312 | MAE = 0.2423\n",
      "\n",
      "ðŸ”§ [194/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9642 | Val Loss: 0.8922 | MAE: 0.7569 | RMSE: 0.8922 | RÂ²: 0.1478\n",
      "Epoch 2/20 | Train Loss: 0.7846 | Val Loss: 0.7704 | MAE: 0.6520 | RMSE: 0.7704 | RÂ²: 0.3646\n",
      "Epoch 3/20 | Train Loss: 0.6886 | Val Loss: 0.6584 | MAE: 0.5569 | RMSE: 0.6584 | RÂ²: 0.5359\n",
      "Epoch 4/20 | Train Loss: 0.6213 | Val Loss: 0.5732 | MAE: 0.4751 | RMSE: 0.5732 | RÂ²: 0.6483\n",
      "Epoch 5/20 | Train Loss: 0.5733 | Val Loss: 0.4788 | MAE: 0.3836 | RMSE: 0.4788 | RÂ²: 0.7546\n",
      "Epoch 6/20 | Train Loss: 0.5274 | Val Loss: 0.3925 | MAE: 0.3063 | RMSE: 0.3925 | RÂ²: 0.8351\n",
      "Epoch 7/20 | Train Loss: 0.5117 | Val Loss: 0.3480 | MAE: 0.2682 | RMSE: 0.3480 | RÂ²: 0.8703\n",
      "Epoch 8/20 | Train Loss: 0.4867 | Val Loss: 0.3270 | MAE: 0.2494 | RMSE: 0.3270 | RÂ²: 0.8855\n",
      "Epoch 9/20 | Train Loss: 0.4749 | Val Loss: 0.3162 | MAE: 0.2374 | RMSE: 0.3162 | RÂ²: 0.8929\n",
      "Epoch 10/20 | Train Loss: 0.4536 | Val Loss: 0.3128 | MAE: 0.2334 | RMSE: 0.3128 | RÂ²: 0.8952\n",
      "Epoch 11/20 | Train Loss: 0.4437 | Val Loss: 0.3081 | MAE: 0.2302 | RMSE: 0.3081 | RÂ²: 0.8984\n",
      "Epoch 12/20 | Train Loss: 0.4312 | Val Loss: 0.3015 | MAE: 0.2236 | RMSE: 0.3015 | RÂ²: 0.9027\n",
      "Epoch 13/20 | Train Loss: 0.4244 | Val Loss: 0.2952 | MAE: 0.2171 | RMSE: 0.2952 | RÂ²: 0.9067\n",
      "Epoch 14/20 | Train Loss: 0.4123 | Val Loss: 0.2930 | MAE: 0.2144 | RMSE: 0.2930 | RÂ²: 0.9081\n",
      "Epoch 15/20 | Train Loss: 0.4156 | Val Loss: 0.2926 | MAE: 0.2133 | RMSE: 0.2926 | RÂ²: 0.9083\n",
      "Epoch 16/20 | Train Loss: 0.4043 | Val Loss: 0.2944 | MAE: 0.2118 | RMSE: 0.2944 | RÂ²: 0.9072\n",
      "Epoch 17/20 | Train Loss: 0.3999 | Val Loss: 0.2920 | MAE: 0.2088 | RMSE: 0.2920 | RÂ²: 0.9087\n",
      "Epoch 18/20 | Train Loss: 0.3917 | Val Loss: 0.2892 | MAE: 0.2070 | RMSE: 0.2892 | RÂ²: 0.9105\n",
      "Epoch 19/20 | Train Loss: 0.3869 | Val Loss: 0.2911 | MAE: 0.2078 | RMSE: 0.2911 | RÂ²: 0.9093\n",
      "Epoch 20/20 | Train Loss: 0.3826 | Val Loss: 0.2872 | MAE: 0.2046 | RMSE: 0.2872 | RÂ²: 0.9117\n",
      "âœ… RMSE = 0.2872 | MAE = 0.2046\n",
      "\n",
      "ðŸ”§ [195/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[1024, 512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9566 | Val Loss: 1.0061 | MAE: 0.8532 | RMSE: 1.0061 | RÂ²: -0.0836\n",
      "Epoch 2/20 | Train Loss: 0.7973 | Val Loss: 0.9434 | MAE: 0.8003 | RMSE: 0.9434 | RÂ²: 0.0471\n",
      "Epoch 3/20 | Train Loss: 0.6918 | Val Loss: 0.8417 | MAE: 0.7096 | RMSE: 0.8417 | RÂ²: 0.2415\n",
      "Epoch 4/20 | Train Loss: 0.6028 | Val Loss: 0.6852 | MAE: 0.5694 | RMSE: 0.6852 | RÂ²: 0.4974\n",
      "Epoch 5/20 | Train Loss: 0.5666 | Val Loss: 0.5251 | MAE: 0.4244 | RMSE: 0.5251 | RÂ²: 0.7049\n",
      "Epoch 6/20 | Train Loss: 0.5306 | Val Loss: 0.4102 | MAE: 0.3185 | RMSE: 0.4102 | RÂ²: 0.8199\n",
      "Epoch 7/20 | Train Loss: 0.5016 | Val Loss: 0.3536 | MAE: 0.2664 | RMSE: 0.3536 | RÂ²: 0.8662\n",
      "Epoch 8/20 | Train Loss: 0.4830 | Val Loss: 0.3254 | MAE: 0.2374 | RMSE: 0.3254 | RÂ²: 0.8866\n",
      "Epoch 9/20 | Train Loss: 0.4610 | Val Loss: 0.3182 | MAE: 0.2287 | RMSE: 0.3182 | RÂ²: 0.8916\n",
      "Epoch 10/20 | Train Loss: 0.4500 | Val Loss: 0.3134 | MAE: 0.2238 | RMSE: 0.3134 | RÂ²: 0.8948\n",
      "Epoch 11/20 | Train Loss: 0.4380 | Val Loss: 0.3138 | MAE: 0.2258 | RMSE: 0.3138 | RÂ²: 0.8946\n",
      "Epoch 12/20 | Train Loss: 0.4253 | Val Loss: 0.3181 | MAE: 0.2293 | RMSE: 0.3181 | RÂ²: 0.8916\n",
      "Epoch 13/20 | Train Loss: 0.4201 | Val Loss: 0.3167 | MAE: 0.2284 | RMSE: 0.3167 | RÂ²: 0.8926\n",
      "Epoch 14/20 | Train Loss: 0.4128 | Val Loss: 0.3109 | MAE: 0.2260 | RMSE: 0.3109 | RÂ²: 0.8965\n",
      "Epoch 15/20 | Train Loss: 0.4118 | Val Loss: 0.3080 | MAE: 0.2230 | RMSE: 0.3080 | RÂ²: 0.8984\n",
      "Epoch 16/20 | Train Loss: 0.3955 | Val Loss: 0.3060 | MAE: 0.2199 | RMSE: 0.3060 | RÂ²: 0.8997\n",
      "Epoch 17/20 | Train Loss: 0.3911 | Val Loss: 0.3032 | MAE: 0.2162 | RMSE: 0.3032 | RÂ²: 0.9016\n",
      "Epoch 18/20 | Train Loss: 0.3839 | Val Loss: 0.3034 | MAE: 0.2167 | RMSE: 0.3034 | RÂ²: 0.9014\n",
      "Epoch 19/20 | Train Loss: 0.3772 | Val Loss: 0.3055 | MAE: 0.2188 | RMSE: 0.3055 | RÂ²: 0.9001\n",
      "Epoch 20/20 | Train Loss: 0.3690 | Val Loss: 0.3069 | MAE: 0.2187 | RMSE: 0.3069 | RÂ²: 0.8992\n",
      "âœ… RMSE = 0.3032 | MAE = 0.2162\n",
      "\n",
      "ðŸ”§ [196/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[512, 512, 512], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8433 | Val Loss: 0.8954 | MAE: 0.7542 | RMSE: 0.8954 | RÂ²: 0.1417\n",
      "Epoch 2/20 | Train Loss: 0.5625 | Val Loss: 0.7697 | MAE: 0.6405 | RMSE: 0.7697 | RÂ²: 0.3657\n",
      "Epoch 3/20 | Train Loss: 0.4900 | Val Loss: 0.6359 | MAE: 0.5123 | RMSE: 0.6359 | RÂ²: 0.5672\n",
      "Epoch 4/20 | Train Loss: 0.4356 | Val Loss: 0.4908 | MAE: 0.3990 | RMSE: 0.4908 | RÂ²: 0.7422\n",
      "Epoch 5/20 | Train Loss: 0.4193 | Val Loss: 0.4173 | MAE: 0.3232 | RMSE: 0.4173 | RÂ²: 0.8135\n",
      "Epoch 6/20 | Train Loss: 0.3904 | Val Loss: 0.3427 | MAE: 0.2451 | RMSE: 0.3427 | RÂ²: 0.8743\n",
      "Epoch 7/20 | Train Loss: 0.3805 | Val Loss: 0.3342 | MAE: 0.2177 | RMSE: 0.3342 | RÂ²: 0.8804\n",
      "Epoch 8/20 | Train Loss: 0.3674 | Val Loss: 0.3193 | MAE: 0.2091 | RMSE: 0.3193 | RÂ²: 0.8909\n",
      "Epoch 9/20 | Train Loss: 0.3610 | Val Loss: 0.2977 | MAE: 0.2087 | RMSE: 0.2977 | RÂ²: 0.9051\n",
      "Epoch 10/20 | Train Loss: 0.3475 | Val Loss: 0.2804 | MAE: 0.1997 | RMSE: 0.2804 | RÂ²: 0.9158\n",
      "Epoch 11/20 | Train Loss: 0.3427 | Val Loss: 0.2751 | MAE: 0.1948 | RMSE: 0.2751 | RÂ²: 0.9190\n",
      "Epoch 12/20 | Train Loss: 0.3322 | Val Loss: 0.2897 | MAE: 0.1977 | RMSE: 0.2897 | RÂ²: 0.9102\n",
      "Epoch 13/20 | Train Loss: 0.3254 | Val Loss: 0.2757 | MAE: 0.1942 | RMSE: 0.2757 | RÂ²: 0.9186\n",
      "Epoch 14/20 | Train Loss: 0.3253 | Val Loss: 0.2854 | MAE: 0.1990 | RMSE: 0.2854 | RÂ²: 0.9128\n",
      "Epoch 15/20 | Train Loss: 0.3175 | Val Loss: 0.2863 | MAE: 0.1989 | RMSE: 0.2863 | RÂ²: 0.9122\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2751 | MAE = 0.1948\n",
      "\n",
      "ðŸ”§ [197/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[1024, 512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.7986 | Val Loss: 0.7738 | MAE: 0.6488 | RMSE: 0.7738 | RÂ²: 0.3590\n",
      "Epoch 2/20 | Train Loss: 0.5293 | Val Loss: 0.4914 | MAE: 0.3857 | RMSE: 0.4914 | RÂ²: 0.7414\n",
      "Epoch 3/20 | Train Loss: 0.4436 | Val Loss: 0.4174 | MAE: 0.3368 | RMSE: 0.4174 | RÂ²: 0.8135\n",
      "Epoch 4/20 | Train Loss: 0.3980 | Val Loss: 0.4334 | MAE: 0.3514 | RMSE: 0.4334 | RÂ²: 0.7989\n",
      "Epoch 5/20 | Train Loss: 0.3704 | Val Loss: 0.4113 | MAE: 0.3321 | RMSE: 0.4113 | RÂ²: 0.8189\n",
      "Epoch 6/20 | Train Loss: 0.3484 | Val Loss: 0.3363 | MAE: 0.2550 | RMSE: 0.3363 | RÂ²: 0.8789\n",
      "Epoch 7/20 | Train Loss: 0.3348 | Val Loss: 0.3204 | MAE: 0.2404 | RMSE: 0.3204 | RÂ²: 0.8901\n",
      "Epoch 8/20 | Train Loss: 0.3249 | Val Loss: 0.2943 | MAE: 0.2112 | RMSE: 0.2943 | RÂ²: 0.9073\n",
      "Epoch 9/20 | Train Loss: 0.3138 | Val Loss: 0.2859 | MAE: 0.2022 | RMSE: 0.2859 | RÂ²: 0.9125\n",
      "Epoch 10/20 | Train Loss: 0.3051 | Val Loss: 0.2699 | MAE: 0.1867 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 11/20 | Train Loss: 0.3041 | Val Loss: 0.2824 | MAE: 0.1976 | RMSE: 0.2824 | RÂ²: 0.9146\n",
      "Epoch 12/20 | Train Loss: 0.2979 | Val Loss: 0.2704 | MAE: 0.1863 | RMSE: 0.2704 | RÂ²: 0.9217\n",
      "Epoch 13/20 | Train Loss: 0.2954 | Val Loss: 0.2818 | MAE: 0.1963 | RMSE: 0.2818 | RÂ²: 0.9150\n",
      "Epoch 14/20 | Train Loss: 0.2896 | Val Loss: 0.2713 | MAE: 0.1902 | RMSE: 0.2713 | RÂ²: 0.9212\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2699 | MAE = 0.1867\n",
      "\n",
      "ðŸ”§ [198/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[512, 256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 1.0432 | Val Loss: 0.9644 | MAE: 0.8202 | RMSE: 0.9644 | RÂ²: 0.0043\n",
      "Epoch 2/20 | Train Loss: 0.9215 | Val Loss: 0.8653 | MAE: 0.7355 | RMSE: 0.8653 | RÂ²: 0.1985\n",
      "Epoch 3/20 | Train Loss: 0.8397 | Val Loss: 0.7595 | MAE: 0.6434 | RMSE: 0.7595 | RÂ²: 0.3825\n",
      "Epoch 4/20 | Train Loss: 0.7849 | Val Loss: 0.6465 | MAE: 0.5414 | RMSE: 0.6465 | RÂ²: 0.5526\n",
      "Epoch 5/20 | Train Loss: 0.7187 | Val Loss: 0.5342 | MAE: 0.4353 | RMSE: 0.5342 | RÂ²: 0.6945\n",
      "Epoch 6/20 | Train Loss: 0.6787 | Val Loss: 0.4459 | MAE: 0.3531 | RMSE: 0.4459 | RÂ²: 0.7871\n",
      "Epoch 7/20 | Train Loss: 0.6427 | Val Loss: 0.3904 | MAE: 0.3003 | RMSE: 0.3904 | RÂ²: 0.8368\n",
      "Epoch 8/20 | Train Loss: 0.6176 | Val Loss: 0.3664 | MAE: 0.2731 | RMSE: 0.3664 | RÂ²: 0.8563\n",
      "Epoch 9/20 | Train Loss: 0.5896 | Val Loss: 0.3597 | MAE: 0.2594 | RMSE: 0.3597 | RÂ²: 0.8615\n",
      "Epoch 10/20 | Train Loss: 0.5663 | Val Loss: 0.3540 | MAE: 0.2525 | RMSE: 0.3540 | RÂ²: 0.8659\n",
      "Epoch 11/20 | Train Loss: 0.5512 | Val Loss: 0.3493 | MAE: 0.2513 | RMSE: 0.3493 | RÂ²: 0.8694\n",
      "Epoch 12/20 | Train Loss: 0.5342 | Val Loss: 0.3492 | MAE: 0.2542 | RMSE: 0.3492 | RÂ²: 0.8695\n",
      "Epoch 13/20 | Train Loss: 0.5111 | Val Loss: 0.3486 | MAE: 0.2537 | RMSE: 0.3486 | RÂ²: 0.8699\n",
      "Epoch 14/20 | Train Loss: 0.4995 | Val Loss: 0.3517 | MAE: 0.2542 | RMSE: 0.3517 | RÂ²: 0.8676\n",
      "Epoch 15/20 | Train Loss: 0.4857 | Val Loss: 0.3531 | MAE: 0.2524 | RMSE: 0.3531 | RÂ²: 0.8665\n",
      "Epoch 16/20 | Train Loss: 0.4844 | Val Loss: 0.3540 | MAE: 0.2519 | RMSE: 0.3540 | RÂ²: 0.8658\n",
      "Epoch 17/20 | Train Loss: 0.4665 | Val Loss: 0.3500 | MAE: 0.2485 | RMSE: 0.3500 | RÂ²: 0.8689\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3486 | MAE = 0.2537\n",
      "\n",
      "ðŸ”§ [199/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[256, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0367 | Val Loss: 0.9253 | MAE: 0.7851 | RMSE: 0.9253 | RÂ²: 0.0834\n",
      "Epoch 2/20 | Train Loss: 0.9148 | Val Loss: 0.8431 | MAE: 0.7130 | RMSE: 0.8431 | RÂ²: 0.2390\n",
      "Epoch 3/20 | Train Loss: 0.8473 | Val Loss: 0.7700 | MAE: 0.6451 | RMSE: 0.7700 | RÂ²: 0.3653\n",
      "Epoch 4/20 | Train Loss: 0.8059 | Val Loss: 0.6990 | MAE: 0.5836 | RMSE: 0.6990 | RÂ²: 0.4769\n",
      "Epoch 5/20 | Train Loss: 0.7592 | Val Loss: 0.6374 | MAE: 0.5328 | RMSE: 0.6374 | RÂ²: 0.5650\n",
      "Epoch 6/20 | Train Loss: 0.7193 | Val Loss: 0.5873 | MAE: 0.4887 | RMSE: 0.5873 | RÂ²: 0.6307\n",
      "Epoch 7/20 | Train Loss: 0.6738 | Val Loss: 0.5378 | MAE: 0.4438 | RMSE: 0.5378 | RÂ²: 0.6903\n",
      "Epoch 8/20 | Train Loss: 0.6557 | Val Loss: 0.4934 | MAE: 0.4024 | RMSE: 0.4934 | RÂ²: 0.7394\n",
      "Epoch 9/20 | Train Loss: 0.6258 | Val Loss: 0.4558 | MAE: 0.3670 | RMSE: 0.4558 | RÂ²: 0.7776\n",
      "Epoch 10/20 | Train Loss: 0.6069 | Val Loss: 0.4239 | MAE: 0.3387 | RMSE: 0.4239 | RÂ²: 0.8076\n",
      "Epoch 11/20 | Train Loss: 0.5923 | Val Loss: 0.3999 | MAE: 0.3179 | RMSE: 0.3999 | RÂ²: 0.8288\n",
      "Epoch 12/20 | Train Loss: 0.5655 | Val Loss: 0.3815 | MAE: 0.3015 | RMSE: 0.3815 | RÂ²: 0.8442\n",
      "Epoch 13/20 | Train Loss: 0.5657 | Val Loss: 0.3685 | MAE: 0.2885 | RMSE: 0.3685 | RÂ²: 0.8546\n",
      "Epoch 14/20 | Train Loss: 0.5454 | Val Loss: 0.3597 | MAE: 0.2786 | RMSE: 0.3597 | RÂ²: 0.8615\n",
      "Epoch 15/20 | Train Loss: 0.5351 | Val Loss: 0.3510 | MAE: 0.2701 | RMSE: 0.3510 | RÂ²: 0.8681\n",
      "Epoch 16/20 | Train Loss: 0.5247 | Val Loss: 0.3461 | MAE: 0.2643 | RMSE: 0.3461 | RÂ²: 0.8718\n",
      "Epoch 17/20 | Train Loss: 0.5234 | Val Loss: 0.3410 | MAE: 0.2585 | RMSE: 0.3410 | RÂ²: 0.8755\n",
      "Epoch 18/20 | Train Loss: 0.5158 | Val Loss: 0.3338 | MAE: 0.2524 | RMSE: 0.3338 | RÂ²: 0.8807\n",
      "Epoch 19/20 | Train Loss: 0.4976 | Val Loss: 0.3269 | MAE: 0.2466 | RMSE: 0.3269 | RÂ²: 0.8856\n",
      "Epoch 20/20 | Train Loss: 0.4928 | Val Loss: 0.3219 | MAE: 0.2422 | RMSE: 0.3219 | RÂ²: 0.8891\n",
      "âœ… RMSE = 0.3219 | MAE = 0.2422\n",
      "\n",
      "ðŸ”§ [200/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8992 | Val Loss: 0.7765 | MAE: 0.6522 | RMSE: 0.7765 | RÂ²: 0.3545\n",
      "Epoch 2/20 | Train Loss: 0.4041 | Val Loss: 0.5408 | MAE: 0.4442 | RMSE: 0.5408 | RÂ²: 0.6869\n",
      "Epoch 3/20 | Train Loss: 0.3390 | Val Loss: 0.2777 | MAE: 0.2020 | RMSE: 0.2777 | RÂ²: 0.9175\n",
      "Epoch 4/20 | Train Loss: 0.2965 | Val Loss: 0.2849 | MAE: 0.1953 | RMSE: 0.2849 | RÂ²: 0.9131\n",
      "Epoch 5/20 | Train Loss: 0.2780 | Val Loss: 0.2680 | MAE: 0.1908 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 6/20 | Train Loss: 0.2646 | Val Loss: 0.3007 | MAE: 0.2018 | RMSE: 0.3007 | RÂ²: 0.9032\n",
      "Epoch 7/20 | Train Loss: 0.2561 | Val Loss: 0.2610 | MAE: 0.1830 | RMSE: 0.2610 | RÂ²: 0.9271\n",
      "Epoch 8/20 | Train Loss: 0.2485 | Val Loss: 0.2639 | MAE: 0.1828 | RMSE: 0.2639 | RÂ²: 0.9254\n",
      "Epoch 9/20 | Train Loss: 0.2458 | Val Loss: 0.2745 | MAE: 0.1836 | RMSE: 0.2745 | RÂ²: 0.9193\n",
      "Epoch 10/20 | Train Loss: 0.2418 | Val Loss: 0.2615 | MAE: 0.1791 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 11/20 | Train Loss: 0.2367 | Val Loss: 0.2609 | MAE: 0.1799 | RMSE: 0.2609 | RÂ²: 0.9272\n",
      "Epoch 12/20 | Train Loss: 0.2341 | Val Loss: 0.2691 | MAE: 0.1797 | RMSE: 0.2691 | RÂ²: 0.9225\n",
      "Epoch 13/20 | Train Loss: 0.2311 | Val Loss: 0.2699 | MAE: 0.1858 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 14/20 | Train Loss: 0.2284 | Val Loss: 0.2660 | MAE: 0.1799 | RMSE: 0.2660 | RÂ²: 0.9243\n",
      "Epoch 15/20 | Train Loss: 0.2289 | Val Loss: 0.2776 | MAE: 0.1956 | RMSE: 0.2776 | RÂ²: 0.9175\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2609 | MAE = 0.1799\n",
      "\n",
      "ðŸ”§ [201/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[512, 512, 512], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9874 | Val Loss: 0.7847 | MAE: 0.6591 | RMSE: 0.7847 | RÂ²: 0.3409\n",
      "Epoch 2/20 | Train Loss: 0.6615 | Val Loss: 0.6284 | MAE: 0.4930 | RMSE: 0.6284 | RÂ²: 0.5773\n",
      "Epoch 3/20 | Train Loss: 0.5518 | Val Loss: 0.3183 | MAE: 0.2429 | RMSE: 0.3183 | RÂ²: 0.8915\n",
      "Epoch 4/20 | Train Loss: 0.4983 | Val Loss: 0.3189 | MAE: 0.2384 | RMSE: 0.3189 | RÂ²: 0.8911\n",
      "Epoch 5/20 | Train Loss: 0.4575 | Val Loss: 0.3802 | MAE: 0.2878 | RMSE: 0.3802 | RÂ²: 0.8453\n",
      "Epoch 6/20 | Train Loss: 0.4274 | Val Loss: 0.3799 | MAE: 0.2800 | RMSE: 0.3799 | RÂ²: 0.8455\n",
      "Epoch 7/20 | Train Loss: 0.4002 | Val Loss: 0.3727 | MAE: 0.2884 | RMSE: 0.3727 | RÂ²: 0.8513\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3183 | MAE = 0.2429\n",
      "\n",
      "ðŸ”§ [202/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9949 | Val Loss: 0.8695 | MAE: 0.7378 | RMSE: 0.8695 | RÂ²: 0.1906\n",
      "Epoch 2/20 | Train Loss: 0.8201 | Val Loss: 0.7356 | MAE: 0.6211 | RMSE: 0.7356 | RÂ²: 0.4207\n",
      "Epoch 3/20 | Train Loss: 0.7589 | Val Loss: 0.6213 | MAE: 0.5261 | RMSE: 0.6213 | RÂ²: 0.5867\n",
      "Epoch 4/20 | Train Loss: 0.6904 | Val Loss: 0.5526 | MAE: 0.4621 | RMSE: 0.5526 | RÂ²: 0.6731\n",
      "Epoch 5/20 | Train Loss: 0.6382 | Val Loss: 0.4777 | MAE: 0.3925 | RMSE: 0.4777 | RÂ²: 0.7557\n",
      "Epoch 6/20 | Train Loss: 0.6056 | Val Loss: 0.3957 | MAE: 0.3207 | RMSE: 0.3957 | RÂ²: 0.8324\n",
      "Epoch 7/20 | Train Loss: 0.5897 | Val Loss: 0.3615 | MAE: 0.2867 | RMSE: 0.3615 | RÂ²: 0.8601\n",
      "Epoch 8/20 | Train Loss: 0.5548 | Val Loss: 0.3556 | MAE: 0.2758 | RMSE: 0.3556 | RÂ²: 0.8646\n",
      "Epoch 9/20 | Train Loss: 0.5412 | Val Loss: 0.3527 | MAE: 0.2666 | RMSE: 0.3527 | RÂ²: 0.8669\n",
      "Epoch 10/20 | Train Loss: 0.5189 | Val Loss: 0.3341 | MAE: 0.2508 | RMSE: 0.3341 | RÂ²: 0.8805\n",
      "Epoch 11/20 | Train Loss: 0.5046 | Val Loss: 0.3197 | MAE: 0.2397 | RMSE: 0.3197 | RÂ²: 0.8906\n",
      "Epoch 12/20 | Train Loss: 0.4844 | Val Loss: 0.3201 | MAE: 0.2397 | RMSE: 0.3201 | RÂ²: 0.8903\n",
      "Epoch 13/20 | Train Loss: 0.4795 | Val Loss: 0.3232 | MAE: 0.2397 | RMSE: 0.3232 | RÂ²: 0.8882\n",
      "Epoch 14/20 | Train Loss: 0.4654 | Val Loss: 0.3228 | MAE: 0.2379 | RMSE: 0.3228 | RÂ²: 0.8885\n",
      "Epoch 15/20 | Train Loss: 0.4534 | Val Loss: 0.3186 | MAE: 0.2338 | RMSE: 0.3186 | RÂ²: 0.8913\n",
      "Epoch 16/20 | Train Loss: 0.4498 | Val Loss: 0.3118 | MAE: 0.2282 | RMSE: 0.3118 | RÂ²: 0.8959\n",
      "Epoch 17/20 | Train Loss: 0.4395 | Val Loss: 0.3040 | MAE: 0.2215 | RMSE: 0.3040 | RÂ²: 0.9010\n",
      "Epoch 18/20 | Train Loss: 0.4326 | Val Loss: 0.3041 | MAE: 0.2179 | RMSE: 0.3041 | RÂ²: 0.9010\n",
      "Epoch 19/20 | Train Loss: 0.4265 | Val Loss: 0.3037 | MAE: 0.2155 | RMSE: 0.3037 | RÂ²: 0.9013\n",
      "Epoch 20/20 | Train Loss: 0.4178 | Val Loss: 0.3019 | MAE: 0.2132 | RMSE: 0.3019 | RÂ²: 0.9025\n",
      "âœ… RMSE = 0.3019 | MAE = 0.2132\n",
      "\n",
      "ðŸ”§ [203/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[256, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8957 | Val Loss: 0.8657 | MAE: 0.7349 | RMSE: 0.8657 | RÂ²: 0.1977\n",
      "Epoch 2/20 | Train Loss: 0.7054 | Val Loss: 0.7374 | MAE: 0.6280 | RMSE: 0.7374 | RÂ²: 0.4179\n",
      "Epoch 3/20 | Train Loss: 0.6010 | Val Loss: 0.6307 | MAE: 0.5304 | RMSE: 0.6307 | RÂ²: 0.5742\n",
      "Epoch 4/20 | Train Loss: 0.5320 | Val Loss: 0.4909 | MAE: 0.4021 | RMSE: 0.4909 | RÂ²: 0.7420\n",
      "Epoch 5/20 | Train Loss: 0.4827 | Val Loss: 0.3927 | MAE: 0.3092 | RMSE: 0.3927 | RÂ²: 0.8349\n",
      "Epoch 6/20 | Train Loss: 0.4523 | Val Loss: 0.3553 | MAE: 0.2723 | RMSE: 0.3553 | RÂ²: 0.8649\n",
      "Epoch 7/20 | Train Loss: 0.4291 | Val Loss: 0.3227 | MAE: 0.2437 | RMSE: 0.3227 | RÂ²: 0.8885\n",
      "Epoch 8/20 | Train Loss: 0.4114 | Val Loss: 0.3071 | MAE: 0.2274 | RMSE: 0.3071 | RÂ²: 0.8990\n",
      "Epoch 9/20 | Train Loss: 0.3976 | Val Loss: 0.3008 | MAE: 0.2205 | RMSE: 0.3008 | RÂ²: 0.9031\n",
      "Epoch 10/20 | Train Loss: 0.3822 | Val Loss: 0.2925 | MAE: 0.2132 | RMSE: 0.2925 | RÂ²: 0.9084\n",
      "Epoch 11/20 | Train Loss: 0.3732 | Val Loss: 0.2887 | MAE: 0.2095 | RMSE: 0.2887 | RÂ²: 0.9108\n",
      "Epoch 12/20 | Train Loss: 0.3621 | Val Loss: 0.2834 | MAE: 0.2046 | RMSE: 0.2834 | RÂ²: 0.9140\n",
      "Epoch 13/20 | Train Loss: 0.3585 | Val Loss: 0.2804 | MAE: 0.1996 | RMSE: 0.2804 | RÂ²: 0.9159\n",
      "Epoch 14/20 | Train Loss: 0.3495 | Val Loss: 0.2779 | MAE: 0.1963 | RMSE: 0.2779 | RÂ²: 0.9173\n",
      "Epoch 15/20 | Train Loss: 0.3429 | Val Loss: 0.2757 | MAE: 0.1954 | RMSE: 0.2757 | RÂ²: 0.9186\n",
      "Epoch 16/20 | Train Loss: 0.3417 | Val Loss: 0.2781 | MAE: 0.1936 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 17/20 | Train Loss: 0.3299 | Val Loss: 0.2691 | MAE: 0.1881 | RMSE: 0.2691 | RÂ²: 0.9225\n",
      "Epoch 18/20 | Train Loss: 0.3276 | Val Loss: 0.2725 | MAE: 0.1894 | RMSE: 0.2725 | RÂ²: 0.9205\n",
      "Epoch 19/20 | Train Loss: 0.3243 | Val Loss: 0.2761 | MAE: 0.1927 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 20/20 | Train Loss: 0.3204 | Val Loss: 0.2742 | MAE: 0.1920 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "âœ… RMSE = 0.2691 | MAE = 0.1881\n",
      "\n",
      "ðŸ”§ [204/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[512, 512, 512], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9660 | Val Loss: 0.8859 | MAE: 0.7519 | RMSE: 0.8859 | RÂ²: 0.1598\n",
      "Epoch 2/20 | Train Loss: 0.7528 | Val Loss: 0.7615 | MAE: 0.6408 | RMSE: 0.7615 | RÂ²: 0.3792\n",
      "Epoch 3/20 | Train Loss: 0.6454 | Val Loss: 0.5946 | MAE: 0.4923 | RMSE: 0.5946 | RÂ²: 0.6215\n",
      "Epoch 4/20 | Train Loss: 0.5913 | Val Loss: 0.4670 | MAE: 0.3642 | RMSE: 0.4670 | RÂ²: 0.7665\n",
      "Epoch 5/20 | Train Loss: 0.5611 | Val Loss: 0.3861 | MAE: 0.2804 | RMSE: 0.3861 | RÂ²: 0.8404\n",
      "Epoch 6/20 | Train Loss: 0.5297 | Val Loss: 0.3108 | MAE: 0.2265 | RMSE: 0.3108 | RÂ²: 0.8966\n",
      "Epoch 7/20 | Train Loss: 0.5121 | Val Loss: 0.3018 | MAE: 0.2183 | RMSE: 0.3018 | RÂ²: 0.9025\n",
      "Epoch 8/20 | Train Loss: 0.4897 | Val Loss: 0.3038 | MAE: 0.2161 | RMSE: 0.3038 | RÂ²: 0.9012\n",
      "Epoch 9/20 | Train Loss: 0.4700 | Val Loss: 0.3039 | MAE: 0.2212 | RMSE: 0.3039 | RÂ²: 0.9011\n",
      "Epoch 10/20 | Train Loss: 0.4551 | Val Loss: 0.3109 | MAE: 0.2256 | RMSE: 0.3109 | RÂ²: 0.8965\n",
      "Epoch 11/20 | Train Loss: 0.4460 | Val Loss: 0.3085 | MAE: 0.2244 | RMSE: 0.3085 | RÂ²: 0.8981\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3018 | MAE = 0.2183\n",
      "\n",
      "ðŸ”§ [205/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[1024, 512, 256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9984 | Val Loss: 0.9052 | MAE: 0.7681 | RMSE: 0.9052 | RÂ²: 0.1228\n",
      "Epoch 2/20 | Train Loss: 0.7583 | Val Loss: 0.7136 | MAE: 0.5967 | RMSE: 0.7136 | RÂ²: 0.4549\n",
      "Epoch 3/20 | Train Loss: 0.6422 | Val Loss: 0.4390 | MAE: 0.3473 | RMSE: 0.4390 | RÂ²: 0.7937\n",
      "Epoch 4/20 | Train Loss: 0.5774 | Val Loss: 0.4513 | MAE: 0.3358 | RMSE: 0.4513 | RÂ²: 0.7820\n",
      "Epoch 5/20 | Train Loss: 0.5141 | Val Loss: 0.4582 | MAE: 0.3479 | RMSE: 0.4582 | RÂ²: 0.7753\n",
      "Epoch 6/20 | Train Loss: 0.4789 | Val Loss: 0.4667 | MAE: 0.3740 | RMSE: 0.4667 | RÂ²: 0.7668\n",
      "Epoch 7/20 | Train Loss: 0.4535 | Val Loss: 0.4971 | MAE: 0.4162 | RMSE: 0.4971 | RÂ²: 0.7354\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.4390 | MAE = 0.3473\n",
      "\n",
      "ðŸ”§ [206/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9110 | Val Loss: 0.6849 | MAE: 0.5815 | RMSE: 0.6849 | RÂ²: 0.4979\n",
      "Epoch 2/20 | Train Loss: 0.6728 | Val Loss: 0.6215 | MAE: 0.4709 | RMSE: 0.6215 | RÂ²: 0.5865\n",
      "Epoch 3/20 | Train Loss: 0.5993 | Val Loss: 0.4022 | MAE: 0.3124 | RMSE: 0.4022 | RÂ²: 0.8268\n",
      "Epoch 4/20 | Train Loss: 0.5442 | Val Loss: 0.3771 | MAE: 0.2906 | RMSE: 0.3771 | RÂ²: 0.8478\n",
      "Epoch 5/20 | Train Loss: 0.5061 | Val Loss: 0.3555 | MAE: 0.2639 | RMSE: 0.3555 | RÂ²: 0.8647\n",
      "Epoch 6/20 | Train Loss: 0.4772 | Val Loss: 0.3313 | MAE: 0.2460 | RMSE: 0.3313 | RÂ²: 0.8825\n",
      "Epoch 7/20 | Train Loss: 0.4597 | Val Loss: 0.3460 | MAE: 0.2515 | RMSE: 0.3460 | RÂ²: 0.8719\n",
      "Epoch 8/20 | Train Loss: 0.4347 | Val Loss: 0.3319 | MAE: 0.2376 | RMSE: 0.3319 | RÂ²: 0.8821\n",
      "Epoch 9/20 | Train Loss: 0.4154 | Val Loss: 0.3069 | MAE: 0.2231 | RMSE: 0.3069 | RÂ²: 0.8991\n",
      "Epoch 10/20 | Train Loss: 0.4017 | Val Loss: 0.3014 | MAE: 0.2174 | RMSE: 0.3014 | RÂ²: 0.9027\n",
      "Epoch 11/20 | Train Loss: 0.3889 | Val Loss: 0.3004 | MAE: 0.2128 | RMSE: 0.3004 | RÂ²: 0.9034\n",
      "Epoch 12/20 | Train Loss: 0.3817 | Val Loss: 0.3003 | MAE: 0.2111 | RMSE: 0.3003 | RÂ²: 0.9035\n",
      "Epoch 13/20 | Train Loss: 0.3768 | Val Loss: 0.3048 | MAE: 0.2111 | RMSE: 0.3048 | RÂ²: 0.9006\n",
      "Epoch 14/20 | Train Loss: 0.3634 | Val Loss: 0.2905 | MAE: 0.2066 | RMSE: 0.2905 | RÂ²: 0.9096\n",
      "Epoch 15/20 | Train Loss: 0.3541 | Val Loss: 0.2869 | MAE: 0.2011 | RMSE: 0.2869 | RÂ²: 0.9119\n",
      "Epoch 16/20 | Train Loss: 0.3491 | Val Loss: 0.2893 | MAE: 0.2005 | RMSE: 0.2893 | RÂ²: 0.9104\n",
      "Epoch 17/20 | Train Loss: 0.3451 | Val Loss: 0.2907 | MAE: 0.2041 | RMSE: 0.2907 | RÂ²: 0.9095\n",
      "Epoch 18/20 | Train Loss: 0.3371 | Val Loss: 0.2862 | MAE: 0.1990 | RMSE: 0.2862 | RÂ²: 0.9123\n",
      "Epoch 19/20 | Train Loss: 0.3365 | Val Loss: 0.2874 | MAE: 0.1999 | RMSE: 0.2874 | RÂ²: 0.9116\n",
      "Epoch 20/20 | Train Loss: 0.3268 | Val Loss: 0.2861 | MAE: 0.2007 | RMSE: 0.2861 | RÂ²: 0.9124\n",
      "âœ… RMSE = 0.2861 | MAE = 0.2007\n",
      "\n",
      "ðŸ”§ [207/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9421 | Val Loss: 0.8659 | MAE: 0.7334 | RMSE: 0.8659 | RÂ²: 0.1974\n",
      "Epoch 2/20 | Train Loss: 0.7294 | Val Loss: 0.7397 | MAE: 0.6271 | RMSE: 0.7397 | RÂ²: 0.4142\n",
      "Epoch 3/20 | Train Loss: 0.5929 | Val Loss: 0.6190 | MAE: 0.5189 | RMSE: 0.6190 | RÂ²: 0.5898\n",
      "Epoch 4/20 | Train Loss: 0.5056 | Val Loss: 0.4904 | MAE: 0.3965 | RMSE: 0.4904 | RÂ²: 0.7425\n",
      "Epoch 5/20 | Train Loss: 0.4471 | Val Loss: 0.3788 | MAE: 0.2942 | RMSE: 0.3788 | RÂ²: 0.8464\n",
      "Epoch 6/20 | Train Loss: 0.4140 | Val Loss: 0.3449 | MAE: 0.2627 | RMSE: 0.3449 | RÂ²: 0.8727\n",
      "Epoch 7/20 | Train Loss: 0.3894 | Val Loss: 0.3322 | MAE: 0.2511 | RMSE: 0.3322 | RÂ²: 0.8819\n",
      "Epoch 8/20 | Train Loss: 0.3681 | Val Loss: 0.3241 | MAE: 0.2401 | RMSE: 0.3241 | RÂ²: 0.8875\n",
      "Epoch 9/20 | Train Loss: 0.3555 | Val Loss: 0.3135 | MAE: 0.2312 | RMSE: 0.3135 | RÂ²: 0.8948\n",
      "Epoch 10/20 | Train Loss: 0.3449 | Val Loss: 0.3062 | MAE: 0.2253 | RMSE: 0.3062 | RÂ²: 0.8996\n",
      "Epoch 11/20 | Train Loss: 0.3360 | Val Loss: 0.2999 | MAE: 0.2168 | RMSE: 0.2999 | RÂ²: 0.9037\n",
      "Epoch 12/20 | Train Loss: 0.3240 | Val Loss: 0.2924 | MAE: 0.2110 | RMSE: 0.2924 | RÂ²: 0.9084\n",
      "Epoch 13/20 | Train Loss: 0.3188 | Val Loss: 0.2905 | MAE: 0.2084 | RMSE: 0.2905 | RÂ²: 0.9097\n",
      "Epoch 14/20 | Train Loss: 0.3145 | Val Loss: 0.2857 | MAE: 0.2048 | RMSE: 0.2857 | RÂ²: 0.9126\n",
      "Epoch 15/20 | Train Loss: 0.3062 | Val Loss: 0.2852 | MAE: 0.2035 | RMSE: 0.2852 | RÂ²: 0.9129\n",
      "Epoch 16/20 | Train Loss: 0.3013 | Val Loss: 0.2823 | MAE: 0.2003 | RMSE: 0.2823 | RÂ²: 0.9147\n",
      "Epoch 17/20 | Train Loss: 0.2959 | Val Loss: 0.2766 | MAE: 0.1960 | RMSE: 0.2766 | RÂ²: 0.9181\n",
      "Epoch 18/20 | Train Loss: 0.2914 | Val Loss: 0.2800 | MAE: 0.1979 | RMSE: 0.2800 | RÂ²: 0.9160\n",
      "Epoch 19/20 | Train Loss: 0.2897 | Val Loss: 0.2777 | MAE: 0.1959 | RMSE: 0.2777 | RÂ²: 0.9174\n",
      "Epoch 20/20 | Train Loss: 0.2837 | Val Loss: 0.2800 | MAE: 0.1973 | RMSE: 0.2800 | RÂ²: 0.9160\n",
      "âœ… RMSE = 0.2766 | MAE = 0.1960\n",
      "\n",
      "ðŸ”§ [208/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 256, 1024], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8195 | Val Loss: 0.8550 | MAE: 0.7171 | RMSE: 0.8550 | RÂ²: 0.2174\n",
      "Epoch 2/20 | Train Loss: 0.5288 | Val Loss: 0.6412 | MAE: 0.5306 | RMSE: 0.6412 | RÂ²: 0.5599\n",
      "Epoch 3/20 | Train Loss: 0.4624 | Val Loss: 0.5031 | MAE: 0.4138 | RMSE: 0.5031 | RÂ²: 0.7290\n",
      "Epoch 4/20 | Train Loss: 0.4443 | Val Loss: 0.3455 | MAE: 0.2451 | RMSE: 0.3455 | RÂ²: 0.8722\n",
      "Epoch 5/20 | Train Loss: 0.4177 | Val Loss: 0.2925 | MAE: 0.2129 | RMSE: 0.2925 | RÂ²: 0.9084\n",
      "Epoch 6/20 | Train Loss: 0.3834 | Val Loss: 0.2785 | MAE: 0.1943 | RMSE: 0.2785 | RÂ²: 0.9170\n",
      "Epoch 7/20 | Train Loss: 0.3763 | Val Loss: 0.2912 | MAE: 0.1988 | RMSE: 0.2912 | RÂ²: 0.9092\n",
      "Epoch 8/20 | Train Loss: 0.3740 | Val Loss: 0.2771 | MAE: 0.1883 | RMSE: 0.2771 | RÂ²: 0.9178\n",
      "Epoch 9/20 | Train Loss: 0.3533 | Val Loss: 0.2613 | MAE: 0.1805 | RMSE: 0.2613 | RÂ²: 0.9269\n",
      "Epoch 10/20 | Train Loss: 0.3378 | Val Loss: 0.2681 | MAE: 0.1862 | RMSE: 0.2681 | RÂ²: 0.9231\n",
      "Epoch 11/20 | Train Loss: 0.3334 | Val Loss: 0.2677 | MAE: 0.1835 | RMSE: 0.2677 | RÂ²: 0.9233\n",
      "Epoch 12/20 | Train Loss: 0.3254 | Val Loss: 0.2638 | MAE: 0.1812 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 13/20 | Train Loss: 0.3241 | Val Loss: 0.2876 | MAE: 0.1926 | RMSE: 0.2876 | RÂ²: 0.9114\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2613 | MAE = 0.1805\n",
      "\n",
      "ðŸ”§ [209/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[1024, 512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0508 | Val Loss: 0.9495 | MAE: 0.7992 | RMSE: 0.9495 | RÂ²: 0.0348\n",
      "Epoch 2/20 | Train Loss: 0.9011 | Val Loss: 0.8623 | MAE: 0.7300 | RMSE: 0.8623 | RÂ²: 0.2040\n",
      "Epoch 3/20 | Train Loss: 0.8355 | Val Loss: 0.7562 | MAE: 0.6436 | RMSE: 0.7562 | RÂ²: 0.3878\n",
      "Epoch 4/20 | Train Loss: 0.7628 | Val Loss: 0.6527 | MAE: 0.5527 | RMSE: 0.6527 | RÂ²: 0.5440\n",
      "Epoch 5/20 | Train Loss: 0.6923 | Val Loss: 0.5614 | MAE: 0.4640 | RMSE: 0.5614 | RÂ²: 0.6626\n",
      "Epoch 6/20 | Train Loss: 0.6361 | Val Loss: 0.4815 | MAE: 0.3820 | RMSE: 0.4815 | RÂ²: 0.7518\n",
      "Epoch 7/20 | Train Loss: 0.5912 | Val Loss: 0.4287 | MAE: 0.3283 | RMSE: 0.4287 | RÂ²: 0.8033\n",
      "Epoch 8/20 | Train Loss: 0.5730 | Val Loss: 0.4240 | MAE: 0.3223 | RMSE: 0.4240 | RÂ²: 0.8075\n",
      "Epoch 9/20 | Train Loss: 0.5521 | Val Loss: 0.4373 | MAE: 0.3317 | RMSE: 0.4373 | RÂ²: 0.7953\n",
      "Epoch 10/20 | Train Loss: 0.5294 | Val Loss: 0.4363 | MAE: 0.3285 | RMSE: 0.4363 | RÂ²: 0.7962\n",
      "Epoch 11/20 | Train Loss: 0.5077 | Val Loss: 0.4138 | MAE: 0.3060 | RMSE: 0.4138 | RÂ²: 0.8167\n",
      "Epoch 12/20 | Train Loss: 0.5016 | Val Loss: 0.3958 | MAE: 0.2863 | RMSE: 0.3958 | RÂ²: 0.8323\n",
      "Epoch 13/20 | Train Loss: 0.4910 | Val Loss: 0.3793 | MAE: 0.2736 | RMSE: 0.3793 | RÂ²: 0.8460\n",
      "Epoch 14/20 | Train Loss: 0.4840 | Val Loss: 0.3710 | MAE: 0.2676 | RMSE: 0.3710 | RÂ²: 0.8526\n",
      "Epoch 15/20 | Train Loss: 0.4694 | Val Loss: 0.3698 | MAE: 0.2643 | RMSE: 0.3698 | RÂ²: 0.8536\n",
      "Epoch 16/20 | Train Loss: 0.4586 | Val Loss: 0.3698 | MAE: 0.2617 | RMSE: 0.3698 | RÂ²: 0.8536\n",
      "Epoch 17/20 | Train Loss: 0.4494 | Val Loss: 0.3632 | MAE: 0.2543 | RMSE: 0.3632 | RÂ²: 0.8588\n",
      "Epoch 18/20 | Train Loss: 0.4450 | Val Loss: 0.3584 | MAE: 0.2476 | RMSE: 0.3584 | RÂ²: 0.8625\n",
      "Epoch 19/20 | Train Loss: 0.4404 | Val Loss: 0.3545 | MAE: 0.2417 | RMSE: 0.3545 | RÂ²: 0.8655\n",
      "Epoch 20/20 | Train Loss: 0.4275 | Val Loss: 0.3541 | MAE: 0.2414 | RMSE: 0.3541 | RÂ²: 0.8658\n",
      "âœ… RMSE = 0.3541 | MAE = 0.2414\n",
      "\n",
      "ðŸ”§ [210/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[512, 512, 512], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8332 | Val Loss: 0.8908 | MAE: 0.7506 | RMSE: 0.8908 | RÂ²: 0.1506\n",
      "Epoch 2/20 | Train Loss: 0.5124 | Val Loss: 0.7123 | MAE: 0.5972 | RMSE: 0.7123 | RÂ²: 0.4569\n",
      "Epoch 3/20 | Train Loss: 0.4388 | Val Loss: 0.5254 | MAE: 0.4371 | RMSE: 0.5254 | RÂ²: 0.7044\n",
      "Epoch 4/20 | Train Loss: 0.3939 | Val Loss: 0.3979 | MAE: 0.3074 | RMSE: 0.3979 | RÂ²: 0.8305\n",
      "Epoch 5/20 | Train Loss: 0.3674 | Val Loss: 0.2988 | MAE: 0.2238 | RMSE: 0.2988 | RÂ²: 0.9044\n",
      "Epoch 6/20 | Train Loss: 0.3460 | Val Loss: 0.2627 | MAE: 0.1835 | RMSE: 0.2627 | RÂ²: 0.9261\n",
      "Epoch 7/20 | Train Loss: 0.3337 | Val Loss: 0.2824 | MAE: 0.2046 | RMSE: 0.2824 | RÂ²: 0.9147\n",
      "Epoch 8/20 | Train Loss: 0.3272 | Val Loss: 0.2637 | MAE: 0.1805 | RMSE: 0.2637 | RÂ²: 0.9255\n",
      "Epoch 9/20 | Train Loss: 0.3141 | Val Loss: 0.2855 | MAE: 0.2054 | RMSE: 0.2855 | RÂ²: 0.9127\n",
      "Epoch 10/20 | Train Loss: 0.3050 | Val Loss: 0.2649 | MAE: 0.1848 | RMSE: 0.2649 | RÂ²: 0.9249\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2627 | MAE = 0.1835\n",
      "\n",
      "ðŸ”§ [211/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8818 | Val Loss: 0.7960 | MAE: 0.6766 | RMSE: 0.7960 | RÂ²: 0.3216\n",
      "Epoch 2/20 | Train Loss: 0.6257 | Val Loss: 0.6859 | MAE: 0.5683 | RMSE: 0.6859 | RÂ²: 0.4963\n",
      "Epoch 3/20 | Train Loss: 0.5253 | Val Loss: 0.5283 | MAE: 0.4236 | RMSE: 0.5283 | RÂ²: 0.7012\n",
      "Epoch 4/20 | Train Loss: 0.4685 | Val Loss: 0.3842 | MAE: 0.3054 | RMSE: 0.3842 | RÂ²: 0.8420\n",
      "Epoch 5/20 | Train Loss: 0.4394 | Val Loss: 0.3706 | MAE: 0.2841 | RMSE: 0.3706 | RÂ²: 0.8530\n",
      "Epoch 6/20 | Train Loss: 0.4075 | Val Loss: 0.3368 | MAE: 0.2552 | RMSE: 0.3368 | RÂ²: 0.8786\n",
      "Epoch 7/20 | Train Loss: 0.3918 | Val Loss: 0.3130 | MAE: 0.2335 | RMSE: 0.3130 | RÂ²: 0.8951\n",
      "Epoch 8/20 | Train Loss: 0.3750 | Val Loss: 0.3028 | MAE: 0.2231 | RMSE: 0.3028 | RÂ²: 0.9018\n",
      "Epoch 9/20 | Train Loss: 0.3562 | Val Loss: 0.2923 | MAE: 0.2123 | RMSE: 0.2923 | RÂ²: 0.9086\n",
      "Epoch 10/20 | Train Loss: 0.3497 | Val Loss: 0.2870 | MAE: 0.2056 | RMSE: 0.2870 | RÂ²: 0.9118\n",
      "Epoch 11/20 | Train Loss: 0.3388 | Val Loss: 0.2816 | MAE: 0.2003 | RMSE: 0.2816 | RÂ²: 0.9151\n",
      "Epoch 12/20 | Train Loss: 0.3385 | Val Loss: 0.2798 | MAE: 0.1982 | RMSE: 0.2798 | RÂ²: 0.9162\n",
      "Epoch 13/20 | Train Loss: 0.3280 | Val Loss: 0.2835 | MAE: 0.2008 | RMSE: 0.2835 | RÂ²: 0.9140\n",
      "Epoch 14/20 | Train Loss: 0.3240 | Val Loss: 0.2800 | MAE: 0.1955 | RMSE: 0.2800 | RÂ²: 0.9161\n",
      "Epoch 15/20 | Train Loss: 0.3196 | Val Loss: 0.2749 | MAE: 0.1928 | RMSE: 0.2749 | RÂ²: 0.9191\n",
      "Epoch 16/20 | Train Loss: 0.3099 | Val Loss: 0.2823 | MAE: 0.1980 | RMSE: 0.2823 | RÂ²: 0.9147\n",
      "Epoch 17/20 | Train Loss: 0.3079 | Val Loss: 0.2791 | MAE: 0.1942 | RMSE: 0.2791 | RÂ²: 0.9166\n",
      "Epoch 18/20 | Train Loss: 0.3004 | Val Loss: 0.2782 | MAE: 0.1944 | RMSE: 0.2782 | RÂ²: 0.9172\n",
      "Epoch 19/20 | Train Loss: 0.3058 | Val Loss: 0.2785 | MAE: 0.1934 | RMSE: 0.2785 | RÂ²: 0.9169\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2749 | MAE = 0.1928\n",
      "\n",
      "ðŸ”§ [212/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[1024, 256, 1024], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.0384 | Val Loss: 0.9250 | MAE: 0.7849 | RMSE: 0.9250 | RÂ²: 0.0840\n",
      "Epoch 2/20 | Train Loss: 0.9035 | Val Loss: 0.8161 | MAE: 0.6927 | RMSE: 0.8161 | RÂ²: 0.2869\n",
      "Epoch 3/20 | Train Loss: 0.8283 | Val Loss: 0.7083 | MAE: 0.5992 | RMSE: 0.7083 | RÂ²: 0.4629\n",
      "Epoch 4/20 | Train Loss: 0.7766 | Val Loss: 0.6011 | MAE: 0.5022 | RMSE: 0.6011 | RÂ²: 0.6132\n",
      "Epoch 5/20 | Train Loss: 0.7265 | Val Loss: 0.5024 | MAE: 0.4099 | RMSE: 0.5024 | RÂ²: 0.7298\n",
      "Epoch 6/20 | Train Loss: 0.6786 | Val Loss: 0.4281 | MAE: 0.3384 | RMSE: 0.4281 | RÂ²: 0.8038\n",
      "Epoch 7/20 | Train Loss: 0.6495 | Val Loss: 0.3865 | MAE: 0.2944 | RMSE: 0.3865 | RÂ²: 0.8401\n",
      "Epoch 8/20 | Train Loss: 0.6233 | Val Loss: 0.3749 | MAE: 0.2715 | RMSE: 0.3749 | RÂ²: 0.8495\n",
      "Epoch 9/20 | Train Loss: 0.6213 | Val Loss: 0.3544 | MAE: 0.2531 | RMSE: 0.3544 | RÂ²: 0.8655\n",
      "Epoch 10/20 | Train Loss: 0.5966 | Val Loss: 0.3394 | MAE: 0.2440 | RMSE: 0.3394 | RÂ²: 0.8767\n",
      "Epoch 11/20 | Train Loss: 0.5855 | Val Loss: 0.3311 | MAE: 0.2388 | RMSE: 0.3311 | RÂ²: 0.8826\n",
      "Epoch 12/20 | Train Loss: 0.5738 | Val Loss: 0.3323 | MAE: 0.2381 | RMSE: 0.3323 | RÂ²: 0.8818\n",
      "Epoch 13/20 | Train Loss: 0.5625 | Val Loss: 0.3278 | MAE: 0.2365 | RMSE: 0.3278 | RÂ²: 0.8849\n",
      "Epoch 14/20 | Train Loss: 0.5652 | Val Loss: 0.3228 | MAE: 0.2367 | RMSE: 0.3228 | RÂ²: 0.8884\n",
      "Epoch 15/20 | Train Loss: 0.5456 | Val Loss: 0.3234 | MAE: 0.2356 | RMSE: 0.3234 | RÂ²: 0.8881\n",
      "Epoch 16/20 | Train Loss: 0.5393 | Val Loss: 0.3321 | MAE: 0.2348 | RMSE: 0.3321 | RÂ²: 0.8819\n",
      "Epoch 17/20 | Train Loss: 0.5423 | Val Loss: 0.3282 | MAE: 0.2331 | RMSE: 0.3282 | RÂ²: 0.8847\n",
      "Epoch 18/20 | Train Loss: 0.5259 | Val Loss: 0.3247 | MAE: 0.2317 | RMSE: 0.3247 | RÂ²: 0.8872\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3228 | MAE = 0.2367\n",
      "\n",
      "ðŸ”§ [213/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[256, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.7979 | Val Loss: 0.7653 | MAE: 0.6353 | RMSE: 0.7653 | RÂ²: 0.3730\n",
      "Epoch 2/20 | Train Loss: 0.4963 | Val Loss: 0.5108 | MAE: 0.4083 | RMSE: 0.5108 | RÂ²: 0.7206\n",
      "Epoch 3/20 | Train Loss: 0.4159 | Val Loss: 0.3450 | MAE: 0.2695 | RMSE: 0.3450 | RÂ²: 0.8726\n",
      "Epoch 4/20 | Train Loss: 0.3735 | Val Loss: 0.3243 | MAE: 0.2439 | RMSE: 0.3243 | RÂ²: 0.8874\n",
      "Epoch 5/20 | Train Loss: 0.3512 | Val Loss: 0.2847 | MAE: 0.2092 | RMSE: 0.2847 | RÂ²: 0.9132\n",
      "Epoch 6/20 | Train Loss: 0.3259 | Val Loss: 0.2683 | MAE: 0.1913 | RMSE: 0.2683 | RÂ²: 0.9229\n",
      "Epoch 7/20 | Train Loss: 0.3146 | Val Loss: 0.2631 | MAE: 0.1836 | RMSE: 0.2631 | RÂ²: 0.9259\n",
      "Epoch 8/20 | Train Loss: 0.3046 | Val Loss: 0.2760 | MAE: 0.1898 | RMSE: 0.2760 | RÂ²: 0.9184\n",
      "Epoch 9/20 | Train Loss: 0.2936 | Val Loss: 0.2536 | MAE: 0.1764 | RMSE: 0.2536 | RÂ²: 0.9311\n",
      "Epoch 10/20 | Train Loss: 0.2868 | Val Loss: 0.2608 | MAE: 0.1744 | RMSE: 0.2608 | RÂ²: 0.9272\n",
      "Epoch 11/20 | Train Loss: 0.2816 | Val Loss: 0.2583 | MAE: 0.1787 | RMSE: 0.2583 | RÂ²: 0.9286\n",
      "Epoch 12/20 | Train Loss: 0.2752 | Val Loss: 0.2500 | MAE: 0.1703 | RMSE: 0.2500 | RÂ²: 0.9331\n",
      "Epoch 13/20 | Train Loss: 0.2726 | Val Loss: 0.2605 | MAE: 0.1767 | RMSE: 0.2605 | RÂ²: 0.9274\n",
      "Epoch 14/20 | Train Loss: 0.2671 | Val Loss: 0.2593 | MAE: 0.1784 | RMSE: 0.2593 | RÂ²: 0.9280\n",
      "Epoch 15/20 | Train Loss: 0.2614 | Val Loss: 0.2662 | MAE: 0.1840 | RMSE: 0.2662 | RÂ²: 0.9241\n",
      "Epoch 16/20 | Train Loss: 0.2584 | Val Loss: 0.2554 | MAE: 0.1731 | RMSE: 0.2554 | RÂ²: 0.9302\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2500 | MAE = 0.1703\n",
      "\n",
      "ðŸ”§ [214/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9105 | Val Loss: 0.9018 | MAE: 0.7636 | RMSE: 0.9018 | RÂ²: 0.1293\n",
      "Epoch 2/20 | Train Loss: 0.7022 | Val Loss: 0.7909 | MAE: 0.6689 | RMSE: 0.7909 | RÂ²: 0.3303\n",
      "Epoch 3/20 | Train Loss: 0.5908 | Val Loss: 0.6671 | MAE: 0.5524 | RMSE: 0.6671 | RÂ²: 0.5236\n",
      "Epoch 4/20 | Train Loss: 0.5303 | Val Loss: 0.5164 | MAE: 0.4142 | RMSE: 0.5164 | RÂ²: 0.7145\n",
      "Epoch 5/20 | Train Loss: 0.5013 | Val Loss: 0.3973 | MAE: 0.3097 | RMSE: 0.3973 | RÂ²: 0.8310\n",
      "Epoch 6/20 | Train Loss: 0.4705 | Val Loss: 0.3441 | MAE: 0.2606 | RMSE: 0.3441 | RÂ²: 0.8732\n",
      "Epoch 7/20 | Train Loss: 0.4497 | Val Loss: 0.3130 | MAE: 0.2359 | RMSE: 0.3130 | RÂ²: 0.8951\n",
      "Epoch 8/20 | Train Loss: 0.4316 | Val Loss: 0.2971 | MAE: 0.2211 | RMSE: 0.2971 | RÂ²: 0.9055\n",
      "Epoch 9/20 | Train Loss: 0.4159 | Val Loss: 0.2918 | MAE: 0.2108 | RMSE: 0.2918 | RÂ²: 0.9088\n",
      "Epoch 10/20 | Train Loss: 0.4128 | Val Loss: 0.2960 | MAE: 0.2127 | RMSE: 0.2960 | RÂ²: 0.9062\n",
      "Epoch 11/20 | Train Loss: 0.4044 | Val Loss: 0.2970 | MAE: 0.2129 | RMSE: 0.2970 | RÂ²: 0.9056\n",
      "Epoch 12/20 | Train Loss: 0.3953 | Val Loss: 0.3028 | MAE: 0.2140 | RMSE: 0.3028 | RÂ²: 0.9018\n",
      "Epoch 13/20 | Train Loss: 0.3881 | Val Loss: 0.3010 | MAE: 0.2141 | RMSE: 0.3010 | RÂ²: 0.9030\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2918 | MAE = 0.2108\n",
      "\n",
      "ðŸ”§ [215/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8962 | Val Loss: 0.7748 | MAE: 0.6480 | RMSE: 0.7748 | RÂ²: 0.3574\n",
      "Epoch 2/20 | Train Loss: 0.4370 | Val Loss: 0.4809 | MAE: 0.3878 | RMSE: 0.4809 | RÂ²: 0.7524\n",
      "Epoch 3/20 | Train Loss: 0.3750 | Val Loss: 0.4482 | MAE: 0.2698 | RMSE: 0.4482 | RÂ²: 0.7849\n",
      "Epoch 4/20 | Train Loss: 0.3417 | Val Loss: 0.3693 | MAE: 0.2616 | RMSE: 0.3693 | RÂ²: 0.8540\n",
      "Epoch 5/20 | Train Loss: 0.3192 | Val Loss: 0.4189 | MAE: 0.2824 | RMSE: 0.4189 | RÂ²: 0.8121\n",
      "Epoch 6/20 | Train Loss: 0.3238 | Val Loss: 0.3609 | MAE: 0.2693 | RMSE: 0.3609 | RÂ²: 0.8605\n",
      "Epoch 7/20 | Train Loss: 0.3060 | Val Loss: 0.2965 | MAE: 0.1902 | RMSE: 0.2965 | RÂ²: 0.9059\n",
      "Epoch 8/20 | Train Loss: 0.2956 | Val Loss: 0.2988 | MAE: 0.1946 | RMSE: 0.2988 | RÂ²: 0.9044\n",
      "Epoch 9/20 | Train Loss: 0.2978 | Val Loss: 0.2629 | MAE: 0.1786 | RMSE: 0.2629 | RÂ²: 0.9260\n",
      "Epoch 10/20 | Train Loss: 0.2887 | Val Loss: 0.2761 | MAE: 0.1958 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 11/20 | Train Loss: 0.2792 | Val Loss: 0.2723 | MAE: 0.1834 | RMSE: 0.2723 | RÂ²: 0.9206\n",
      "Epoch 12/20 | Train Loss: 0.2751 | Val Loss: 0.2764 | MAE: 0.1816 | RMSE: 0.2764 | RÂ²: 0.9182\n",
      "Epoch 13/20 | Train Loss: 0.2714 | Val Loss: 0.2636 | MAE: 0.1776 | RMSE: 0.2636 | RÂ²: 0.9256\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2629 | MAE = 0.1786\n",
      "\n",
      "ðŸ”§ [216/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[512, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9340 | Val Loss: 0.8388 | MAE: 0.7094 | RMSE: 0.8388 | RÂ²: 0.2467\n",
      "Epoch 2/20 | Train Loss: 0.7151 | Val Loss: 0.6488 | MAE: 0.5396 | RMSE: 0.6488 | RÂ²: 0.5494\n",
      "Epoch 3/20 | Train Loss: 0.6068 | Val Loss: 0.5206 | MAE: 0.3875 | RMSE: 0.5206 | RÂ²: 0.7099\n",
      "Epoch 4/20 | Train Loss: 0.5479 | Val Loss: 0.3909 | MAE: 0.2818 | RMSE: 0.3909 | RÂ²: 0.8364\n",
      "Epoch 5/20 | Train Loss: 0.5048 | Val Loss: 0.4091 | MAE: 0.2987 | RMSE: 0.4091 | RÂ²: 0.8209\n",
      "Epoch 6/20 | Train Loss: 0.4670 | Val Loss: 0.4119 | MAE: 0.3177 | RMSE: 0.4119 | RÂ²: 0.8183\n",
      "Epoch 7/20 | Train Loss: 0.4431 | Val Loss: 0.4174 | MAE: 0.3220 | RMSE: 0.4174 | RÂ²: 0.8134\n",
      "Epoch 8/20 | Train Loss: 0.4267 | Val Loss: 0.4180 | MAE: 0.3151 | RMSE: 0.4180 | RÂ²: 0.8130\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3909 | MAE = 0.2818\n",
      "\n",
      "ðŸ”§ [217/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.7592 | Val Loss: 0.7861 | MAE: 0.6514 | RMSE: 0.7861 | RÂ²: 0.3385\n",
      "Epoch 2/20 | Train Loss: 0.4684 | Val Loss: 0.5408 | MAE: 0.4504 | RMSE: 0.5408 | RÂ²: 0.6869\n",
      "Epoch 3/20 | Train Loss: 0.3943 | Val Loss: 0.3402 | MAE: 0.2690 | RMSE: 0.3402 | RÂ²: 0.8761\n",
      "Epoch 4/20 | Train Loss: 0.3677 | Val Loss: 0.3223 | MAE: 0.2364 | RMSE: 0.3223 | RÂ²: 0.8888\n",
      "Epoch 5/20 | Train Loss: 0.3399 | Val Loss: 0.2863 | MAE: 0.2053 | RMSE: 0.2863 | RÂ²: 0.9122\n",
      "Epoch 6/20 | Train Loss: 0.3262 | Val Loss: 0.2819 | MAE: 0.2065 | RMSE: 0.2819 | RÂ²: 0.9149\n",
      "Epoch 7/20 | Train Loss: 0.3029 | Val Loss: 0.2780 | MAE: 0.2024 | RMSE: 0.2780 | RÂ²: 0.9173\n",
      "Epoch 8/20 | Train Loss: 0.2992 | Val Loss: 0.2700 | MAE: 0.1895 | RMSE: 0.2700 | RÂ²: 0.9219\n",
      "Epoch 9/20 | Train Loss: 0.2975 | Val Loss: 0.2787 | MAE: 0.1896 | RMSE: 0.2787 | RÂ²: 0.9168\n",
      "Epoch 10/20 | Train Loss: 0.2927 | Val Loss: 0.2574 | MAE: 0.1781 | RMSE: 0.2574 | RÂ²: 0.9290\n",
      "Epoch 11/20 | Train Loss: 0.2857 | Val Loss: 0.2632 | MAE: 0.1803 | RMSE: 0.2632 | RÂ²: 0.9258\n",
      "Epoch 12/20 | Train Loss: 0.2772 | Val Loss: 0.2636 | MAE: 0.1828 | RMSE: 0.2636 | RÂ²: 0.9256\n",
      "Epoch 13/20 | Train Loss: 0.2755 | Val Loss: 0.2706 | MAE: 0.1885 | RMSE: 0.2706 | RÂ²: 0.9216\n",
      "Epoch 14/20 | Train Loss: 0.2652 | Val Loss: 0.2699 | MAE: 0.1859 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2574 | MAE = 0.1781\n",
      "\n",
      "ðŸ”§ [218/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[1024, 512, 256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9235 | Val Loss: 0.9057 | MAE: 0.7668 | RMSE: 0.9057 | RÂ²: 0.1217\n",
      "Epoch 2/20 | Train Loss: 0.6330 | Val Loss: 0.6633 | MAE: 0.5500 | RMSE: 0.6633 | RÂ²: 0.5289\n",
      "Epoch 3/20 | Train Loss: 0.5352 | Val Loss: 0.4423 | MAE: 0.3506 | RMSE: 0.4423 | RÂ²: 0.7906\n",
      "Epoch 4/20 | Train Loss: 0.4801 | Val Loss: 0.4017 | MAE: 0.2927 | RMSE: 0.4017 | RÂ²: 0.8273\n",
      "Epoch 5/20 | Train Loss: 0.4356 | Val Loss: 0.4127 | MAE: 0.3288 | RMSE: 0.4127 | RÂ²: 0.8176\n",
      "Epoch 6/20 | Train Loss: 0.4127 | Val Loss: 0.4179 | MAE: 0.3371 | RMSE: 0.4179 | RÂ²: 0.8130\n",
      "Epoch 7/20 | Train Loss: 0.3944 | Val Loss: 0.3733 | MAE: 0.2918 | RMSE: 0.3733 | RÂ²: 0.8508\n",
      "Epoch 8/20 | Train Loss: 0.3724 | Val Loss: 0.3651 | MAE: 0.2841 | RMSE: 0.3651 | RÂ²: 0.8573\n",
      "Epoch 9/20 | Train Loss: 0.3626 | Val Loss: 0.3369 | MAE: 0.2598 | RMSE: 0.3369 | RÂ²: 0.8785\n",
      "Epoch 10/20 | Train Loss: 0.3472 | Val Loss: 0.2978 | MAE: 0.2170 | RMSE: 0.2978 | RÂ²: 0.9051\n",
      "Epoch 11/20 | Train Loss: 0.3420 | Val Loss: 0.2908 | MAE: 0.2113 | RMSE: 0.2908 | RÂ²: 0.9095\n",
      "Epoch 12/20 | Train Loss: 0.3336 | Val Loss: 0.2890 | MAE: 0.2061 | RMSE: 0.2890 | RÂ²: 0.9106\n",
      "Epoch 13/20 | Train Loss: 0.3229 | Val Loss: 0.2817 | MAE: 0.2009 | RMSE: 0.2817 | RÂ²: 0.9151\n",
      "Epoch 14/20 | Train Loss: 0.3186 | Val Loss: 0.2788 | MAE: 0.1954 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "Epoch 15/20 | Train Loss: 0.3161 | Val Loss: 0.2744 | MAE: 0.1928 | RMSE: 0.2744 | RÂ²: 0.9194\n",
      "Epoch 16/20 | Train Loss: 0.3074 | Val Loss: 0.2748 | MAE: 0.1871 | RMSE: 0.2748 | RÂ²: 0.9192\n",
      "Epoch 17/20 | Train Loss: 0.3092 | Val Loss: 0.2841 | MAE: 0.1962 | RMSE: 0.2841 | RÂ²: 0.9136\n",
      "Epoch 18/20 | Train Loss: 0.3046 | Val Loss: 0.2662 | MAE: 0.1814 | RMSE: 0.2662 | RÂ²: 0.9242\n",
      "Epoch 19/20 | Train Loss: 0.3029 | Val Loss: 0.2751 | MAE: 0.1925 | RMSE: 0.2751 | RÂ²: 0.9190\n",
      "Epoch 20/20 | Train Loss: 0.3001 | Val Loss: 0.2673 | MAE: 0.1831 | RMSE: 0.2673 | RÂ²: 0.9235\n",
      "âœ… RMSE = 0.2662 | MAE = 0.1814\n",
      "\n",
      "ðŸ”§ [219/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[512, 256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9547 | Val Loss: 0.9891 | MAE: 0.8404 | RMSE: 0.9891 | RÂ²: -0.0473\n",
      "Epoch 2/20 | Train Loss: 0.8010 | Val Loss: 0.9143 | MAE: 0.7787 | RMSE: 0.9143 | RÂ²: 0.1051\n",
      "Epoch 3/20 | Train Loss: 0.7003 | Val Loss: 0.8207 | MAE: 0.6979 | RMSE: 0.8207 | RÂ²: 0.2789\n",
      "Epoch 4/20 | Train Loss: 0.6260 | Val Loss: 0.7010 | MAE: 0.5907 | RMSE: 0.7010 | RÂ²: 0.4740\n",
      "Epoch 5/20 | Train Loss: 0.5625 | Val Loss: 0.5599 | MAE: 0.4618 | RMSE: 0.5599 | RÂ²: 0.6644\n",
      "Epoch 6/20 | Train Loss: 0.5207 | Val Loss: 0.4414 | MAE: 0.3512 | RMSE: 0.4414 | RÂ²: 0.7914\n",
      "Epoch 7/20 | Train Loss: 0.4902 | Val Loss: 0.3677 | MAE: 0.2832 | RMSE: 0.3677 | RÂ²: 0.8553\n",
      "Epoch 8/20 | Train Loss: 0.4635 | Val Loss: 0.3327 | MAE: 0.2532 | RMSE: 0.3327 | RÂ²: 0.8815\n",
      "Epoch 9/20 | Train Loss: 0.4393 | Val Loss: 0.3154 | MAE: 0.2358 | RMSE: 0.3154 | RÂ²: 0.8935\n",
      "Epoch 10/20 | Train Loss: 0.4222 | Val Loss: 0.3051 | MAE: 0.2236 | RMSE: 0.3051 | RÂ²: 0.9003\n",
      "Epoch 11/20 | Train Loss: 0.4101 | Val Loss: 0.2997 | MAE: 0.2173 | RMSE: 0.2997 | RÂ²: 0.9038\n",
      "Epoch 12/20 | Train Loss: 0.4006 | Val Loss: 0.2940 | MAE: 0.2133 | RMSE: 0.2940 | RÂ²: 0.9075\n",
      "Epoch 13/20 | Train Loss: 0.3881 | Val Loss: 0.2940 | MAE: 0.2124 | RMSE: 0.2940 | RÂ²: 0.9075\n",
      "Epoch 14/20 | Train Loss: 0.3829 | Val Loss: 0.2927 | MAE: 0.2108 | RMSE: 0.2927 | RÂ²: 0.9083\n",
      "Epoch 15/20 | Train Loss: 0.3738 | Val Loss: 0.2872 | MAE: 0.2061 | RMSE: 0.2872 | RÂ²: 0.9117\n",
      "Epoch 16/20 | Train Loss: 0.3710 | Val Loss: 0.2826 | MAE: 0.2026 | RMSE: 0.2826 | RÂ²: 0.9145\n",
      "Epoch 17/20 | Train Loss: 0.3602 | Val Loss: 0.2802 | MAE: 0.1999 | RMSE: 0.2802 | RÂ²: 0.9159\n",
      "Epoch 18/20 | Train Loss: 0.3545 | Val Loss: 0.2824 | MAE: 0.2014 | RMSE: 0.2824 | RÂ²: 0.9146\n",
      "Epoch 19/20 | Train Loss: 0.3489 | Val Loss: 0.2815 | MAE: 0.2009 | RMSE: 0.2815 | RÂ²: 0.9152\n",
      "Epoch 20/20 | Train Loss: 0.3503 | Val Loss: 0.2804 | MAE: 0.1991 | RMSE: 0.2804 | RÂ²: 0.9158\n",
      "âœ… RMSE = 0.2802 | MAE = 0.1999\n",
      "\n",
      "ðŸ”§ [220/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[512, 256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9863 | Val Loss: 0.8339 | MAE: 0.7074 | RMSE: 0.8339 | RÂ²: 0.2556\n",
      "Epoch 2/20 | Train Loss: 0.7489 | Val Loss: 0.6415 | MAE: 0.5359 | RMSE: 0.6415 | RÂ²: 0.5595\n",
      "Epoch 3/20 | Train Loss: 0.6359 | Val Loss: 0.4902 | MAE: 0.3786 | RMSE: 0.4902 | RÂ²: 0.7427\n",
      "Epoch 4/20 | Train Loss: 0.5592 | Val Loss: 0.4241 | MAE: 0.3034 | RMSE: 0.4241 | RÂ²: 0.8074\n",
      "Epoch 5/20 | Train Loss: 0.5136 | Val Loss: 0.3893 | MAE: 0.2804 | RMSE: 0.3893 | RÂ²: 0.8377\n",
      "Epoch 6/20 | Train Loss: 0.4838 | Val Loss: 0.3589 | MAE: 0.2605 | RMSE: 0.3589 | RÂ²: 0.8621\n",
      "Epoch 7/20 | Train Loss: 0.4602 | Val Loss: 0.3394 | MAE: 0.2491 | RMSE: 0.3394 | RÂ²: 0.8767\n",
      "Epoch 8/20 | Train Loss: 0.4443 | Val Loss: 0.3313 | MAE: 0.2425 | RMSE: 0.3313 | RÂ²: 0.8825\n",
      "Epoch 9/20 | Train Loss: 0.4274 | Val Loss: 0.3251 | MAE: 0.2331 | RMSE: 0.3251 | RÂ²: 0.8868\n",
      "Epoch 10/20 | Train Loss: 0.4116 | Val Loss: 0.3218 | MAE: 0.2284 | RMSE: 0.3218 | RÂ²: 0.8892\n",
      "Epoch 11/20 | Train Loss: 0.4049 | Val Loss: 0.3099 | MAE: 0.2196 | RMSE: 0.3099 | RÂ²: 0.8972\n",
      "Epoch 12/20 | Train Loss: 0.3919 | Val Loss: 0.3043 | MAE: 0.2173 | RMSE: 0.3043 | RÂ²: 0.9009\n",
      "Epoch 13/20 | Train Loss: 0.3776 | Val Loss: 0.2999 | MAE: 0.2135 | RMSE: 0.2999 | RÂ²: 0.9037\n",
      "Epoch 14/20 | Train Loss: 0.3758 | Val Loss: 0.2968 | MAE: 0.2093 | RMSE: 0.2968 | RÂ²: 0.9057\n",
      "Epoch 15/20 | Train Loss: 0.3690 | Val Loss: 0.2969 | MAE: 0.2084 | RMSE: 0.2969 | RÂ²: 0.9056\n",
      "Epoch 16/20 | Train Loss: 0.3663 | Val Loss: 0.2958 | MAE: 0.2085 | RMSE: 0.2958 | RÂ²: 0.9063\n",
      "Epoch 17/20 | Train Loss: 0.3548 | Val Loss: 0.2908 | MAE: 0.2059 | RMSE: 0.2908 | RÂ²: 0.9095\n",
      "Epoch 18/20 | Train Loss: 0.3520 | Val Loss: 0.2893 | MAE: 0.2059 | RMSE: 0.2893 | RÂ²: 0.9104\n",
      "Epoch 19/20 | Train Loss: 0.3468 | Val Loss: 0.2881 | MAE: 0.2038 | RMSE: 0.2881 | RÂ²: 0.9111\n",
      "Epoch 20/20 | Train Loss: 0.3441 | Val Loss: 0.2901 | MAE: 0.2060 | RMSE: 0.2901 | RÂ²: 0.9099\n",
      "âœ… RMSE = 0.2881 | MAE = 0.2038\n",
      "\n",
      "ðŸ”§ [221/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[1024, 256, 1024], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9384 | Val Loss: 0.7116 | MAE: 0.5993 | RMSE: 0.7116 | RÂ²: 0.4578\n",
      "Epoch 2/20 | Train Loss: 0.6521 | Val Loss: 0.5226 | MAE: 0.4094 | RMSE: 0.5226 | RÂ²: 0.7077\n",
      "Epoch 3/20 | Train Loss: 0.5530 | Val Loss: 0.4039 | MAE: 0.3206 | RMSE: 0.4039 | RÂ²: 0.8253\n",
      "Epoch 4/20 | Train Loss: 0.5063 | Val Loss: 0.3607 | MAE: 0.2676 | RMSE: 0.3607 | RÂ²: 0.8607\n",
      "Epoch 5/20 | Train Loss: 0.4664 | Val Loss: 0.3475 | MAE: 0.2563 | RMSE: 0.3475 | RÂ²: 0.8707\n",
      "Epoch 6/20 | Train Loss: 0.4350 | Val Loss: 0.3560 | MAE: 0.2581 | RMSE: 0.3560 | RÂ²: 0.8643\n",
      "Epoch 7/20 | Train Loss: 0.4171 | Val Loss: 0.3449 | MAE: 0.2401 | RMSE: 0.3449 | RÂ²: 0.8727\n",
      "Epoch 8/20 | Train Loss: 0.3878 | Val Loss: 0.2994 | MAE: 0.2060 | RMSE: 0.2994 | RÂ²: 0.9040\n",
      "Epoch 9/20 | Train Loss: 0.3671 | Val Loss: 0.2978 | MAE: 0.2106 | RMSE: 0.2978 | RÂ²: 0.9050\n",
      "Epoch 10/20 | Train Loss: 0.3595 | Val Loss: 0.2832 | MAE: 0.1961 | RMSE: 0.2832 | RÂ²: 0.9141\n",
      "Epoch 11/20 | Train Loss: 0.3360 | Val Loss: 0.2856 | MAE: 0.1972 | RMSE: 0.2856 | RÂ²: 0.9127\n",
      "Epoch 12/20 | Train Loss: 0.3290 | Val Loss: 0.2745 | MAE: 0.1884 | RMSE: 0.2745 | RÂ²: 0.9193\n",
      "Epoch 13/20 | Train Loss: 0.3176 | Val Loss: 0.2763 | MAE: 0.1887 | RMSE: 0.2763 | RÂ²: 0.9183\n",
      "Epoch 14/20 | Train Loss: 0.3075 | Val Loss: 0.2769 | MAE: 0.1933 | RMSE: 0.2769 | RÂ²: 0.9179\n",
      "Epoch 15/20 | Train Loss: 0.2977 | Val Loss: 0.2742 | MAE: 0.1897 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "Epoch 16/20 | Train Loss: 0.2948 | Val Loss: 0.2794 | MAE: 0.1962 | RMSE: 0.2794 | RÂ²: 0.9165\n",
      "Epoch 17/20 | Train Loss: 0.2912 | Val Loss: 0.2724 | MAE: 0.1898 | RMSE: 0.2724 | RÂ²: 0.9205\n",
      "Epoch 18/20 | Train Loss: 0.2851 | Val Loss: 0.2837 | MAE: 0.1995 | RMSE: 0.2837 | RÂ²: 0.9139\n",
      "Epoch 19/20 | Train Loss: 0.2844 | Val Loss: 0.2663 | MAE: 0.1843 | RMSE: 0.2663 | RÂ²: 0.9241\n",
      "Epoch 20/20 | Train Loss: 0.2790 | Val Loss: 0.2932 | MAE: 0.2020 | RMSE: 0.2932 | RÂ²: 0.9079\n",
      "âœ… RMSE = 0.2663 | MAE = 0.1843\n",
      "\n",
      "ðŸ”§ [222/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[512, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9867 | Val Loss: 0.9291 | MAE: 0.7847 | RMSE: 0.9291 | RÂ²: 0.0758\n",
      "Epoch 2/20 | Train Loss: 0.8025 | Val Loss: 0.8393 | MAE: 0.7114 | RMSE: 0.8393 | RÂ²: 0.2459\n",
      "Epoch 3/20 | Train Loss: 0.7041 | Val Loss: 0.7398 | MAE: 0.6246 | RMSE: 0.7398 | RÂ²: 0.4140\n",
      "Epoch 4/20 | Train Loss: 0.6224 | Val Loss: 0.6091 | MAE: 0.5055 | RMSE: 0.6091 | RÂ²: 0.6028\n",
      "Epoch 5/20 | Train Loss: 0.5674 | Val Loss: 0.4904 | MAE: 0.3914 | RMSE: 0.4904 | RÂ²: 0.7425\n",
      "Epoch 6/20 | Train Loss: 0.5390 | Val Loss: 0.4042 | MAE: 0.3040 | RMSE: 0.4042 | RÂ²: 0.8251\n",
      "Epoch 7/20 | Train Loss: 0.5193 | Val Loss: 0.3741 | MAE: 0.2658 | RMSE: 0.3741 | RÂ²: 0.8502\n",
      "Epoch 8/20 | Train Loss: 0.4902 | Val Loss: 0.3506 | MAE: 0.2492 | RMSE: 0.3506 | RÂ²: 0.8684\n",
      "Epoch 9/20 | Train Loss: 0.4751 | Val Loss: 0.3241 | MAE: 0.2358 | RMSE: 0.3241 | RÂ²: 0.8876\n",
      "Epoch 10/20 | Train Loss: 0.4622 | Val Loss: 0.3132 | MAE: 0.2272 | RMSE: 0.3132 | RÂ²: 0.8950\n",
      "Epoch 11/20 | Train Loss: 0.4469 | Val Loss: 0.3083 | MAE: 0.2242 | RMSE: 0.3083 | RÂ²: 0.8983\n",
      "Epoch 12/20 | Train Loss: 0.4351 | Val Loss: 0.3054 | MAE: 0.2241 | RMSE: 0.3054 | RÂ²: 0.9002\n",
      "Epoch 13/20 | Train Loss: 0.4326 | Val Loss: 0.3033 | MAE: 0.2206 | RMSE: 0.3033 | RÂ²: 0.9015\n",
      "Epoch 14/20 | Train Loss: 0.4237 | Val Loss: 0.3065 | MAE: 0.2208 | RMSE: 0.3065 | RÂ²: 0.8994\n",
      "Epoch 15/20 | Train Loss: 0.4156 | Val Loss: 0.3062 | MAE: 0.2213 | RMSE: 0.3062 | RÂ²: 0.8996\n",
      "Epoch 16/20 | Train Loss: 0.4075 | Val Loss: 0.3068 | MAE: 0.2207 | RMSE: 0.3068 | RÂ²: 0.8992\n",
      "Epoch 17/20 | Train Loss: 0.4046 | Val Loss: 0.3052 | MAE: 0.2192 | RMSE: 0.3052 | RÂ²: 0.9003\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3033 | MAE = 0.2206\n",
      "\n",
      "ðŸ”§ [223/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9208 | Val Loss: 0.7450 | MAE: 0.6281 | RMSE: 0.7450 | RÂ²: 0.4058\n",
      "Epoch 2/20 | Train Loss: 0.6996 | Val Loss: 0.5485 | MAE: 0.4561 | RMSE: 0.5485 | RÂ²: 0.6779\n",
      "Epoch 3/20 | Train Loss: 0.6107 | Val Loss: 0.4135 | MAE: 0.3372 | RMSE: 0.4135 | RÂ²: 0.8169\n",
      "Epoch 4/20 | Train Loss: 0.5556 | Val Loss: 0.3509 | MAE: 0.2658 | RMSE: 0.3509 | RÂ²: 0.8682\n",
      "Epoch 5/20 | Train Loss: 0.5055 | Val Loss: 0.3237 | MAE: 0.2339 | RMSE: 0.3237 | RÂ²: 0.8878\n",
      "Epoch 6/20 | Train Loss: 0.4751 | Val Loss: 0.3302 | MAE: 0.2324 | RMSE: 0.3302 | RÂ²: 0.8833\n",
      "Epoch 7/20 | Train Loss: 0.4503 | Val Loss: 0.3337 | MAE: 0.2334 | RMSE: 0.3337 | RÂ²: 0.8808\n",
      "Epoch 8/20 | Train Loss: 0.4386 | Val Loss: 0.3232 | MAE: 0.2290 | RMSE: 0.3232 | RÂ²: 0.8882\n",
      "Epoch 9/20 | Train Loss: 0.4136 | Val Loss: 0.3253 | MAE: 0.2326 | RMSE: 0.3253 | RÂ²: 0.8867\n",
      "Epoch 10/20 | Train Loss: 0.3982 | Val Loss: 0.3153 | MAE: 0.2230 | RMSE: 0.3153 | RÂ²: 0.8936\n",
      "Epoch 11/20 | Train Loss: 0.3890 | Val Loss: 0.3139 | MAE: 0.2163 | RMSE: 0.3139 | RÂ²: 0.8945\n",
      "Epoch 12/20 | Train Loss: 0.3779 | Val Loss: 0.3061 | MAE: 0.2131 | RMSE: 0.3061 | RÂ²: 0.8997\n",
      "Epoch 13/20 | Train Loss: 0.3654 | Val Loss: 0.3042 | MAE: 0.2089 | RMSE: 0.3042 | RÂ²: 0.9009\n",
      "Epoch 14/20 | Train Loss: 0.3608 | Val Loss: 0.3056 | MAE: 0.2095 | RMSE: 0.3056 | RÂ²: 0.9000\n",
      "Epoch 15/20 | Train Loss: 0.3594 | Val Loss: 0.3016 | MAE: 0.2095 | RMSE: 0.3016 | RÂ²: 0.9026\n",
      "Epoch 16/20 | Train Loss: 0.3476 | Val Loss: 0.2964 | MAE: 0.2071 | RMSE: 0.2964 | RÂ²: 0.9060\n",
      "Epoch 17/20 | Train Loss: 0.3436 | Val Loss: 0.2949 | MAE: 0.2072 | RMSE: 0.2949 | RÂ²: 0.9069\n",
      "Epoch 18/20 | Train Loss: 0.3379 | Val Loss: 0.3010 | MAE: 0.2112 | RMSE: 0.3010 | RÂ²: 0.9030\n",
      "Epoch 19/20 | Train Loss: 0.3357 | Val Loss: 0.2968 | MAE: 0.2069 | RMSE: 0.2968 | RÂ²: 0.9057\n",
      "Epoch 20/20 | Train Loss: 0.3319 | Val Loss: 0.3018 | MAE: 0.2078 | RMSE: 0.3018 | RÂ²: 0.9025\n",
      "âœ… RMSE = 0.2949 | MAE = 0.2072\n",
      "\n",
      "ðŸ”§ [224/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[512, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9873 | Val Loss: 0.9180 | MAE: 0.7750 | RMSE: 0.9180 | RÂ²: 0.0978\n",
      "Epoch 2/20 | Train Loss: 0.8285 | Val Loss: 0.8128 | MAE: 0.6885 | RMSE: 0.8128 | RÂ²: 0.2928\n",
      "Epoch 3/20 | Train Loss: 0.7528 | Val Loss: 0.6911 | MAE: 0.5842 | RMSE: 0.6911 | RÂ²: 0.4887\n",
      "Epoch 4/20 | Train Loss: 0.6814 | Val Loss: 0.5596 | MAE: 0.4650 | RMSE: 0.5596 | RÂ²: 0.6647\n",
      "Epoch 5/20 | Train Loss: 0.6337 | Val Loss: 0.4458 | MAE: 0.3573 | RMSE: 0.4458 | RÂ²: 0.7873\n",
      "Epoch 6/20 | Train Loss: 0.6015 | Val Loss: 0.3770 | MAE: 0.2880 | RMSE: 0.3770 | RÂ²: 0.8478\n",
      "Epoch 7/20 | Train Loss: 0.5690 | Val Loss: 0.3624 | MAE: 0.2628 | RMSE: 0.3624 | RÂ²: 0.8594\n",
      "Epoch 8/20 | Train Loss: 0.5477 | Val Loss: 0.3609 | MAE: 0.2536 | RMSE: 0.3609 | RÂ²: 0.8605\n",
      "Epoch 9/20 | Train Loss: 0.5306 | Val Loss: 0.3536 | MAE: 0.2492 | RMSE: 0.3536 | RÂ²: 0.8662\n",
      "Epoch 10/20 | Train Loss: 0.5202 | Val Loss: 0.3356 | MAE: 0.2436 | RMSE: 0.3356 | RÂ²: 0.8794\n",
      "Epoch 11/20 | Train Loss: 0.5039 | Val Loss: 0.3392 | MAE: 0.2435 | RMSE: 0.3392 | RÂ²: 0.8768\n",
      "Epoch 12/20 | Train Loss: 0.4963 | Val Loss: 0.3434 | MAE: 0.2423 | RMSE: 0.3434 | RÂ²: 0.8737\n",
      "Epoch 13/20 | Train Loss: 0.4905 | Val Loss: 0.3444 | MAE: 0.2444 | RMSE: 0.3444 | RÂ²: 0.8730\n",
      "Epoch 14/20 | Train Loss: 0.4754 | Val Loss: 0.3391 | MAE: 0.2431 | RMSE: 0.3391 | RÂ²: 0.8769\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3356 | MAE = 0.2436\n",
      "\n",
      "ðŸ”§ [225/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.7808 | Val Loss: 0.9060 | MAE: 0.7602 | RMSE: 0.9060 | RÂ²: 0.1213\n",
      "Epoch 2/20 | Train Loss: 0.5272 | Val Loss: 0.7545 | MAE: 0.6265 | RMSE: 0.7545 | RÂ²: 0.3905\n",
      "Epoch 3/20 | Train Loss: 0.4656 | Val Loss: 0.5323 | MAE: 0.4475 | RMSE: 0.5323 | RÂ²: 0.6967\n",
      "Epoch 4/20 | Train Loss: 0.4250 | Val Loss: 0.4048 | MAE: 0.3275 | RMSE: 0.4048 | RÂ²: 0.8245\n",
      "Epoch 5/20 | Train Loss: 0.4093 | Val Loss: 0.3091 | MAE: 0.2232 | RMSE: 0.3091 | RÂ²: 0.8977\n",
      "Epoch 6/20 | Train Loss: 0.3820 | Val Loss: 0.2915 | MAE: 0.2121 | RMSE: 0.2915 | RÂ²: 0.9090\n",
      "Epoch 7/20 | Train Loss: 0.3693 | Val Loss: 0.2933 | MAE: 0.2132 | RMSE: 0.2933 | RÂ²: 0.9079\n",
      "Epoch 8/20 | Train Loss: 0.3551 | Val Loss: 0.3069 | MAE: 0.2307 | RMSE: 0.3069 | RÂ²: 0.8992\n",
      "Epoch 9/20 | Train Loss: 0.3500 | Val Loss: 0.2888 | MAE: 0.2068 | RMSE: 0.2888 | RÂ²: 0.9107\n",
      "Epoch 10/20 | Train Loss: 0.3329 | Val Loss: 0.2939 | MAE: 0.2173 | RMSE: 0.2939 | RÂ²: 0.9075\n",
      "Epoch 11/20 | Train Loss: 0.3290 | Val Loss: 0.3012 | MAE: 0.2212 | RMSE: 0.3012 | RÂ²: 0.9029\n",
      "Epoch 12/20 | Train Loss: 0.3160 | Val Loss: 0.2808 | MAE: 0.1996 | RMSE: 0.2808 | RÂ²: 0.9156\n",
      "Epoch 13/20 | Train Loss: 0.3145 | Val Loss: 0.3023 | MAE: 0.2163 | RMSE: 0.3023 | RÂ²: 0.9022\n",
      "Epoch 14/20 | Train Loss: 0.3071 | Val Loss: 0.2744 | MAE: 0.1933 | RMSE: 0.2744 | RÂ²: 0.9194\n",
      "Epoch 15/20 | Train Loss: 0.3015 | Val Loss: 0.2740 | MAE: 0.1925 | RMSE: 0.2740 | RÂ²: 0.9196\n",
      "Epoch 16/20 | Train Loss: 0.2983 | Val Loss: 0.2777 | MAE: 0.1976 | RMSE: 0.2777 | RÂ²: 0.9174\n",
      "Epoch 17/20 | Train Loss: 0.2964 | Val Loss: 0.2775 | MAE: 0.1946 | RMSE: 0.2775 | RÂ²: 0.9175\n",
      "Epoch 18/20 | Train Loss: 0.2919 | Val Loss: 0.2827 | MAE: 0.1973 | RMSE: 0.2827 | RÂ²: 0.9144\n",
      "Epoch 19/20 | Train Loss: 0.2888 | Val Loss: 0.2759 | MAE: 0.1915 | RMSE: 0.2759 | RÂ²: 0.9185\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2740 | MAE = 0.1925\n",
      "\n",
      "ðŸ”§ [226/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 1.0494 | Val Loss: 0.8790 | MAE: 0.7429 | RMSE: 0.8790 | RÂ²: 0.1728\n",
      "Epoch 2/20 | Train Loss: 0.8833 | Val Loss: 0.7668 | MAE: 0.6380 | RMSE: 0.7668 | RÂ²: 0.3706\n",
      "Epoch 3/20 | Train Loss: 0.8303 | Val Loss: 0.6680 | MAE: 0.5449 | RMSE: 0.6680 | RÂ²: 0.5223\n",
      "Epoch 4/20 | Train Loss: 0.7573 | Val Loss: 0.5754 | MAE: 0.4735 | RMSE: 0.5754 | RÂ²: 0.6455\n",
      "Epoch 5/20 | Train Loss: 0.7176 | Val Loss: 0.5167 | MAE: 0.4217 | RMSE: 0.5167 | RÂ²: 0.7142\n",
      "Epoch 6/20 | Train Loss: 0.6829 | Val Loss: 0.4693 | MAE: 0.3741 | RMSE: 0.4693 | RÂ²: 0.7642\n",
      "Epoch 7/20 | Train Loss: 0.6538 | Val Loss: 0.4193 | MAE: 0.3280 | RMSE: 0.4193 | RÂ²: 0.8118\n",
      "Epoch 8/20 | Train Loss: 0.6236 | Val Loss: 0.3769 | MAE: 0.2900 | RMSE: 0.3769 | RÂ²: 0.8479\n",
      "Epoch 9/20 | Train Loss: 0.5997 | Val Loss: 0.3533 | MAE: 0.2691 | RMSE: 0.3533 | RÂ²: 0.8664\n",
      "Epoch 10/20 | Train Loss: 0.5937 | Val Loss: 0.3467 | MAE: 0.2631 | RMSE: 0.3467 | RÂ²: 0.8713\n",
      "Epoch 11/20 | Train Loss: 0.5643 | Val Loss: 0.3441 | MAE: 0.2572 | RMSE: 0.3441 | RÂ²: 0.8732\n",
      "Epoch 12/20 | Train Loss: 0.5556 | Val Loss: 0.3435 | MAE: 0.2530 | RMSE: 0.3435 | RÂ²: 0.8736\n",
      "Epoch 13/20 | Train Loss: 0.5417 | Val Loss: 0.3380 | MAE: 0.2470 | RMSE: 0.3380 | RÂ²: 0.8777\n",
      "Epoch 14/20 | Train Loss: 0.5257 | Val Loss: 0.3304 | MAE: 0.2413 | RMSE: 0.3304 | RÂ²: 0.8832\n",
      "Epoch 15/20 | Train Loss: 0.5100 | Val Loss: 0.3293 | MAE: 0.2395 | RMSE: 0.3293 | RÂ²: 0.8839\n",
      "Epoch 16/20 | Train Loss: 0.4959 | Val Loss: 0.3265 | MAE: 0.2366 | RMSE: 0.3265 | RÂ²: 0.8859\n",
      "Epoch 17/20 | Train Loss: 0.4952 | Val Loss: 0.3228 | MAE: 0.2333 | RMSE: 0.3228 | RÂ²: 0.8885\n",
      "Epoch 18/20 | Train Loss: 0.4832 | Val Loss: 0.3224 | MAE: 0.2318 | RMSE: 0.3224 | RÂ²: 0.8887\n",
      "Epoch 19/20 | Train Loss: 0.4719 | Val Loss: 0.3224 | MAE: 0.2304 | RMSE: 0.3224 | RÂ²: 0.8887\n",
      "Epoch 20/20 | Train Loss: 0.4669 | Val Loss: 0.3225 | MAE: 0.2295 | RMSE: 0.3225 | RÂ²: 0.8887\n",
      "âœ… RMSE = 0.3224 | MAE = 0.2318\n",
      "\n",
      "ðŸ”§ [227/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[512, 512, 512], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8170 | Val Loss: 0.8755 | MAE: 0.7284 | RMSE: 0.8755 | RÂ²: 0.1794\n",
      "Epoch 2/20 | Train Loss: 0.5030 | Val Loss: 0.6979 | MAE: 0.5801 | RMSE: 0.6979 | RÂ²: 0.4786\n",
      "Epoch 3/20 | Train Loss: 0.4404 | Val Loss: 0.5501 | MAE: 0.4560 | RMSE: 0.5501 | RÂ²: 0.6760\n",
      "Epoch 4/20 | Train Loss: 0.3842 | Val Loss: 0.3928 | MAE: 0.2947 | RMSE: 0.3928 | RÂ²: 0.8348\n",
      "Epoch 5/20 | Train Loss: 0.3600 | Val Loss: 0.3169 | MAE: 0.2356 | RMSE: 0.3169 | RÂ²: 0.8925\n",
      "Epoch 6/20 | Train Loss: 0.3416 | Val Loss: 0.2855 | MAE: 0.2011 | RMSE: 0.2855 | RÂ²: 0.9128\n",
      "Epoch 7/20 | Train Loss: 0.3315 | Val Loss: 0.2743 | MAE: 0.1919 | RMSE: 0.2743 | RÂ²: 0.9195\n",
      "Epoch 8/20 | Train Loss: 0.3208 | Val Loss: 0.2888 | MAE: 0.2036 | RMSE: 0.2888 | RÂ²: 0.9107\n",
      "Epoch 9/20 | Train Loss: 0.3126 | Val Loss: 0.2604 | MAE: 0.1817 | RMSE: 0.2604 | RÂ²: 0.9274\n",
      "Epoch 10/20 | Train Loss: 0.2992 | Val Loss: 0.2771 | MAE: 0.2029 | RMSE: 0.2771 | RÂ²: 0.9178\n",
      "Epoch 11/20 | Train Loss: 0.2953 | Val Loss: 0.2602 | MAE: 0.1821 | RMSE: 0.2602 | RÂ²: 0.9275\n",
      "Epoch 12/20 | Train Loss: 0.2886 | Val Loss: 0.2714 | MAE: 0.1950 | RMSE: 0.2714 | RÂ²: 0.9212\n",
      "Epoch 13/20 | Train Loss: 0.2810 | Val Loss: 0.2660 | MAE: 0.1858 | RMSE: 0.2660 | RÂ²: 0.9243\n",
      "Epoch 14/20 | Train Loss: 0.2807 | Val Loss: 0.2866 | MAE: 0.2049 | RMSE: 0.2866 | RÂ²: 0.9121\n",
      "Epoch 15/20 | Train Loss: 0.2759 | Val Loss: 0.2620 | MAE: 0.1815 | RMSE: 0.2620 | RÂ²: 0.9265\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2602 | MAE = 0.1821\n",
      "\n",
      "ðŸ”§ [228/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8386 | Val Loss: 0.6944 | MAE: 0.5853 | RMSE: 0.6944 | RÂ²: 0.4838\n",
      "Epoch 2/20 | Train Loss: 0.5829 | Val Loss: 0.4241 | MAE: 0.3446 | RMSE: 0.4241 | RÂ²: 0.8074\n",
      "Epoch 3/20 | Train Loss: 0.5070 | Val Loss: 0.3769 | MAE: 0.2853 | RMSE: 0.3769 | RÂ²: 0.8479\n",
      "Epoch 4/20 | Train Loss: 0.4578 | Val Loss: 0.3339 | MAE: 0.2483 | RMSE: 0.3339 | RÂ²: 0.8806\n",
      "Epoch 5/20 | Train Loss: 0.4212 | Val Loss: 0.3252 | MAE: 0.2431 | RMSE: 0.3252 | RÂ²: 0.8868\n",
      "Epoch 6/20 | Train Loss: 0.4014 | Val Loss: 0.3203 | MAE: 0.2338 | RMSE: 0.3203 | RÂ²: 0.8902\n",
      "Epoch 7/20 | Train Loss: 0.3919 | Val Loss: 0.2990 | MAE: 0.2152 | RMSE: 0.2990 | RÂ²: 0.9043\n",
      "Epoch 8/20 | Train Loss: 0.3686 | Val Loss: 0.2915 | MAE: 0.2100 | RMSE: 0.2915 | RÂ²: 0.9091\n",
      "Epoch 9/20 | Train Loss: 0.3609 | Val Loss: 0.2757 | MAE: 0.1956 | RMSE: 0.2757 | RÂ²: 0.9186\n",
      "Epoch 10/20 | Train Loss: 0.3493 | Val Loss: 0.2772 | MAE: 0.1987 | RMSE: 0.2772 | RÂ²: 0.9178\n",
      "Epoch 11/20 | Train Loss: 0.3371 | Val Loss: 0.2739 | MAE: 0.1952 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 12/20 | Train Loss: 0.3323 | Val Loss: 0.2806 | MAE: 0.2015 | RMSE: 0.2806 | RÂ²: 0.9157\n",
      "Epoch 13/20 | Train Loss: 0.3228 | Val Loss: 0.2776 | MAE: 0.1982 | RMSE: 0.2776 | RÂ²: 0.9175\n",
      "Epoch 14/20 | Train Loss: 0.3175 | Val Loss: 0.2843 | MAE: 0.2023 | RMSE: 0.2843 | RÂ²: 0.9135\n",
      "Epoch 15/20 | Train Loss: 0.3130 | Val Loss: 0.2756 | MAE: 0.1933 | RMSE: 0.2756 | RÂ²: 0.9187\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2739 | MAE = 0.1952\n",
      "\n",
      "ðŸ”§ [229/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[256, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 1.0170 | Val Loss: 0.9388 | MAE: 0.7893 | RMSE: 0.9388 | RÂ²: 0.0565\n",
      "Epoch 2/20 | Train Loss: 0.8129 | Val Loss: 0.8206 | MAE: 0.6935 | RMSE: 0.8206 | RÂ²: 0.2792\n",
      "Epoch 3/20 | Train Loss: 0.7308 | Val Loss: 0.7172 | MAE: 0.5957 | RMSE: 0.7172 | RÂ²: 0.4493\n",
      "Epoch 4/20 | Train Loss: 0.6738 | Val Loss: 0.6251 | MAE: 0.5131 | RMSE: 0.6251 | RÂ²: 0.5817\n",
      "Epoch 5/20 | Train Loss: 0.6092 | Val Loss: 0.5545 | MAE: 0.4524 | RMSE: 0.5545 | RÂ²: 0.6708\n",
      "Epoch 6/20 | Train Loss: 0.5642 | Val Loss: 0.4974 | MAE: 0.3980 | RMSE: 0.4974 | RÂ²: 0.7351\n",
      "Epoch 7/20 | Train Loss: 0.5281 | Val Loss: 0.4409 | MAE: 0.3424 | RMSE: 0.4409 | RÂ²: 0.7919\n",
      "Epoch 8/20 | Train Loss: 0.4932 | Val Loss: 0.3913 | MAE: 0.2983 | RMSE: 0.3913 | RÂ²: 0.8361\n",
      "Epoch 9/20 | Train Loss: 0.4715 | Val Loss: 0.3587 | MAE: 0.2696 | RMSE: 0.3587 | RÂ²: 0.8622\n",
      "Epoch 10/20 | Train Loss: 0.4480 | Val Loss: 0.3420 | MAE: 0.2553 | RMSE: 0.3420 | RÂ²: 0.8748\n",
      "Epoch 11/20 | Train Loss: 0.4345 | Val Loss: 0.3322 | MAE: 0.2480 | RMSE: 0.3322 | RÂ²: 0.8818\n",
      "Epoch 12/20 | Train Loss: 0.4307 | Val Loss: 0.3262 | MAE: 0.2442 | RMSE: 0.3262 | RÂ²: 0.8861\n",
      "Epoch 13/20 | Train Loss: 0.4200 | Val Loss: 0.3214 | MAE: 0.2403 | RMSE: 0.3214 | RÂ²: 0.8894\n",
      "Epoch 14/20 | Train Loss: 0.4140 | Val Loss: 0.3175 | MAE: 0.2355 | RMSE: 0.3175 | RÂ²: 0.8921\n",
      "Epoch 15/20 | Train Loss: 0.3972 | Val Loss: 0.3083 | MAE: 0.2276 | RMSE: 0.3083 | RÂ²: 0.8982\n",
      "Epoch 16/20 | Train Loss: 0.3923 | Val Loss: 0.3012 | MAE: 0.2212 | RMSE: 0.3012 | RÂ²: 0.9029\n",
      "Epoch 17/20 | Train Loss: 0.3923 | Val Loss: 0.2978 | MAE: 0.2175 | RMSE: 0.2978 | RÂ²: 0.9051\n",
      "Epoch 18/20 | Train Loss: 0.3866 | Val Loss: 0.2946 | MAE: 0.2164 | RMSE: 0.2946 | RÂ²: 0.9071\n",
      "Epoch 19/20 | Train Loss: 0.3740 | Val Loss: 0.2958 | MAE: 0.2166 | RMSE: 0.2958 | RÂ²: 0.9064\n",
      "Epoch 20/20 | Train Loss: 0.3736 | Val Loss: 0.2970 | MAE: 0.2162 | RMSE: 0.2970 | RÂ²: 0.9056\n",
      "âœ… RMSE = 0.2946 | MAE = 0.2164\n",
      "\n",
      "ðŸ”§ [230/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9046 | Val Loss: 0.8231 | MAE: 0.6961 | RMSE: 0.8231 | RÂ²: 0.2746\n",
      "Epoch 2/20 | Train Loss: 0.7009 | Val Loss: 0.7009 | MAE: 0.5882 | RMSE: 0.7009 | RÂ²: 0.4741\n",
      "Epoch 3/20 | Train Loss: 0.5929 | Val Loss: 0.5552 | MAE: 0.4583 | RMSE: 0.5552 | RÂ²: 0.6700\n",
      "Epoch 4/20 | Train Loss: 0.5228 | Val Loss: 0.4186 | MAE: 0.3412 | RMSE: 0.4186 | RÂ²: 0.8124\n",
      "Epoch 5/20 | Train Loss: 0.4823 | Val Loss: 0.3272 | MAE: 0.2546 | RMSE: 0.3272 | RÂ²: 0.8854\n",
      "Epoch 6/20 | Train Loss: 0.4502 | Val Loss: 0.3045 | MAE: 0.2293 | RMSE: 0.3045 | RÂ²: 0.9007\n",
      "Epoch 7/20 | Train Loss: 0.4302 | Val Loss: 0.2997 | MAE: 0.2225 | RMSE: 0.2997 | RÂ²: 0.9038\n",
      "Epoch 8/20 | Train Loss: 0.4203 | Val Loss: 0.2871 | MAE: 0.2097 | RMSE: 0.2871 | RÂ²: 0.9118\n",
      "Epoch 9/20 | Train Loss: 0.4007 | Val Loss: 0.2868 | MAE: 0.2051 | RMSE: 0.2868 | RÂ²: 0.9119\n",
      "Epoch 10/20 | Train Loss: 0.3907 | Val Loss: 0.2784 | MAE: 0.1981 | RMSE: 0.2784 | RÂ²: 0.9170\n",
      "Epoch 11/20 | Train Loss: 0.3799 | Val Loss: 0.2734 | MAE: 0.1949 | RMSE: 0.2734 | RÂ²: 0.9200\n",
      "Epoch 12/20 | Train Loss: 0.3774 | Val Loss: 0.2794 | MAE: 0.1985 | RMSE: 0.2794 | RÂ²: 0.9164\n",
      "Epoch 13/20 | Train Loss: 0.3633 | Val Loss: 0.2823 | MAE: 0.1997 | RMSE: 0.2823 | RÂ²: 0.9147\n",
      "Epoch 14/20 | Train Loss: 0.3566 | Val Loss: 0.2770 | MAE: 0.1961 | RMSE: 0.2770 | RÂ²: 0.9179\n",
      "Epoch 15/20 | Train Loss: 0.3524 | Val Loss: 0.2757 | MAE: 0.1954 | RMSE: 0.2757 | RÂ²: 0.9187\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2734 | MAE = 0.1949\n",
      "\n",
      "ðŸ”§ [231/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[256, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 1.0798 | Val Loss: 0.9608 | MAE: 0.8120 | RMSE: 0.9608 | RÂ²: 0.0117\n",
      "Epoch 2/20 | Train Loss: 0.9297 | Val Loss: 0.8579 | MAE: 0.7270 | RMSE: 0.8579 | RÂ²: 0.2122\n",
      "Epoch 3/20 | Train Loss: 0.8470 | Val Loss: 0.7626 | MAE: 0.6389 | RMSE: 0.7626 | RÂ²: 0.3773\n",
      "Epoch 4/20 | Train Loss: 0.7946 | Val Loss: 0.6846 | MAE: 0.5604 | RMSE: 0.6846 | RÂ²: 0.4983\n",
      "Epoch 5/20 | Train Loss: 0.7463 | Val Loss: 0.6148 | MAE: 0.4956 | RMSE: 0.6148 | RÂ²: 0.5953\n",
      "Epoch 6/20 | Train Loss: 0.7151 | Val Loss: 0.5505 | MAE: 0.4435 | RMSE: 0.5505 | RÂ²: 0.6755\n",
      "Epoch 7/20 | Train Loss: 0.6641 | Val Loss: 0.4996 | MAE: 0.3990 | RMSE: 0.4996 | RÂ²: 0.7328\n",
      "Epoch 8/20 | Train Loss: 0.6460 | Val Loss: 0.4565 | MAE: 0.3599 | RMSE: 0.4565 | RÂ²: 0.7769\n",
      "Epoch 9/20 | Train Loss: 0.6263 | Val Loss: 0.4228 | MAE: 0.3288 | RMSE: 0.4228 | RÂ²: 0.8086\n",
      "Epoch 10/20 | Train Loss: 0.6055 | Val Loss: 0.3990 | MAE: 0.3088 | RMSE: 0.3990 | RÂ²: 0.8296\n",
      "Epoch 11/20 | Train Loss: 0.5869 | Val Loss: 0.3792 | MAE: 0.2925 | RMSE: 0.3792 | RÂ²: 0.8460\n",
      "Epoch 12/20 | Train Loss: 0.5791 | Val Loss: 0.3701 | MAE: 0.2844 | RMSE: 0.3701 | RÂ²: 0.8534\n",
      "Epoch 13/20 | Train Loss: 0.5599 | Val Loss: 0.3652 | MAE: 0.2771 | RMSE: 0.3652 | RÂ²: 0.8572\n",
      "Epoch 14/20 | Train Loss: 0.5452 | Val Loss: 0.3543 | MAE: 0.2671 | RMSE: 0.3543 | RÂ²: 0.8656\n",
      "Epoch 15/20 | Train Loss: 0.5297 | Val Loss: 0.3429 | MAE: 0.2587 | RMSE: 0.3429 | RÂ²: 0.8741\n",
      "Epoch 16/20 | Train Loss: 0.5354 | Val Loss: 0.3373 | MAE: 0.2535 | RMSE: 0.3373 | RÂ²: 0.8782\n",
      "Epoch 17/20 | Train Loss: 0.5246 | Val Loss: 0.3373 | MAE: 0.2523 | RMSE: 0.3373 | RÂ²: 0.8782\n",
      "Epoch 18/20 | Train Loss: 0.5156 | Val Loss: 0.3357 | MAE: 0.2501 | RMSE: 0.3357 | RÂ²: 0.8793\n",
      "Epoch 19/20 | Train Loss: 0.5033 | Val Loss: 0.3291 | MAE: 0.2453 | RMSE: 0.3291 | RÂ²: 0.8841\n",
      "Epoch 20/20 | Train Loss: 0.4978 | Val Loss: 0.3240 | MAE: 0.2411 | RMSE: 0.3240 | RÂ²: 0.8876\n",
      "âœ… RMSE = 0.3240 | MAE = 0.2411\n",
      "\n",
      "ðŸ”§ [232/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8031 | Val Loss: 0.7514 | MAE: 0.6321 | RMSE: 0.7514 | RÂ²: 0.3955\n",
      "Epoch 2/20 | Train Loss: 0.5300 | Val Loss: 0.5789 | MAE: 0.4800 | RMSE: 0.5789 | RÂ²: 0.6413\n",
      "Epoch 3/20 | Train Loss: 0.4380 | Val Loss: 0.4007 | MAE: 0.3319 | RMSE: 0.4007 | RÂ²: 0.8281\n",
      "Epoch 4/20 | Train Loss: 0.3936 | Val Loss: 0.3669 | MAE: 0.2890 | RMSE: 0.3669 | RÂ²: 0.8559\n",
      "Epoch 5/20 | Train Loss: 0.3596 | Val Loss: 0.2890 | MAE: 0.2148 | RMSE: 0.2890 | RÂ²: 0.9106\n",
      "Epoch 6/20 | Train Loss: 0.3445 | Val Loss: 0.2732 | MAE: 0.1958 | RMSE: 0.2732 | RÂ²: 0.9201\n",
      "Epoch 7/20 | Train Loss: 0.3321 | Val Loss: 0.2770 | MAE: 0.1997 | RMSE: 0.2770 | RÂ²: 0.9179\n",
      "Epoch 8/20 | Train Loss: 0.3169 | Val Loss: 0.2615 | MAE: 0.1842 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 9/20 | Train Loss: 0.3068 | Val Loss: 0.2675 | MAE: 0.1866 | RMSE: 0.2675 | RÂ²: 0.9234\n",
      "Epoch 10/20 | Train Loss: 0.3019 | Val Loss: 0.2615 | MAE: 0.1828 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 11/20 | Train Loss: 0.2954 | Val Loss: 0.2717 | MAE: 0.1899 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "Epoch 12/20 | Train Loss: 0.2971 | Val Loss: 0.2695 | MAE: 0.1845 | RMSE: 0.2695 | RÂ²: 0.9222\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2615 | MAE = 0.1842\n",
      "\n",
      "ðŸ”§ [233/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8102 | Val Loss: 0.8274 | MAE: 0.7001 | RMSE: 0.8274 | RÂ²: 0.2672\n",
      "Epoch 2/20 | Train Loss: 0.5409 | Val Loss: 0.7072 | MAE: 0.5876 | RMSE: 0.7072 | RÂ²: 0.4646\n",
      "Epoch 3/20 | Train Loss: 0.4385 | Val Loss: 0.5146 | MAE: 0.4222 | RMSE: 0.5146 | RÂ²: 0.7165\n",
      "Epoch 4/20 | Train Loss: 0.3869 | Val Loss: 0.4026 | MAE: 0.3217 | RMSE: 0.4026 | RÂ²: 0.8265\n",
      "Epoch 5/20 | Train Loss: 0.3538 | Val Loss: 0.3463 | MAE: 0.2663 | RMSE: 0.3463 | RÂ²: 0.8716\n",
      "Epoch 6/20 | Train Loss: 0.3311 | Val Loss: 0.3072 | MAE: 0.2296 | RMSE: 0.3072 | RÂ²: 0.8990\n",
      "Epoch 7/20 | Train Loss: 0.3194 | Val Loss: 0.3041 | MAE: 0.2239 | RMSE: 0.3041 | RÂ²: 0.9010\n",
      "Epoch 8/20 | Train Loss: 0.3085 | Val Loss: 0.2989 | MAE: 0.2157 | RMSE: 0.2989 | RÂ²: 0.9043\n",
      "Epoch 9/20 | Train Loss: 0.2979 | Val Loss: 0.2910 | MAE: 0.2044 | RMSE: 0.2910 | RÂ²: 0.9093\n",
      "Epoch 10/20 | Train Loss: 0.2910 | Val Loss: 0.3104 | MAE: 0.2259 | RMSE: 0.3104 | RÂ²: 0.8969\n",
      "Epoch 11/20 | Train Loss: 0.2888 | Val Loss: 0.3023 | MAE: 0.2167 | RMSE: 0.3023 | RÂ²: 0.9022\n",
      "Epoch 12/20 | Train Loss: 0.2804 | Val Loss: 0.2884 | MAE: 0.2030 | RMSE: 0.2884 | RÂ²: 0.9110\n",
      "Epoch 13/20 | Train Loss: 0.2757 | Val Loss: 0.2863 | MAE: 0.2001 | RMSE: 0.2863 | RÂ²: 0.9123\n",
      "Epoch 14/20 | Train Loss: 0.2717 | Val Loss: 0.2934 | MAE: 0.2080 | RMSE: 0.2934 | RÂ²: 0.9079\n",
      "Epoch 15/20 | Train Loss: 0.2720 | Val Loss: 0.2911 | MAE: 0.2051 | RMSE: 0.2911 | RÂ²: 0.9093\n",
      "Epoch 16/20 | Train Loss: 0.2638 | Val Loss: 0.2900 | MAE: 0.2048 | RMSE: 0.2900 | RÂ²: 0.9100\n",
      "Epoch 17/20 | Train Loss: 0.2670 | Val Loss: 0.2856 | MAE: 0.2018 | RMSE: 0.2856 | RÂ²: 0.9127\n",
      "Epoch 18/20 | Train Loss: 0.2615 | Val Loss: 0.2851 | MAE: 0.1989 | RMSE: 0.2851 | RÂ²: 0.9130\n",
      "Epoch 19/20 | Train Loss: 0.2597 | Val Loss: 0.2866 | MAE: 0.2018 | RMSE: 0.2866 | RÂ²: 0.9121\n",
      "Epoch 20/20 | Train Loss: 0.2592 | Val Loss: 0.2894 | MAE: 0.2076 | RMSE: 0.2894 | RÂ²: 0.9103\n",
      "âœ… RMSE = 0.2851 | MAE = 0.1989\n",
      "\n",
      "ðŸ”§ [234/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9044 | Val Loss: 0.7062 | MAE: 0.5945 | RMSE: 0.7062 | RÂ²: 0.4660\n",
      "Epoch 2/20 | Train Loss: 0.6717 | Val Loss: 0.5780 | MAE: 0.4621 | RMSE: 0.5780 | RÂ²: 0.6424\n",
      "Epoch 3/20 | Train Loss: 0.5866 | Val Loss: 0.3890 | MAE: 0.3061 | RMSE: 0.3890 | RÂ²: 0.8380\n",
      "Epoch 4/20 | Train Loss: 0.5405 | Val Loss: 0.3520 | MAE: 0.2598 | RMSE: 0.3520 | RÂ²: 0.8673\n",
      "Epoch 5/20 | Train Loss: 0.5106 | Val Loss: 0.3303 | MAE: 0.2370 | RMSE: 0.3303 | RÂ²: 0.8832\n",
      "Epoch 6/20 | Train Loss: 0.4702 | Val Loss: 0.3091 | MAE: 0.2225 | RMSE: 0.3091 | RÂ²: 0.8977\n",
      "Epoch 7/20 | Train Loss: 0.4479 | Val Loss: 0.3169 | MAE: 0.2247 | RMSE: 0.3169 | RÂ²: 0.8925\n",
      "Epoch 8/20 | Train Loss: 0.4260 | Val Loss: 0.3188 | MAE: 0.2270 | RMSE: 0.3188 | RÂ²: 0.8912\n",
      "Epoch 9/20 | Train Loss: 0.4092 | Val Loss: 0.3075 | MAE: 0.2252 | RMSE: 0.3075 | RÂ²: 0.8987\n",
      "Epoch 10/20 | Train Loss: 0.3963 | Val Loss: 0.2986 | MAE: 0.2146 | RMSE: 0.2986 | RÂ²: 0.9045\n",
      "Epoch 11/20 | Train Loss: 0.3847 | Val Loss: 0.2928 | MAE: 0.2064 | RMSE: 0.2928 | RÂ²: 0.9082\n",
      "Epoch 12/20 | Train Loss: 0.3774 | Val Loss: 0.2961 | MAE: 0.2098 | RMSE: 0.2961 | RÂ²: 0.9061\n",
      "Epoch 13/20 | Train Loss: 0.3675 | Val Loss: 0.2959 | MAE: 0.2098 | RMSE: 0.2959 | RÂ²: 0.9063\n",
      "Epoch 14/20 | Train Loss: 0.3566 | Val Loss: 0.2890 | MAE: 0.2035 | RMSE: 0.2890 | RÂ²: 0.9106\n",
      "Epoch 15/20 | Train Loss: 0.3529 | Val Loss: 0.2881 | MAE: 0.2013 | RMSE: 0.2881 | RÂ²: 0.9111\n",
      "Epoch 16/20 | Train Loss: 0.3448 | Val Loss: 0.2959 | MAE: 0.2092 | RMSE: 0.2959 | RÂ²: 0.9063\n",
      "Epoch 17/20 | Train Loss: 0.3427 | Val Loss: 0.2921 | MAE: 0.2059 | RMSE: 0.2921 | RÂ²: 0.9086\n",
      "Epoch 18/20 | Train Loss: 0.3354 | Val Loss: 0.2909 | MAE: 0.2041 | RMSE: 0.2909 | RÂ²: 0.9094\n",
      "Epoch 19/20 | Train Loss: 0.3293 | Val Loss: 0.2861 | MAE: 0.2009 | RMSE: 0.2861 | RÂ²: 0.9124\n",
      "Epoch 20/20 | Train Loss: 0.3279 | Val Loss: 0.2902 | MAE: 0.2013 | RMSE: 0.2902 | RÂ²: 0.9098\n",
      "âœ… RMSE = 0.2861 | MAE = 0.2009\n",
      "\n",
      "ðŸ”§ [235/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9542 | Val Loss: 0.8539 | MAE: 0.7238 | RMSE: 0.8539 | RÂ²: 0.2194\n",
      "Epoch 2/20 | Train Loss: 0.7754 | Val Loss: 0.7199 | MAE: 0.6090 | RMSE: 0.7199 | RÂ²: 0.4452\n",
      "Epoch 3/20 | Train Loss: 0.6753 | Val Loss: 0.5846 | MAE: 0.4783 | RMSE: 0.5846 | RÂ²: 0.6342\n",
      "Epoch 4/20 | Train Loss: 0.6171 | Val Loss: 0.4580 | MAE: 0.3487 | RMSE: 0.4580 | RÂ²: 0.7755\n",
      "Epoch 5/20 | Train Loss: 0.5811 | Val Loss: 0.4107 | MAE: 0.2883 | RMSE: 0.4107 | RÂ²: 0.8194\n",
      "Epoch 6/20 | Train Loss: 0.5475 | Val Loss: 0.3990 | MAE: 0.2714 | RMSE: 0.3990 | RÂ²: 0.8295\n",
      "Epoch 7/20 | Train Loss: 0.5181 | Val Loss: 0.3778 | MAE: 0.2581 | RMSE: 0.3778 | RÂ²: 0.8472\n",
      "Epoch 8/20 | Train Loss: 0.4971 | Val Loss: 0.3688 | MAE: 0.2600 | RMSE: 0.3688 | RÂ²: 0.8544\n",
      "Epoch 9/20 | Train Loss: 0.4774 | Val Loss: 0.3824 | MAE: 0.2736 | RMSE: 0.3824 | RÂ²: 0.8435\n",
      "Epoch 10/20 | Train Loss: 0.4624 | Val Loss: 0.3885 | MAE: 0.2819 | RMSE: 0.3885 | RÂ²: 0.8385\n",
      "Epoch 11/20 | Train Loss: 0.4484 | Val Loss: 0.3843 | MAE: 0.2823 | RMSE: 0.3843 | RÂ²: 0.8419\n",
      "Epoch 12/20 | Train Loss: 0.4395 | Val Loss: 0.3781 | MAE: 0.2726 | RMSE: 0.3781 | RÂ²: 0.8470\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3688 | MAE = 0.2600\n",
      "\n",
      "ðŸ”§ [236/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.7357 | Val Loss: 0.7157 | MAE: 0.5985 | RMSE: 0.7157 | RÂ²: 0.4516\n",
      "Epoch 2/20 | Train Loss: 0.4801 | Val Loss: 0.4221 | MAE: 0.3504 | RMSE: 0.4221 | RÂ²: 0.8093\n",
      "Epoch 3/20 | Train Loss: 0.4024 | Val Loss: 0.3738 | MAE: 0.2906 | RMSE: 0.3738 | RÂ²: 0.8504\n",
      "Epoch 4/20 | Train Loss: 0.3676 | Val Loss: 0.3060 | MAE: 0.2282 | RMSE: 0.3060 | RÂ²: 0.8998\n",
      "Epoch 5/20 | Train Loss: 0.3523 | Val Loss: 0.2939 | MAE: 0.2165 | RMSE: 0.2939 | RÂ²: 0.9075\n",
      "Epoch 6/20 | Train Loss: 0.3335 | Val Loss: 0.2771 | MAE: 0.1954 | RMSE: 0.2771 | RÂ²: 0.9178\n",
      "Epoch 7/20 | Train Loss: 0.3242 | Val Loss: 0.2745 | MAE: 0.1972 | RMSE: 0.2745 | RÂ²: 0.9193\n",
      "Epoch 8/20 | Train Loss: 0.3081 | Val Loss: 0.2680 | MAE: 0.1901 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 9/20 | Train Loss: 0.2970 | Val Loss: 0.2685 | MAE: 0.1880 | RMSE: 0.2685 | RÂ²: 0.9228\n",
      "Epoch 10/20 | Train Loss: 0.2895 | Val Loss: 0.2797 | MAE: 0.1972 | RMSE: 0.2797 | RÂ²: 0.9162\n",
      "Epoch 11/20 | Train Loss: 0.2846 | Val Loss: 0.2650 | MAE: 0.1888 | RMSE: 0.2650 | RÂ²: 0.9248\n",
      "Epoch 12/20 | Train Loss: 0.2795 | Val Loss: 0.2689 | MAE: 0.1888 | RMSE: 0.2689 | RÂ²: 0.9226\n",
      "Epoch 13/20 | Train Loss: 0.2742 | Val Loss: 0.2751 | MAE: 0.1969 | RMSE: 0.2751 | RÂ²: 0.9190\n",
      "Epoch 14/20 | Train Loss: 0.2691 | Val Loss: 0.2704 | MAE: 0.1888 | RMSE: 0.2704 | RÂ²: 0.9217\n",
      "Epoch 15/20 | Train Loss: 0.2662 | Val Loss: 0.2670 | MAE: 0.1873 | RMSE: 0.2670 | RÂ²: 0.9237\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2650 | MAE = 0.1888\n",
      "\n",
      "ðŸ”§ [237/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 1.0204 | Val Loss: 0.8092 | MAE: 0.6815 | RMSE: 0.8092 | RÂ²: 0.2990\n",
      "Epoch 2/20 | Train Loss: 0.8422 | Val Loss: 0.6643 | MAE: 0.5527 | RMSE: 0.6643 | RÂ²: 0.5275\n",
      "Epoch 3/20 | Train Loss: 0.7408 | Val Loss: 0.5590 | MAE: 0.4684 | RMSE: 0.5590 | RÂ²: 0.6655\n",
      "Epoch 4/20 | Train Loss: 0.6677 | Val Loss: 0.4962 | MAE: 0.3954 | RMSE: 0.4962 | RÂ²: 0.7364\n",
      "Epoch 5/20 | Train Loss: 0.6277 | Val Loss: 0.4193 | MAE: 0.3158 | RMSE: 0.4193 | RÂ²: 0.8117\n",
      "Epoch 6/20 | Train Loss: 0.5954 | Val Loss: 0.3912 | MAE: 0.2833 | RMSE: 0.3912 | RÂ²: 0.8361\n",
      "Epoch 7/20 | Train Loss: 0.5671 | Val Loss: 0.3978 | MAE: 0.2782 | RMSE: 0.3978 | RÂ²: 0.8306\n",
      "Epoch 8/20 | Train Loss: 0.5420 | Val Loss: 0.3687 | MAE: 0.2622 | RMSE: 0.3687 | RÂ²: 0.8544\n",
      "Epoch 9/20 | Train Loss: 0.5266 | Val Loss: 0.3437 | MAE: 0.2490 | RMSE: 0.3437 | RÂ²: 0.8736\n",
      "Epoch 10/20 | Train Loss: 0.5142 | Val Loss: 0.3385 | MAE: 0.2462 | RMSE: 0.3385 | RÂ²: 0.8773\n",
      "Epoch 11/20 | Train Loss: 0.4953 | Val Loss: 0.3367 | MAE: 0.2452 | RMSE: 0.3367 | RÂ²: 0.8787\n",
      "Epoch 12/20 | Train Loss: 0.4851 | Val Loss: 0.3422 | MAE: 0.2443 | RMSE: 0.3422 | RÂ²: 0.8746\n",
      "Epoch 13/20 | Train Loss: 0.4697 | Val Loss: 0.3375 | MAE: 0.2424 | RMSE: 0.3375 | RÂ²: 0.8781\n",
      "Epoch 14/20 | Train Loss: 0.4651 | Val Loss: 0.3243 | MAE: 0.2366 | RMSE: 0.3243 | RÂ²: 0.8874\n",
      "Epoch 15/20 | Train Loss: 0.4514 | Val Loss: 0.3218 | MAE: 0.2322 | RMSE: 0.3218 | RÂ²: 0.8892\n",
      "Epoch 16/20 | Train Loss: 0.4504 | Val Loss: 0.3289 | MAE: 0.2330 | RMSE: 0.3289 | RÂ²: 0.8842\n",
      "Epoch 17/20 | Train Loss: 0.4396 | Val Loss: 0.3294 | MAE: 0.2326 | RMSE: 0.3294 | RÂ²: 0.8839\n",
      "Epoch 18/20 | Train Loss: 0.4313 | Val Loss: 0.3271 | MAE: 0.2303 | RMSE: 0.3271 | RÂ²: 0.8855\n",
      "Epoch 19/20 | Train Loss: 0.4257 | Val Loss: 0.3271 | MAE: 0.2289 | RMSE: 0.3271 | RÂ²: 0.8855\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3218 | MAE = 0.2322\n",
      "\n",
      "ðŸ”§ [238/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8308 | Val Loss: 0.8066 | MAE: 0.6813 | RMSE: 0.8066 | RÂ²: 0.3034\n",
      "Epoch 2/20 | Train Loss: 0.5707 | Val Loss: 0.6571 | MAE: 0.5400 | RMSE: 0.6571 | RÂ²: 0.5378\n",
      "Epoch 3/20 | Train Loss: 0.4734 | Val Loss: 0.4512 | MAE: 0.3676 | RMSE: 0.4512 | RÂ²: 0.7820\n",
      "Epoch 4/20 | Train Loss: 0.4185 | Val Loss: 0.3801 | MAE: 0.2930 | RMSE: 0.3801 | RÂ²: 0.8453\n",
      "Epoch 5/20 | Train Loss: 0.3927 | Val Loss: 0.3195 | MAE: 0.2415 | RMSE: 0.3195 | RÂ²: 0.8907\n",
      "Epoch 6/20 | Train Loss: 0.3666 | Val Loss: 0.3096 | MAE: 0.2250 | RMSE: 0.3096 | RÂ²: 0.8974\n",
      "Epoch 7/20 | Train Loss: 0.3483 | Val Loss: 0.2838 | MAE: 0.2029 | RMSE: 0.2838 | RÂ²: 0.9138\n",
      "Epoch 8/20 | Train Loss: 0.3441 | Val Loss: 0.2757 | MAE: 0.1959 | RMSE: 0.2757 | RÂ²: 0.9186\n",
      "Epoch 9/20 | Train Loss: 0.3299 | Val Loss: 0.2709 | MAE: 0.1932 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 10/20 | Train Loss: 0.3224 | Val Loss: 0.2736 | MAE: 0.1934 | RMSE: 0.2736 | RÂ²: 0.9199\n",
      "Epoch 11/20 | Train Loss: 0.3102 | Val Loss: 0.2689 | MAE: 0.1885 | RMSE: 0.2689 | RÂ²: 0.9226\n",
      "Epoch 12/20 | Train Loss: 0.3052 | Val Loss: 0.2691 | MAE: 0.1897 | RMSE: 0.2691 | RÂ²: 0.9225\n",
      "Epoch 13/20 | Train Loss: 0.2995 | Val Loss: 0.2713 | MAE: 0.1914 | RMSE: 0.2713 | RÂ²: 0.9212\n",
      "Epoch 14/20 | Train Loss: 0.2988 | Val Loss: 0.2624 | MAE: 0.1826 | RMSE: 0.2624 | RÂ²: 0.9263\n",
      "Epoch 15/20 | Train Loss: 0.2930 | Val Loss: 0.2666 | MAE: 0.1872 | RMSE: 0.2666 | RÂ²: 0.9239\n",
      "Epoch 16/20 | Train Loss: 0.2889 | Val Loss: 0.2678 | MAE: 0.1887 | RMSE: 0.2678 | RÂ²: 0.9232\n",
      "Epoch 17/20 | Train Loss: 0.2852 | Val Loss: 0.2599 | MAE: 0.1816 | RMSE: 0.2599 | RÂ²: 0.9277\n",
      "Epoch 18/20 | Train Loss: 0.2829 | Val Loss: 0.2756 | MAE: 0.1951 | RMSE: 0.2756 | RÂ²: 0.9187\n",
      "Epoch 19/20 | Train Loss: 0.2757 | Val Loss: 0.2644 | MAE: 0.1852 | RMSE: 0.2644 | RÂ²: 0.9252\n",
      "Epoch 20/20 | Train Loss: 0.2741 | Val Loss: 0.2706 | MAE: 0.1903 | RMSE: 0.2706 | RÂ²: 0.9216\n",
      "âœ… RMSE = 0.2599 | MAE = 0.1816\n",
      "\n",
      "ðŸ”§ [239/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[256, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8835 | Val Loss: 0.7578 | MAE: 0.6420 | RMSE: 0.7578 | RÂ²: 0.3852\n",
      "Epoch 2/20 | Train Loss: 0.6929 | Val Loss: 0.6456 | MAE: 0.5296 | RMSE: 0.6456 | RÂ²: 0.5538\n",
      "Epoch 3/20 | Train Loss: 0.5864 | Val Loss: 0.4622 | MAE: 0.3679 | RMSE: 0.4622 | RÂ²: 0.7712\n",
      "Epoch 4/20 | Train Loss: 0.5318 | Val Loss: 0.3841 | MAE: 0.2889 | RMSE: 0.3841 | RÂ²: 0.8420\n",
      "Epoch 5/20 | Train Loss: 0.4922 | Val Loss: 0.3582 | MAE: 0.2615 | RMSE: 0.3582 | RÂ²: 0.8627\n",
      "Epoch 6/20 | Train Loss: 0.4642 | Val Loss: 0.3406 | MAE: 0.2542 | RMSE: 0.3406 | RÂ²: 0.8758\n",
      "Epoch 7/20 | Train Loss: 0.4457 | Val Loss: 0.3401 | MAE: 0.2505 | RMSE: 0.3401 | RÂ²: 0.8762\n",
      "Epoch 8/20 | Train Loss: 0.4283 | Val Loss: 0.3372 | MAE: 0.2415 | RMSE: 0.3372 | RÂ²: 0.8783\n",
      "Epoch 9/20 | Train Loss: 0.4151 | Val Loss: 0.3354 | MAE: 0.2369 | RMSE: 0.3354 | RÂ²: 0.8796\n",
      "Epoch 10/20 | Train Loss: 0.3937 | Val Loss: 0.3278 | MAE: 0.2292 | RMSE: 0.3278 | RÂ²: 0.8850\n",
      "Epoch 11/20 | Train Loss: 0.3940 | Val Loss: 0.3038 | MAE: 0.2200 | RMSE: 0.3038 | RÂ²: 0.9012\n",
      "Epoch 12/20 | Train Loss: 0.3845 | Val Loss: 0.2886 | MAE: 0.2089 | RMSE: 0.2886 | RÂ²: 0.9108\n",
      "Epoch 13/20 | Train Loss: 0.3709 | Val Loss: 0.2989 | MAE: 0.2077 | RMSE: 0.2989 | RÂ²: 0.9044\n",
      "Epoch 14/20 | Train Loss: 0.3587 | Val Loss: 0.2835 | MAE: 0.2017 | RMSE: 0.2835 | RÂ²: 0.9139\n",
      "Epoch 15/20 | Train Loss: 0.3544 | Val Loss: 0.2815 | MAE: 0.1971 | RMSE: 0.2815 | RÂ²: 0.9151\n",
      "Epoch 16/20 | Train Loss: 0.3481 | Val Loss: 0.2968 | MAE: 0.1998 | RMSE: 0.2968 | RÂ²: 0.9057\n",
      "Epoch 17/20 | Train Loss: 0.3406 | Val Loss: 0.2797 | MAE: 0.1937 | RMSE: 0.2797 | RÂ²: 0.9162\n",
      "Epoch 18/20 | Train Loss: 0.3377 | Val Loss: 0.2803 | MAE: 0.1920 | RMSE: 0.2803 | RÂ²: 0.9159\n",
      "Epoch 19/20 | Train Loss: 0.3307 | Val Loss: 0.2881 | MAE: 0.1961 | RMSE: 0.2881 | RÂ²: 0.9112\n",
      "Epoch 20/20 | Train Loss: 0.3237 | Val Loss: 0.2804 | MAE: 0.1929 | RMSE: 0.2804 | RÂ²: 0.9159\n",
      "âœ… RMSE = 0.2797 | MAE = 0.1937\n",
      "\n",
      "ðŸ”§ [240/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[512, 512, 512], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.9347 | Val Loss: 0.8753 | MAE: 0.7414 | RMSE: 0.8753 | RÂ²: 0.1798\n",
      "Epoch 2/20 | Train Loss: 0.7337 | Val Loss: 0.7625 | MAE: 0.6403 | RMSE: 0.7625 | RÂ²: 0.3775\n",
      "Epoch 3/20 | Train Loss: 0.6272 | Val Loss: 0.6101 | MAE: 0.4967 | RMSE: 0.6101 | RÂ²: 0.6015\n",
      "Epoch 4/20 | Train Loss: 0.5805 | Val Loss: 0.5291 | MAE: 0.3897 | RMSE: 0.5291 | RÂ²: 0.7003\n",
      "Epoch 5/20 | Train Loss: 0.5464 | Val Loss: 0.4313 | MAE: 0.2973 | RMSE: 0.4313 | RÂ²: 0.8009\n",
      "Epoch 6/20 | Train Loss: 0.5113 | Val Loss: 0.3629 | MAE: 0.2575 | RMSE: 0.3629 | RÂ²: 0.8590\n",
      "Epoch 7/20 | Train Loss: 0.4976 | Val Loss: 0.3534 | MAE: 0.2462 | RMSE: 0.3534 | RÂ²: 0.8663\n",
      "Epoch 8/20 | Train Loss: 0.4746 | Val Loss: 0.3467 | MAE: 0.2424 | RMSE: 0.3467 | RÂ²: 0.8713\n",
      "Epoch 9/20 | Train Loss: 0.4533 | Val Loss: 0.3494 | MAE: 0.2473 | RMSE: 0.3494 | RÂ²: 0.8693\n",
      "Epoch 10/20 | Train Loss: 0.4533 | Val Loss: 0.3509 | MAE: 0.2417 | RMSE: 0.3509 | RÂ²: 0.8682\n",
      "Epoch 11/20 | Train Loss: 0.4458 | Val Loss: 0.3422 | MAE: 0.2410 | RMSE: 0.3422 | RÂ²: 0.8747\n",
      "Epoch 12/20 | Train Loss: 0.4322 | Val Loss: 0.3582 | MAE: 0.2491 | RMSE: 0.3582 | RÂ²: 0.8627\n",
      "Epoch 13/20 | Train Loss: 0.4167 | Val Loss: 0.3689 | MAE: 0.2503 | RMSE: 0.3689 | RÂ²: 0.8543\n",
      "Epoch 14/20 | Train Loss: 0.4103 | Val Loss: 0.3371 | MAE: 0.2343 | RMSE: 0.3371 | RÂ²: 0.8783\n",
      "Epoch 15/20 | Train Loss: 0.4088 | Val Loss: 0.3379 | MAE: 0.2284 | RMSE: 0.3379 | RÂ²: 0.8778\n",
      "Epoch 16/20 | Train Loss: 0.3952 | Val Loss: 0.3354 | MAE: 0.2314 | RMSE: 0.3354 | RÂ²: 0.8795\n",
      "Epoch 17/20 | Train Loss: 0.3901 | Val Loss: 0.3718 | MAE: 0.2489 | RMSE: 0.3718 | RÂ²: 0.8520\n",
      "Epoch 18/20 | Train Loss: 0.3793 | Val Loss: 0.3463 | MAE: 0.2348 | RMSE: 0.3463 | RÂ²: 0.8716\n",
      "Epoch 19/20 | Train Loss: 0.3814 | Val Loss: 0.3462 | MAE: 0.2263 | RMSE: 0.3462 | RÂ²: 0.8717\n",
      "Epoch 20/20 | Train Loss: 0.3701 | Val Loss: 0.3346 | MAE: 0.2259 | RMSE: 0.3346 | RÂ²: 0.8801\n",
      "âœ… RMSE = 0.3346 | MAE = 0.2259\n",
      "\n",
      "ðŸ”§ [241/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[2048, 1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8554 | Val Loss: 0.8446 | MAE: 0.7080 | RMSE: 0.8446 | RÂ²: 0.2363\n",
      "Epoch 2/20 | Train Loss: 0.5295 | Val Loss: 0.5873 | MAE: 0.4694 | RMSE: 0.5873 | RÂ²: 0.6308\n",
      "Epoch 3/20 | Train Loss: 0.4592 | Val Loss: 0.3489 | MAE: 0.2769 | RMSE: 0.3489 | RÂ²: 0.8697\n",
      "Epoch 4/20 | Train Loss: 0.4092 | Val Loss: 0.3448 | MAE: 0.2535 | RMSE: 0.3448 | RÂ²: 0.8728\n",
      "Epoch 5/20 | Train Loss: 0.3864 | Val Loss: 0.3413 | MAE: 0.2532 | RMSE: 0.3413 | RÂ²: 0.8753\n",
      "Epoch 6/20 | Train Loss: 0.3668 | Val Loss: 0.3186 | MAE: 0.2421 | RMSE: 0.3186 | RÂ²: 0.8913\n",
      "Epoch 7/20 | Train Loss: 0.3520 | Val Loss: 0.3215 | MAE: 0.2457 | RMSE: 0.3215 | RÂ²: 0.8894\n",
      "Epoch 8/20 | Train Loss: 0.3311 | Val Loss: 0.3126 | MAE: 0.2345 | RMSE: 0.3126 | RÂ²: 0.8954\n",
      "Epoch 9/20 | Train Loss: 0.3249 | Val Loss: 0.3020 | MAE: 0.2262 | RMSE: 0.3020 | RÂ²: 0.9024\n",
      "Epoch 10/20 | Train Loss: 0.3176 | Val Loss: 0.2852 | MAE: 0.2071 | RMSE: 0.2852 | RÂ²: 0.9129\n",
      "Epoch 11/20 | Train Loss: 0.3104 | Val Loss: 0.2912 | MAE: 0.2141 | RMSE: 0.2912 | RÂ²: 0.9092\n",
      "Epoch 12/20 | Train Loss: 0.3094 | Val Loss: 0.2737 | MAE: 0.1877 | RMSE: 0.2737 | RÂ²: 0.9198\n",
      "Epoch 13/20 | Train Loss: 0.2980 | Val Loss: 0.2698 | MAE: 0.1844 | RMSE: 0.2698 | RÂ²: 0.9221\n",
      "Epoch 14/20 | Train Loss: 0.2893 | Val Loss: 0.2637 | MAE: 0.1844 | RMSE: 0.2637 | RÂ²: 0.9256\n",
      "Epoch 15/20 | Train Loss: 0.2888 | Val Loss: 0.2716 | MAE: 0.1854 | RMSE: 0.2716 | RÂ²: 0.9211\n",
      "Epoch 16/20 | Train Loss: 0.2808 | Val Loss: 0.2678 | MAE: 0.1803 | RMSE: 0.2678 | RÂ²: 0.9232\n",
      "Epoch 17/20 | Train Loss: 0.2802 | Val Loss: 0.2702 | MAE: 0.1856 | RMSE: 0.2702 | RÂ²: 0.9219\n",
      "Epoch 18/20 | Train Loss: 0.2767 | Val Loss: 0.2672 | MAE: 0.1808 | RMSE: 0.2672 | RÂ²: 0.9235\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2637 | MAE = 0.1844\n",
      "\n",
      "ðŸ”§ [242/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[1024, 512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.7196 | Val Loss: 0.8776 | MAE: 0.7351 | RMSE: 0.8776 | RÂ²: 0.1754\n",
      "Epoch 2/20 | Train Loss: 0.3983 | Val Loss: 0.7189 | MAE: 0.5953 | RMSE: 0.7189 | RÂ²: 0.4467\n",
      "Epoch 3/20 | Train Loss: 0.3392 | Val Loss: 0.4737 | MAE: 0.3864 | RMSE: 0.4737 | RÂ²: 0.7598\n",
      "Epoch 4/20 | Train Loss: 0.3030 | Val Loss: 0.3534 | MAE: 0.2629 | RMSE: 0.3534 | RÂ²: 0.8663\n",
      "Epoch 5/20 | Train Loss: 0.2890 | Val Loss: 0.2770 | MAE: 0.1930 | RMSE: 0.2770 | RÂ²: 0.9179\n",
      "Epoch 6/20 | Train Loss: 0.2782 | Val Loss: 0.2679 | MAE: 0.1927 | RMSE: 0.2679 | RÂ²: 0.9231\n",
      "Epoch 7/20 | Train Loss: 0.2678 | Val Loss: 0.2578 | MAE: 0.1786 | RMSE: 0.2578 | RÂ²: 0.9288\n",
      "Epoch 8/20 | Train Loss: 0.2593 | Val Loss: 0.2620 | MAE: 0.1833 | RMSE: 0.2620 | RÂ²: 0.9265\n",
      "Epoch 9/20 | Train Loss: 0.2581 | Val Loss: 0.2539 | MAE: 0.1739 | RMSE: 0.2539 | RÂ²: 0.9310\n",
      "Epoch 10/20 | Train Loss: 0.2538 | Val Loss: 0.2553 | MAE: 0.1691 | RMSE: 0.2553 | RÂ²: 0.9302\n",
      "Epoch 11/20 | Train Loss: 0.2511 | Val Loss: 0.2714 | MAE: 0.1803 | RMSE: 0.2714 | RÂ²: 0.9212\n",
      "Epoch 12/20 | Train Loss: 0.2469 | Val Loss: 0.2867 | MAE: 0.1833 | RMSE: 0.2867 | RÂ²: 0.9120\n",
      "Epoch 13/20 | Train Loss: 0.2445 | Val Loss: 0.2605 | MAE: 0.1721 | RMSE: 0.2605 | RÂ²: 0.9274\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2539 | MAE = 0.1739\n",
      "\n",
      "ðŸ”§ [243/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[1024, 512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.1104 | Val Loss: 0.9580 | MAE: 0.8051 | RMSE: 0.9580 | RÂ²: 0.0174\n",
      "Epoch 2/20 | Train Loss: 0.9994 | Val Loss: 0.8840 | MAE: 0.7469 | RMSE: 0.8840 | RÂ²: 0.1635\n",
      "Epoch 3/20 | Train Loss: 0.9268 | Val Loss: 0.7946 | MAE: 0.6752 | RMSE: 0.7946 | RÂ²: 0.3240\n",
      "Epoch 4/20 | Train Loss: 0.8937 | Val Loss: 0.7109 | MAE: 0.6053 | RMSE: 0.7109 | RÂ²: 0.4590\n",
      "Epoch 5/20 | Train Loss: 0.8442 | Val Loss: 0.6340 | MAE: 0.5399 | RMSE: 0.6340 | RÂ²: 0.5697\n",
      "Epoch 6/20 | Train Loss: 0.7962 | Val Loss: 0.5651 | MAE: 0.4789 | RMSE: 0.5651 | RÂ²: 0.6581\n",
      "Epoch 7/20 | Train Loss: 0.7434 | Val Loss: 0.5035 | MAE: 0.4212 | RMSE: 0.5035 | RÂ²: 0.7286\n",
      "Epoch 8/20 | Train Loss: 0.7135 | Val Loss: 0.4564 | MAE: 0.3734 | RMSE: 0.4564 | RÂ²: 0.7770\n",
      "Epoch 9/20 | Train Loss: 0.6803 | Val Loss: 0.4332 | MAE: 0.3448 | RMSE: 0.4332 | RÂ²: 0.7991\n",
      "Epoch 10/20 | Train Loss: 0.6599 | Val Loss: 0.4320 | MAE: 0.3361 | RMSE: 0.4320 | RÂ²: 0.8002\n",
      "Epoch 11/20 | Train Loss: 0.6252 | Val Loss: 0.4329 | MAE: 0.3314 | RMSE: 0.4329 | RÂ²: 0.7993\n",
      "Epoch 12/20 | Train Loss: 0.6112 | Val Loss: 0.4342 | MAE: 0.3262 | RMSE: 0.4342 | RÂ²: 0.7981\n",
      "Epoch 13/20 | Train Loss: 0.5934 | Val Loss: 0.4365 | MAE: 0.3251 | RMSE: 0.4365 | RÂ²: 0.7960\n",
      "Epoch 14/20 | Train Loss: 0.5876 | Val Loss: 0.4358 | MAE: 0.3224 | RMSE: 0.4358 | RÂ²: 0.7967\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.4320 | MAE = 0.3361\n",
      "\n",
      "ðŸ”§ [244/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[512, 512, 512], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8266 | Val Loss: 0.8572 | MAE: 0.7183 | RMSE: 0.8572 | RÂ²: 0.2134\n",
      "Epoch 2/20 | Train Loss: 0.5562 | Val Loss: 0.6995 | MAE: 0.5765 | RMSE: 0.6995 | RÂ²: 0.4762\n",
      "Epoch 3/20 | Train Loss: 0.4896 | Val Loss: 0.4797 | MAE: 0.3998 | RMSE: 0.4797 | RÂ²: 0.7536\n",
      "Epoch 4/20 | Train Loss: 0.4522 | Val Loss: 0.4070 | MAE: 0.3101 | RMSE: 0.4070 | RÂ²: 0.8226\n",
      "Epoch 5/20 | Train Loss: 0.4211 | Val Loss: 0.3061 | MAE: 0.2238 | RMSE: 0.3061 | RÂ²: 0.8997\n",
      "Epoch 6/20 | Train Loss: 0.3889 | Val Loss: 0.2935 | MAE: 0.2104 | RMSE: 0.2935 | RÂ²: 0.9078\n",
      "Epoch 7/20 | Train Loss: 0.3731 | Val Loss: 0.3075 | MAE: 0.2273 | RMSE: 0.3075 | RÂ²: 0.8988\n",
      "Epoch 8/20 | Train Loss: 0.3594 | Val Loss: 0.3124 | MAE: 0.2343 | RMSE: 0.3124 | RÂ²: 0.8955\n",
      "Epoch 9/20 | Train Loss: 0.3395 | Val Loss: 0.3017 | MAE: 0.2234 | RMSE: 0.3017 | RÂ²: 0.9026\n",
      "Epoch 10/20 | Train Loss: 0.3359 | Val Loss: 0.2946 | MAE: 0.2195 | RMSE: 0.2946 | RÂ²: 0.9071\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2935 | MAE = 0.2104\n",
      "\n",
      "ðŸ”§ [245/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 1.0530 | Val Loss: 0.7217 | MAE: 0.6028 | RMSE: 0.7217 | RÂ²: 0.4424\n",
      "Epoch 2/20 | Train Loss: 0.4843 | Val Loss: 0.4193 | MAE: 0.3342 | RMSE: 0.4193 | RÂ²: 0.8118\n",
      "Epoch 3/20 | Train Loss: 0.4041 | Val Loss: 0.6506 | MAE: 0.5254 | RMSE: 0.6506 | RÂ²: 0.5469\n",
      "Epoch 4/20 | Train Loss: 0.3559 | Val Loss: 0.4624 | MAE: 0.3835 | RMSE: 0.4624 | RÂ²: 0.7711\n",
      "Epoch 5/20 | Train Loss: 0.3386 | Val Loss: 0.5898 | MAE: 0.4455 | RMSE: 0.5898 | RÂ²: 0.6276\n",
      "Epoch 6/20 | Train Loss: 0.3278 | Val Loss: 0.3325 | MAE: 0.2470 | RMSE: 0.3325 | RÂ²: 0.8816\n",
      "Epoch 7/20 | Train Loss: 0.3128 | Val Loss: 0.3680 | MAE: 0.2787 | RMSE: 0.3680 | RÂ²: 0.8550\n",
      "Epoch 8/20 | Train Loss: 0.3016 | Val Loss: 0.2731 | MAE: 0.1864 | RMSE: 0.2731 | RÂ²: 0.9201\n",
      "Epoch 9/20 | Train Loss: 0.2948 | Val Loss: 0.2769 | MAE: 0.1934 | RMSE: 0.2769 | RÂ²: 0.9179\n",
      "Epoch 10/20 | Train Loss: 0.2841 | Val Loss: 0.2819 | MAE: 0.1881 | RMSE: 0.2819 | RÂ²: 0.9149\n",
      "Epoch 11/20 | Train Loss: 0.2825 | Val Loss: 0.2550 | MAE: 0.1744 | RMSE: 0.2550 | RÂ²: 0.9304\n",
      "Epoch 12/20 | Train Loss: 0.2826 | Val Loss: 0.2687 | MAE: 0.1839 | RMSE: 0.2687 | RÂ²: 0.9227\n",
      "Epoch 13/20 | Train Loss: 0.2729 | Val Loss: 0.2818 | MAE: 0.1979 | RMSE: 0.2818 | RÂ²: 0.9150\n",
      "Epoch 14/20 | Train Loss: 0.2673 | Val Loss: 0.2679 | MAE: 0.1834 | RMSE: 0.2679 | RÂ²: 0.9232\n",
      "Epoch 15/20 | Train Loss: 0.2625 | Val Loss: 0.2637 | MAE: 0.1831 | RMSE: 0.2637 | RÂ²: 0.9256\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2550 | MAE = 0.1744\n",
      "\n",
      "ðŸ”§ [246/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 256, 1024], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8517 | Val Loss: 0.8415 | MAE: 0.7020 | RMSE: 0.8415 | RÂ²: 0.2419\n",
      "Epoch 2/20 | Train Loss: 0.5373 | Val Loss: 0.6933 | MAE: 0.5521 | RMSE: 0.6933 | RÂ²: 0.4854\n",
      "Epoch 3/20 | Train Loss: 0.4911 | Val Loss: 0.4190 | MAE: 0.3477 | RMSE: 0.4190 | RÂ²: 0.8121\n",
      "Epoch 4/20 | Train Loss: 0.4447 | Val Loss: 0.3714 | MAE: 0.2798 | RMSE: 0.3714 | RÂ²: 0.8523\n",
      "Epoch 5/20 | Train Loss: 0.4123 | Val Loss: 0.2978 | MAE: 0.2128 | RMSE: 0.2978 | RÂ²: 0.9051\n",
      "Epoch 6/20 | Train Loss: 0.3973 | Val Loss: 0.2784 | MAE: 0.1973 | RMSE: 0.2784 | RÂ²: 0.9170\n",
      "Epoch 7/20 | Train Loss: 0.3790 | Val Loss: 0.3049 | MAE: 0.2074 | RMSE: 0.3049 | RÂ²: 0.9005\n",
      "Epoch 8/20 | Train Loss: 0.3817 | Val Loss: 0.2749 | MAE: 0.1947 | RMSE: 0.2749 | RÂ²: 0.9191\n",
      "Epoch 9/20 | Train Loss: 0.3649 | Val Loss: 0.2797 | MAE: 0.1970 | RMSE: 0.2797 | RÂ²: 0.9163\n",
      "Epoch 10/20 | Train Loss: 0.3498 | Val Loss: 0.2847 | MAE: 0.1919 | RMSE: 0.2847 | RÂ²: 0.9132\n",
      "Epoch 11/20 | Train Loss: 0.3451 | Val Loss: 0.2735 | MAE: 0.1846 | RMSE: 0.2735 | RÂ²: 0.9199\n",
      "Epoch 12/20 | Train Loss: 0.3366 | Val Loss: 0.2754 | MAE: 0.1950 | RMSE: 0.2754 | RÂ²: 0.9188\n",
      "Epoch 13/20 | Train Loss: 0.3265 | Val Loss: 0.3179 | MAE: 0.2038 | RMSE: 0.3179 | RÂ²: 0.8918\n",
      "Epoch 14/20 | Train Loss: 0.3257 | Val Loss: 0.2853 | MAE: 0.2026 | RMSE: 0.2853 | RÂ²: 0.9129\n",
      "Epoch 15/20 | Train Loss: 0.3131 | Val Loss: 0.2574 | MAE: 0.1760 | RMSE: 0.2574 | RÂ²: 0.9291\n",
      "Epoch 16/20 | Train Loss: 0.3090 | Val Loss: 0.2662 | MAE: 0.1838 | RMSE: 0.2662 | RÂ²: 0.9241\n",
      "Epoch 17/20 | Train Loss: 0.3047 | Val Loss: 0.2688 | MAE: 0.1826 | RMSE: 0.2688 | RÂ²: 0.9226\n",
      "Epoch 18/20 | Train Loss: 0.3000 | Val Loss: 0.2894 | MAE: 0.1895 | RMSE: 0.2894 | RÂ²: 0.9103\n",
      "Epoch 19/20 | Train Loss: 0.2971 | Val Loss: 0.2653 | MAE: 0.1783 | RMSE: 0.2653 | RÂ²: 0.9247\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2574 | MAE = 0.1760\n",
      "\n",
      "ðŸ”§ [247/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[512, 256, 128], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9216 | Val Loss: 0.8724 | MAE: 0.7368 | RMSE: 0.8724 | RÂ²: 0.1853\n",
      "Epoch 2/20 | Train Loss: 0.6999 | Val Loss: 0.7254 | MAE: 0.6037 | RMSE: 0.7254 | RÂ²: 0.4367\n",
      "Epoch 3/20 | Train Loss: 0.5902 | Val Loss: 0.5096 | MAE: 0.4140 | RMSE: 0.5096 | RÂ²: 0.7220\n",
      "Epoch 4/20 | Train Loss: 0.5270 | Val Loss: 0.4604 | MAE: 0.3336 | RMSE: 0.4604 | RÂ²: 0.7731\n",
      "Epoch 5/20 | Train Loss: 0.4935 | Val Loss: 0.3586 | MAE: 0.2642 | RMSE: 0.3586 | RÂ²: 0.8623\n",
      "Epoch 6/20 | Train Loss: 0.4664 | Val Loss: 0.3514 | MAE: 0.2563 | RMSE: 0.3514 | RÂ²: 0.8678\n",
      "Epoch 7/20 | Train Loss: 0.4451 | Val Loss: 0.3521 | MAE: 0.2563 | RMSE: 0.3521 | RÂ²: 0.8673\n",
      "Epoch 8/20 | Train Loss: 0.4153 | Val Loss: 0.3364 | MAE: 0.2537 | RMSE: 0.3364 | RÂ²: 0.8789\n",
      "Epoch 9/20 | Train Loss: 0.4023 | Val Loss: 0.3267 | MAE: 0.2465 | RMSE: 0.3267 | RÂ²: 0.8857\n",
      "Epoch 10/20 | Train Loss: 0.3930 | Val Loss: 0.3156 | MAE: 0.2307 | RMSE: 0.3156 | RÂ²: 0.8934\n",
      "Epoch 11/20 | Train Loss: 0.3815 | Val Loss: 0.3001 | MAE: 0.2147 | RMSE: 0.3001 | RÂ²: 0.9036\n",
      "Epoch 12/20 | Train Loss: 0.3670 | Val Loss: 0.2936 | MAE: 0.2047 | RMSE: 0.2936 | RÂ²: 0.9077\n",
      "Epoch 13/20 | Train Loss: 0.3597 | Val Loss: 0.2891 | MAE: 0.2012 | RMSE: 0.2891 | RÂ²: 0.9105\n",
      "Epoch 14/20 | Train Loss: 0.3527 | Val Loss: 0.2843 | MAE: 0.1974 | RMSE: 0.2843 | RÂ²: 0.9135\n",
      "Epoch 15/20 | Train Loss: 0.3490 | Val Loss: 0.2877 | MAE: 0.1998 | RMSE: 0.2877 | RÂ²: 0.9114\n",
      "Epoch 16/20 | Train Loss: 0.3428 | Val Loss: 0.2812 | MAE: 0.1918 | RMSE: 0.2812 | RÂ²: 0.9154\n",
      "Epoch 17/20 | Train Loss: 0.3341 | Val Loss: 0.2850 | MAE: 0.1961 | RMSE: 0.2850 | RÂ²: 0.9130\n",
      "Epoch 18/20 | Train Loss: 0.3274 | Val Loss: 0.2818 | MAE: 0.1912 | RMSE: 0.2818 | RÂ²: 0.9150\n",
      "Epoch 19/20 | Train Loss: 0.3229 | Val Loss: 0.2748 | MAE: 0.1882 | RMSE: 0.2748 | RÂ²: 0.9192\n",
      "Epoch 20/20 | Train Loss: 0.3155 | Val Loss: 0.2739 | MAE: 0.1909 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "âœ… RMSE = 0.2739 | MAE = 0.1909\n",
      "\n",
      "ðŸ”§ [248/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[2048, 1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9824 | Val Loss: 0.7329 | MAE: 0.6037 | RMSE: 0.7329 | RÂ²: 0.4250\n",
      "Epoch 2/20 | Train Loss: 0.5775 | Val Loss: 0.4687 | MAE: 0.3648 | RMSE: 0.4687 | RÂ²: 0.7648\n",
      "Epoch 3/20 | Train Loss: 0.4920 | Val Loss: 0.6361 | MAE: 0.4892 | RMSE: 0.6361 | RÂ²: 0.5668\n",
      "Epoch 4/20 | Train Loss: 0.4407 | Val Loss: 0.4965 | MAE: 0.4109 | RMSE: 0.4965 | RÂ²: 0.7361\n",
      "Epoch 5/20 | Train Loss: 0.4023 | Val Loss: 0.5383 | MAE: 0.4210 | RMSE: 0.5383 | RÂ²: 0.6897\n",
      "Epoch 6/20 | Train Loss: 0.3776 | Val Loss: 0.3984 | MAE: 0.3106 | RMSE: 0.3984 | RÂ²: 0.8301\n",
      "Epoch 7/20 | Train Loss: 0.3594 | Val Loss: 0.3490 | MAE: 0.2723 | RMSE: 0.3490 | RÂ²: 0.8696\n",
      "Epoch 8/20 | Train Loss: 0.3443 | Val Loss: 0.3104 | MAE: 0.2269 | RMSE: 0.3104 | RÂ²: 0.8969\n",
      "Epoch 9/20 | Train Loss: 0.3316 | Val Loss: 0.2872 | MAE: 0.2092 | RMSE: 0.2872 | RÂ²: 0.9117\n",
      "Epoch 10/20 | Train Loss: 0.3179 | Val Loss: 0.2742 | MAE: 0.1878 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "Epoch 11/20 | Train Loss: 0.3074 | Val Loss: 0.2808 | MAE: 0.1987 | RMSE: 0.2808 | RÂ²: 0.9156\n",
      "Epoch 12/20 | Train Loss: 0.3013 | Val Loss: 0.2616 | MAE: 0.1767 | RMSE: 0.2616 | RÂ²: 0.9268\n",
      "Epoch 13/20 | Train Loss: 0.2935 | Val Loss: 0.2669 | MAE: 0.1843 | RMSE: 0.2669 | RÂ²: 0.9237\n",
      "Epoch 14/20 | Train Loss: 0.2918 | Val Loss: 0.2717 | MAE: 0.1869 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "Epoch 15/20 | Train Loss: 0.2891 | Val Loss: 0.2739 | MAE: 0.1888 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 16/20 | Train Loss: 0.2847 | Val Loss: 0.2704 | MAE: 0.1825 | RMSE: 0.2704 | RÂ²: 0.9217\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2616 | MAE = 0.1767\n",
      "\n",
      "ðŸ”§ [249/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8425 | Val Loss: 0.8698 | MAE: 0.7401 | RMSE: 0.8698 | RÂ²: 0.1901\n",
      "Epoch 2/20 | Train Loss: 0.5167 | Val Loss: 0.6860 | MAE: 0.5779 | RMSE: 0.6860 | RÂ²: 0.4963\n",
      "Epoch 3/20 | Train Loss: 0.4234 | Val Loss: 0.4590 | MAE: 0.3825 | RMSE: 0.4590 | RÂ²: 0.7745\n",
      "Epoch 4/20 | Train Loss: 0.3850 | Val Loss: 0.3534 | MAE: 0.2806 | RMSE: 0.3534 | RÂ²: 0.8663\n",
      "Epoch 5/20 | Train Loss: 0.3604 | Val Loss: 0.2984 | MAE: 0.2256 | RMSE: 0.2984 | RÂ²: 0.9047\n",
      "Epoch 6/20 | Train Loss: 0.3432 | Val Loss: 0.2701 | MAE: 0.1962 | RMSE: 0.2701 | RÂ²: 0.9219\n",
      "Epoch 7/20 | Train Loss: 0.3299 | Val Loss: 0.2632 | MAE: 0.1891 | RMSE: 0.2632 | RÂ²: 0.9258\n",
      "Epoch 8/20 | Train Loss: 0.3113 | Val Loss: 0.2652 | MAE: 0.1886 | RMSE: 0.2652 | RÂ²: 0.9247\n",
      "Epoch 9/20 | Train Loss: 0.3034 | Val Loss: 0.2554 | MAE: 0.1772 | RMSE: 0.2554 | RÂ²: 0.9302\n",
      "Epoch 10/20 | Train Loss: 0.2976 | Val Loss: 0.2611 | MAE: 0.1806 | RMSE: 0.2611 | RÂ²: 0.9270\n",
      "Epoch 11/20 | Train Loss: 0.2927 | Val Loss: 0.2535 | MAE: 0.1743 | RMSE: 0.2535 | RÂ²: 0.9312\n",
      "Epoch 12/20 | Train Loss: 0.2849 | Val Loss: 0.2540 | MAE: 0.1750 | RMSE: 0.2540 | RÂ²: 0.9309\n",
      "Epoch 13/20 | Train Loss: 0.2799 | Val Loss: 0.2604 | MAE: 0.1809 | RMSE: 0.2604 | RÂ²: 0.9274\n",
      "Epoch 14/20 | Train Loss: 0.2778 | Val Loss: 0.2603 | MAE: 0.1766 | RMSE: 0.2603 | RÂ²: 0.9275\n",
      "Epoch 15/20 | Train Loss: 0.2754 | Val Loss: 0.2717 | MAE: 0.1873 | RMSE: 0.2717 | RÂ²: 0.9210\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2535 | MAE = 0.1743\n",
      "\n",
      "ðŸ”§ [250/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[1024, 512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8973 | Val Loss: 0.8103 | MAE: 0.6823 | RMSE: 0.8103 | RÂ²: 0.2971\n",
      "Epoch 2/20 | Train Loss: 0.6177 | Val Loss: 0.4291 | MAE: 0.3443 | RMSE: 0.4291 | RÂ²: 0.8029\n",
      "Epoch 3/20 | Train Loss: 0.5105 | Val Loss: 0.4482 | MAE: 0.3579 | RMSE: 0.4482 | RÂ²: 0.7850\n",
      "Epoch 4/20 | Train Loss: 0.4539 | Val Loss: 0.5212 | MAE: 0.4352 | RMSE: 0.5212 | RÂ²: 0.7092\n",
      "Epoch 5/20 | Train Loss: 0.4295 | Val Loss: 0.4754 | MAE: 0.3915 | RMSE: 0.4754 | RÂ²: 0.7580\n",
      "Epoch 6/20 | Train Loss: 0.3946 | Val Loss: 0.4268 | MAE: 0.3405 | RMSE: 0.4268 | RÂ²: 0.8050\n",
      "Epoch 7/20 | Train Loss: 0.3781 | Val Loss: 0.3542 | MAE: 0.2665 | RMSE: 0.3542 | RÂ²: 0.8657\n",
      "Epoch 8/20 | Train Loss: 0.3680 | Val Loss: 0.3117 | MAE: 0.2226 | RMSE: 0.3117 | RÂ²: 0.8960\n",
      "Epoch 9/20 | Train Loss: 0.3502 | Val Loss: 0.3000 | MAE: 0.2162 | RMSE: 0.3000 | RÂ²: 0.9036\n",
      "Epoch 10/20 | Train Loss: 0.3418 | Val Loss: 0.2770 | MAE: 0.1947 | RMSE: 0.2770 | RÂ²: 0.9178\n",
      "Epoch 11/20 | Train Loss: 0.3335 | Val Loss: 0.2813 | MAE: 0.1980 | RMSE: 0.2813 | RÂ²: 0.9153\n",
      "Epoch 12/20 | Train Loss: 0.3330 | Val Loss: 0.2816 | MAE: 0.1994 | RMSE: 0.2816 | RÂ²: 0.9151\n",
      "Epoch 13/20 | Train Loss: 0.3275 | Val Loss: 0.2779 | MAE: 0.1924 | RMSE: 0.2779 | RÂ²: 0.9173\n",
      "Epoch 14/20 | Train Loss: 0.3179 | Val Loss: 0.2776 | MAE: 0.1960 | RMSE: 0.2776 | RÂ²: 0.9175\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2770 | MAE = 0.1947\n",
      "\n",
      "ðŸ”§ [251/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[512, 512, 512], seed=2029\n",
      "Epoch 1/20 | Train Loss: 1.0296 | Val Loss: 0.8240 | MAE: 0.6843 | RMSE: 0.8240 | RÂ²: 0.2731\n",
      "Epoch 2/20 | Train Loss: 0.6879 | Val Loss: 0.5801 | MAE: 0.4743 | RMSE: 0.5801 | RÂ²: 0.6398\n",
      "Epoch 3/20 | Train Loss: 0.5745 | Val Loss: 0.3670 | MAE: 0.2793 | RMSE: 0.3670 | RÂ²: 0.8558\n",
      "Epoch 4/20 | Train Loss: 0.5097 | Val Loss: 0.3778 | MAE: 0.2925 | RMSE: 0.3778 | RÂ²: 0.8472\n",
      "Epoch 5/20 | Train Loss: 0.4775 | Val Loss: 0.4114 | MAE: 0.3170 | RMSE: 0.4114 | RÂ²: 0.8189\n",
      "Epoch 6/20 | Train Loss: 0.4424 | Val Loss: 0.3683 | MAE: 0.2796 | RMSE: 0.3683 | RÂ²: 0.8548\n",
      "Epoch 7/20 | Train Loss: 0.4104 | Val Loss: 0.3935 | MAE: 0.3129 | RMSE: 0.3935 | RÂ²: 0.8343\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3670 | MAE = 0.2793\n",
      "\n",
      "ðŸ”§ [252/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[1024, 256, 1024], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8109 | Val Loss: 0.9093 | MAE: 0.7677 | RMSE: 0.9093 | RÂ²: 0.1148\n",
      "Epoch 2/20 | Train Loss: 0.5596 | Val Loss: 0.7616 | MAE: 0.6405 | RMSE: 0.7616 | RÂ²: 0.3790\n",
      "Epoch 3/20 | Train Loss: 0.4847 | Val Loss: 0.6045 | MAE: 0.5009 | RMSE: 0.6045 | RÂ²: 0.6088\n",
      "Epoch 4/20 | Train Loss: 0.4316 | Val Loss: 0.4571 | MAE: 0.3766 | RMSE: 0.4571 | RÂ²: 0.7764\n",
      "Epoch 5/20 | Train Loss: 0.4108 | Val Loss: 0.3704 | MAE: 0.2866 | RMSE: 0.3704 | RÂ²: 0.8531\n",
      "Epoch 6/20 | Train Loss: 0.3811 | Val Loss: 0.2977 | MAE: 0.2123 | RMSE: 0.2977 | RÂ²: 0.9052\n",
      "Epoch 7/20 | Train Loss: 0.3655 | Val Loss: 0.2772 | MAE: 0.1982 | RMSE: 0.2772 | RÂ²: 0.9178\n",
      "Epoch 8/20 | Train Loss: 0.3610 | Val Loss: 0.2726 | MAE: 0.1915 | RMSE: 0.2726 | RÂ²: 0.9205\n",
      "Epoch 9/20 | Train Loss: 0.3509 | Val Loss: 0.3035 | MAE: 0.1978 | RMSE: 0.3035 | RÂ²: 0.9014\n",
      "Epoch 10/20 | Train Loss: 0.3468 | Val Loss: 0.2797 | MAE: 0.1921 | RMSE: 0.2797 | RÂ²: 0.9163\n",
      "Epoch 11/20 | Train Loss: 0.3315 | Val Loss: 0.2727 | MAE: 0.1915 | RMSE: 0.2727 | RÂ²: 0.9204\n",
      "Epoch 12/20 | Train Loss: 0.3253 | Val Loss: 0.2636 | MAE: 0.1814 | RMSE: 0.2636 | RÂ²: 0.9256\n",
      "Epoch 13/20 | Train Loss: 0.3298 | Val Loss: 0.2726 | MAE: 0.1902 | RMSE: 0.2726 | RÂ²: 0.9205\n",
      "Epoch 14/20 | Train Loss: 0.3207 | Val Loss: 0.2761 | MAE: 0.1885 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 15/20 | Train Loss: 0.3219 | Val Loss: 0.2725 | MAE: 0.1894 | RMSE: 0.2725 | RÂ²: 0.9205\n",
      "Epoch 16/20 | Train Loss: 0.3160 | Val Loss: 0.2691 | MAE: 0.1860 | RMSE: 0.2691 | RÂ²: 0.9225\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2636 | MAE = 0.1814\n",
      "\n",
      "ðŸ”§ [253/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 1.1136 | Val Loss: 0.9859 | MAE: 0.8342 | RMSE: 0.9859 | RÂ²: -0.0406\n",
      "Epoch 2/20 | Train Loss: 0.9321 | Val Loss: 0.9016 | MAE: 0.7667 | RMSE: 0.9016 | RÂ²: 0.1298\n",
      "Epoch 3/20 | Train Loss: 0.8367 | Val Loss: 0.7892 | MAE: 0.6707 | RMSE: 0.7892 | RÂ²: 0.3332\n",
      "Epoch 4/20 | Train Loss: 0.7895 | Val Loss: 0.6751 | MAE: 0.5693 | RMSE: 0.6751 | RÂ²: 0.5121\n",
      "Epoch 5/20 | Train Loss: 0.7255 | Val Loss: 0.5691 | MAE: 0.4758 | RMSE: 0.5691 | RÂ²: 0.6532\n",
      "Epoch 6/20 | Train Loss: 0.6969 | Val Loss: 0.4847 | MAE: 0.4023 | RMSE: 0.4847 | RÂ²: 0.7485\n",
      "Epoch 7/20 | Train Loss: 0.6476 | Val Loss: 0.4282 | MAE: 0.3529 | RMSE: 0.4282 | RÂ²: 0.8037\n",
      "Epoch 8/20 | Train Loss: 0.6168 | Val Loss: 0.3887 | MAE: 0.3150 | RMSE: 0.3887 | RÂ²: 0.8383\n",
      "Epoch 9/20 | Train Loss: 0.5858 | Val Loss: 0.3593 | MAE: 0.2847 | RMSE: 0.3593 | RÂ²: 0.8618\n",
      "Epoch 10/20 | Train Loss: 0.5787 | Val Loss: 0.3393 | MAE: 0.2635 | RMSE: 0.3393 | RÂ²: 0.8768\n",
      "Epoch 11/20 | Train Loss: 0.5422 | Val Loss: 0.3301 | MAE: 0.2523 | RMSE: 0.3301 | RÂ²: 0.8834\n",
      "Epoch 12/20 | Train Loss: 0.5318 | Val Loss: 0.3264 | MAE: 0.2454 | RMSE: 0.3264 | RÂ²: 0.8859\n",
      "Epoch 13/20 | Train Loss: 0.5149 | Val Loss: 0.3233 | MAE: 0.2400 | RMSE: 0.3233 | RÂ²: 0.8881\n",
      "Epoch 14/20 | Train Loss: 0.5062 | Val Loss: 0.3194 | MAE: 0.2360 | RMSE: 0.3194 | RÂ²: 0.8908\n",
      "Epoch 15/20 | Train Loss: 0.5042 | Val Loss: 0.3197 | MAE: 0.2350 | RMSE: 0.3197 | RÂ²: 0.8906\n",
      "Epoch 16/20 | Train Loss: 0.4959 | Val Loss: 0.3182 | MAE: 0.2330 | RMSE: 0.3182 | RÂ²: 0.8916\n",
      "Epoch 17/20 | Train Loss: 0.4831 | Val Loss: 0.3157 | MAE: 0.2314 | RMSE: 0.3157 | RÂ²: 0.8933\n",
      "Epoch 18/20 | Train Loss: 0.4772 | Val Loss: 0.3125 | MAE: 0.2292 | RMSE: 0.3125 | RÂ²: 0.8954\n",
      "Epoch 19/20 | Train Loss: 0.4705 | Val Loss: 0.3121 | MAE: 0.2280 | RMSE: 0.3121 | RÂ²: 0.8957\n",
      "Epoch 20/20 | Train Loss: 0.4621 | Val Loss: 0.3141 | MAE: 0.2285 | RMSE: 0.3141 | RÂ²: 0.8944\n",
      "âœ… RMSE = 0.3121 | MAE = 0.2280\n",
      "\n",
      "ðŸ”§ [254/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8477 | Val Loss: 0.7735 | MAE: 0.6540 | RMSE: 0.7735 | RÂ²: 0.3595\n",
      "Epoch 2/20 | Train Loss: 0.6074 | Val Loss: 0.6148 | MAE: 0.5102 | RMSE: 0.6148 | RÂ²: 0.5954\n",
      "Epoch 3/20 | Train Loss: 0.5206 | Val Loss: 0.4137 | MAE: 0.3356 | RMSE: 0.4137 | RÂ²: 0.8168\n",
      "Epoch 4/20 | Train Loss: 0.4727 | Val Loss: 0.3403 | MAE: 0.2671 | RMSE: 0.3403 | RÂ²: 0.8761\n",
      "Epoch 5/20 | Train Loss: 0.4390 | Val Loss: 0.3139 | MAE: 0.2405 | RMSE: 0.3139 | RÂ²: 0.8945\n",
      "Epoch 6/20 | Train Loss: 0.4163 | Val Loss: 0.3018 | MAE: 0.2268 | RMSE: 0.3018 | RÂ²: 0.9025\n",
      "Epoch 7/20 | Train Loss: 0.4044 | Val Loss: 0.3082 | MAE: 0.2264 | RMSE: 0.3082 | RÂ²: 0.8983\n",
      "Epoch 8/20 | Train Loss: 0.3875 | Val Loss: 0.2974 | MAE: 0.2165 | RMSE: 0.2974 | RÂ²: 0.9053\n",
      "Epoch 9/20 | Train Loss: 0.3750 | Val Loss: 0.2810 | MAE: 0.2035 | RMSE: 0.2810 | RÂ²: 0.9155\n",
      "Epoch 10/20 | Train Loss: 0.3645 | Val Loss: 0.2795 | MAE: 0.2018 | RMSE: 0.2795 | RÂ²: 0.9164\n",
      "Epoch 11/20 | Train Loss: 0.3574 | Val Loss: 0.2761 | MAE: 0.1999 | RMSE: 0.2761 | RÂ²: 0.9184\n",
      "Epoch 12/20 | Train Loss: 0.3515 | Val Loss: 0.2779 | MAE: 0.1985 | RMSE: 0.2779 | RÂ²: 0.9173\n",
      "Epoch 13/20 | Train Loss: 0.3384 | Val Loss: 0.2772 | MAE: 0.1985 | RMSE: 0.2772 | RÂ²: 0.9177\n",
      "Epoch 14/20 | Train Loss: 0.3332 | Val Loss: 0.2772 | MAE: 0.1980 | RMSE: 0.2772 | RÂ²: 0.9177\n",
      "Epoch 15/20 | Train Loss: 0.3285 | Val Loss: 0.2829 | MAE: 0.2027 | RMSE: 0.2829 | RÂ²: 0.9143\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2761 | MAE = 0.1999\n",
      "\n",
      "ðŸ”§ [255/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[512, 512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 1.0369 | Val Loss: 0.9467 | MAE: 0.8022 | RMSE: 0.9467 | RÂ²: 0.0405\n",
      "Epoch 2/20 | Train Loss: 0.9318 | Val Loss: 0.8516 | MAE: 0.7231 | RMSE: 0.8516 | RÂ²: 0.2235\n",
      "Epoch 3/20 | Train Loss: 0.8585 | Val Loss: 0.7593 | MAE: 0.6427 | RMSE: 0.7593 | RÂ²: 0.3828\n",
      "Epoch 4/20 | Train Loss: 0.8111 | Val Loss: 0.6713 | MAE: 0.5651 | RMSE: 0.6713 | RÂ²: 0.5175\n",
      "Epoch 5/20 | Train Loss: 0.7474 | Val Loss: 0.5898 | MAE: 0.4893 | RMSE: 0.5898 | RÂ²: 0.6276\n",
      "Epoch 6/20 | Train Loss: 0.7153 | Val Loss: 0.5159 | MAE: 0.4183 | RMSE: 0.5159 | RÂ²: 0.7151\n",
      "Epoch 7/20 | Train Loss: 0.6785 | Val Loss: 0.4595 | MAE: 0.3643 | RMSE: 0.4595 | RÂ²: 0.7739\n",
      "Epoch 8/20 | Train Loss: 0.6400 | Val Loss: 0.4216 | MAE: 0.3255 | RMSE: 0.4216 | RÂ²: 0.8097\n",
      "Epoch 9/20 | Train Loss: 0.6292 | Val Loss: 0.4042 | MAE: 0.2992 | RMSE: 0.4042 | RÂ²: 0.8251\n",
      "Epoch 10/20 | Train Loss: 0.6182 | Val Loss: 0.3963 | MAE: 0.2834 | RMSE: 0.3963 | RÂ²: 0.8318\n",
      "Epoch 11/20 | Train Loss: 0.5973 | Val Loss: 0.3811 | MAE: 0.2703 | RMSE: 0.3811 | RÂ²: 0.8445\n",
      "Epoch 12/20 | Train Loss: 0.5885 | Val Loss: 0.3660 | MAE: 0.2615 | RMSE: 0.3660 | RÂ²: 0.8566\n",
      "Epoch 13/20 | Train Loss: 0.5737 | Val Loss: 0.3561 | MAE: 0.2562 | RMSE: 0.3561 | RÂ²: 0.8643\n",
      "Epoch 14/20 | Train Loss: 0.5593 | Val Loss: 0.3508 | MAE: 0.2544 | RMSE: 0.3508 | RÂ²: 0.8683\n",
      "Epoch 15/20 | Train Loss: 0.5578 | Val Loss: 0.3495 | MAE: 0.2535 | RMSE: 0.3495 | RÂ²: 0.8693\n",
      "Epoch 16/20 | Train Loss: 0.5369 | Val Loss: 0.3530 | MAE: 0.2530 | RMSE: 0.3530 | RÂ²: 0.8666\n",
      "Epoch 17/20 | Train Loss: 0.5257 | Val Loss: 0.3548 | MAE: 0.2526 | RMSE: 0.3548 | RÂ²: 0.8653\n",
      "Epoch 18/20 | Train Loss: 0.5201 | Val Loss: 0.3531 | MAE: 0.2519 | RMSE: 0.3531 | RÂ²: 0.8665\n",
      "Epoch 19/20 | Train Loss: 0.5049 | Val Loss: 0.3488 | MAE: 0.2509 | RMSE: 0.3488 | RÂ²: 0.8697\n",
      "Epoch 20/20 | Train Loss: 0.5073 | Val Loss: 0.3499 | MAE: 0.2493 | RMSE: 0.3499 | RÂ²: 0.8689\n",
      "âœ… RMSE = 0.3488 | MAE = 0.2509\n",
      "\n",
      "ðŸ”§ [256/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[1024, 512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8944 | Val Loss: 0.8148 | MAE: 0.6813 | RMSE: 0.8148 | RÂ²: 0.2892\n",
      "Epoch 2/20 | Train Loss: 0.6477 | Val Loss: 0.5967 | MAE: 0.4907 | RMSE: 0.5967 | RÂ²: 0.6188\n",
      "Epoch 3/20 | Train Loss: 0.5818 | Val Loss: 0.4736 | MAE: 0.3562 | RMSE: 0.4736 | RÂ²: 0.7599\n",
      "Epoch 4/20 | Train Loss: 0.5121 | Val Loss: 0.3122 | MAE: 0.2285 | RMSE: 0.3122 | RÂ²: 0.8956\n",
      "Epoch 5/20 | Train Loss: 0.4788 | Val Loss: 0.3225 | MAE: 0.2301 | RMSE: 0.3225 | RÂ²: 0.8887\n",
      "Epoch 6/20 | Train Loss: 0.4481 | Val Loss: 0.3439 | MAE: 0.2455 | RMSE: 0.3439 | RÂ²: 0.8734\n",
      "Epoch 7/20 | Train Loss: 0.4240 | Val Loss: 0.3696 | MAE: 0.2812 | RMSE: 0.3696 | RÂ²: 0.8538\n",
      "Epoch 8/20 | Train Loss: 0.3986 | Val Loss: 0.3422 | MAE: 0.2593 | RMSE: 0.3422 | RÂ²: 0.8746\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3122 | MAE = 0.2285\n",
      "\n",
      "ðŸ”§ [257/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[512, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 1.0570 | Val Loss: 0.9345 | MAE: 0.7886 | RMSE: 0.9345 | RÂ²: 0.0652\n",
      "Epoch 2/20 | Train Loss: 0.9283 | Val Loss: 0.8515 | MAE: 0.7219 | RMSE: 0.8515 | RÂ²: 0.2237\n",
      "Epoch 3/20 | Train Loss: 0.8792 | Val Loss: 0.7659 | MAE: 0.6494 | RMSE: 0.7659 | RÂ²: 0.3720\n",
      "Epoch 4/20 | Train Loss: 0.8177 | Val Loss: 0.6813 | MAE: 0.5758 | RMSE: 0.6813 | RÂ²: 0.5031\n",
      "Epoch 5/20 | Train Loss: 0.7693 | Val Loss: 0.5999 | MAE: 0.5013 | RMSE: 0.5999 | RÂ²: 0.6148\n",
      "Epoch 6/20 | Train Loss: 0.7281 | Val Loss: 0.5203 | MAE: 0.4261 | RMSE: 0.5203 | RÂ²: 0.7102\n",
      "Epoch 7/20 | Train Loss: 0.7084 | Val Loss: 0.4605 | MAE: 0.3699 | RMSE: 0.4605 | RÂ²: 0.7730\n",
      "Epoch 8/20 | Train Loss: 0.6732 | Val Loss: 0.4181 | MAE: 0.3248 | RMSE: 0.4181 | RÂ²: 0.8128\n",
      "Epoch 9/20 | Train Loss: 0.6489 | Val Loss: 0.3992 | MAE: 0.2993 | RMSE: 0.3992 | RÂ²: 0.8294\n",
      "Epoch 10/20 | Train Loss: 0.6257 | Val Loss: 0.3845 | MAE: 0.2790 | RMSE: 0.3845 | RÂ²: 0.8418\n",
      "Epoch 11/20 | Train Loss: 0.6093 | Val Loss: 0.3715 | MAE: 0.2662 | RMSE: 0.3715 | RÂ²: 0.8522\n",
      "Epoch 12/20 | Train Loss: 0.5924 | Val Loss: 0.3631 | MAE: 0.2610 | RMSE: 0.3631 | RÂ²: 0.8589\n",
      "Epoch 13/20 | Train Loss: 0.5842 | Val Loss: 0.3652 | MAE: 0.2611 | RMSE: 0.3652 | RÂ²: 0.8572\n",
      "Epoch 14/20 | Train Loss: 0.5777 | Val Loss: 0.3694 | MAE: 0.2614 | RMSE: 0.3694 | RÂ²: 0.8539\n",
      "Epoch 15/20 | Train Loss: 0.5583 | Val Loss: 0.3665 | MAE: 0.2605 | RMSE: 0.3665 | RÂ²: 0.8562\n",
      "Epoch 16/20 | Train Loss: 0.5461 | Val Loss: 0.3687 | MAE: 0.2618 | RMSE: 0.3687 | RÂ²: 0.8545\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3631 | MAE = 0.2610\n",
      "\n",
      "ðŸ”§ [258/300] Entrenando con: lr=0.0001, dropout=0.2, hidden_sizes=[512, 512, 512], seed=7777\n",
      "Epoch 1/20 | Train Loss: 1.0317 | Val Loss: 0.9485 | MAE: 0.8046 | RMSE: 0.9485 | RÂ²: 0.0369\n",
      "Epoch 2/20 | Train Loss: 0.7585 | Val Loss: 0.8269 | MAE: 0.7025 | RMSE: 0.8269 | RÂ²: 0.2680\n",
      "Epoch 3/20 | Train Loss: 0.6219 | Val Loss: 0.7191 | MAE: 0.6069 | RMSE: 0.7191 | RÂ²: 0.4463\n",
      "Epoch 4/20 | Train Loss: 0.5471 | Val Loss: 0.5929 | MAE: 0.4872 | RMSE: 0.5929 | RÂ²: 0.6236\n",
      "Epoch 5/20 | Train Loss: 0.4987 | Val Loss: 0.4487 | MAE: 0.3565 | RMSE: 0.4487 | RÂ²: 0.7844\n",
      "Epoch 6/20 | Train Loss: 0.4716 | Val Loss: 0.3694 | MAE: 0.2796 | RMSE: 0.3694 | RÂ²: 0.8539\n",
      "Epoch 7/20 | Train Loss: 0.4459 | Val Loss: 0.3301 | MAE: 0.2483 | RMSE: 0.3301 | RÂ²: 0.8833\n",
      "Epoch 8/20 | Train Loss: 0.4315 | Val Loss: 0.3150 | MAE: 0.2344 | RMSE: 0.3150 | RÂ²: 0.8938\n",
      "Epoch 9/20 | Train Loss: 0.4169 | Val Loss: 0.3065 | MAE: 0.2221 | RMSE: 0.3065 | RÂ²: 0.8995\n",
      "Epoch 10/20 | Train Loss: 0.4092 | Val Loss: 0.2989 | MAE: 0.2128 | RMSE: 0.2989 | RÂ²: 0.9044\n",
      "Epoch 11/20 | Train Loss: 0.3970 | Val Loss: 0.2903 | MAE: 0.2063 | RMSE: 0.2903 | RÂ²: 0.9098\n",
      "Epoch 12/20 | Train Loss: 0.3945 | Val Loss: 0.2877 | MAE: 0.2050 | RMSE: 0.2877 | RÂ²: 0.9114\n",
      "Epoch 13/20 | Train Loss: 0.3851 | Val Loss: 0.2886 | MAE: 0.2031 | RMSE: 0.2886 | RÂ²: 0.9108\n",
      "Epoch 14/20 | Train Loss: 0.3856 | Val Loss: 0.2913 | MAE: 0.2053 | RMSE: 0.2913 | RÂ²: 0.9091\n",
      "Epoch 15/20 | Train Loss: 0.3693 | Val Loss: 0.3042 | MAE: 0.2090 | RMSE: 0.3042 | RÂ²: 0.9009\n",
      "Epoch 16/20 | Train Loss: 0.3701 | Val Loss: 0.2924 | MAE: 0.2045 | RMSE: 0.2924 | RÂ²: 0.9084\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2877 | MAE = 0.2050\n",
      "\n",
      "ðŸ”§ [259/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[1024, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9789 | Val Loss: 0.9144 | MAE: 0.7747 | RMSE: 0.9144 | RÂ²: 0.1049\n",
      "Epoch 2/20 | Train Loss: 0.7927 | Val Loss: 0.7943 | MAE: 0.6743 | RMSE: 0.7943 | RÂ²: 0.3247\n",
      "Epoch 3/20 | Train Loss: 0.6909 | Val Loss: 0.6788 | MAE: 0.5688 | RMSE: 0.6788 | RÂ²: 0.5067\n",
      "Epoch 4/20 | Train Loss: 0.6273 | Val Loss: 0.5514 | MAE: 0.4439 | RMSE: 0.5514 | RÂ²: 0.6745\n",
      "Epoch 5/20 | Train Loss: 0.5874 | Val Loss: 0.4468 | MAE: 0.3356 | RMSE: 0.4468 | RÂ²: 0.7862\n",
      "Epoch 6/20 | Train Loss: 0.5473 | Val Loss: 0.3989 | MAE: 0.2816 | RMSE: 0.3989 | RÂ²: 0.8296\n",
      "Epoch 7/20 | Train Loss: 0.5416 | Val Loss: 0.3724 | MAE: 0.2586 | RMSE: 0.3724 | RÂ²: 0.8516\n",
      "Epoch 8/20 | Train Loss: 0.5202 | Val Loss: 0.3423 | MAE: 0.2447 | RMSE: 0.3423 | RÂ²: 0.8745\n",
      "Epoch 9/20 | Train Loss: 0.4985 | Val Loss: 0.3365 | MAE: 0.2407 | RMSE: 0.3365 | RÂ²: 0.8788\n",
      "Epoch 10/20 | Train Loss: 0.4857 | Val Loss: 0.3372 | MAE: 0.2413 | RMSE: 0.3372 | RÂ²: 0.8783\n",
      "Epoch 11/20 | Train Loss: 0.4816 | Val Loss: 0.3443 | MAE: 0.2449 | RMSE: 0.3443 | RÂ²: 0.8731\n",
      "Epoch 12/20 | Train Loss: 0.4707 | Val Loss: 0.3504 | MAE: 0.2469 | RMSE: 0.3504 | RÂ²: 0.8686\n",
      "Epoch 13/20 | Train Loss: 0.4633 | Val Loss: 0.3564 | MAE: 0.2505 | RMSE: 0.3564 | RÂ²: 0.8640\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3365 | MAE = 0.2407\n",
      "\n",
      "ðŸ”§ [260/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[2048, 1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8815 | Val Loss: 0.8808 | MAE: 0.7355 | RMSE: 0.8808 | RÂ²: 0.1694\n",
      "Epoch 2/20 | Train Loss: 0.5000 | Val Loss: 0.5864 | MAE: 0.4855 | RMSE: 0.5864 | RÂ²: 0.6319\n",
      "Epoch 3/20 | Train Loss: 0.4240 | Val Loss: 0.3467 | MAE: 0.2677 | RMSE: 0.3467 | RÂ²: 0.8713\n",
      "Epoch 4/20 | Train Loss: 0.3934 | Val Loss: 0.3369 | MAE: 0.2389 | RMSE: 0.3369 | RÂ²: 0.8785\n",
      "Epoch 5/20 | Train Loss: 0.3615 | Val Loss: 0.3559 | MAE: 0.2609 | RMSE: 0.3559 | RÂ²: 0.8644\n",
      "Epoch 6/20 | Train Loss: 0.3559 | Val Loss: 0.3033 | MAE: 0.2295 | RMSE: 0.3033 | RÂ²: 0.9015\n",
      "Epoch 7/20 | Train Loss: 0.3335 | Val Loss: 0.3038 | MAE: 0.2279 | RMSE: 0.3038 | RÂ²: 0.9012\n",
      "Epoch 8/20 | Train Loss: 0.3276 | Val Loss: 0.2804 | MAE: 0.2042 | RMSE: 0.2804 | RÂ²: 0.9158\n",
      "Epoch 9/20 | Train Loss: 0.3153 | Val Loss: 0.2772 | MAE: 0.1959 | RMSE: 0.2772 | RÂ²: 0.9178\n",
      "Epoch 10/20 | Train Loss: 0.3094 | Val Loss: 0.2808 | MAE: 0.1932 | RMSE: 0.2808 | RÂ²: 0.9156\n",
      "Epoch 11/20 | Train Loss: 0.3043 | Val Loss: 0.2615 | MAE: 0.1790 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 12/20 | Train Loss: 0.3019 | Val Loss: 0.2742 | MAE: 0.1882 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "Epoch 13/20 | Train Loss: 0.2963 | Val Loss: 0.2687 | MAE: 0.1866 | RMSE: 0.2687 | RÂ²: 0.9227\n",
      "Epoch 14/20 | Train Loss: 0.2874 | Val Loss: 0.2647 | MAE: 0.1819 | RMSE: 0.2647 | RÂ²: 0.9250\n",
      "Epoch 15/20 | Train Loss: 0.2890 | Val Loss: 0.2680 | MAE: 0.1850 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2615 | MAE = 0.1790\n",
      "\n",
      "ðŸ”§ [261/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[512, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8947 | Val Loss: 0.8716 | MAE: 0.7391 | RMSE: 0.8716 | RÂ²: 0.1867\n",
      "Epoch 2/20 | Train Loss: 0.6781 | Val Loss: 0.7786 | MAE: 0.6482 | RMSE: 0.7786 | RÂ²: 0.3511\n",
      "Epoch 3/20 | Train Loss: 0.5701 | Val Loss: 0.6051 | MAE: 0.4976 | RMSE: 0.6051 | RÂ²: 0.6080\n",
      "Epoch 4/20 | Train Loss: 0.5162 | Val Loss: 0.4899 | MAE: 0.3788 | RMSE: 0.4899 | RÂ²: 0.7430\n",
      "Epoch 5/20 | Train Loss: 0.4845 | Val Loss: 0.3923 | MAE: 0.2925 | RMSE: 0.3923 | RÂ²: 0.8353\n",
      "Epoch 6/20 | Train Loss: 0.4519 | Val Loss: 0.3403 | MAE: 0.2468 | RMSE: 0.3403 | RÂ²: 0.8760\n",
      "Epoch 7/20 | Train Loss: 0.4371 | Val Loss: 0.3267 | MAE: 0.2343 | RMSE: 0.3267 | RÂ²: 0.8857\n",
      "Epoch 8/20 | Train Loss: 0.4172 | Val Loss: 0.3311 | MAE: 0.2367 | RMSE: 0.3311 | RÂ²: 0.8826\n",
      "Epoch 9/20 | Train Loss: 0.4112 | Val Loss: 0.3222 | MAE: 0.2416 | RMSE: 0.3222 | RÂ²: 0.8888\n",
      "Epoch 10/20 | Train Loss: 0.3952 | Val Loss: 0.3213 | MAE: 0.2401 | RMSE: 0.3213 | RÂ²: 0.8895\n",
      "Epoch 11/20 | Train Loss: 0.3935 | Val Loss: 0.3062 | MAE: 0.2285 | RMSE: 0.3062 | RÂ²: 0.8996\n",
      "Epoch 12/20 | Train Loss: 0.3818 | Val Loss: 0.3002 | MAE: 0.2210 | RMSE: 0.3002 | RÂ²: 0.9035\n",
      "Epoch 13/20 | Train Loss: 0.3708 | Val Loss: 0.2889 | MAE: 0.2075 | RMSE: 0.2889 | RÂ²: 0.9106\n",
      "Epoch 14/20 | Train Loss: 0.3688 | Val Loss: 0.2919 | MAE: 0.2118 | RMSE: 0.2919 | RÂ²: 0.9088\n",
      "Epoch 15/20 | Train Loss: 0.3593 | Val Loss: 0.2954 | MAE: 0.2129 | RMSE: 0.2954 | RÂ²: 0.9066\n",
      "Epoch 16/20 | Train Loss: 0.3537 | Val Loss: 0.2881 | MAE: 0.2054 | RMSE: 0.2881 | RÂ²: 0.9112\n",
      "Epoch 17/20 | Train Loss: 0.3482 | Val Loss: 0.2806 | MAE: 0.2032 | RMSE: 0.2806 | RÂ²: 0.9157\n",
      "Epoch 18/20 | Train Loss: 0.3406 | Val Loss: 0.2770 | MAE: 0.1998 | RMSE: 0.2770 | RÂ²: 0.9178\n",
      "Epoch 19/20 | Train Loss: 0.3361 | Val Loss: 0.2767 | MAE: 0.1982 | RMSE: 0.2767 | RÂ²: 0.9181\n",
      "Epoch 20/20 | Train Loss: 0.3288 | Val Loss: 0.2781 | MAE: 0.1955 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "âœ… RMSE = 0.2767 | MAE = 0.1982\n",
      "\n",
      "ðŸ”§ [262/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[256, 128], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7291 | Val Loss: 0.7417 | MAE: 0.6134 | RMSE: 0.7417 | RÂ²: 0.4111\n",
      "Epoch 2/20 | Train Loss: 0.4366 | Val Loss: 0.4603 | MAE: 0.3802 | RMSE: 0.4603 | RÂ²: 0.7731\n",
      "Epoch 3/20 | Train Loss: 0.3677 | Val Loss: 0.3535 | MAE: 0.2731 | RMSE: 0.3535 | RÂ²: 0.8663\n",
      "Epoch 4/20 | Train Loss: 0.3234 | Val Loss: 0.2911 | MAE: 0.2174 | RMSE: 0.2911 | RÂ²: 0.9093\n",
      "Epoch 5/20 | Train Loss: 0.2994 | Val Loss: 0.2613 | MAE: 0.1842 | RMSE: 0.2613 | RÂ²: 0.9269\n",
      "Epoch 6/20 | Train Loss: 0.2858 | Val Loss: 0.2686 | MAE: 0.1834 | RMSE: 0.2686 | RÂ²: 0.9228\n",
      "Epoch 7/20 | Train Loss: 0.2779 | Val Loss: 0.2641 | MAE: 0.1802 | RMSE: 0.2641 | RÂ²: 0.9253\n",
      "Epoch 8/20 | Train Loss: 0.2701 | Val Loss: 0.2752 | MAE: 0.1891 | RMSE: 0.2752 | RÂ²: 0.9189\n",
      "Epoch 9/20 | Train Loss: 0.2645 | Val Loss: 0.2608 | MAE: 0.1779 | RMSE: 0.2608 | RÂ²: 0.9272\n",
      "Epoch 10/20 | Train Loss: 0.2597 | Val Loss: 0.2926 | MAE: 0.2020 | RMSE: 0.2926 | RÂ²: 0.9084\n",
      "Epoch 11/20 | Train Loss: 0.2540 | Val Loss: 0.2658 | MAE: 0.1816 | RMSE: 0.2658 | RÂ²: 0.9243\n",
      "Epoch 12/20 | Train Loss: 0.2481 | Val Loss: 0.2616 | MAE: 0.1791 | RMSE: 0.2616 | RÂ²: 0.9267\n",
      "Epoch 13/20 | Train Loss: 0.2511 | Val Loss: 0.2804 | MAE: 0.1961 | RMSE: 0.2804 | RÂ²: 0.9158\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2608 | MAE = 0.1779\n",
      "\n",
      "ðŸ”§ [263/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 1.0522 | Val Loss: 0.9190 | MAE: 0.7811 | RMSE: 0.9190 | RÂ²: 0.0959\n",
      "Epoch 2/20 | Train Loss: 0.9731 | Val Loss: 0.8601 | MAE: 0.7319 | RMSE: 0.8601 | RÂ²: 0.2080\n",
      "Epoch 3/20 | Train Loss: 0.9303 | Val Loss: 0.8075 | MAE: 0.6839 | RMSE: 0.8075 | RÂ²: 0.3019\n",
      "Epoch 4/20 | Train Loss: 0.8810 | Val Loss: 0.7583 | MAE: 0.6382 | RMSE: 0.7583 | RÂ²: 0.3843\n",
      "Epoch 5/20 | Train Loss: 0.8546 | Val Loss: 0.7130 | MAE: 0.5992 | RMSE: 0.7130 | RÂ²: 0.4558\n",
      "Epoch 6/20 | Train Loss: 0.8222 | Val Loss: 0.6732 | MAE: 0.5657 | RMSE: 0.6732 | RÂ²: 0.5148\n",
      "Epoch 7/20 | Train Loss: 0.7862 | Val Loss: 0.6381 | MAE: 0.5341 | RMSE: 0.6381 | RÂ²: 0.5640\n",
      "Epoch 8/20 | Train Loss: 0.7579 | Val Loss: 0.6060 | MAE: 0.5033 | RMSE: 0.6060 | RÂ²: 0.6069\n",
      "Epoch 9/20 | Train Loss: 0.7344 | Val Loss: 0.5737 | MAE: 0.4715 | RMSE: 0.5737 | RÂ²: 0.6477\n",
      "Epoch 10/20 | Train Loss: 0.7203 | Val Loss: 0.5447 | MAE: 0.4431 | RMSE: 0.5447 | RÂ²: 0.6823\n",
      "Epoch 11/20 | Train Loss: 0.6918 | Val Loss: 0.5178 | MAE: 0.4174 | RMSE: 0.5178 | RÂ²: 0.7129\n",
      "Epoch 12/20 | Train Loss: 0.6799 | Val Loss: 0.4931 | MAE: 0.3950 | RMSE: 0.4931 | RÂ²: 0.7396\n",
      "Epoch 13/20 | Train Loss: 0.6576 | Val Loss: 0.4697 | MAE: 0.3736 | RMSE: 0.4697 | RÂ²: 0.7638\n",
      "Epoch 14/20 | Train Loss: 0.6460 | Val Loss: 0.4526 | MAE: 0.3579 | RMSE: 0.4526 | RÂ²: 0.7807\n",
      "Epoch 15/20 | Train Loss: 0.6324 | Val Loss: 0.4385 | MAE: 0.3452 | RMSE: 0.4385 | RÂ²: 0.7942\n",
      "Epoch 16/20 | Train Loss: 0.6161 | Val Loss: 0.4253 | MAE: 0.3326 | RMSE: 0.4253 | RÂ²: 0.8064\n",
      "Epoch 17/20 | Train Loss: 0.6008 | Val Loss: 0.4125 | MAE: 0.3206 | RMSE: 0.4125 | RÂ²: 0.8178\n",
      "Epoch 18/20 | Train Loss: 0.5836 | Val Loss: 0.3984 | MAE: 0.3079 | RMSE: 0.3984 | RÂ²: 0.8301\n",
      "Epoch 19/20 | Train Loss: 0.5848 | Val Loss: 0.3859 | MAE: 0.2964 | RMSE: 0.3859 | RÂ²: 0.8406\n",
      "Epoch 20/20 | Train Loss: 0.5700 | Val Loss: 0.3753 | MAE: 0.2871 | RMSE: 0.3753 | RÂ²: 0.8492\n",
      "âœ… RMSE = 0.3753 | MAE = 0.2871\n",
      "\n",
      "ðŸ”§ [264/300] Entrenando con: lr=0.001, dropout=0.3, hidden_sizes=[1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8844 | Val Loss: 0.7756 | MAE: 0.6481 | RMSE: 0.7756 | RÂ²: 0.3561\n",
      "Epoch 2/20 | Train Loss: 0.4847 | Val Loss: 0.4517 | MAE: 0.3674 | RMSE: 0.4517 | RÂ²: 0.7816\n",
      "Epoch 3/20 | Train Loss: 0.4087 | Val Loss: 0.3020 | MAE: 0.2142 | RMSE: 0.3020 | RÂ²: 0.9024\n",
      "Epoch 4/20 | Train Loss: 0.3565 | Val Loss: 0.3001 | MAE: 0.2199 | RMSE: 0.3001 | RÂ²: 0.9036\n",
      "Epoch 5/20 | Train Loss: 0.3378 | Val Loss: 0.3185 | MAE: 0.2422 | RMSE: 0.3185 | RÂ²: 0.8914\n",
      "Epoch 6/20 | Train Loss: 0.3231 | Val Loss: 0.3240 | MAE: 0.2385 | RMSE: 0.3240 | RÂ²: 0.8876\n",
      "Epoch 7/20 | Train Loss: 0.3077 | Val Loss: 0.3111 | MAE: 0.2312 | RMSE: 0.3111 | RÂ²: 0.8964\n",
      "Epoch 8/20 | Train Loss: 0.3009 | Val Loss: 0.2754 | MAE: 0.1835 | RMSE: 0.2754 | RÂ²: 0.9188\n",
      "Epoch 9/20 | Train Loss: 0.2930 | Val Loss: 0.2708 | MAE: 0.1894 | RMSE: 0.2708 | RÂ²: 0.9215\n",
      "Epoch 10/20 | Train Loss: 0.2820 | Val Loss: 0.2591 | MAE: 0.1761 | RMSE: 0.2591 | RÂ²: 0.9281\n",
      "Epoch 11/20 | Train Loss: 0.2790 | Val Loss: 0.2599 | MAE: 0.1759 | RMSE: 0.2599 | RÂ²: 0.9277\n",
      "Epoch 12/20 | Train Loss: 0.2736 | Val Loss: 0.2590 | MAE: 0.1760 | RMSE: 0.2590 | RÂ²: 0.9282\n",
      "Epoch 13/20 | Train Loss: 0.2689 | Val Loss: 0.2677 | MAE: 0.1815 | RMSE: 0.2677 | RÂ²: 0.9233\n",
      "Epoch 14/20 | Train Loss: 0.2658 | Val Loss: 0.2529 | MAE: 0.1721 | RMSE: 0.2529 | RÂ²: 0.9315\n",
      "Epoch 15/20 | Train Loss: 0.2675 | Val Loss: 0.2951 | MAE: 0.2088 | RMSE: 0.2951 | RÂ²: 0.9068\n",
      "Epoch 16/20 | Train Loss: 0.2590 | Val Loss: 0.2539 | MAE: 0.1727 | RMSE: 0.2539 | RÂ²: 0.9310\n",
      "Epoch 17/20 | Train Loss: 0.2560 | Val Loss: 0.2680 | MAE: 0.1845 | RMSE: 0.2680 | RÂ²: 0.9231\n",
      "Epoch 18/20 | Train Loss: 0.2521 | Val Loss: 0.2529 | MAE: 0.1727 | RMSE: 0.2529 | RÂ²: 0.9315\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2529 | MAE = 0.1721\n",
      "\n",
      "ðŸ”§ [265/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[256, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 1.0556 | Val Loss: 0.9441 | MAE: 0.7967 | RMSE: 0.9441 | RÂ²: 0.0458\n",
      "Epoch 2/20 | Train Loss: 0.9316 | Val Loss: 0.8427 | MAE: 0.7139 | RMSE: 0.8427 | RÂ²: 0.2398\n",
      "Epoch 3/20 | Train Loss: 0.8650 | Val Loss: 0.7547 | MAE: 0.6292 | RMSE: 0.7547 | RÂ²: 0.3902\n",
      "Epoch 4/20 | Train Loss: 0.8180 | Val Loss: 0.6815 | MAE: 0.5572 | RMSE: 0.6815 | RÂ²: 0.5027\n",
      "Epoch 5/20 | Train Loss: 0.7968 | Val Loss: 0.6149 | MAE: 0.5003 | RMSE: 0.6149 | RÂ²: 0.5952\n",
      "Epoch 6/20 | Train Loss: 0.7575 | Val Loss: 0.5601 | MAE: 0.4557 | RMSE: 0.5601 | RÂ²: 0.6642\n",
      "Epoch 7/20 | Train Loss: 0.7376 | Val Loss: 0.5184 | MAE: 0.4233 | RMSE: 0.5184 | RÂ²: 0.7123\n",
      "Epoch 8/20 | Train Loss: 0.7212 | Val Loss: 0.4874 | MAE: 0.3964 | RMSE: 0.4874 | RÂ²: 0.7457\n",
      "Epoch 9/20 | Train Loss: 0.7054 | Val Loss: 0.4600 | MAE: 0.3719 | RMSE: 0.4600 | RÂ²: 0.7734\n",
      "Epoch 10/20 | Train Loss: 0.6746 | Val Loss: 0.4369 | MAE: 0.3493 | RMSE: 0.4369 | RÂ²: 0.7957\n",
      "Epoch 11/20 | Train Loss: 0.6637 | Val Loss: 0.4190 | MAE: 0.3312 | RMSE: 0.4190 | RÂ²: 0.8121\n",
      "Epoch 12/20 | Train Loss: 0.6476 | Val Loss: 0.4016 | MAE: 0.3146 | RMSE: 0.4016 | RÂ²: 0.8273\n",
      "Epoch 13/20 | Train Loss: 0.6305 | Val Loss: 0.3896 | MAE: 0.3027 | RMSE: 0.3896 | RÂ²: 0.8375\n",
      "Epoch 14/20 | Train Loss: 0.6137 | Val Loss: 0.3780 | MAE: 0.2911 | RMSE: 0.3780 | RÂ²: 0.8471\n",
      "Epoch 15/20 | Train Loss: 0.6006 | Val Loss: 0.3699 | MAE: 0.2825 | RMSE: 0.3699 | RÂ²: 0.8536\n",
      "Epoch 16/20 | Train Loss: 0.5998 | Val Loss: 0.3644 | MAE: 0.2756 | RMSE: 0.3644 | RÂ²: 0.8578\n",
      "Epoch 17/20 | Train Loss: 0.5873 | Val Loss: 0.3628 | MAE: 0.2709 | RMSE: 0.3628 | RÂ²: 0.8591\n",
      "Epoch 18/20 | Train Loss: 0.5815 | Val Loss: 0.3615 | MAE: 0.2667 | RMSE: 0.3615 | RÂ²: 0.8601\n",
      "Epoch 19/20 | Train Loss: 0.5659 | Val Loss: 0.3597 | MAE: 0.2631 | RMSE: 0.3597 | RÂ²: 0.8615\n",
      "Epoch 20/20 | Train Loss: 0.5624 | Val Loss: 0.3567 | MAE: 0.2597 | RMSE: 0.3567 | RÂ²: 0.8638\n",
      "âœ… RMSE = 0.3567 | MAE = 0.2597\n",
      "\n",
      "ðŸ”§ [266/300] Entrenando con: lr=0.001, dropout=0.1, hidden_sizes=[1024, 512, 256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.6866 | Val Loss: 0.8391 | MAE: 0.7090 | RMSE: 0.8391 | RÂ²: 0.2463\n",
      "Epoch 2/20 | Train Loss: 0.3518 | Val Loss: 0.5538 | MAE: 0.4600 | RMSE: 0.5538 | RÂ²: 0.6716\n",
      "Epoch 3/20 | Train Loss: 0.3023 | Val Loss: 0.2897 | MAE: 0.2245 | RMSE: 0.2897 | RÂ²: 0.9101\n",
      "Epoch 4/20 | Train Loss: 0.2794 | Val Loss: 0.2567 | MAE: 0.1845 | RMSE: 0.2567 | RÂ²: 0.9294\n",
      "Epoch 5/20 | Train Loss: 0.2680 | Val Loss: 0.2747 | MAE: 0.1910 | RMSE: 0.2747 | RÂ²: 0.9192\n",
      "Epoch 6/20 | Train Loss: 0.2569 | Val Loss: 0.2773 | MAE: 0.1987 | RMSE: 0.2773 | RÂ²: 0.9177\n",
      "Epoch 7/20 | Train Loss: 0.2478 | Val Loss: 0.2640 | MAE: 0.1801 | RMSE: 0.2640 | RÂ²: 0.9254\n",
      "Epoch 8/20 | Train Loss: 0.2405 | Val Loss: 0.2578 | MAE: 0.1727 | RMSE: 0.2578 | RÂ²: 0.9289\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2567 | MAE = 0.1845\n",
      "\n",
      "ðŸ”§ [267/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[1024, 256, 1024], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8627 | Val Loss: 0.9206 | MAE: 0.7810 | RMSE: 0.9206 | RÂ²: 0.0926\n",
      "Epoch 2/20 | Train Loss: 0.5949 | Val Loss: 0.8368 | MAE: 0.7030 | RMSE: 0.8368 | RÂ²: 0.2504\n",
      "Epoch 3/20 | Train Loss: 0.4798 | Val Loss: 0.6728 | MAE: 0.5595 | RMSE: 0.6728 | RÂ²: 0.5154\n",
      "Epoch 4/20 | Train Loss: 0.4151 | Val Loss: 0.5120 | MAE: 0.4190 | RMSE: 0.5120 | RÂ²: 0.7194\n",
      "Epoch 5/20 | Train Loss: 0.3837 | Val Loss: 0.4273 | MAE: 0.3478 | RMSE: 0.4273 | RÂ²: 0.8045\n",
      "Epoch 6/20 | Train Loss: 0.3630 | Val Loss: 0.3564 | MAE: 0.2774 | RMSE: 0.3564 | RÂ²: 0.8640\n",
      "Epoch 7/20 | Train Loss: 0.3449 | Val Loss: 0.3040 | MAE: 0.2251 | RMSE: 0.3040 | RÂ²: 0.9011\n",
      "Epoch 8/20 | Train Loss: 0.3266 | Val Loss: 0.2935 | MAE: 0.2108 | RMSE: 0.2935 | RÂ²: 0.9078\n",
      "Epoch 9/20 | Train Loss: 0.3209 | Val Loss: 0.2819 | MAE: 0.1987 | RMSE: 0.2819 | RÂ²: 0.9150\n",
      "Epoch 10/20 | Train Loss: 0.3127 | Val Loss: 0.2853 | MAE: 0.2007 | RMSE: 0.2853 | RÂ²: 0.9129\n",
      "Epoch 11/20 | Train Loss: 0.3053 | Val Loss: 0.2897 | MAE: 0.2010 | RMSE: 0.2897 | RÂ²: 0.9102\n",
      "Epoch 12/20 | Train Loss: 0.3021 | Val Loss: 0.2838 | MAE: 0.1994 | RMSE: 0.2838 | RÂ²: 0.9137\n",
      "Epoch 13/20 | Train Loss: 0.2941 | Val Loss: 0.2820 | MAE: 0.1980 | RMSE: 0.2820 | RÂ²: 0.9148\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2819 | MAE = 0.1987\n",
      "\n",
      "ðŸ”§ [268/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[1024, 512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.8276 | Val Loss: 0.9097 | MAE: 0.7685 | RMSE: 0.9097 | RÂ²: 0.1140\n",
      "Epoch 2/20 | Train Loss: 0.5344 | Val Loss: 0.8135 | MAE: 0.6796 | RMSE: 0.8135 | RÂ²: 0.2914\n",
      "Epoch 3/20 | Train Loss: 0.4354 | Val Loss: 0.6424 | MAE: 0.5365 | RMSE: 0.6424 | RÂ²: 0.5582\n",
      "Epoch 4/20 | Train Loss: 0.3937 | Val Loss: 0.5125 | MAE: 0.4278 | RMSE: 0.5125 | RÂ²: 0.7188\n",
      "Epoch 5/20 | Train Loss: 0.3542 | Val Loss: 0.4139 | MAE: 0.3414 | RMSE: 0.4139 | RÂ²: 0.8166\n",
      "Epoch 6/20 | Train Loss: 0.3377 | Val Loss: 0.3288 | MAE: 0.2623 | RMSE: 0.3288 | RÂ²: 0.8843\n",
      "Epoch 7/20 | Train Loss: 0.3199 | Val Loss: 0.2992 | MAE: 0.2242 | RMSE: 0.2992 | RÂ²: 0.9041\n",
      "Epoch 8/20 | Train Loss: 0.3070 | Val Loss: 0.2754 | MAE: 0.1987 | RMSE: 0.2754 | RÂ²: 0.9188\n",
      "Epoch 9/20 | Train Loss: 0.3005 | Val Loss: 0.2691 | MAE: 0.1901 | RMSE: 0.2691 | RÂ²: 0.9225\n",
      "Epoch 10/20 | Train Loss: 0.2950 | Val Loss: 0.2686 | MAE: 0.1899 | RMSE: 0.2686 | RÂ²: 0.9227\n",
      "Epoch 11/20 | Train Loss: 0.2890 | Val Loss: 0.2713 | MAE: 0.1897 | RMSE: 0.2713 | RÂ²: 0.9212\n",
      "Epoch 12/20 | Train Loss: 0.2822 | Val Loss: 0.2678 | MAE: 0.1880 | RMSE: 0.2678 | RÂ²: 0.9233\n",
      "Epoch 13/20 | Train Loss: 0.2800 | Val Loss: 0.2699 | MAE: 0.1886 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 14/20 | Train Loss: 0.2794 | Val Loss: 0.2743 | MAE: 0.1916 | RMSE: 0.2743 | RÂ²: 0.9195\n",
      "Epoch 15/20 | Train Loss: 0.2735 | Val Loss: 0.2690 | MAE: 0.1873 | RMSE: 0.2690 | RÂ²: 0.9225\n",
      "Epoch 16/20 | Train Loss: 0.2694 | Val Loss: 0.2669 | MAE: 0.1859 | RMSE: 0.2669 | RÂ²: 0.9237\n",
      "Epoch 17/20 | Train Loss: 0.2656 | Val Loss: 0.2713 | MAE: 0.1888 | RMSE: 0.2713 | RÂ²: 0.9212\n",
      "Epoch 18/20 | Train Loss: 0.2671 | Val Loss: 0.2739 | MAE: 0.1907 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 19/20 | Train Loss: 0.2660 | Val Loss: 0.2772 | MAE: 0.1922 | RMSE: 0.2772 | RÂ²: 0.9177\n",
      "Epoch 20/20 | Train Loss: 0.2604 | Val Loss: 0.2717 | MAE: 0.1907 | RMSE: 0.2717 | RÂ²: 0.9209\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2669 | MAE = 0.1859\n",
      "\n",
      "ðŸ”§ [269/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[256, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9705 | Val Loss: 0.9197 | MAE: 0.7801 | RMSE: 0.9197 | RÂ²: 0.0945\n",
      "Epoch 2/20 | Train Loss: 0.7980 | Val Loss: 0.8288 | MAE: 0.7018 | RMSE: 0.8288 | RÂ²: 0.2646\n",
      "Epoch 3/20 | Train Loss: 0.6931 | Val Loss: 0.7329 | MAE: 0.6194 | RMSE: 0.7329 | RÂ²: 0.4250\n",
      "Epoch 4/20 | Train Loss: 0.6140 | Val Loss: 0.6311 | MAE: 0.5310 | RMSE: 0.6311 | RÂ²: 0.5735\n",
      "Epoch 5/20 | Train Loss: 0.5468 | Val Loss: 0.5317 | MAE: 0.4401 | RMSE: 0.5317 | RÂ²: 0.6974\n",
      "Epoch 6/20 | Train Loss: 0.4933 | Val Loss: 0.4496 | MAE: 0.3620 | RMSE: 0.4496 | RÂ²: 0.7836\n",
      "Epoch 7/20 | Train Loss: 0.4589 | Val Loss: 0.3898 | MAE: 0.3063 | RMSE: 0.3898 | RÂ²: 0.8374\n",
      "Epoch 8/20 | Train Loss: 0.4283 | Val Loss: 0.3534 | MAE: 0.2730 | RMSE: 0.3534 | RÂ²: 0.8663\n",
      "Epoch 9/20 | Train Loss: 0.4055 | Val Loss: 0.3272 | MAE: 0.2488 | RMSE: 0.3272 | RÂ²: 0.8854\n",
      "Epoch 10/20 | Train Loss: 0.3908 | Val Loss: 0.3189 | MAE: 0.2374 | RMSE: 0.3189 | RÂ²: 0.8912\n",
      "Epoch 11/20 | Train Loss: 0.3782 | Val Loss: 0.3150 | MAE: 0.2329 | RMSE: 0.3150 | RÂ²: 0.8938\n",
      "Epoch 12/20 | Train Loss: 0.3654 | Val Loss: 0.3085 | MAE: 0.2290 | RMSE: 0.3085 | RÂ²: 0.8981\n",
      "Epoch 13/20 | Train Loss: 0.3575 | Val Loss: 0.3052 | MAE: 0.2259 | RMSE: 0.3052 | RÂ²: 0.9003\n",
      "Epoch 14/20 | Train Loss: 0.3487 | Val Loss: 0.2993 | MAE: 0.2200 | RMSE: 0.2993 | RÂ²: 0.9041\n",
      "Epoch 15/20 | Train Loss: 0.3404 | Val Loss: 0.2930 | MAE: 0.2144 | RMSE: 0.2930 | RÂ²: 0.9081\n",
      "Epoch 16/20 | Train Loss: 0.3348 | Val Loss: 0.2902 | MAE: 0.2120 | RMSE: 0.2902 | RÂ²: 0.9099\n",
      "Epoch 17/20 | Train Loss: 0.3284 | Val Loss: 0.2882 | MAE: 0.2093 | RMSE: 0.2882 | RÂ²: 0.9111\n",
      "Epoch 18/20 | Train Loss: 0.3209 | Val Loss: 0.2854 | MAE: 0.2064 | RMSE: 0.2854 | RÂ²: 0.9128\n",
      "Epoch 19/20 | Train Loss: 0.3181 | Val Loss: 0.2854 | MAE: 0.2059 | RMSE: 0.2854 | RÂ²: 0.9128\n",
      "Epoch 20/20 | Train Loss: 0.3147 | Val Loss: 0.2845 | MAE: 0.2050 | RMSE: 0.2845 | RÂ²: 0.9133\n",
      "âœ… RMSE = 0.2845 | MAE = 0.2050\n",
      "\n",
      "ðŸ”§ [270/300] Entrenando con: lr=0.001, dropout=0.2, hidden_sizes=[512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8269 | Val Loss: 0.6700 | MAE: 0.5641 | RMSE: 0.6700 | RÂ²: 0.5195\n",
      "Epoch 2/20 | Train Loss: 0.4923 | Val Loss: 0.5025 | MAE: 0.4102 | RMSE: 0.5025 | RÂ²: 0.7296\n",
      "Epoch 3/20 | Train Loss: 0.4029 | Val Loss: 0.3380 | MAE: 0.2723 | RMSE: 0.3380 | RÂ²: 0.8777\n",
      "Epoch 4/20 | Train Loss: 0.3615 | Val Loss: 0.3211 | MAE: 0.2410 | RMSE: 0.3211 | RÂ²: 0.8896\n",
      "Epoch 5/20 | Train Loss: 0.3333 | Val Loss: 0.2818 | MAE: 0.1940 | RMSE: 0.2818 | RÂ²: 0.9150\n",
      "Epoch 6/20 | Train Loss: 0.3169 | Val Loss: 0.2672 | MAE: 0.1882 | RMSE: 0.2672 | RÂ²: 0.9235\n",
      "Epoch 7/20 | Train Loss: 0.3069 | Val Loss: 0.2615 | MAE: 0.1815 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 8/20 | Train Loss: 0.2948 | Val Loss: 0.2664 | MAE: 0.1889 | RMSE: 0.2664 | RÂ²: 0.9240\n",
      "Epoch 9/20 | Train Loss: 0.2870 | Val Loss: 0.2572 | MAE: 0.1771 | RMSE: 0.2572 | RÂ²: 0.9292\n",
      "Epoch 10/20 | Train Loss: 0.2791 | Val Loss: 0.2701 | MAE: 0.1886 | RMSE: 0.2701 | RÂ²: 0.9219\n",
      "Epoch 11/20 | Train Loss: 0.2741 | Val Loss: 0.2614 | MAE: 0.1809 | RMSE: 0.2614 | RÂ²: 0.9269\n",
      "Epoch 12/20 | Train Loss: 0.2739 | Val Loss: 0.2718 | MAE: 0.1899 | RMSE: 0.2718 | RÂ²: 0.9209\n",
      "Epoch 13/20 | Train Loss: 0.2652 | Val Loss: 0.2686 | MAE: 0.1849 | RMSE: 0.2686 | RÂ²: 0.9228\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2572 | MAE = 0.1771\n",
      "\n",
      "ðŸ”§ [271/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 1.0801 | Val Loss: 0.8772 | MAE: 0.7436 | RMSE: 0.8772 | RÂ²: 0.1762\n",
      "Epoch 2/20 | Train Loss: 0.9181 | Val Loss: 0.7639 | MAE: 0.6382 | RMSE: 0.7639 | RÂ²: 0.3752\n",
      "Epoch 3/20 | Train Loss: 0.8566 | Val Loss: 0.6780 | MAE: 0.5523 | RMSE: 0.6780 | RÂ²: 0.5078\n",
      "Epoch 4/20 | Train Loss: 0.7915 | Val Loss: 0.5875 | MAE: 0.4828 | RMSE: 0.5875 | RÂ²: 0.6305\n",
      "Epoch 5/20 | Train Loss: 0.7466 | Val Loss: 0.5205 | MAE: 0.4336 | RMSE: 0.5205 | RÂ²: 0.7099\n",
      "Epoch 6/20 | Train Loss: 0.7067 | Val Loss: 0.4769 | MAE: 0.3947 | RMSE: 0.4769 | RÂ²: 0.7565\n",
      "Epoch 7/20 | Train Loss: 0.6714 | Val Loss: 0.4359 | MAE: 0.3525 | RMSE: 0.4359 | RÂ²: 0.7966\n",
      "Epoch 8/20 | Train Loss: 0.6424 | Val Loss: 0.3980 | MAE: 0.3145 | RMSE: 0.3980 | RÂ²: 0.8305\n",
      "Epoch 9/20 | Train Loss: 0.6218 | Val Loss: 0.3753 | MAE: 0.2908 | RMSE: 0.3753 | RÂ²: 0.8492\n",
      "Epoch 10/20 | Train Loss: 0.6054 | Val Loss: 0.3680 | MAE: 0.2798 | RMSE: 0.3680 | RÂ²: 0.8550\n",
      "Epoch 11/20 | Train Loss: 0.5867 | Val Loss: 0.3629 | MAE: 0.2716 | RMSE: 0.3629 | RÂ²: 0.8590\n",
      "Epoch 12/20 | Train Loss: 0.5774 | Val Loss: 0.3628 | MAE: 0.2660 | RMSE: 0.3628 | RÂ²: 0.8591\n",
      "Epoch 13/20 | Train Loss: 0.5558 | Val Loss: 0.3526 | MAE: 0.2571 | RMSE: 0.3526 | RÂ²: 0.8669\n",
      "Epoch 14/20 | Train Loss: 0.5538 | Val Loss: 0.3362 | MAE: 0.2476 | RMSE: 0.3362 | RÂ²: 0.8790\n",
      "Epoch 15/20 | Train Loss: 0.5368 | Val Loss: 0.3302 | MAE: 0.2428 | RMSE: 0.3302 | RÂ²: 0.8833\n",
      "Epoch 16/20 | Train Loss: 0.5384 | Val Loss: 0.3356 | MAE: 0.2435 | RMSE: 0.3356 | RÂ²: 0.8794\n",
      "Epoch 17/20 | Train Loss: 0.5224 | Val Loss: 0.3416 | MAE: 0.2438 | RMSE: 0.3416 | RÂ²: 0.8751\n",
      "Epoch 18/20 | Train Loss: 0.5147 | Val Loss: 0.3449 | MAE: 0.2422 | RMSE: 0.3449 | RÂ²: 0.8727\n",
      "Epoch 19/20 | Train Loss: 0.5102 | Val Loss: 0.3474 | MAE: 0.2411 | RMSE: 0.3474 | RÂ²: 0.8708\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3302 | MAE = 0.2428\n",
      "\n",
      "ðŸ”§ [272/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[512, 256], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.8845 | Val Loss: 0.6634 | MAE: 0.5387 | RMSE: 0.6634 | RÂ²: 0.5288\n",
      "Epoch 2/20 | Train Loss: 0.5936 | Val Loss: 0.4070 | MAE: 0.3271 | RMSE: 0.4070 | RÂ²: 0.8227\n",
      "Epoch 3/20 | Train Loss: 0.5276 | Val Loss: 0.3790 | MAE: 0.2805 | RMSE: 0.3790 | RÂ²: 0.8462\n",
      "Epoch 4/20 | Train Loss: 0.4643 | Val Loss: 0.3499 | MAE: 0.2631 | RMSE: 0.3499 | RÂ²: 0.8689\n",
      "Epoch 5/20 | Train Loss: 0.4323 | Val Loss: 0.3273 | MAE: 0.2433 | RMSE: 0.3273 | RÂ²: 0.8853\n",
      "Epoch 6/20 | Train Loss: 0.4036 | Val Loss: 0.3055 | MAE: 0.2192 | RMSE: 0.3055 | RÂ²: 0.9001\n",
      "Epoch 7/20 | Train Loss: 0.3803 | Val Loss: 0.3053 | MAE: 0.2258 | RMSE: 0.3053 | RÂ²: 0.9002\n",
      "Epoch 8/20 | Train Loss: 0.3666 | Val Loss: 0.2886 | MAE: 0.2101 | RMSE: 0.2886 | RÂ²: 0.9108\n",
      "Epoch 9/20 | Train Loss: 0.3483 | Val Loss: 0.2921 | MAE: 0.2166 | RMSE: 0.2921 | RÂ²: 0.9086\n",
      "Epoch 10/20 | Train Loss: 0.3427 | Val Loss: 0.2757 | MAE: 0.1951 | RMSE: 0.2757 | RÂ²: 0.9186\n",
      "Epoch 11/20 | Train Loss: 0.3315 | Val Loss: 0.2789 | MAE: 0.1949 | RMSE: 0.2789 | RÂ²: 0.9168\n",
      "Epoch 12/20 | Train Loss: 0.3226 | Val Loss: 0.2800 | MAE: 0.1970 | RMSE: 0.2800 | RÂ²: 0.9160\n",
      "Epoch 13/20 | Train Loss: 0.3186 | Val Loss: 0.2825 | MAE: 0.1979 | RMSE: 0.2825 | RÂ²: 0.9146\n",
      "Epoch 14/20 | Train Loss: 0.3115 | Val Loss: 0.2875 | MAE: 0.2033 | RMSE: 0.2875 | RÂ²: 0.9115\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2757 | MAE = 0.1951\n",
      "\n",
      "ðŸ”§ [273/300] Entrenando con: lr=0.001, dropout=0.5, hidden_sizes=[512, 256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9125 | Val Loss: 0.7387 | MAE: 0.6251 | RMSE: 0.7387 | RÂ²: 0.4158\n",
      "Epoch 2/20 | Train Loss: 0.6301 | Val Loss: 0.5224 | MAE: 0.4079 | RMSE: 0.5224 | RÂ²: 0.7078\n",
      "Epoch 3/20 | Train Loss: 0.5309 | Val Loss: 0.4095 | MAE: 0.3153 | RMSE: 0.4095 | RÂ²: 0.8205\n",
      "Epoch 4/20 | Train Loss: 0.4870 | Val Loss: 0.3869 | MAE: 0.2806 | RMSE: 0.3869 | RÂ²: 0.8397\n",
      "Epoch 5/20 | Train Loss: 0.4384 | Val Loss: 0.3925 | MAE: 0.2976 | RMSE: 0.3925 | RÂ²: 0.8351\n",
      "Epoch 6/20 | Train Loss: 0.4128 | Val Loss: 0.3798 | MAE: 0.2939 | RMSE: 0.3798 | RÂ²: 0.8456\n",
      "Epoch 7/20 | Train Loss: 0.3955 | Val Loss: 0.3392 | MAE: 0.2462 | RMSE: 0.3392 | RÂ²: 0.8769\n",
      "Epoch 8/20 | Train Loss: 0.3826 | Val Loss: 0.3114 | MAE: 0.2279 | RMSE: 0.3114 | RÂ²: 0.8962\n",
      "Epoch 9/20 | Train Loss: 0.3693 | Val Loss: 0.2939 | MAE: 0.2110 | RMSE: 0.2939 | RÂ²: 0.9075\n",
      "Epoch 10/20 | Train Loss: 0.3583 | Val Loss: 0.2819 | MAE: 0.2018 | RMSE: 0.2819 | RÂ²: 0.9149\n",
      "Epoch 11/20 | Train Loss: 0.3516 | Val Loss: 0.2827 | MAE: 0.2031 | RMSE: 0.2827 | RÂ²: 0.9144\n",
      "Epoch 12/20 | Train Loss: 0.3424 | Val Loss: 0.2807 | MAE: 0.2001 | RMSE: 0.2807 | RÂ²: 0.9156\n",
      "Epoch 13/20 | Train Loss: 0.3339 | Val Loss: 0.2786 | MAE: 0.1966 | RMSE: 0.2786 | RÂ²: 0.9169\n",
      "Epoch 14/20 | Train Loss: 0.3300 | Val Loss: 0.2814 | MAE: 0.1995 | RMSE: 0.2814 | RÂ²: 0.9152\n",
      "Epoch 15/20 | Train Loss: 0.3254 | Val Loss: 0.2862 | MAE: 0.2024 | RMSE: 0.2862 | RÂ²: 0.9123\n",
      "Epoch 16/20 | Train Loss: 0.3234 | Val Loss: 0.2792 | MAE: 0.2000 | RMSE: 0.2792 | RÂ²: 0.9166\n",
      "Epoch 17/20 | Train Loss: 0.3157 | Val Loss: 0.2792 | MAE: 0.1965 | RMSE: 0.2792 | RÂ²: 0.9166\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2786 | MAE = 0.1966\n",
      "\n",
      "ðŸ”§ [274/300] Entrenando con: lr=0.0001, dropout=0.5, hidden_sizes=[512, 256, 128], seed=1009\n",
      "Epoch 1/20 | Train Loss: 1.1304 | Val Loss: 0.9630 | MAE: 0.8111 | RMSE: 0.9630 | RÂ²: 0.0071\n",
      "Epoch 2/20 | Train Loss: 0.9924 | Val Loss: 0.8810 | MAE: 0.7457 | RMSE: 0.8810 | RÂ²: 0.1691\n",
      "Epoch 3/20 | Train Loss: 0.9234 | Val Loss: 0.7817 | MAE: 0.6618 | RMSE: 0.7817 | RÂ²: 0.3458\n",
      "Epoch 4/20 | Train Loss: 0.8653 | Val Loss: 0.6875 | MAE: 0.5821 | RMSE: 0.6875 | RÂ²: 0.4940\n",
      "Epoch 5/20 | Train Loss: 0.8175 | Val Loss: 0.6063 | MAE: 0.5095 | RMSE: 0.6063 | RÂ²: 0.6064\n",
      "Epoch 6/20 | Train Loss: 0.7842 | Val Loss: 0.5386 | MAE: 0.4508 | RMSE: 0.5386 | RÂ²: 0.6895\n",
      "Epoch 7/20 | Train Loss: 0.7471 | Val Loss: 0.4839 | MAE: 0.4036 | RMSE: 0.4839 | RÂ²: 0.7494\n",
      "Epoch 8/20 | Train Loss: 0.7076 | Val Loss: 0.4446 | MAE: 0.3684 | RMSE: 0.4446 | RÂ²: 0.7884\n",
      "Epoch 9/20 | Train Loss: 0.6869 | Val Loss: 0.4194 | MAE: 0.3416 | RMSE: 0.4194 | RÂ²: 0.8117\n",
      "Epoch 10/20 | Train Loss: 0.6525 | Val Loss: 0.4032 | MAE: 0.3200 | RMSE: 0.4032 | RÂ²: 0.8260\n",
      "Epoch 11/20 | Train Loss: 0.6389 | Val Loss: 0.3938 | MAE: 0.3033 | RMSE: 0.3938 | RÂ²: 0.8339\n",
      "Epoch 12/20 | Train Loss: 0.6158 | Val Loss: 0.3898 | MAE: 0.2908 | RMSE: 0.3898 | RÂ²: 0.8373\n",
      "Epoch 13/20 | Train Loss: 0.5901 | Val Loss: 0.3835 | MAE: 0.2796 | RMSE: 0.3835 | RÂ²: 0.8426\n",
      "Epoch 14/20 | Train Loss: 0.5850 | Val Loss: 0.3845 | MAE: 0.2732 | RMSE: 0.3845 | RÂ²: 0.8418\n",
      "Epoch 15/20 | Train Loss: 0.5652 | Val Loss: 0.3870 | MAE: 0.2688 | RMSE: 0.3870 | RÂ²: 0.8396\n",
      "Epoch 16/20 | Train Loss: 0.5602 | Val Loss: 0.3890 | MAE: 0.2650 | RMSE: 0.3890 | RÂ²: 0.8380\n",
      "Epoch 17/20 | Train Loss: 0.5500 | Val Loss: 0.3880 | MAE: 0.2623 | RMSE: 0.3880 | RÂ²: 0.8389\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3835 | MAE = 0.2796\n",
      "\n",
      "ðŸ”§ [275/300] Entrenando con: lr=0.0001, dropout=0.4, hidden_sizes=[1024, 256, 1024], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9762 | Val Loss: 0.9300 | MAE: 0.7882 | RMSE: 0.9300 | RÂ²: 0.0741\n",
      "Epoch 2/20 | Train Loss: 0.8083 | Val Loss: 0.8156 | MAE: 0.6911 | RMSE: 0.8156 | RÂ²: 0.2879\n",
      "Epoch 3/20 | Train Loss: 0.7153 | Val Loss: 0.6859 | MAE: 0.5772 | RMSE: 0.6859 | RÂ²: 0.4963\n",
      "Epoch 4/20 | Train Loss: 0.6556 | Val Loss: 0.5566 | MAE: 0.4581 | RMSE: 0.5566 | RÂ²: 0.6683\n",
      "Epoch 5/20 | Train Loss: 0.6114 | Val Loss: 0.4481 | MAE: 0.3537 | RMSE: 0.4481 | RÂ²: 0.7850\n",
      "Epoch 6/20 | Train Loss: 0.5941 | Val Loss: 0.3935 | MAE: 0.2956 | RMSE: 0.3935 | RÂ²: 0.8342\n",
      "Epoch 7/20 | Train Loss: 0.5718 | Val Loss: 0.3613 | MAE: 0.2604 | RMSE: 0.3613 | RÂ²: 0.8602\n",
      "Epoch 8/20 | Train Loss: 0.5449 | Val Loss: 0.3302 | MAE: 0.2417 | RMSE: 0.3302 | RÂ²: 0.8833\n",
      "Epoch 9/20 | Train Loss: 0.5310 | Val Loss: 0.3250 | MAE: 0.2376 | RMSE: 0.3250 | RÂ²: 0.8869\n",
      "Epoch 10/20 | Train Loss: 0.5212 | Val Loss: 0.3143 | MAE: 0.2304 | RMSE: 0.3143 | RÂ²: 0.8942\n",
      "Epoch 11/20 | Train Loss: 0.4999 | Val Loss: 0.3114 | MAE: 0.2264 | RMSE: 0.3114 | RÂ²: 0.8962\n",
      "Epoch 12/20 | Train Loss: 0.5030 | Val Loss: 0.3074 | MAE: 0.2234 | RMSE: 0.3074 | RÂ²: 0.8989\n",
      "Epoch 13/20 | Train Loss: 0.4896 | Val Loss: 0.3070 | MAE: 0.2222 | RMSE: 0.3070 | RÂ²: 0.8991\n",
      "Epoch 14/20 | Train Loss: 0.4907 | Val Loss: 0.3063 | MAE: 0.2209 | RMSE: 0.3063 | RÂ²: 0.8995\n",
      "Epoch 15/20 | Train Loss: 0.4742 | Val Loss: 0.3037 | MAE: 0.2196 | RMSE: 0.3037 | RÂ²: 0.9012\n",
      "Epoch 16/20 | Train Loss: 0.4721 | Val Loss: 0.3002 | MAE: 0.2161 | RMSE: 0.3002 | RÂ²: 0.9035\n",
      "Epoch 17/20 | Train Loss: 0.4606 | Val Loss: 0.3003 | MAE: 0.2165 | RMSE: 0.3003 | RÂ²: 0.9034\n",
      "Epoch 18/20 | Train Loss: 0.4557 | Val Loss: 0.2987 | MAE: 0.2151 | RMSE: 0.2987 | RÂ²: 0.9045\n",
      "Epoch 19/20 | Train Loss: 0.4528 | Val Loss: 0.2982 | MAE: 0.2121 | RMSE: 0.2982 | RÂ²: 0.9048\n",
      "Epoch 20/20 | Train Loss: 0.4456 | Val Loss: 0.3050 | MAE: 0.2155 | RMSE: 0.3050 | RÂ²: 0.9004\n",
      "âœ… RMSE = 0.2982 | MAE = 0.2121\n",
      "\n",
      "ðŸ”§ [276/300] Entrenando con: lr=0.0005, dropout=0.5, hidden_sizes=[512, 512, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.9244 | Val Loss: 0.8360 | MAE: 0.7114 | RMSE: 0.8360 | RÂ²: 0.2518\n",
      "Epoch 2/20 | Train Loss: 0.7201 | Val Loss: 0.6461 | MAE: 0.5410 | RMSE: 0.6461 | RÂ²: 0.5531\n",
      "Epoch 3/20 | Train Loss: 0.6087 | Val Loss: 0.4781 | MAE: 0.3715 | RMSE: 0.4781 | RÂ²: 0.7553\n",
      "Epoch 4/20 | Train Loss: 0.5666 | Val Loss: 0.3665 | MAE: 0.2623 | RMSE: 0.3665 | RÂ²: 0.8562\n",
      "Epoch 5/20 | Train Loss: 0.5105 | Val Loss: 0.3732 | MAE: 0.2818 | RMSE: 0.3732 | RÂ²: 0.8509\n",
      "Epoch 6/20 | Train Loss: 0.4758 | Val Loss: 0.3913 | MAE: 0.3016 | RMSE: 0.3913 | RÂ²: 0.8360\n",
      "Epoch 7/20 | Train Loss: 0.4481 | Val Loss: 0.4244 | MAE: 0.3387 | RMSE: 0.4244 | RÂ²: 0.8072\n",
      "Epoch 8/20 | Train Loss: 0.4373 | Val Loss: 0.4303 | MAE: 0.3390 | RMSE: 0.4303 | RÂ²: 0.8017\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3665 | MAE = 0.2623\n",
      "\n",
      "ðŸ”§ [277/300] Entrenando con: lr=0.0002, dropout=0.2, hidden_sizes=[1024, 256, 1024], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.8557 | Val Loss: 0.8872 | MAE: 0.7495 | RMSE: 0.8872 | RÂ²: 0.1574\n",
      "Epoch 2/20 | Train Loss: 0.5573 | Val Loss: 0.7668 | MAE: 0.6344 | RMSE: 0.7668 | RÂ²: 0.3706\n",
      "Epoch 3/20 | Train Loss: 0.4744 | Val Loss: 0.5977 | MAE: 0.4910 | RMSE: 0.5977 | RÂ²: 0.6175\n",
      "Epoch 4/20 | Train Loss: 0.4279 | Val Loss: 0.4350 | MAE: 0.3601 | RMSE: 0.4350 | RÂ²: 0.7974\n",
      "Epoch 5/20 | Train Loss: 0.3944 | Val Loss: 0.3701 | MAE: 0.2887 | RMSE: 0.3701 | RÂ²: 0.8534\n",
      "Epoch 6/20 | Train Loss: 0.3852 | Val Loss: 0.2922 | MAE: 0.2178 | RMSE: 0.2922 | RÂ²: 0.9086\n",
      "Epoch 7/20 | Train Loss: 0.3698 | Val Loss: 0.2976 | MAE: 0.2031 | RMSE: 0.2976 | RÂ²: 0.9052\n",
      "Epoch 8/20 | Train Loss: 0.3586 | Val Loss: 0.2683 | MAE: 0.1893 | RMSE: 0.2683 | RÂ²: 0.9229\n",
      "Epoch 9/20 | Train Loss: 0.3514 | Val Loss: 0.2713 | MAE: 0.1901 | RMSE: 0.2713 | RÂ²: 0.9212\n",
      "Epoch 10/20 | Train Loss: 0.3381 | Val Loss: 0.2644 | MAE: 0.1864 | RMSE: 0.2644 | RÂ²: 0.9252\n",
      "Epoch 11/20 | Train Loss: 0.3339 | Val Loss: 0.2649 | MAE: 0.1857 | RMSE: 0.2649 | RÂ²: 0.9249\n",
      "Epoch 12/20 | Train Loss: 0.3302 | Val Loss: 0.2699 | MAE: 0.1882 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 13/20 | Train Loss: 0.3286 | Val Loss: 0.2644 | MAE: 0.1827 | RMSE: 0.2644 | RÂ²: 0.9252\n",
      "Epoch 14/20 | Train Loss: 0.3264 | Val Loss: 0.2739 | MAE: 0.1926 | RMSE: 0.2739 | RÂ²: 0.9197\n",
      "Epoch 15/20 | Train Loss: 0.3213 | Val Loss: 0.2743 | MAE: 0.1896 | RMSE: 0.2743 | RÂ²: 0.9195\n",
      "Epoch 16/20 | Train Loss: 0.3120 | Val Loss: 0.2803 | MAE: 0.1878 | RMSE: 0.2803 | RÂ²: 0.9159\n",
      "Epoch 17/20 | Train Loss: 0.3102 | Val Loss: 0.2757 | MAE: 0.1890 | RMSE: 0.2757 | RÂ²: 0.9187\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2644 | MAE = 0.1827\n",
      "\n",
      "ðŸ”§ [278/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[1024, 256, 1024], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.9171 | Val Loss: 0.8800 | MAE: 0.7459 | RMSE: 0.8800 | RÂ²: 0.1710\n",
      "Epoch 2/20 | Train Loss: 0.7084 | Val Loss: 0.7398 | MAE: 0.6230 | RMSE: 0.7398 | RÂ²: 0.4141\n",
      "Epoch 3/20 | Train Loss: 0.6009 | Val Loss: 0.5744 | MAE: 0.4711 | RMSE: 0.5744 | RÂ²: 0.6468\n",
      "Epoch 4/20 | Train Loss: 0.5724 | Val Loss: 0.4570 | MAE: 0.3496 | RMSE: 0.4570 | RÂ²: 0.7764\n",
      "Epoch 5/20 | Train Loss: 0.5404 | Val Loss: 0.3423 | MAE: 0.2574 | RMSE: 0.3423 | RÂ²: 0.8746\n",
      "Epoch 6/20 | Train Loss: 0.5166 | Val Loss: 0.3333 | MAE: 0.2393 | RMSE: 0.3333 | RÂ²: 0.8810\n",
      "Epoch 7/20 | Train Loss: 0.4983 | Val Loss: 0.3204 | MAE: 0.2234 | RMSE: 0.3204 | RÂ²: 0.8901\n",
      "Epoch 8/20 | Train Loss: 0.4720 | Val Loss: 0.3174 | MAE: 0.2250 | RMSE: 0.3174 | RÂ²: 0.8921\n",
      "Epoch 9/20 | Train Loss: 0.4629 | Val Loss: 0.3138 | MAE: 0.2218 | RMSE: 0.3138 | RÂ²: 0.8946\n",
      "Epoch 10/20 | Train Loss: 0.4523 | Val Loss: 0.3051 | MAE: 0.2160 | RMSE: 0.3051 | RÂ²: 0.9003\n",
      "Epoch 11/20 | Train Loss: 0.4334 | Val Loss: 0.2972 | MAE: 0.2134 | RMSE: 0.2972 | RÂ²: 0.9054\n",
      "Epoch 12/20 | Train Loss: 0.4349 | Val Loss: 0.2905 | MAE: 0.2044 | RMSE: 0.2905 | RÂ²: 0.9096\n",
      "Epoch 13/20 | Train Loss: 0.4306 | Val Loss: 0.2977 | MAE: 0.2086 | RMSE: 0.2977 | RÂ²: 0.9051\n",
      "Epoch 14/20 | Train Loss: 0.4244 | Val Loss: 0.2959 | MAE: 0.2080 | RMSE: 0.2959 | RÂ²: 0.9063\n",
      "Epoch 15/20 | Train Loss: 0.4064 | Val Loss: 0.2984 | MAE: 0.2074 | RMSE: 0.2984 | RÂ²: 0.9047\n",
      "Epoch 16/20 | Train Loss: 0.4019 | Val Loss: 0.2884 | MAE: 0.2022 | RMSE: 0.2884 | RÂ²: 0.9109\n",
      "Epoch 17/20 | Train Loss: 0.3939 | Val Loss: 0.2933 | MAE: 0.2020 | RMSE: 0.2933 | RÂ²: 0.9079\n",
      "Epoch 18/20 | Train Loss: 0.3880 | Val Loss: 0.2980 | MAE: 0.2035 | RMSE: 0.2980 | RÂ²: 0.9049\n",
      "Epoch 19/20 | Train Loss: 0.3818 | Val Loss: 0.3045 | MAE: 0.2081 | RMSE: 0.3045 | RÂ²: 0.9007\n",
      "Epoch 20/20 | Train Loss: 0.3772 | Val Loss: 0.3097 | MAE: 0.2095 | RMSE: 0.3097 | RÂ²: 0.8973\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2884 | MAE = 0.2022\n",
      "\n",
      "ðŸ”§ [279/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 1.0604 | Val Loss: 0.8699 | MAE: 0.7372 | RMSE: 0.8699 | RÂ²: 0.1899\n",
      "Epoch 2/20 | Train Loss: 0.9122 | Val Loss: 0.7603 | MAE: 0.6350 | RMSE: 0.7603 | RÂ²: 0.3812\n",
      "Epoch 3/20 | Train Loss: 0.8495 | Val Loss: 0.6699 | MAE: 0.5474 | RMSE: 0.6699 | RÂ²: 0.5196\n",
      "Epoch 4/20 | Train Loss: 0.7832 | Val Loss: 0.5839 | MAE: 0.4772 | RMSE: 0.5839 | RÂ²: 0.6350\n",
      "Epoch 5/20 | Train Loss: 0.7348 | Val Loss: 0.5171 | MAE: 0.4209 | RMSE: 0.5171 | RÂ²: 0.7137\n",
      "Epoch 6/20 | Train Loss: 0.6751 | Val Loss: 0.4676 | MAE: 0.3743 | RMSE: 0.4676 | RÂ²: 0.7659\n",
      "Epoch 7/20 | Train Loss: 0.6458 | Val Loss: 0.4289 | MAE: 0.3354 | RMSE: 0.4289 | RÂ²: 0.8030\n",
      "Epoch 8/20 | Train Loss: 0.6184 | Val Loss: 0.3976 | MAE: 0.3047 | RMSE: 0.3976 | RÂ²: 0.8308\n",
      "Epoch 9/20 | Train Loss: 0.5972 | Val Loss: 0.3772 | MAE: 0.2856 | RMSE: 0.3772 | RÂ²: 0.8476\n",
      "Epoch 10/20 | Train Loss: 0.5738 | Val Loss: 0.3646 | MAE: 0.2738 | RMSE: 0.3646 | RÂ²: 0.8577\n",
      "Epoch 11/20 | Train Loss: 0.5596 | Val Loss: 0.3598 | MAE: 0.2670 | RMSE: 0.3598 | RÂ²: 0.8614\n",
      "Epoch 12/20 | Train Loss: 0.5478 | Val Loss: 0.3536 | MAE: 0.2603 | RMSE: 0.3536 | RÂ²: 0.8662\n",
      "Epoch 13/20 | Train Loss: 0.5335 | Val Loss: 0.3470 | MAE: 0.2552 | RMSE: 0.3470 | RÂ²: 0.8711\n",
      "Epoch 14/20 | Train Loss: 0.5278 | Val Loss: 0.3460 | MAE: 0.2543 | RMSE: 0.3460 | RÂ²: 0.8719\n",
      "Epoch 15/20 | Train Loss: 0.5163 | Val Loss: 0.3438 | MAE: 0.2529 | RMSE: 0.3438 | RÂ²: 0.8735\n",
      "Epoch 16/20 | Train Loss: 0.5126 | Val Loss: 0.3400 | MAE: 0.2499 | RMSE: 0.3400 | RÂ²: 0.8763\n",
      "Epoch 17/20 | Train Loss: 0.4936 | Val Loss: 0.3317 | MAE: 0.2445 | RMSE: 0.3317 | RÂ²: 0.8822\n",
      "Epoch 18/20 | Train Loss: 0.4904 | Val Loss: 0.3293 | MAE: 0.2418 | RMSE: 0.3293 | RÂ²: 0.8839\n",
      "Epoch 19/20 | Train Loss: 0.4833 | Val Loss: 0.3335 | MAE: 0.2424 | RMSE: 0.3335 | RÂ²: 0.8809\n",
      "Epoch 20/20 | Train Loss: 0.4715 | Val Loss: 0.3353 | MAE: 0.2416 | RMSE: 0.3353 | RÂ²: 0.8797\n",
      "âœ… RMSE = 0.3293 | MAE = 0.2418\n",
      "\n",
      "ðŸ”§ [280/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=2029\n",
      "Epoch 1/20 | Train Loss: 0.9263 | Val Loss: 0.8208 | MAE: 0.6894 | RMSE: 0.8208 | RÂ²: 0.2787\n",
      "Epoch 2/20 | Train Loss: 0.3726 | Val Loss: 0.4990 | MAE: 0.4178 | RMSE: 0.4990 | RÂ²: 0.7334\n",
      "Epoch 3/20 | Train Loss: 0.3149 | Val Loss: 0.3410 | MAE: 0.2602 | RMSE: 0.3410 | RÂ²: 0.8755\n",
      "Epoch 4/20 | Train Loss: 0.2890 | Val Loss: 0.2981 | MAE: 0.2197 | RMSE: 0.2981 | RÂ²: 0.9049\n",
      "Epoch 5/20 | Train Loss: 0.2742 | Val Loss: 0.3569 | MAE: 0.2422 | RMSE: 0.3569 | RÂ²: 0.8637\n",
      "Epoch 6/20 | Train Loss: 0.2690 | Val Loss: 0.3075 | MAE: 0.2242 | RMSE: 0.3075 | RÂ²: 0.8988\n",
      "Epoch 7/20 | Train Loss: 0.2615 | Val Loss: 0.3086 | MAE: 0.2194 | RMSE: 0.3086 | RÂ²: 0.8981\n",
      "Epoch 8/20 | Train Loss: 0.2566 | Val Loss: 0.2630 | MAE: 0.1818 | RMSE: 0.2630 | RÂ²: 0.9260\n",
      "Epoch 9/20 | Train Loss: 0.2542 | Val Loss: 0.2759 | MAE: 0.1834 | RMSE: 0.2759 | RÂ²: 0.9185\n",
      "Epoch 10/20 | Train Loss: 0.2493 | Val Loss: 0.2647 | MAE: 0.1781 | RMSE: 0.2647 | RÂ²: 0.9250\n",
      "Epoch 11/20 | Train Loss: 0.2483 | Val Loss: 0.2640 | MAE: 0.1754 | RMSE: 0.2640 | RÂ²: 0.9254\n",
      "Epoch 12/20 | Train Loss: 0.2429 | Val Loss: 0.2541 | MAE: 0.1703 | RMSE: 0.2541 | RÂ²: 0.9309\n",
      "Epoch 13/20 | Train Loss: 0.2395 | Val Loss: 0.2588 | MAE: 0.1730 | RMSE: 0.2588 | RÂ²: 0.9283\n",
      "Epoch 14/20 | Train Loss: 0.2420 | Val Loss: 0.2728 | MAE: 0.1795 | RMSE: 0.2728 | RÂ²: 0.9203\n",
      "Epoch 15/20 | Train Loss: 0.2383 | Val Loss: 0.2773 | MAE: 0.1965 | RMSE: 0.2773 | RÂ²: 0.9177\n",
      "Epoch 16/20 | Train Loss: 0.2419 | Val Loss: 0.2659 | MAE: 0.1754 | RMSE: 0.2659 | RÂ²: 0.9243\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2541 | MAE = 0.1703\n",
      "\n",
      "ðŸ”§ [281/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 1.1045 | Val Loss: 0.9549 | MAE: 0.8104 | RMSE: 0.9549 | RÂ²: 0.0238\n",
      "Epoch 2/20 | Train Loss: 0.9052 | Val Loss: 0.8300 | MAE: 0.7060 | RMSE: 0.8300 | RÂ²: 0.2625\n",
      "Epoch 3/20 | Train Loss: 0.8273 | Val Loss: 0.6915 | MAE: 0.5865 | RMSE: 0.6915 | RÂ²: 0.4880\n",
      "Epoch 4/20 | Train Loss: 0.7547 | Val Loss: 0.5609 | MAE: 0.4749 | RMSE: 0.5609 | RÂ²: 0.6632\n",
      "Epoch 5/20 | Train Loss: 0.6905 | Val Loss: 0.4552 | MAE: 0.3779 | RMSE: 0.4552 | RÂ²: 0.7782\n",
      "Epoch 6/20 | Train Loss: 0.6621 | Val Loss: 0.3911 | MAE: 0.3146 | RMSE: 0.3911 | RÂ²: 0.8362\n",
      "Epoch 7/20 | Train Loss: 0.6190 | Val Loss: 0.3554 | MAE: 0.2776 | RMSE: 0.3554 | RÂ²: 0.8648\n",
      "Epoch 8/20 | Train Loss: 0.5920 | Val Loss: 0.3384 | MAE: 0.2583 | RMSE: 0.3384 | RÂ²: 0.8774\n",
      "Epoch 9/20 | Train Loss: 0.5680 | Val Loss: 0.3312 | MAE: 0.2481 | RMSE: 0.3312 | RÂ²: 0.8826\n",
      "Epoch 10/20 | Train Loss: 0.5482 | Val Loss: 0.3297 | MAE: 0.2444 | RMSE: 0.3297 | RÂ²: 0.8836\n",
      "Epoch 11/20 | Train Loss: 0.5263 | Val Loss: 0.3315 | MAE: 0.2439 | RMSE: 0.3315 | RÂ²: 0.8823\n",
      "Epoch 12/20 | Train Loss: 0.5197 | Val Loss: 0.3303 | MAE: 0.2427 | RMSE: 0.3303 | RÂ²: 0.8832\n",
      "Epoch 13/20 | Train Loss: 0.4986 | Val Loss: 0.3268 | MAE: 0.2416 | RMSE: 0.3268 | RÂ²: 0.8857\n",
      "Epoch 14/20 | Train Loss: 0.4908 | Val Loss: 0.3240 | MAE: 0.2415 | RMSE: 0.3240 | RÂ²: 0.8876\n",
      "Epoch 15/20 | Train Loss: 0.4854 | Val Loss: 0.3224 | MAE: 0.2402 | RMSE: 0.3224 | RÂ²: 0.8887\n",
      "Epoch 16/20 | Train Loss: 0.4729 | Val Loss: 0.3195 | MAE: 0.2359 | RMSE: 0.3195 | RÂ²: 0.8907\n",
      "Epoch 17/20 | Train Loss: 0.4659 | Val Loss: 0.3201 | MAE: 0.2355 | RMSE: 0.3201 | RÂ²: 0.8903\n",
      "Epoch 18/20 | Train Loss: 0.4554 | Val Loss: 0.3189 | MAE: 0.2333 | RMSE: 0.3189 | RÂ²: 0.8911\n",
      "Epoch 19/20 | Train Loss: 0.4507 | Val Loss: 0.3168 | MAE: 0.2298 | RMSE: 0.3168 | RÂ²: 0.8925\n",
      "Epoch 20/20 | Train Loss: 0.4407 | Val Loss: 0.3186 | MAE: 0.2305 | RMSE: 0.3186 | RÂ²: 0.8913\n",
      "âœ… RMSE = 0.3168 | MAE = 0.2298\n",
      "\n",
      "ðŸ”§ [282/300] Entrenando con: lr=0.0001, dropout=0.3, hidden_sizes=[512, 256, 128], seed=7777\n",
      "Epoch 1/20 | Train Loss: 1.0848 | Val Loss: 0.9831 | MAE: 0.8323 | RMSE: 0.9831 | RÂ²: -0.0347\n",
      "Epoch 2/20 | Train Loss: 0.8648 | Val Loss: 0.8908 | MAE: 0.7577 | RMSE: 0.8908 | RÂ²: 0.1505\n",
      "Epoch 3/20 | Train Loss: 0.7670 | Val Loss: 0.7670 | MAE: 0.6522 | RMSE: 0.7670 | RÂ²: 0.3702\n",
      "Epoch 4/20 | Train Loss: 0.7144 | Val Loss: 0.6350 | MAE: 0.5378 | RMSE: 0.6350 | RÂ²: 0.5683\n",
      "Epoch 5/20 | Train Loss: 0.6457 | Val Loss: 0.5190 | MAE: 0.4364 | RMSE: 0.5190 | RÂ²: 0.7116\n",
      "Epoch 6/20 | Train Loss: 0.6038 | Val Loss: 0.4447 | MAE: 0.3651 | RMSE: 0.4447 | RÂ²: 0.7883\n",
      "Epoch 7/20 | Train Loss: 0.5557 | Val Loss: 0.4027 | MAE: 0.3187 | RMSE: 0.4027 | RÂ²: 0.8264\n",
      "Epoch 8/20 | Train Loss: 0.5278 | Val Loss: 0.3683 | MAE: 0.2834 | RMSE: 0.3683 | RÂ²: 0.8548\n",
      "Epoch 9/20 | Train Loss: 0.5057 | Val Loss: 0.3394 | MAE: 0.2575 | RMSE: 0.3394 | RÂ²: 0.8767\n",
      "Epoch 10/20 | Train Loss: 0.4822 | Val Loss: 0.3206 | MAE: 0.2411 | RMSE: 0.3206 | RÂ²: 0.8899\n",
      "Epoch 11/20 | Train Loss: 0.4665 | Val Loss: 0.3125 | MAE: 0.2329 | RMSE: 0.3125 | RÂ²: 0.8954\n",
      "Epoch 12/20 | Train Loss: 0.4581 | Val Loss: 0.3109 | MAE: 0.2284 | RMSE: 0.3109 | RÂ²: 0.8965\n",
      "Epoch 13/20 | Train Loss: 0.4430 | Val Loss: 0.3084 | MAE: 0.2260 | RMSE: 0.3084 | RÂ²: 0.8981\n",
      "Epoch 14/20 | Train Loss: 0.4384 | Val Loss: 0.3035 | MAE: 0.2239 | RMSE: 0.3035 | RÂ²: 0.9014\n",
      "Epoch 15/20 | Train Loss: 0.4372 | Val Loss: 0.3018 | MAE: 0.2216 | RMSE: 0.3018 | RÂ²: 0.9025\n",
      "Epoch 16/20 | Train Loss: 0.4251 | Val Loss: 0.2991 | MAE: 0.2181 | RMSE: 0.2991 | RÂ²: 0.9042\n",
      "Epoch 17/20 | Train Loss: 0.4200 | Val Loss: 0.2944 | MAE: 0.2148 | RMSE: 0.2944 | RÂ²: 0.9072\n",
      "Epoch 18/20 | Train Loss: 0.4116 | Val Loss: 0.2930 | MAE: 0.2130 | RMSE: 0.2930 | RÂ²: 0.9081\n",
      "Epoch 19/20 | Train Loss: 0.4105 | Val Loss: 0.2925 | MAE: 0.2112 | RMSE: 0.2925 | RÂ²: 0.9084\n",
      "Epoch 20/20 | Train Loss: 0.4015 | Val Loss: 0.2941 | MAE: 0.2109 | RMSE: 0.2941 | RÂ²: 0.9074\n",
      "âœ… RMSE = 0.2925 | MAE = 0.2112\n",
      "\n",
      "ðŸ”§ [283/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[512, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8022 | Val Loss: 0.8453 | MAE: 0.7062 | RMSE: 0.8453 | RÂ²: 0.2350\n",
      "Epoch 2/20 | Train Loss: 0.5436 | Val Loss: 0.6552 | MAE: 0.5376 | RMSE: 0.6552 | RÂ²: 0.5404\n",
      "Epoch 3/20 | Train Loss: 0.4695 | Val Loss: 0.4273 | MAE: 0.3516 | RMSE: 0.4273 | RÂ²: 0.8045\n",
      "Epoch 4/20 | Train Loss: 0.4216 | Val Loss: 0.3744 | MAE: 0.2849 | RMSE: 0.3744 | RÂ²: 0.8500\n",
      "Epoch 5/20 | Train Loss: 0.4014 | Val Loss: 0.3464 | MAE: 0.2590 | RMSE: 0.3464 | RÂ²: 0.8715\n",
      "Epoch 6/20 | Train Loss: 0.3748 | Val Loss: 0.3214 | MAE: 0.2451 | RMSE: 0.3214 | RÂ²: 0.8894\n",
      "Epoch 7/20 | Train Loss: 0.3555 | Val Loss: 0.3010 | MAE: 0.2269 | RMSE: 0.3010 | RÂ²: 0.9030\n",
      "Epoch 8/20 | Train Loss: 0.3475 | Val Loss: 0.3118 | MAE: 0.2326 | RMSE: 0.3118 | RÂ²: 0.8959\n",
      "Epoch 9/20 | Train Loss: 0.3349 | Val Loss: 0.3053 | MAE: 0.2296 | RMSE: 0.3053 | RÂ²: 0.9002\n",
      "Epoch 10/20 | Train Loss: 0.3246 | Val Loss: 0.2846 | MAE: 0.2022 | RMSE: 0.2846 | RÂ²: 0.9133\n",
      "Epoch 11/20 | Train Loss: 0.3188 | Val Loss: 0.2869 | MAE: 0.2095 | RMSE: 0.2869 | RÂ²: 0.9119\n",
      "Epoch 12/20 | Train Loss: 0.3135 | Val Loss: 0.2657 | MAE: 0.1847 | RMSE: 0.2657 | RÂ²: 0.9244\n",
      "Epoch 13/20 | Train Loss: 0.3046 | Val Loss: 0.2781 | MAE: 0.1928 | RMSE: 0.2781 | RÂ²: 0.9172\n",
      "Epoch 14/20 | Train Loss: 0.2984 | Val Loss: 0.2650 | MAE: 0.1872 | RMSE: 0.2650 | RÂ²: 0.9248\n",
      "Epoch 15/20 | Train Loss: 0.2953 | Val Loss: 0.2701 | MAE: 0.1904 | RMSE: 0.2701 | RÂ²: 0.9219\n",
      "Epoch 16/20 | Train Loss: 0.2923 | Val Loss: 0.2709 | MAE: 0.1896 | RMSE: 0.2709 | RÂ²: 0.9214\n",
      "Epoch 17/20 | Train Loss: 0.2876 | Val Loss: 0.2644 | MAE: 0.1836 | RMSE: 0.2644 | RÂ²: 0.9251\n",
      "Epoch 18/20 | Train Loss: 0.2813 | Val Loss: 0.2663 | MAE: 0.1841 | RMSE: 0.2663 | RÂ²: 0.9241\n",
      "Epoch 19/20 | Train Loss: 0.2773 | Val Loss: 0.2674 | MAE: 0.1848 | RMSE: 0.2674 | RÂ²: 0.9235\n",
      "Epoch 20/20 | Train Loss: 0.2727 | Val Loss: 0.2605 | MAE: 0.1785 | RMSE: 0.2605 | RÂ²: 0.9274\n",
      "âœ… RMSE = 0.2605 | MAE = 0.1785\n",
      "\n",
      "ðŸ”§ [284/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[256, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.7780 | Val Loss: 0.7875 | MAE: 0.6704 | RMSE: 0.7875 | RÂ²: 0.3361\n",
      "Epoch 2/20 | Train Loss: 0.5011 | Val Loss: 0.6072 | MAE: 0.5102 | RMSE: 0.6072 | RÂ²: 0.6053\n",
      "Epoch 3/20 | Train Loss: 0.4135 | Val Loss: 0.4637 | MAE: 0.3772 | RMSE: 0.4637 | RÂ²: 0.7698\n",
      "Epoch 4/20 | Train Loss: 0.3659 | Val Loss: 0.3428 | MAE: 0.2721 | RMSE: 0.3428 | RÂ²: 0.8742\n",
      "Epoch 5/20 | Train Loss: 0.3365 | Val Loss: 0.3300 | MAE: 0.2476 | RMSE: 0.3300 | RÂ²: 0.8834\n",
      "Epoch 6/20 | Train Loss: 0.3203 | Val Loss: 0.2825 | MAE: 0.2072 | RMSE: 0.2825 | RÂ²: 0.9145\n",
      "Epoch 7/20 | Train Loss: 0.3061 | Val Loss: 0.2641 | MAE: 0.1887 | RMSE: 0.2641 | RÂ²: 0.9253\n",
      "Epoch 8/20 | Train Loss: 0.2930 | Val Loss: 0.2566 | MAE: 0.1799 | RMSE: 0.2566 | RÂ²: 0.9295\n",
      "Epoch 9/20 | Train Loss: 0.2833 | Val Loss: 0.2589 | MAE: 0.1790 | RMSE: 0.2589 | RÂ²: 0.9283\n",
      "Epoch 10/20 | Train Loss: 0.2768 | Val Loss: 0.2605 | MAE: 0.1816 | RMSE: 0.2605 | RÂ²: 0.9274\n",
      "Epoch 11/20 | Train Loss: 0.2728 | Val Loss: 0.2635 | MAE: 0.1862 | RMSE: 0.2635 | RÂ²: 0.9257\n",
      "Epoch 12/20 | Train Loss: 0.2677 | Val Loss: 0.2597 | MAE: 0.1796 | RMSE: 0.2597 | RÂ²: 0.9278\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2566 | MAE = 0.1799\n",
      "\n",
      "ðŸ”§ [285/300] Entrenando con: lr=0.0002, dropout=0.5, hidden_sizes=[512, 256], seed=9871\n",
      "Epoch 1/20 | Train Loss: 0.9914 | Val Loss: 0.8037 | MAE: 0.6796 | RMSE: 0.8037 | RÂ²: 0.3085\n",
      "Epoch 2/20 | Train Loss: 0.8253 | Val Loss: 0.6615 | MAE: 0.5585 | RMSE: 0.6615 | RÂ²: 0.5316\n",
      "Epoch 3/20 | Train Loss: 0.7102 | Val Loss: 0.5706 | MAE: 0.4730 | RMSE: 0.5706 | RÂ²: 0.6514\n",
      "Epoch 4/20 | Train Loss: 0.6661 | Val Loss: 0.5014 | MAE: 0.3909 | RMSE: 0.5014 | RÂ²: 0.7309\n",
      "Epoch 5/20 | Train Loss: 0.6167 | Val Loss: 0.4140 | MAE: 0.3132 | RMSE: 0.4140 | RÂ²: 0.8165\n",
      "Epoch 6/20 | Train Loss: 0.5859 | Val Loss: 0.3703 | MAE: 0.2766 | RMSE: 0.3703 | RÂ²: 0.8532\n",
      "Epoch 7/20 | Train Loss: 0.5652 | Val Loss: 0.3672 | MAE: 0.2686 | RMSE: 0.3672 | RÂ²: 0.8557\n",
      "Epoch 8/20 | Train Loss: 0.5419 | Val Loss: 0.3669 | MAE: 0.2620 | RMSE: 0.3669 | RÂ²: 0.8559\n",
      "Epoch 9/20 | Train Loss: 0.5256 | Val Loss: 0.3431 | MAE: 0.2486 | RMSE: 0.3431 | RÂ²: 0.8740\n",
      "Epoch 10/20 | Train Loss: 0.5080 | Val Loss: 0.3270 | MAE: 0.2406 | RMSE: 0.3270 | RÂ²: 0.8855\n",
      "Epoch 11/20 | Train Loss: 0.4947 | Val Loss: 0.3271 | MAE: 0.2392 | RMSE: 0.3271 | RÂ²: 0.8855\n",
      "Epoch 12/20 | Train Loss: 0.4857 | Val Loss: 0.3309 | MAE: 0.2368 | RMSE: 0.3309 | RÂ²: 0.8828\n",
      "Epoch 13/20 | Train Loss: 0.4799 | Val Loss: 0.3367 | MAE: 0.2363 | RMSE: 0.3367 | RÂ²: 0.8787\n",
      "Epoch 14/20 | Train Loss: 0.4662 | Val Loss: 0.3197 | MAE: 0.2284 | RMSE: 0.3197 | RÂ²: 0.8906\n",
      "Epoch 15/20 | Train Loss: 0.4527 | Val Loss: 0.3109 | MAE: 0.2237 | RMSE: 0.3109 | RÂ²: 0.8965\n",
      "Epoch 16/20 | Train Loss: 0.4442 | Val Loss: 0.3144 | MAE: 0.2232 | RMSE: 0.3144 | RÂ²: 0.8942\n",
      "Epoch 17/20 | Train Loss: 0.4402 | Val Loss: 0.3187 | MAE: 0.2217 | RMSE: 0.3187 | RÂ²: 0.8913\n",
      "Epoch 18/20 | Train Loss: 0.4294 | Val Loss: 0.3122 | MAE: 0.2178 | RMSE: 0.3122 | RÂ²: 0.8957\n",
      "Epoch 19/20 | Train Loss: 0.4312 | Val Loss: 0.3063 | MAE: 0.2152 | RMSE: 0.3063 | RÂ²: 0.8996\n",
      "Epoch 20/20 | Train Loss: 0.4173 | Val Loss: 0.3019 | MAE: 0.2128 | RMSE: 0.3019 | RÂ²: 0.9024\n",
      "âœ… RMSE = 0.3019 | MAE = 0.2128\n",
      "\n",
      "ðŸ”§ [286/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[1024, 512, 256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8677 | Val Loss: 0.8340 | MAE: 0.7010 | RMSE: 0.8340 | RÂ²: 0.2554\n",
      "Epoch 2/20 | Train Loss: 0.5487 | Val Loss: 0.4785 | MAE: 0.3932 | RMSE: 0.4785 | RÂ²: 0.7549\n",
      "Epoch 3/20 | Train Loss: 0.4594 | Val Loss: 0.3845 | MAE: 0.2985 | RMSE: 0.3845 | RÂ²: 0.8417\n",
      "Epoch 4/20 | Train Loss: 0.4187 | Val Loss: 0.4741 | MAE: 0.3865 | RMSE: 0.4741 | RÂ²: 0.7594\n",
      "Epoch 5/20 | Train Loss: 0.3811 | Val Loss: 0.4214 | MAE: 0.3329 | RMSE: 0.4214 | RÂ²: 0.8099\n",
      "Epoch 6/20 | Train Loss: 0.3596 | Val Loss: 0.4092 | MAE: 0.3221 | RMSE: 0.4092 | RÂ²: 0.8207\n",
      "Epoch 7/20 | Train Loss: 0.3476 | Val Loss: 0.3171 | MAE: 0.2335 | RMSE: 0.3171 | RÂ²: 0.8924\n",
      "Epoch 8/20 | Train Loss: 0.3332 | Val Loss: 0.3134 | MAE: 0.2288 | RMSE: 0.3134 | RÂ²: 0.8949\n",
      "Epoch 9/20 | Train Loss: 0.3265 | Val Loss: 0.2871 | MAE: 0.2070 | RMSE: 0.2871 | RÂ²: 0.9118\n",
      "Epoch 10/20 | Train Loss: 0.3104 | Val Loss: 0.2847 | MAE: 0.1980 | RMSE: 0.2847 | RÂ²: 0.9132\n",
      "Epoch 11/20 | Train Loss: 0.3061 | Val Loss: 0.2666 | MAE: 0.1827 | RMSE: 0.2666 | RÂ²: 0.9239\n",
      "Epoch 12/20 | Train Loss: 0.3017 | Val Loss: 0.2715 | MAE: 0.1867 | RMSE: 0.2715 | RÂ²: 0.9211\n",
      "Epoch 13/20 | Train Loss: 0.2914 | Val Loss: 0.2618 | MAE: 0.1786 | RMSE: 0.2618 | RÂ²: 0.9266\n",
      "Epoch 14/20 | Train Loss: 0.2903 | Val Loss: 0.2661 | MAE: 0.1789 | RMSE: 0.2661 | RÂ²: 0.9242\n",
      "Epoch 15/20 | Train Loss: 0.2858 | Val Loss: 0.2615 | MAE: 0.1779 | RMSE: 0.2615 | RÂ²: 0.9268\n",
      "Epoch 16/20 | Train Loss: 0.2793 | Val Loss: 0.2650 | MAE: 0.1805 | RMSE: 0.2650 | RÂ²: 0.9248\n",
      "Epoch 17/20 | Train Loss: 0.2819 | Val Loss: 0.2657 | MAE: 0.1814 | RMSE: 0.2657 | RÂ²: 0.9244\n",
      "Epoch 18/20 | Train Loss: 0.2787 | Val Loss: 0.2744 | MAE: 0.1926 | RMSE: 0.2744 | RÂ²: 0.9194\n",
      "Epoch 19/20 | Train Loss: 0.2764 | Val Loss: 0.2643 | MAE: 0.1796 | RMSE: 0.2643 | RÂ²: 0.9252\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2615 | MAE = 0.1779\n",
      "\n",
      "ðŸ”§ [287/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[256, 256], seed=7777\n",
      "Epoch 1/20 | Train Loss: 0.8265 | Val Loss: 0.7874 | MAE: 0.6698 | RMSE: 0.7874 | RÂ²: 0.3362\n",
      "Epoch 2/20 | Train Loss: 0.5532 | Val Loss: 0.5971 | MAE: 0.4982 | RMSE: 0.5971 | RÂ²: 0.6183\n",
      "Epoch 3/20 | Train Loss: 0.4561 | Val Loss: 0.4542 | MAE: 0.3621 | RMSE: 0.4542 | RÂ²: 0.7791\n",
      "Epoch 4/20 | Train Loss: 0.4147 | Val Loss: 0.3414 | MAE: 0.2697 | RMSE: 0.3414 | RÂ²: 0.8753\n",
      "Epoch 5/20 | Train Loss: 0.3871 | Val Loss: 0.3363 | MAE: 0.2543 | RMSE: 0.3363 | RÂ²: 0.8789\n",
      "Epoch 6/20 | Train Loss: 0.3677 | Val Loss: 0.2996 | MAE: 0.2220 | RMSE: 0.2996 | RÂ²: 0.9039\n",
      "Epoch 7/20 | Train Loss: 0.3480 | Val Loss: 0.2893 | MAE: 0.2097 | RMSE: 0.2893 | RÂ²: 0.9104\n",
      "Epoch 8/20 | Train Loss: 0.3364 | Val Loss: 0.2735 | MAE: 0.1972 | RMSE: 0.2735 | RÂ²: 0.9199\n",
      "Epoch 9/20 | Train Loss: 0.3265 | Val Loss: 0.2698 | MAE: 0.1900 | RMSE: 0.2698 | RÂ²: 0.9220\n",
      "Epoch 10/20 | Train Loss: 0.3180 | Val Loss: 0.2649 | MAE: 0.1875 | RMSE: 0.2649 | RÂ²: 0.9249\n",
      "Epoch 11/20 | Train Loss: 0.3107 | Val Loss: 0.2617 | MAE: 0.1846 | RMSE: 0.2617 | RÂ²: 0.9267\n",
      "Epoch 12/20 | Train Loss: 0.3056 | Val Loss: 0.2660 | MAE: 0.1857 | RMSE: 0.2660 | RÂ²: 0.9242\n",
      "Epoch 13/20 | Train Loss: 0.2986 | Val Loss: 0.2652 | MAE: 0.1854 | RMSE: 0.2652 | RÂ²: 0.9247\n",
      "Epoch 14/20 | Train Loss: 0.2959 | Val Loss: 0.2693 | MAE: 0.1882 | RMSE: 0.2693 | RÂ²: 0.9224\n",
      "Epoch 15/20 | Train Loss: 0.2882 | Val Loss: 0.2622 | MAE: 0.1812 | RMSE: 0.2622 | RÂ²: 0.9264\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2617 | MAE = 0.1846\n",
      "\n",
      "ðŸ”§ [288/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[2048, 1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8711 | Val Loss: 0.8368 | MAE: 0.7004 | RMSE: 0.8368 | RÂ²: 0.2504\n",
      "Epoch 2/20 | Train Loss: 0.3816 | Val Loss: 0.6242 | MAE: 0.5119 | RMSE: 0.6242 | RÂ²: 0.5829\n",
      "Epoch 3/20 | Train Loss: 0.3169 | Val Loss: 0.3413 | MAE: 0.2723 | RMSE: 0.3413 | RÂ²: 0.8753\n",
      "Epoch 4/20 | Train Loss: 0.2835 | Val Loss: 0.2804 | MAE: 0.2095 | RMSE: 0.2804 | RÂ²: 0.9158\n",
      "Epoch 5/20 | Train Loss: 0.2720 | Val Loss: 0.2849 | MAE: 0.2002 | RMSE: 0.2849 | RÂ²: 0.9131\n",
      "Epoch 6/20 | Train Loss: 0.2613 | Val Loss: 0.2492 | MAE: 0.1737 | RMSE: 0.2492 | RÂ²: 0.9335\n",
      "Epoch 7/20 | Train Loss: 0.2554 | Val Loss: 0.2581 | MAE: 0.1769 | RMSE: 0.2581 | RÂ²: 0.9287\n",
      "Epoch 8/20 | Train Loss: 0.2462 | Val Loss: 0.2658 | MAE: 0.1792 | RMSE: 0.2658 | RÂ²: 0.9243\n",
      "Epoch 9/20 | Train Loss: 0.2461 | Val Loss: 0.2636 | MAE: 0.1784 | RMSE: 0.2636 | RÂ²: 0.9256\n",
      "Epoch 10/20 | Train Loss: 0.2415 | Val Loss: 0.2694 | MAE: 0.1805 | RMSE: 0.2694 | RÂ²: 0.9223\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2492 | MAE = 0.1737\n",
      "\n",
      "ðŸ”§ [289/300] Entrenando con: lr=0.0005, dropout=0.3, hidden_sizes=[1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8171 | Val Loss: 0.8518 | MAE: 0.7119 | RMSE: 0.8518 | RÂ²: 0.2232\n",
      "Epoch 2/20 | Train Loss: 0.5215 | Val Loss: 0.6067 | MAE: 0.5017 | RMSE: 0.6067 | RÂ²: 0.6059\n",
      "Epoch 3/20 | Train Loss: 0.4462 | Val Loss: 0.4037 | MAE: 0.3271 | RMSE: 0.4037 | RÂ²: 0.8255\n",
      "Epoch 4/20 | Train Loss: 0.3963 | Val Loss: 0.3207 | MAE: 0.2340 | RMSE: 0.3207 | RÂ²: 0.8899\n",
      "Epoch 5/20 | Train Loss: 0.3658 | Val Loss: 0.2731 | MAE: 0.1997 | RMSE: 0.2731 | RÂ²: 0.9202\n",
      "Epoch 6/20 | Train Loss: 0.3507 | Val Loss: 0.2830 | MAE: 0.2038 | RMSE: 0.2830 | RÂ²: 0.9143\n",
      "Epoch 7/20 | Train Loss: 0.3353 | Val Loss: 0.2902 | MAE: 0.2139 | RMSE: 0.2902 | RÂ²: 0.9098\n",
      "Epoch 8/20 | Train Loss: 0.3241 | Val Loss: 0.2862 | MAE: 0.2094 | RMSE: 0.2862 | RÂ²: 0.9123\n",
      "Epoch 9/20 | Train Loss: 0.3160 | Val Loss: 0.2816 | MAE: 0.2031 | RMSE: 0.2816 | RÂ²: 0.9151\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2731 | MAE = 0.1997\n",
      "\n",
      "ðŸ”§ [290/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9201 | Val Loss: 0.8857 | MAE: 0.7515 | RMSE: 0.8857 | RÂ²: 0.1601\n",
      "Epoch 2/20 | Train Loss: 0.6948 | Val Loss: 0.7578 | MAE: 0.6374 | RMSE: 0.7578 | RÂ²: 0.3853\n",
      "Epoch 3/20 | Train Loss: 0.5905 | Val Loss: 0.5857 | MAE: 0.4787 | RMSE: 0.5857 | RÂ²: 0.6327\n",
      "Epoch 4/20 | Train Loss: 0.5443 | Val Loss: 0.4460 | MAE: 0.3403 | RMSE: 0.4460 | RÂ²: 0.7870\n",
      "Epoch 5/20 | Train Loss: 0.5029 | Val Loss: 0.3446 | MAE: 0.2556 | RMSE: 0.3446 | RÂ²: 0.8729\n",
      "Epoch 6/20 | Train Loss: 0.4708 | Val Loss: 0.3202 | MAE: 0.2349 | RMSE: 0.3202 | RÂ²: 0.8902\n",
      "Epoch 7/20 | Train Loss: 0.4450 | Val Loss: 0.3190 | MAE: 0.2308 | RMSE: 0.3190 | RÂ²: 0.8911\n",
      "Epoch 8/20 | Train Loss: 0.4292 | Val Loss: 0.3309 | MAE: 0.2446 | RMSE: 0.3309 | RÂ²: 0.8828\n",
      "Epoch 9/20 | Train Loss: 0.4148 | Val Loss: 0.3455 | MAE: 0.2584 | RMSE: 0.3455 | RÂ²: 0.8722\n",
      "Epoch 10/20 | Train Loss: 0.3970 | Val Loss: 0.3470 | MAE: 0.2576 | RMSE: 0.3470 | RÂ²: 0.8711\n",
      "Epoch 11/20 | Train Loss: 0.3900 | Val Loss: 0.3292 | MAE: 0.2448 | RMSE: 0.3292 | RÂ²: 0.8840\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.3190 | MAE = 0.2308\n",
      "\n",
      "ðŸ”§ [291/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[256, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.9367 | Val Loss: 0.9250 | MAE: 0.7830 | RMSE: 0.9250 | RÂ²: 0.0839\n",
      "Epoch 2/20 | Train Loss: 0.7623 | Val Loss: 0.8245 | MAE: 0.6988 | RMSE: 0.8245 | RÂ²: 0.2722\n",
      "Epoch 3/20 | Train Loss: 0.6772 | Val Loss: 0.7214 | MAE: 0.6125 | RMSE: 0.7214 | RÂ²: 0.4428\n",
      "Epoch 4/20 | Train Loss: 0.6020 | Val Loss: 0.6245 | MAE: 0.5295 | RMSE: 0.6245 | RÂ²: 0.5824\n",
      "Epoch 5/20 | Train Loss: 0.5458 | Val Loss: 0.5345 | MAE: 0.4445 | RMSE: 0.5345 | RÂ²: 0.6941\n",
      "Epoch 6/20 | Train Loss: 0.5000 | Val Loss: 0.4471 | MAE: 0.3595 | RMSE: 0.4471 | RÂ²: 0.7860\n",
      "Epoch 7/20 | Train Loss: 0.4679 | Val Loss: 0.3905 | MAE: 0.3051 | RMSE: 0.3905 | RÂ²: 0.8367\n",
      "Epoch 8/20 | Train Loss: 0.4408 | Val Loss: 0.3586 | MAE: 0.2781 | RMSE: 0.3586 | RÂ²: 0.8623\n",
      "Epoch 9/20 | Train Loss: 0.4162 | Val Loss: 0.3355 | MAE: 0.2591 | RMSE: 0.3355 | RÂ²: 0.8795\n",
      "Epoch 10/20 | Train Loss: 0.3991 | Val Loss: 0.3244 | MAE: 0.2456 | RMSE: 0.3244 | RÂ²: 0.8874\n",
      "Epoch 11/20 | Train Loss: 0.3845 | Val Loss: 0.3157 | MAE: 0.2370 | RMSE: 0.3157 | RÂ²: 0.8933\n",
      "Epoch 12/20 | Train Loss: 0.3678 | Val Loss: 0.3068 | MAE: 0.2282 | RMSE: 0.3068 | RÂ²: 0.8992\n",
      "Epoch 13/20 | Train Loss: 0.3604 | Val Loss: 0.2967 | MAE: 0.2189 | RMSE: 0.2967 | RÂ²: 0.9057\n",
      "Epoch 14/20 | Train Loss: 0.3523 | Val Loss: 0.2915 | MAE: 0.2140 | RMSE: 0.2915 | RÂ²: 0.9090\n",
      "Epoch 15/20 | Train Loss: 0.3412 | Val Loss: 0.2892 | MAE: 0.2104 | RMSE: 0.2892 | RÂ²: 0.9104\n",
      "Epoch 16/20 | Train Loss: 0.3389 | Val Loss: 0.2836 | MAE: 0.2058 | RMSE: 0.2836 | RÂ²: 0.9139\n",
      "Epoch 17/20 | Train Loss: 0.3290 | Val Loss: 0.2786 | MAE: 0.2008 | RMSE: 0.2786 | RÂ²: 0.9169\n",
      "Epoch 18/20 | Train Loss: 0.3239 | Val Loss: 0.2773 | MAE: 0.1985 | RMSE: 0.2773 | RÂ²: 0.9177\n",
      "Epoch 19/20 | Train Loss: 0.3175 | Val Loss: 0.2774 | MAE: 0.1987 | RMSE: 0.2774 | RÂ²: 0.9176\n",
      "Epoch 20/20 | Train Loss: 0.3152 | Val Loss: 0.2773 | MAE: 0.1980 | RMSE: 0.2773 | RÂ²: 0.9177\n",
      "âœ… RMSE = 0.2773 | MAE = 0.1985\n",
      "\n",
      "ðŸ”§ [292/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[1024, 256, 1024], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.7243 | Val Loss: 0.9083 | MAE: 0.7647 | RMSE: 0.9083 | RÂ²: 0.1167\n",
      "Epoch 2/20 | Train Loss: 0.4721 | Val Loss: 0.7532 | MAE: 0.6351 | RMSE: 0.7532 | RÂ²: 0.3927\n",
      "Epoch 3/20 | Train Loss: 0.3977 | Val Loss: 0.6166 | MAE: 0.5197 | RMSE: 0.6166 | RÂ²: 0.5930\n",
      "Epoch 4/20 | Train Loss: 0.3622 | Val Loss: 0.4701 | MAE: 0.3855 | RMSE: 0.4701 | RÂ²: 0.7635\n",
      "Epoch 5/20 | Train Loss: 0.3355 | Val Loss: 0.3546 | MAE: 0.2831 | RMSE: 0.3546 | RÂ²: 0.8654\n",
      "Epoch 6/20 | Train Loss: 0.3179 | Val Loss: 0.2812 | MAE: 0.2120 | RMSE: 0.2812 | RÂ²: 0.9154\n",
      "Epoch 7/20 | Train Loss: 0.3081 | Val Loss: 0.2668 | MAE: 0.1863 | RMSE: 0.2668 | RÂ²: 0.9238\n",
      "Epoch 8/20 | Train Loss: 0.2997 | Val Loss: 0.2631 | MAE: 0.1861 | RMSE: 0.2631 | RÂ²: 0.9259\n",
      "Epoch 9/20 | Train Loss: 0.2910 | Val Loss: 0.2679 | MAE: 0.1792 | RMSE: 0.2679 | RÂ²: 0.9232\n",
      "Epoch 10/20 | Train Loss: 0.2816 | Val Loss: 0.2630 | MAE: 0.1838 | RMSE: 0.2630 | RÂ²: 0.9260\n",
      "Epoch 11/20 | Train Loss: 0.2829 | Val Loss: 0.2649 | MAE: 0.1830 | RMSE: 0.2649 | RÂ²: 0.9249\n",
      "Epoch 12/20 | Train Loss: 0.2743 | Val Loss: 0.2582 | MAE: 0.1788 | RMSE: 0.2582 | RÂ²: 0.9286\n",
      "Epoch 13/20 | Train Loss: 0.2719 | Val Loss: 0.2685 | MAE: 0.1850 | RMSE: 0.2685 | RÂ²: 0.9228\n",
      "Epoch 14/20 | Train Loss: 0.2728 | Val Loss: 0.2681 | MAE: 0.1844 | RMSE: 0.2681 | RÂ²: 0.9230\n",
      "Epoch 15/20 | Train Loss: 0.2701 | Val Loss: 0.2675 | MAE: 0.1847 | RMSE: 0.2675 | RÂ²: 0.9234\n",
      "Epoch 16/20 | Train Loss: 0.2619 | Val Loss: 0.2667 | MAE: 0.1840 | RMSE: 0.2667 | RÂ²: 0.9239\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2582 | MAE = 0.1788\n",
      "\n",
      "ðŸ”§ [293/300] Entrenando con: lr=0.0002, dropout=0.1, hidden_sizes=[512, 512, 512], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8467 | Val Loss: 0.9053 | MAE: 0.7670 | RMSE: 0.9053 | RÂ²: 0.1226\n",
      "Epoch 2/20 | Train Loss: 0.5186 | Val Loss: 0.7969 | MAE: 0.6638 | RMSE: 0.7969 | RÂ²: 0.3202\n",
      "Epoch 3/20 | Train Loss: 0.4255 | Val Loss: 0.6294 | MAE: 0.5232 | RMSE: 0.6294 | RÂ²: 0.5759\n",
      "Epoch 4/20 | Train Loss: 0.3807 | Val Loss: 0.5099 | MAE: 0.4225 | RMSE: 0.5099 | RÂ²: 0.7217\n",
      "Epoch 5/20 | Train Loss: 0.3458 | Val Loss: 0.4185 | MAE: 0.3391 | RMSE: 0.4185 | RÂ²: 0.8125\n",
      "Epoch 6/20 | Train Loss: 0.3358 | Val Loss: 0.2989 | MAE: 0.2293 | RMSE: 0.2989 | RÂ²: 0.9044\n",
      "Epoch 7/20 | Train Loss: 0.3183 | Val Loss: 0.2965 | MAE: 0.2218 | RMSE: 0.2965 | RÂ²: 0.9059\n",
      "Epoch 8/20 | Train Loss: 0.3073 | Val Loss: 0.2891 | MAE: 0.2101 | RMSE: 0.2891 | RÂ²: 0.9105\n",
      "Epoch 9/20 | Train Loss: 0.2981 | Val Loss: 0.2766 | MAE: 0.1994 | RMSE: 0.2766 | RÂ²: 0.9181\n",
      "Epoch 10/20 | Train Loss: 0.2923 | Val Loss: 0.2638 | MAE: 0.1839 | RMSE: 0.2638 | RÂ²: 0.9255\n",
      "Epoch 11/20 | Train Loss: 0.2851 | Val Loss: 0.2726 | MAE: 0.1917 | RMSE: 0.2726 | RÂ²: 0.9204\n",
      "Epoch 12/20 | Train Loss: 0.2825 | Val Loss: 0.2758 | MAE: 0.1941 | RMSE: 0.2758 | RÂ²: 0.9186\n",
      "Epoch 13/20 | Train Loss: 0.2723 | Val Loss: 0.2699 | MAE: 0.1901 | RMSE: 0.2699 | RÂ²: 0.9220\n",
      "Epoch 14/20 | Train Loss: 0.2736 | Val Loss: 0.2762 | MAE: 0.1933 | RMSE: 0.2762 | RÂ²: 0.9183\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2638 | MAE = 0.1839\n",
      "\n",
      "ðŸ”§ [294/300] Entrenando con: lr=0.0005, dropout=0.1, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9042 | Val Loss: 0.8001 | MAE: 0.6778 | RMSE: 0.8001 | RÂ²: 0.3148\n",
      "Epoch 2/20 | Train Loss: 0.3802 | Val Loss: 0.4888 | MAE: 0.4095 | RMSE: 0.4888 | RÂ²: 0.7442\n",
      "Epoch 3/20 | Train Loss: 0.3181 | Val Loss: 0.3706 | MAE: 0.2911 | RMSE: 0.3706 | RÂ²: 0.8530\n",
      "Epoch 4/20 | Train Loss: 0.2940 | Val Loss: 0.3511 | MAE: 0.2337 | RMSE: 0.3511 | RÂ²: 0.8680\n",
      "Epoch 5/20 | Train Loss: 0.2951 | Val Loss: 0.3906 | MAE: 0.3195 | RMSE: 0.3906 | RÂ²: 0.8367\n",
      "Epoch 6/20 | Train Loss: 0.2833 | Val Loss: 0.3236 | MAE: 0.2359 | RMSE: 0.3236 | RÂ²: 0.8879\n",
      "Epoch 7/20 | Train Loss: 0.2676 | Val Loss: 0.2748 | MAE: 0.1972 | RMSE: 0.2748 | RÂ²: 0.9191\n",
      "Epoch 8/20 | Train Loss: 0.2676 | Val Loss: 0.2921 | MAE: 0.2157 | RMSE: 0.2921 | RÂ²: 0.9087\n",
      "Epoch 9/20 | Train Loss: 0.2565 | Val Loss: 0.2700 | MAE: 0.1872 | RMSE: 0.2700 | RÂ²: 0.9220\n",
      "Epoch 10/20 | Train Loss: 0.2505 | Val Loss: 0.2665 | MAE: 0.1812 | RMSE: 0.2665 | RÂ²: 0.9240\n",
      "Epoch 11/20 | Train Loss: 0.2508 | Val Loss: 0.2684 | MAE: 0.1833 | RMSE: 0.2684 | RÂ²: 0.9228\n",
      "Epoch 12/20 | Train Loss: 0.2471 | Val Loss: 0.3001 | MAE: 0.1967 | RMSE: 0.3001 | RÂ²: 0.9036\n",
      "Epoch 13/20 | Train Loss: 0.2408 | Val Loss: 0.2707 | MAE: 0.1860 | RMSE: 0.2707 | RÂ²: 0.9216\n",
      "Epoch 14/20 | Train Loss: 0.2429 | Val Loss: 0.2835 | MAE: 0.1936 | RMSE: 0.2835 | RÂ²: 0.9139\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2665 | MAE = 0.1812\n",
      "\n",
      "ðŸ”§ [295/300] Entrenando con: lr=0.0002, dropout=0.3, hidden_sizes=[4096, 2048, 1024, 512, 256], seed=101\n",
      "Epoch 1/20 | Train Loss: 0.8788 | Val Loss: 0.9005 | MAE: 0.7544 | RMSE: 0.9005 | RÂ²: 0.1318\n",
      "Epoch 2/20 | Train Loss: 0.5570 | Val Loss: 0.6938 | MAE: 0.5781 | RMSE: 0.6938 | RÂ²: 0.4847\n",
      "Epoch 3/20 | Train Loss: 0.4590 | Val Loss: 0.5034 | MAE: 0.4086 | RMSE: 0.5034 | RÂ²: 0.7287\n",
      "Epoch 4/20 | Train Loss: 0.4056 | Val Loss: 0.3903 | MAE: 0.2932 | RMSE: 0.3903 | RÂ²: 0.8369\n",
      "Epoch 5/20 | Train Loss: 0.3908 | Val Loss: 0.3511 | MAE: 0.2446 | RMSE: 0.3511 | RÂ²: 0.8681\n",
      "Epoch 6/20 | Train Loss: 0.3655 | Val Loss: 0.3088 | MAE: 0.2160 | RMSE: 0.3088 | RÂ²: 0.8979\n",
      "Epoch 7/20 | Train Loss: 0.3595 | Val Loss: 0.3144 | MAE: 0.2283 | RMSE: 0.3144 | RÂ²: 0.8941\n",
      "Epoch 8/20 | Train Loss: 0.3435 | Val Loss: 0.2861 | MAE: 0.1890 | RMSE: 0.2861 | RÂ²: 0.9124\n",
      "Epoch 9/20 | Train Loss: 0.3471 | Val Loss: 0.3029 | MAE: 0.2028 | RMSE: 0.3029 | RÂ²: 0.9018\n",
      "Epoch 10/20 | Train Loss: 0.3328 | Val Loss: 0.2749 | MAE: 0.1926 | RMSE: 0.2749 | RÂ²: 0.9191\n",
      "Epoch 11/20 | Train Loss: 0.3231 | Val Loss: 0.2788 | MAE: 0.1905 | RMSE: 0.2788 | RÂ²: 0.9168\n",
      "Epoch 12/20 | Train Loss: 0.3101 | Val Loss: 0.2753 | MAE: 0.1824 | RMSE: 0.2753 | RÂ²: 0.9189\n",
      "Epoch 13/20 | Train Loss: 0.3082 | Val Loss: 0.2731 | MAE: 0.1885 | RMSE: 0.2731 | RÂ²: 0.9202\n",
      "Epoch 14/20 | Train Loss: 0.3042 | Val Loss: 0.2732 | MAE: 0.1819 | RMSE: 0.2732 | RÂ²: 0.9201\n",
      "Epoch 15/20 | Train Loss: 0.3004 | Val Loss: 0.2730 | MAE: 0.1832 | RMSE: 0.2730 | RÂ²: 0.9202\n",
      "Epoch 16/20 | Train Loss: 0.3010 | Val Loss: 0.2738 | MAE: 0.1862 | RMSE: 0.2738 | RÂ²: 0.9197\n",
      "Epoch 17/20 | Train Loss: 0.3006 | Val Loss: 0.2794 | MAE: 0.1898 | RMSE: 0.2794 | RÂ²: 0.9164\n",
      "Epoch 18/20 | Train Loss: 0.2918 | Val Loss: 0.2799 | MAE: 0.1870 | RMSE: 0.2799 | RÂ²: 0.9161\n",
      "Epoch 19/20 | Train Loss: 0.2917 | Val Loss: 0.2656 | MAE: 0.1787 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "Epoch 20/20 | Train Loss: 0.2842 | Val Loss: 0.2742 | MAE: 0.1855 | RMSE: 0.2742 | RÂ²: 0.9195\n",
      "âœ… RMSE = 0.2656 | MAE = 0.1787\n",
      "\n",
      "ðŸ”§ [296/300] Entrenando con: lr=0.0005, dropout=0.4, hidden_sizes=[512, 256], seed=1009\n",
      "Epoch 1/20 | Train Loss: 0.8531 | Val Loss: 0.7283 | MAE: 0.6151 | RMSE: 0.7283 | RÂ²: 0.4321\n",
      "Epoch 2/20 | Train Loss: 0.5931 | Val Loss: 0.5379 | MAE: 0.4382 | RMSE: 0.5379 | RÂ²: 0.6903\n",
      "Epoch 3/20 | Train Loss: 0.5175 | Val Loss: 0.3898 | MAE: 0.3076 | RMSE: 0.3898 | RÂ²: 0.8374\n",
      "Epoch 4/20 | Train Loss: 0.4786 | Val Loss: 0.3322 | MAE: 0.2484 | RMSE: 0.3322 | RÂ²: 0.8819\n",
      "Epoch 5/20 | Train Loss: 0.4489 | Val Loss: 0.3083 | MAE: 0.2253 | RMSE: 0.3083 | RÂ²: 0.8983\n",
      "Epoch 6/20 | Train Loss: 0.4195 | Val Loss: 0.3038 | MAE: 0.2145 | RMSE: 0.3038 | RÂ²: 0.9012\n",
      "Epoch 7/20 | Train Loss: 0.3999 | Val Loss: 0.2938 | MAE: 0.2065 | RMSE: 0.2938 | RÂ²: 0.9076\n",
      "Epoch 8/20 | Train Loss: 0.3858 | Val Loss: 0.2869 | MAE: 0.1998 | RMSE: 0.2869 | RÂ²: 0.9119\n",
      "Epoch 9/20 | Train Loss: 0.3659 | Val Loss: 0.2774 | MAE: 0.1955 | RMSE: 0.2774 | RÂ²: 0.9176\n",
      "Epoch 10/20 | Train Loss: 0.3608 | Val Loss: 0.2858 | MAE: 0.2020 | RMSE: 0.2858 | RÂ²: 0.9126\n",
      "Epoch 11/20 | Train Loss: 0.3519 | Val Loss: 0.2802 | MAE: 0.1966 | RMSE: 0.2802 | RÂ²: 0.9160\n",
      "Epoch 12/20 | Train Loss: 0.3422 | Val Loss: 0.2824 | MAE: 0.1975 | RMSE: 0.2824 | RÂ²: 0.9146\n",
      "Epoch 13/20 | Train Loss: 0.3344 | Val Loss: 0.2845 | MAE: 0.2003 | RMSE: 0.2845 | RÂ²: 0.9134\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2774 | MAE = 0.1955\n",
      "\n",
      "ðŸ”§ [297/300] Entrenando con: lr=0.0002, dropout=0.4, hidden_sizes=[512, 256, 128], seed=9871\n",
      "Epoch 1/20 | Train Loss: 1.0080 | Val Loss: 0.8942 | MAE: 0.7562 | RMSE: 0.8942 | RÂ²: 0.1440\n",
      "Epoch 2/20 | Train Loss: 0.8171 | Val Loss: 0.7780 | MAE: 0.6602 | RMSE: 0.7780 | RÂ²: 0.3520\n",
      "Epoch 3/20 | Train Loss: 0.7162 | Val Loss: 0.6597 | MAE: 0.5546 | RMSE: 0.6597 | RÂ²: 0.5341\n",
      "Epoch 4/20 | Train Loss: 0.6615 | Val Loss: 0.5589 | MAE: 0.4441 | RMSE: 0.5589 | RÂ²: 0.6656\n",
      "Epoch 5/20 | Train Loss: 0.6001 | Val Loss: 0.4616 | MAE: 0.3419 | RMSE: 0.4616 | RÂ²: 0.7719\n",
      "Epoch 6/20 | Train Loss: 0.5723 | Val Loss: 0.3988 | MAE: 0.2853 | RMSE: 0.3988 | RÂ²: 0.8298\n",
      "Epoch 7/20 | Train Loss: 0.5553 | Val Loss: 0.3759 | MAE: 0.2695 | RMSE: 0.3759 | RÂ²: 0.8487\n",
      "Epoch 8/20 | Train Loss: 0.5359 | Val Loss: 0.3680 | MAE: 0.2656 | RMSE: 0.3680 | RÂ²: 0.8550\n",
      "Epoch 9/20 | Train Loss: 0.5177 | Val Loss: 0.3545 | MAE: 0.2561 | RMSE: 0.3545 | RÂ²: 0.8655\n",
      "Epoch 10/20 | Train Loss: 0.4963 | Val Loss: 0.3408 | MAE: 0.2490 | RMSE: 0.3408 | RÂ²: 0.8756\n",
      "Epoch 11/20 | Train Loss: 0.4790 | Val Loss: 0.3341 | MAE: 0.2434 | RMSE: 0.3341 | RÂ²: 0.8805\n",
      "Epoch 12/20 | Train Loss: 0.4745 | Val Loss: 0.3302 | MAE: 0.2386 | RMSE: 0.3302 | RÂ²: 0.8833\n",
      "Epoch 13/20 | Train Loss: 0.4720 | Val Loss: 0.3323 | MAE: 0.2355 | RMSE: 0.3323 | RÂ²: 0.8818\n",
      "Epoch 14/20 | Train Loss: 0.4573 | Val Loss: 0.3365 | MAE: 0.2372 | RMSE: 0.3365 | RÂ²: 0.8788\n",
      "Epoch 15/20 | Train Loss: 0.4403 | Val Loss: 0.3297 | MAE: 0.2338 | RMSE: 0.3297 | RÂ²: 0.8836\n",
      "Epoch 16/20 | Train Loss: 0.4366 | Val Loss: 0.3273 | MAE: 0.2306 | RMSE: 0.3273 | RÂ²: 0.8853\n",
      "Epoch 17/20 | Train Loss: 0.4257 | Val Loss: 0.3338 | MAE: 0.2286 | RMSE: 0.3338 | RÂ²: 0.8807\n",
      "Epoch 18/20 | Train Loss: 0.4202 | Val Loss: 0.3327 | MAE: 0.2295 | RMSE: 0.3327 | RÂ²: 0.8815\n",
      "Epoch 19/20 | Train Loss: 0.4082 | Val Loss: 0.3265 | MAE: 0.2265 | RMSE: 0.3265 | RÂ²: 0.8859\n",
      "Epoch 20/20 | Train Loss: 0.4006 | Val Loss: 0.3245 | MAE: 0.2240 | RMSE: 0.3245 | RÂ²: 0.8872\n",
      "âœ… RMSE = 0.3245 | MAE = 0.2240\n",
      "\n",
      "ðŸ”§ [298/300] Entrenando con: lr=0.0005, dropout=0.2, hidden_sizes=[1024, 512, 256, 128], seed=5003\n",
      "Epoch 1/20 | Train Loss: 0.7039 | Val Loss: 0.8833 | MAE: 0.7377 | RMSE: 0.8833 | RÂ²: 0.1648\n",
      "Epoch 2/20 | Train Loss: 0.4426 | Val Loss: 0.6880 | MAE: 0.5738 | RMSE: 0.6880 | RÂ²: 0.4932\n",
      "Epoch 3/20 | Train Loss: 0.3850 | Val Loss: 0.4453 | MAE: 0.3651 | RMSE: 0.4453 | RÂ²: 0.7877\n",
      "Epoch 4/20 | Train Loss: 0.3488 | Val Loss: 0.3355 | MAE: 0.2465 | RMSE: 0.3355 | RÂ²: 0.8795\n",
      "Epoch 5/20 | Train Loss: 0.3308 | Val Loss: 0.2626 | MAE: 0.1900 | RMSE: 0.2626 | RÂ²: 0.9262\n",
      "Epoch 6/20 | Train Loss: 0.3128 | Val Loss: 0.2764 | MAE: 0.1958 | RMSE: 0.2764 | RÂ²: 0.9182\n",
      "Epoch 7/20 | Train Loss: 0.3038 | Val Loss: 0.2764 | MAE: 0.1996 | RMSE: 0.2764 | RÂ²: 0.9182\n",
      "Epoch 8/20 | Train Loss: 0.2935 | Val Loss: 0.2656 | MAE: 0.1908 | RMSE: 0.2656 | RÂ²: 0.9245\n",
      "Epoch 9/20 | Train Loss: 0.2873 | Val Loss: 0.2724 | MAE: 0.1892 | RMSE: 0.2724 | RÂ²: 0.9206\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2626 | MAE = 0.1900\n",
      "\n",
      "ðŸ”§ [299/300] Entrenando con: lr=0.0001, dropout=0.1, hidden_sizes=[256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.9543 | Val Loss: 0.9140 | MAE: 0.7773 | RMSE: 0.9140 | RÂ²: 0.1056\n",
      "Epoch 2/20 | Train Loss: 0.8316 | Val Loss: 0.8478 | MAE: 0.7231 | RMSE: 0.8478 | RÂ²: 0.2306\n",
      "Epoch 3/20 | Train Loss: 0.7445 | Val Loss: 0.7778 | MAE: 0.6626 | RMSE: 0.7778 | RÂ²: 0.3523\n",
      "Epoch 4/20 | Train Loss: 0.6733 | Val Loss: 0.7023 | MAE: 0.5931 | RMSE: 0.7023 | RÂ²: 0.4719\n",
      "Epoch 5/20 | Train Loss: 0.6209 | Val Loss: 0.6209 | MAE: 0.5126 | RMSE: 0.6209 | RÂ²: 0.5872\n",
      "Epoch 6/20 | Train Loss: 0.5679 | Val Loss: 0.5494 | MAE: 0.4402 | RMSE: 0.5494 | RÂ²: 0.6768\n",
      "Epoch 7/20 | Train Loss: 0.5305 | Val Loss: 0.4915 | MAE: 0.3866 | RMSE: 0.4915 | RÂ²: 0.7414\n",
      "Epoch 8/20 | Train Loss: 0.4923 | Val Loss: 0.4464 | MAE: 0.3473 | RMSE: 0.4464 | RÂ²: 0.7866\n",
      "Epoch 9/20 | Train Loss: 0.4630 | Val Loss: 0.4107 | MAE: 0.3169 | RMSE: 0.4107 | RÂ²: 0.8194\n",
      "Epoch 10/20 | Train Loss: 0.4342 | Val Loss: 0.3840 | MAE: 0.2938 | RMSE: 0.3840 | RÂ²: 0.8421\n",
      "Epoch 11/20 | Train Loss: 0.4168 | Val Loss: 0.3620 | MAE: 0.2743 | RMSE: 0.3620 | RÂ²: 0.8597\n",
      "Epoch 12/20 | Train Loss: 0.3957 | Val Loss: 0.3485 | MAE: 0.2609 | RMSE: 0.3485 | RÂ²: 0.8700\n",
      "Epoch 13/20 | Train Loss: 0.3809 | Val Loss: 0.3361 | MAE: 0.2505 | RMSE: 0.3361 | RÂ²: 0.8791\n",
      "Epoch 14/20 | Train Loss: 0.3650 | Val Loss: 0.3231 | MAE: 0.2420 | RMSE: 0.3231 | RÂ²: 0.8882\n",
      "Epoch 15/20 | Train Loss: 0.3582 | Val Loss: 0.3134 | MAE: 0.2347 | RMSE: 0.3134 | RÂ²: 0.8948\n",
      "Epoch 16/20 | Train Loss: 0.3444 | Val Loss: 0.3080 | MAE: 0.2290 | RMSE: 0.3080 | RÂ²: 0.8984\n",
      "Epoch 17/20 | Train Loss: 0.3387 | Val Loss: 0.3020 | MAE: 0.2247 | RMSE: 0.3020 | RÂ²: 0.9023\n",
      "Epoch 18/20 | Train Loss: 0.3335 | Val Loss: 0.2988 | MAE: 0.2213 | RMSE: 0.2988 | RÂ²: 0.9044\n",
      "Epoch 19/20 | Train Loss: 0.3252 | Val Loss: 0.2963 | MAE: 0.2170 | RMSE: 0.2963 | RÂ²: 0.9060\n",
      "Epoch 20/20 | Train Loss: 0.3183 | Val Loss: 0.2910 | MAE: 0.2144 | RMSE: 0.2910 | RÂ²: 0.9093\n",
      "âœ… RMSE = 0.2910 | MAE = 0.2144\n",
      "\n",
      "ðŸ”§ [300/300] Entrenando con: lr=0.001, dropout=0.4, hidden_sizes=[256, 128], seed=307\n",
      "Epoch 1/20 | Train Loss: 0.8534 | Val Loss: 0.7200 | MAE: 0.6087 | RMSE: 0.7200 | RÂ²: 0.4451\n",
      "Epoch 2/20 | Train Loss: 0.5972 | Val Loss: 0.4666 | MAE: 0.3819 | RMSE: 0.4666 | RÂ²: 0.7669\n",
      "Epoch 3/20 | Train Loss: 0.5083 | Val Loss: 0.3823 | MAE: 0.2940 | RMSE: 0.3823 | RÂ²: 0.8435\n",
      "Epoch 4/20 | Train Loss: 0.4545 | Val Loss: 0.3533 | MAE: 0.2727 | RMSE: 0.3533 | RÂ²: 0.8663\n",
      "Epoch 5/20 | Train Loss: 0.4210 | Val Loss: 0.3711 | MAE: 0.2855 | RMSE: 0.3711 | RÂ²: 0.8525\n",
      "Epoch 6/20 | Train Loss: 0.4032 | Val Loss: 0.3383 | MAE: 0.2516 | RMSE: 0.3383 | RÂ²: 0.8775\n",
      "Epoch 7/20 | Train Loss: 0.3809 | Val Loss: 0.3059 | MAE: 0.2256 | RMSE: 0.3059 | RÂ²: 0.8998\n",
      "Epoch 8/20 | Train Loss: 0.3632 | Val Loss: 0.2873 | MAE: 0.2091 | RMSE: 0.2873 | RÂ²: 0.9116\n",
      "Epoch 9/20 | Train Loss: 0.3538 | Val Loss: 0.2770 | MAE: 0.1967 | RMSE: 0.2770 | RÂ²: 0.9179\n",
      "Epoch 10/20 | Train Loss: 0.3465 | Val Loss: 0.2789 | MAE: 0.1967 | RMSE: 0.2789 | RÂ²: 0.9167\n",
      "Epoch 11/20 | Train Loss: 0.3317 | Val Loss: 0.2748 | MAE: 0.1918 | RMSE: 0.2748 | RÂ²: 0.9191\n",
      "Epoch 12/20 | Train Loss: 0.3270 | Val Loss: 0.2695 | MAE: 0.1883 | RMSE: 0.2695 | RÂ²: 0.9223\n",
      "Epoch 13/20 | Train Loss: 0.3211 | Val Loss: 0.2842 | MAE: 0.2053 | RMSE: 0.2842 | RÂ²: 0.9135\n",
      "Epoch 14/20 | Train Loss: 0.3132 | Val Loss: 0.2846 | MAE: 0.2039 | RMSE: 0.2846 | RÂ²: 0.9133\n",
      "Epoch 15/20 | Train Loss: 0.3078 | Val Loss: 0.2825 | MAE: 0.2006 | RMSE: 0.2825 | RÂ²: 0.9146\n",
      "Epoch 16/20 | Train Loss: 0.3020 | Val Loss: 0.2719 | MAE: 0.1902 | RMSE: 0.2719 | RÂ²: 0.9209\n",
      "ðŸ”´ Early stopping triggered\n",
      "âœ… RMSE = 0.2695 | MAE = 0.1883\n",
      "\n",
      "ðŸ“Š Mejores combinaciones:\n",
      "         lr  dropout                  hidden_sizes  seed      rmse       mae  \\\n",
      "169  0.0005      0.1         [1024, 512, 256, 128]  7777  0.240731  0.161526   \n",
      "5    0.0002      0.1  [4096, 2048, 1024, 512, 256]  5003  0.242738  0.166108   \n",
      "68   0.0002      0.1        [2048, 1024, 512, 256]   307  0.244617  0.171043   \n",
      "132  0.0001      0.1  [4096, 2048, 1024, 512, 256]  2029  0.245374  0.171914   \n",
      "287  0.0005      0.1        [2048, 1024, 512, 256]   101  0.249183  0.173728   \n",
      "122  0.0002      0.1         [1024, 512, 256, 128]  2029  0.249538  0.175310   \n",
      "23   0.0010      0.3               [512, 256, 128]  2029  0.249793  0.170107   \n",
      "212  0.0010      0.2                    [256, 256]  2029  0.250029  0.170332   \n",
      "190  0.0005      0.1        [2048, 1024, 512, 256]  1009  0.250736  0.172152   \n",
      "59   0.0005      0.1  [4096, 2048, 1024, 512, 256]  5003  0.251189  0.171999   \n",
      "124  0.0005      0.1               [512, 512, 256]  9871  0.252202  0.174694   \n",
      "123  0.0010      0.1             [1024, 256, 1024]  7777  0.252399  0.176447   \n",
      "63   0.0005      0.1              [1024, 512, 256]  1009  0.252566  0.177751   \n",
      "91   0.0010      0.1                    [512, 256]  7777  0.252597  0.173447   \n",
      "13   0.0010      0.1                    [256, 256]   307  0.252647  0.176165   \n",
      "159  0.0010      0.4              [1024, 512, 256]   307  0.252767  0.172420   \n",
      "263  0.0010      0.3              [1024, 512, 256]   101  0.252870  0.172058   \n",
      "30   0.0002      0.1              [1024, 512, 256]   101  0.253064  0.175548   \n",
      "67   0.0005      0.1             [1024, 256, 1024]  1009  0.253106  0.171345   \n",
      "55   0.0010      0.3               [512, 512, 256]   307  0.253255  0.174658   \n",
      "179  0.0005      0.1               [512, 512, 512]  5003  0.253388  0.177084   \n",
      "248  0.0005      0.2               [512, 256, 128]  7777  0.253487  0.174337   \n",
      "147  0.0010      0.4             [1024, 256, 1024]   101  0.253497  0.173681   \n",
      "241  0.0005      0.1         [1024, 512, 256, 128]  2029  0.253947  0.173912   \n",
      "279  0.0005      0.1  [4096, 2048, 1024, 512, 256]  2029  0.254099  0.170329   \n",
      "150  0.0010      0.1               [512, 256, 128]   307  0.254178  0.174773   \n",
      "60   0.0005      0.2        [2048, 1024, 512, 256]  7777  0.254361  0.169831   \n",
      "85   0.0005      0.3              [1024, 512, 256]  1009  0.254622  0.180868   \n",
      "4    0.0010      0.2         [1024, 512, 256, 128]  2029  0.254661  0.173422   \n",
      "28   0.0010      0.5              [1024, 512, 256]   307  0.254737  0.174493   \n",
      "189  0.0010      0.2             [1024, 256, 1024]   101  0.254890  0.174795   \n",
      "244  0.0010      0.3        [2048, 1024, 512, 256]   307  0.255023  0.174362   \n",
      "17   0.0010      0.2               [512, 512, 256]   101  0.255133  0.176097   \n",
      "111  0.0010      0.1                    [256, 128]  1009  0.255634  0.174784   \n",
      "108  0.0010      0.2                    [512, 256]  7777  0.255634  0.174690   \n",
      "106  0.0002      0.1               [512, 512, 256]   307  0.255712  0.180601   \n",
      "56   0.0010      0.2               [512, 512, 512]  1009  0.255792  0.177540   \n",
      "160  0.0010      0.1               [512, 512, 256]  2029  0.255821  0.180457   \n",
      "2    0.0005      0.4  [4096, 2048, 1024, 512, 256]  9871  0.255907  0.170087   \n",
      "83   0.0010      0.4               [512, 512, 512]  2029  0.256009  0.173554   \n",
      "137  0.0002      0.2        [2048, 1024, 512, 256]  1009  0.256083  0.174991   \n",
      "82   0.0005      0.2             [1024, 256, 1024]  9871  0.256441  0.176651   \n",
      "168  0.0010      0.2        [2048, 1024, 512, 256]  5003  0.256532  0.175361   \n",
      "73   0.0010      0.4        [2048, 1024, 512, 256]   101  0.256596  0.173346   \n",
      "283  0.0005      0.1                    [256, 256]   101  0.256610  0.179907   \n",
      "265  0.0010      0.1         [1024, 512, 256, 128]  1009  0.256733  0.184455   \n",
      "14   0.0005      0.2              [1024, 512, 256]   101  0.256770  0.181899   \n",
      "93   0.0005      0.3                    [512, 256]  5003  0.257143  0.178183   \n",
      "131  0.0005      0.2         [1024, 512, 256, 128]   101  0.257212  0.175534   \n",
      "269  0.0010      0.2                    [512, 256]  2029  0.257232  0.177052   \n",
      "\n",
      "     best_epoch  \n",
      "169           7  \n",
      "5             7  \n",
      "68            7  \n",
      "132           7  \n",
      "287           6  \n",
      "122           9  \n",
      "23           11  \n",
      "212          12  \n",
      "190           5  \n",
      "59            9  \n",
      "124          10  \n",
      "123           9  \n",
      "63            6  \n",
      "91            8  \n",
      "13            9  \n",
      "159          16  \n",
      "263          14  \n",
      "30           10  \n",
      "67           14  \n",
      "55           13  \n",
      "179           7  \n",
      "248          11  \n",
      "147          15  \n",
      "241           9  \n",
      "279          12  \n",
      "150          12  \n",
      "60            7  \n",
      "85            6  \n",
      "4            13  \n",
      "28           16  \n",
      "189           8  \n",
      "244          11  \n",
      "17            9  \n",
      "111           9  \n",
      "108           8  \n",
      "106          10  \n",
      "56            8  \n",
      "160           5  \n",
      "2            13  \n",
      "83           20  \n",
      "137           8  \n",
      "82            8  \n",
      "168          10  \n",
      "73           10  \n",
      "283           8  \n",
      "265           4  \n",
      "14            5  \n",
      "93           12  \n",
      "131          13  \n",
      "269           9  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from itertools import product\n",
    "\n",
    "# ---- FUNCIÃ“N DE STRATIFIED SAMPLING ----\n",
    "def stratified_sampling_grid(param_grid, stratify_by, N_TOTAL, seed=42):\n",
    "    keys = list(param_grid.keys())\n",
    "    all_combos = [\n",
    "        dict(zip(keys, vals))\n",
    "        for vals in product(*(param_grid[k] for k in keys))\n",
    "    ]\n",
    "    df = pd.DataFrame(all_combos)\n",
    "\n",
    "    # --- Nueva columna hashable para deduplicar y comparar ---\n",
    "    for col in stratify_by:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            df[col + '_str'] = df[col].apply(lambda x: str(x))\n",
    "    # Usar '_str' si existe, sino el nombre original\n",
    "    stratify_keys = [col + '_str' if (col + '_str') in df.columns else col for col in stratify_by]\n",
    "\n",
    "    random.seed(seed)\n",
    "    uniq_strata = df.drop_duplicates(subset=stratify_keys)\n",
    "    uniq_strata = uniq_strata.sample(frac=1, random_state=seed)  # shuffle\n",
    "\n",
    "    sampled_rows = []\n",
    "    for _, row in uniq_strata.iterrows():\n",
    "        cond = pd.Series(True, index=df.index)\n",
    "        for col in stratify_by:\n",
    "            key = col + '_str' if (col + '_str') in df.columns else col\n",
    "            val = str(row[col]) if key.endswith('_str') else row[col]\n",
    "            cond &= (df[key] == val)\n",
    "        group = df[cond]\n",
    "        sampled_rows.append(group.sample(1, random_state=random.randint(0, 99999)).iloc[0])\n",
    "\n",
    "    if len(sampled_rows) < N_TOTAL:\n",
    "        ids_usados = set()\n",
    "        for row in sampled_rows:\n",
    "            row_tuple = tuple(\n",
    "                str(row[k]) if (k + '_str') in df.columns else row[k] for k in keys\n",
    "            )\n",
    "            ids_usados.add(row_tuple)\n",
    "        resto = df[\n",
    "            ~df.apply(lambda r: tuple(\n",
    "                str(r[k]) if (k + '_str') in df.columns else r[k] for k in keys\n",
    "            ) in ids_usados, axis=1)\n",
    "        ]\n",
    "        faltan = N_TOTAL - len(sampled_rows)\n",
    "        if len(resto) > 0:\n",
    "            sampled_rows += list(resto.sample(n=min(faltan, len(resto)), random_state=seed+1).to_dict('records'))\n",
    "\n",
    "    sampled_rows = sampled_rows[:N_TOTAL]\n",
    "    # --- Devolver solo las columnas originales ---\n",
    "    # Â¡Asegurarse de que todos sean dicts!\n",
    "    sampled_rows = [dict(r) for r in sampled_rows]\n",
    "    result = pd.DataFrame(sampled_rows)[keys].reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "# ---- DEFINICIÃ“N DEL GRID Y PARÃMETROS ----\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4, 2e-4, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'hidden_sizes': [\n",
    "        [512, 256], [1024, 512, 256], [2048, 1024, 512, 256], [512, 256, 128],\n",
    "        [1024, 512, 256, 128], [4096, 2048, 1024, 512, 256], [256, 128],\n",
    "        [512, 512, 256], [512, 512, 512], [256, 256], [1024, 256, 1024]\n",
    "    ],\n",
    "    'seed': [101, 307, 1009, 2029, 5003, 7777, 9871]\n",
    "}\n",
    "N_TOTAL = 300  # AjustÃ¡ segÃºn tus recursos\n",
    "stratify_by = ['lr', 'dropout', 'hidden_sizes']\n",
    "\n",
    "# ---- GENERAR COMBINACIONES A ENTRENAR ----\n",
    "combos_to_run = stratified_sampling_grid(param_grid, stratify_by=stratify_by, N_TOTAL=N_TOTAL)\n",
    "\n",
    "# ---- LOOP DE ENTRENAMIENTO ----\n",
    "results = []\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for i, row in combos_to_run.iterrows():\n",
    "    lr = row['lr']\n",
    "    dropout = row['dropout']\n",
    "    hidden_sizes = row['hidden_sizes']\n",
    "    seed = int(row['seed'])\n",
    "\n",
    "    print(f\"\\nðŸ”§ [{i+1}/{len(combos_to_run)}] Entrenando con: lr={lr}, dropout={dropout}, hidden_sizes={hidden_sizes}, seed={seed}\")\n",
    "\n",
    "    # Setear semillas\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Crear modelo y mover a dispositivo\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrenamiento (ajustÃ¡ la funciÃ³n train_model segÃºn tu cÃ³digo)\n",
    "    y_true_gs, y_pred_gs, best_epoch = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        n_epochs=20, lr=lr, patience=4)\n",
    "\n",
    "    rmse = np.sqrt(np.mean((np.array(y_true_gs) - np.array(y_pred_gs)) ** 2))\n",
    "    mae = mean_absolute_error(y_true_gs, y_pred_gs)\n",
    "\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'dropout': dropout,\n",
    "        'hidden_sizes': hidden_sizes,\n",
    "        'seed': seed,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'best_epoch': best_epoch\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… RMSE = {rmse:.4f} | MAE = {mae:.4f}\")\n",
    "\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        torch.save(model.state_dict(), f\"best_model_rmse{rmse:.4f}_lr{lr}_do{dropout}_seed{seed}.pth\")\n",
    "        print(\"ðŸ’¾ Modelo guardado (mejor hasta ahora)\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ---- GUARDAR Y MOSTRAR RESULTADOS ----\n",
    "results_df = pd.DataFrame(results).sort_values(by='rmse')\n",
    "print(\"\\nðŸ“Š Mejores combinaciones:\")\n",
    "print(results_df.head(50))\n",
    "results_df.to_csv(\"gridsearch_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb90a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# ------------------------\n",
    "# Prepara el dataset final\n",
    "# ------------------------\n",
    "# train_dataset y val_dataset ya deberÃ­an estar definidos\n",
    "train_val_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader_full = DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974a992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelos seleccionados para ensemble (total=50):\n",
      "{'lr': 0.0005, 'dropout': 0.1, 'hidden_sizes': [1024, 512, 256, 128], 'seed': 7777, 'n_epochs': 7, 'name': 'modelo_1', 'name_desc': 'mlp_lr_0.0005_drop_0.1_hs_1024-512-256-128_seed_7777'}\n",
      "{'lr': 0.0002, 'dropout': 0.1, 'hidden_sizes': [4096, 2048, 1024, 512, 256], 'seed': 5003, 'n_epochs': 7, 'name': 'modelo_2', 'name_desc': 'mlp_lr_0.0002_drop_0.1_hs_4096-2048-1024-512-256_seed_5003'}\n",
      "{'lr': 0.0002, 'dropout': 0.1, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 307, 'n_epochs': 7, 'name': 'modelo_3', 'name_desc': 'mlp_lr_0.0002_drop_0.1_hs_2048-1024-512-256_seed_307'}\n",
      "{'lr': 0.0001, 'dropout': 0.1, 'hidden_sizes': [4096, 2048, 1024, 512, 256], 'seed': 2029, 'n_epochs': 7, 'name': 'modelo_4', 'name_desc': 'mlp_lr_0.0001_drop_0.1_hs_4096-2048-1024-512-256_seed_2029'}\n",
      "{'lr': 0.001, 'dropout': 0.3, 'hidden_sizes': [512, 256, 128], 'seed': 2029, 'n_epochs': 11, 'name': 'modelo_5', 'name_desc': 'mlp_lr_0.001_drop_0.3_hs_512-256-128_seed_2029'}\n",
      "{'lr': 0.001, 'dropout': 0.2, 'hidden_sizes': [256, 256], 'seed': 2029, 'n_epochs': 12, 'name': 'modelo_6', 'name_desc': 'mlp_lr_0.001_drop_0.2_hs_256-256_seed_2029'}\n",
      "{'lr': 0.0005, 'dropout': 0.1, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 1009, 'n_epochs': 5, 'name': 'modelo_7', 'name_desc': 'mlp_lr_0.0005_drop_0.1_hs_2048-1024-512-256_seed_1009'}\n",
      "{'lr': 0.0005, 'dropout': 0.1, 'hidden_sizes': [4096, 2048, 1024, 512, 256], 'seed': 5003, 'n_epochs': 9, 'name': 'modelo_8', 'name_desc': 'mlp_lr_0.0005_drop_0.1_hs_4096-2048-1024-512-256_seed_5003'}\n",
      "{'lr': 0.0005, 'dropout': 0.2, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 7777, 'n_epochs': 7, 'name': 'modelo_9', 'name_desc': 'mlp_lr_0.0005_drop_0.2_hs_2048-1024-512-256_seed_7777'}\n",
      "{'lr': 0.0005, 'dropout': 0.1, 'hidden_sizes': [1024, 256, 1024], 'seed': 1009, 'n_epochs': 14, 'name': 'modelo_10', 'name_desc': 'mlp_lr_0.0005_drop_0.1_hs_1024-256-1024_seed_1009'}\n",
      "{'lr': 0.001, 'dropout': 0.3, 'hidden_sizes': [1024, 512, 256], 'seed': 101, 'n_epochs': 14, 'name': 'modelo_11', 'name_desc': 'mlp_lr_0.001_drop_0.3_hs_1024-512-256_seed_101'}\n",
      "{'lr': 0.001, 'dropout': 0.4, 'hidden_sizes': [1024, 512, 256], 'seed': 307, 'n_epochs': 16, 'name': 'modelo_12', 'name_desc': 'mlp_lr_0.001_drop_0.4_hs_1024-512-256_seed_307'}\n",
      "{'lr': 0.0002, 'dropout': 0.1, 'hidden_sizes': [1024, 512, 256, 128], 'seed': 2029, 'n_epochs': 9, 'name': 'modelo_13', 'name_desc': 'mlp_lr_0.0002_drop_0.1_hs_1024-512-256-128_seed_2029'}\n",
      "{'lr': 0.0005, 'dropout': 0.4, 'hidden_sizes': [4096, 2048, 1024, 512, 256], 'seed': 9871, 'n_epochs': 13, 'name': 'modelo_14', 'name_desc': 'mlp_lr_0.0005_drop_0.4_hs_4096-2048-1024-512-256_seed_9871'}\n",
      "{'lr': 0.001, 'dropout': 0.1, 'hidden_sizes': [512, 256], 'seed': 7777, 'n_epochs': 8, 'name': 'modelo_15', 'name_desc': 'mlp_lr_0.001_drop_0.1_hs_512-256_seed_7777'}\n",
      "{'lr': 0.0005, 'dropout': 0.1, 'hidden_sizes': [512, 512, 256], 'seed': 9871, 'n_epochs': 10, 'name': 'modelo_16', 'name_desc': 'mlp_lr_0.0005_drop_0.1_hs_512-512-256_seed_9871'}\n",
      "{'lr': 0.001, 'dropout': 0.4, 'hidden_sizes': [1024, 256, 1024], 'seed': 101, 'n_epochs': 15, 'name': 'modelo_17', 'name_desc': 'mlp_lr_0.001_drop_0.4_hs_1024-256-1024_seed_101'}\n",
      "{'lr': 0.0005, 'dropout': 0.2, 'hidden_sizes': [512, 256, 128], 'seed': 7777, 'n_epochs': 11, 'name': 'modelo_18', 'name_desc': 'mlp_lr_0.0005_drop_0.2_hs_512-256-128_seed_7777'}\n",
      "{'lr': 0.001, 'dropout': 0.2, 'hidden_sizes': [1024, 512, 256, 128], 'seed': 2029, 'n_epochs': 13, 'name': 'modelo_19', 'name_desc': 'mlp_lr_0.001_drop_0.2_hs_1024-512-256-128_seed_2029'}\n",
      "{'lr': 0.001, 'dropout': 0.3, 'hidden_sizes': [512, 512, 256], 'seed': 307, 'n_epochs': 13, 'name': 'modelo_20', 'name_desc': 'mlp_lr_0.001_drop_0.3_hs_512-512-256_seed_307'}\n",
      "{'lr': 0.0002, 'dropout': 0.1, 'hidden_sizes': [1024, 512, 256], 'seed': 101, 'n_epochs': 10, 'name': 'modelo_21', 'name_desc': 'mlp_lr_0.0002_drop_0.1_hs_1024-512-256_seed_101'}\n",
      "{'lr': 0.001, 'dropout': 0.1, 'hidden_sizes': [512, 256, 128], 'seed': 307, 'n_epochs': 12, 'name': 'modelo_22', 'name_desc': 'mlp_lr_0.001_drop_0.1_hs_512-256-128_seed_307'}\n",
      "{'lr': 0.001, 'dropout': 0.1, 'hidden_sizes': [256, 256], 'seed': 307, 'n_epochs': 9, 'name': 'modelo_23', 'name_desc': 'mlp_lr_0.001_drop_0.1_hs_256-256_seed_307'}\n",
      "{'lr': 0.001, 'dropout': 0.1, 'hidden_sizes': [1024, 256, 1024], 'seed': 7777, 'n_epochs': 9, 'name': 'modelo_24', 'name_desc': 'mlp_lr_0.001_drop_0.1_hs_1024-256-1024_seed_7777'}\n",
      "{'lr': 0.001, 'dropout': 0.5, 'hidden_sizes': [1024, 512, 256], 'seed': 307, 'n_epochs': 16, 'name': 'modelo_25', 'name_desc': 'mlp_lr_0.001_drop_0.5_hs_1024-512-256_seed_307'}\n",
      "{'lr': 0.001, 'dropout': 0.3, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 307, 'n_epochs': 11, 'name': 'modelo_26', 'name_desc': 'mlp_lr_0.001_drop_0.3_hs_2048-1024-512-256_seed_307'}\n",
      "{'lr': 0.001, 'dropout': 0.4, 'hidden_sizes': [512, 512, 512], 'seed': 2029, 'n_epochs': 20, 'name': 'modelo_27', 'name_desc': 'mlp_lr_0.001_drop_0.4_hs_512-512-512_seed_2029'}\n",
      "{'lr': 0.001, 'dropout': 0.2, 'hidden_sizes': [1024, 256, 1024], 'seed': 101, 'n_epochs': 8, 'name': 'modelo_28', 'name_desc': 'mlp_lr_0.001_drop_0.2_hs_1024-256-1024_seed_101'}\n",
      "{'lr': 0.001, 'dropout': 0.4, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 101, 'n_epochs': 10, 'name': 'modelo_29', 'name_desc': 'mlp_lr_0.001_drop_0.4_hs_2048-1024-512-256_seed_101'}\n",
      "{'lr': 0.001, 'dropout': 0.2, 'hidden_sizes': [512, 256], 'seed': 7777, 'n_epochs': 8, 'name': 'modelo_30', 'name_desc': 'mlp_lr_0.001_drop_0.2_hs_512-256_seed_7777'}\n",
      "{'lr': 0.0002, 'dropout': 0.2, 'hidden_sizes': [4096, 2048, 1024, 512, 256], 'seed': 1009, 'n_epochs': 6, 'name': 'modelo_31', 'name_desc': 'mlp_lr_0.0002_drop_0.2_hs_4096-2048-1024-512-256_seed_1009'}\n",
      "{'lr': 0.001, 'dropout': 0.1, 'hidden_sizes': [256, 128], 'seed': 1009, 'n_epochs': 9, 'name': 'modelo_32', 'name_desc': 'mlp_lr_0.001_drop_0.1_hs_256-128_seed_1009'}\n",
      "{'lr': 0.0005, 'dropout': 0.1, 'hidden_sizes': [1024, 512, 256], 'seed': 1009, 'n_epochs': 6, 'name': 'modelo_33', 'name_desc': 'mlp_lr_0.0005_drop_0.1_hs_1024-512-256_seed_1009'}\n",
      "{'lr': 0.0005, 'dropout': 0.1, 'hidden_sizes': [512, 512, 512], 'seed': 5003, 'n_epochs': 7, 'name': 'modelo_34', 'name_desc': 'mlp_lr_0.0005_drop_0.1_hs_512-512-512_seed_5003'}\n",
      "{'lr': 0.0002, 'dropout': 0.2, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 1009, 'n_epochs': 8, 'name': 'modelo_35', 'name_desc': 'mlp_lr_0.0002_drop_0.2_hs_2048-1024-512-256_seed_1009'}\n",
      "{'lr': 0.001, 'dropout': 0.2, 'hidden_sizes': [512, 512, 256], 'seed': 101, 'n_epochs': 9, 'name': 'modelo_36', 'name_desc': 'mlp_lr_0.001_drop_0.2_hs_512-512-256_seed_101'}\n",
      "{'lr': 0.001, 'dropout': 0.1, 'hidden_sizes': [512, 512, 512], 'seed': 101, 'n_epochs': 9, 'name': 'modelo_37', 'name_desc': 'mlp_lr_0.001_drop_0.1_hs_512-512-512_seed_101'}\n",
      "{'lr': 0.001, 'dropout': 0.2, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 5003, 'n_epochs': 10, 'name': 'modelo_38', 'name_desc': 'mlp_lr_0.001_drop_0.2_hs_2048-1024-512-256_seed_5003'}\n",
      "{'lr': 0.0005, 'dropout': 0.2, 'hidden_sizes': [1024, 512, 256, 128], 'seed': 101, 'n_epochs': 13, 'name': 'modelo_39', 'name_desc': 'mlp_lr_0.0005_drop_0.2_hs_1024-512-256-128_seed_101'}\n",
      "{'lr': 0.0002, 'dropout': 0.3, 'hidden_sizes': [4096, 2048, 1024, 512, 256], 'seed': 7777, 'n_epochs': 10, 'name': 'modelo_40', 'name_desc': 'mlp_lr_0.0002_drop_0.3_hs_4096-2048-1024-512-256_seed_7777'}\n",
      "{'lr': 0.0005, 'dropout': 0.2, 'hidden_sizes': [1024, 256, 1024], 'seed': 9871, 'n_epochs': 8, 'name': 'modelo_41', 'name_desc': 'mlp_lr_0.0005_drop_0.2_hs_1024-256-1024_seed_9871'}\n",
      "{'lr': 0.0005, 'dropout': 0.3, 'hidden_sizes': [1024, 256, 1024], 'seed': 2029, 'n_epochs': 15, 'name': 'modelo_42', 'name_desc': 'mlp_lr_0.0005_drop_0.3_hs_1024-256-1024_seed_2029'}\n",
      "{'lr': 0.001, 'dropout': 0.2, 'hidden_sizes': [512, 512, 512], 'seed': 1009, 'n_epochs': 8, 'name': 'modelo_43', 'name_desc': 'mlp_lr_0.001_drop_0.2_hs_512-512-512_seed_1009'}\n",
      "{'lr': 0.001, 'dropout': 0.3, 'hidden_sizes': [1024, 512, 256, 128], 'seed': 307, 'n_epochs': 10, 'name': 'modelo_44', 'name_desc': 'mlp_lr_0.001_drop_0.3_hs_1024-512-256-128_seed_307'}\n",
      "{'lr': 0.0002, 'dropout': 0.1, 'hidden_sizes': [1024, 256, 1024], 'seed': 101, 'n_epochs': 10, 'name': 'modelo_45', 'name_desc': 'mlp_lr_0.0002_drop_0.1_hs_1024-256-1024_seed_101'}\n",
      "{'lr': 0.0005, 'dropout': 0.3, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 5003, 'n_epochs': 11, 'name': 'modelo_46', 'name_desc': 'mlp_lr_0.0005_drop_0.3_hs_2048-1024-512-256_seed_5003'}\n",
      "{'lr': 0.001, 'dropout': 0.5, 'hidden_sizes': [2048, 1024, 512, 256], 'seed': 7777, 'n_epochs': 13, 'name': 'modelo_47', 'name_desc': 'mlp_lr_0.001_drop_0.5_hs_2048-1024-512-256_seed_7777'}\n",
      "{'lr': 0.0005, 'dropout': 0.3, 'hidden_sizes': [512, 256], 'seed': 5003, 'n_epochs': 12, 'name': 'modelo_48', 'name_desc': 'mlp_lr_0.0005_drop_0.3_hs_512-256_seed_5003'}\n",
      "{'lr': 0.001, 'dropout': 0.2, 'hidden_sizes': [512, 256, 128], 'seed': 9871, 'n_epochs': 10, 'name': 'modelo_49', 'name_desc': 'mlp_lr_0.001_drop_0.2_hs_512-256-128_seed_9871'}\n",
      "{'lr': 0.001, 'dropout': 0.3, 'hidden_sizes': [256, 256], 'seed': 2029, 'n_epochs': 12, 'name': 'modelo_50', 'name_desc': 'mlp_lr_0.001_drop_0.3_hs_256-256_seed_2029'}\n",
      "\n",
      "Resumen por (lr, dropout):\n",
      "dropout  0.1  0.2  0.3  0.4  0.5\n",
      "lr                              \n",
      "0.0001     1    0    0    0    0\n",
      "0.0002     5    2    1    0    0\n",
      "0.0005     7    4    3    1    0\n",
      "0.0010     6    8    6    4    2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "\n",
    "# --- CONFIGURACIÃ“N ---\n",
    "csv_path = \"gridsearch_results.csv\"\n",
    "ensemble_size = 50\n",
    "top_n = 100  # Top-N por mÃ©trica combinada a considerar para maximizar diversidad\n",
    "\n",
    "# --- CARGA Y PREPROCESAMIENTO ---\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# NormalizaciÃ³n z-score de mÃ©tricas\n",
    "df['mae_z'] = (df['mae'] - df['mae'].mean()) / df['mae'].std()\n",
    "df['rmse_z'] = (df['rmse'] - df['rmse'].mean()) / df['rmse'].std()\n",
    "df['score'] = df['mae_z'] + df['rmse_z']\n",
    "\n",
    "# SelecciÃ³n top-N mejores por score combinado\n",
    "df_top = df.sort_values('score').head(top_n).copy()\n",
    "\n",
    "# Convertir string a lista en 'hidden_sizes' si corresponde\n",
    "def safe_eval(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return val  # Si falla, lo deja como estÃ¡\n",
    "    return val\n",
    "\n",
    "df_top['hidden_sizes'] = df_top['hidden_sizes'].apply(safe_eval)\n",
    "# Crear columna auxiliar como string para deduplicar\n",
    "df_top['hidden_sizes_str'] = df_top['hidden_sizes'].apply(str)\n",
    "\n",
    "# Elegir modelos diversos: por combinaciÃ³n Ãºnica de (lr, dropout, hidden_sizes_str)\n",
    "diverse = df_top.drop_duplicates(subset=['lr', 'dropout', 'hidden_sizes_str'])\n",
    "\n",
    "# Si hay menos de ensemble_size, completar con otros buenos modelos (distinta seed)\n",
    "if len(diverse) < ensemble_size:\n",
    "    faltan = ensemble_size - len(diverse)\n",
    "    restantes = df_top[~df_top.index.isin(diverse.index)].sort_values('score')\n",
    "    # Elegir por combinaciÃ³n diferente de seed\n",
    "    ya = set(tuple(x) for x in diverse[['lr', 'dropout', 'hidden_sizes_str']].values)\n",
    "    extras = []\n",
    "    for _, row in restantes.iterrows():\n",
    "        key = (row['lr'], row['dropout'], row['hidden_sizes_str'])\n",
    "        if key not in ya:\n",
    "            extras.append(row)\n",
    "            ya.add(key)\n",
    "        if len(extras) >= faltan:\n",
    "            break\n",
    "    diverse = pd.concat([diverse, pd.DataFrame(extras)], ignore_index=True)\n",
    "else:\n",
    "    diverse = diverse.head(ensemble_size)\n",
    "\n",
    "# --- CREACIÃ“N DE CONFIGS ---\n",
    "configs = []\n",
    "for i, row in diverse.reset_index(drop=True).iterrows():\n",
    "    name = f\"mlp_lr_{row['lr']}_drop_{row['dropout']}_hs_{'-'.join(map(str, row['hidden_sizes']))}_seed_{int(row['seed'])}\"\n",
    "    name_short = f\"modelo_{i+1}\"\n",
    "    configs.append({\n",
    "        \"lr\": float(row['lr']),\n",
    "        \"dropout\": float(row['dropout']),\n",
    "        \"hidden_sizes\": list(row['hidden_sizes']),\n",
    "        \"seed\": int(row['seed']),\n",
    "        \"n_epochs\": int(row['best_epoch']),\n",
    "        \"name\": name_short,\n",
    "        \"name_desc\": name\n",
    "    })\n",
    "\n",
    "# --- IMPRESIÃ“N DE RESUMEN ---\n",
    "print(f\"\\nModelos seleccionados para ensemble (total={len(configs)}):\")\n",
    "for cfg in configs:\n",
    "    print(cfg)\n",
    "\n",
    "print(\"\\nResumen por (lr, dropout):\")\n",
    "print(diverse.groupby(['lr', 'dropout']).size().unstack(fill_value=0))\n",
    "\n",
    "# (Opcional) Guardar configs a archivo para uso posterior\n",
    "with open(\"ensemble_configs.json\", \"w\") as f:\n",
    "    json.dump(configs, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc3bc99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Entrenando modelo_1 - lr=0.0005 dropout=0.1 hidden=[1024, 512, 256, 128] seed=7777 epochs=7\n",
      "Epoch 1/7 | Train Loss: 0.4411\n",
      "Epoch 2/7 | Train Loss: 0.3021\n",
      "Epoch 3/7 | Train Loss: 0.2766\n",
      "Epoch 4/7 | Train Loss: 0.2624\n",
      "Epoch 5/7 | Train Loss: 0.2543\n",
      "Epoch 6/7 | Train Loss: 0.2501\n",
      "Epoch 7/7 | Train Loss: 0.2431\n",
      "ðŸ’¾ Modelo guardado: modelo_1_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_2 - lr=0.0002 dropout=0.1 hidden=[4096, 2048, 1024, 512, 256] seed=5003 epochs=7\n",
      "Epoch 1/7 | Train Loss: 0.4394\n",
      "Epoch 2/7 | Train Loss: 0.3036\n",
      "Epoch 3/7 | Train Loss: 0.2873\n",
      "Epoch 4/7 | Train Loss: 0.2744\n",
      "Epoch 5/7 | Train Loss: 0.2608\n",
      "Epoch 6/7 | Train Loss: 0.2496\n",
      "Epoch 7/7 | Train Loss: 0.2455\n",
      "ðŸ’¾ Modelo guardado: modelo_2_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_3 - lr=0.0002 dropout=0.1 hidden=[2048, 1024, 512, 256] seed=307 epochs=7\n",
      "Epoch 1/7 | Train Loss: 0.4778\n",
      "Epoch 2/7 | Train Loss: 0.3125\n",
      "Epoch 3/7 | Train Loss: 0.2883\n",
      "Epoch 4/7 | Train Loss: 0.2755\n",
      "Epoch 5/7 | Train Loss: 0.2611\n",
      "Epoch 6/7 | Train Loss: 0.2593\n",
      "Epoch 7/7 | Train Loss: 0.2531\n",
      "ðŸ’¾ Modelo guardado: modelo_3_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_4 - lr=0.0001 dropout=0.1 hidden=[4096, 2048, 1024, 512, 256] seed=2029 epochs=7\n",
      "Epoch 1/7 | Train Loss: 0.4913\n",
      "Epoch 2/7 | Train Loss: 0.3198\n",
      "Epoch 3/7 | Train Loss: 0.3048\n",
      "Epoch 4/7 | Train Loss: 0.2888\n",
      "Epoch 5/7 | Train Loss: 0.2788\n",
      "Epoch 6/7 | Train Loss: 0.2698\n",
      "Epoch 7/7 | Train Loss: 0.2636\n",
      "ðŸ’¾ Modelo guardado: modelo_4_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_5 - lr=0.001 dropout=0.3 hidden=[512, 256, 128] seed=2029 epochs=11\n",
      "Epoch 1/11 | Train Loss: 0.5907\n",
      "Epoch 2/11 | Train Loss: 0.3816\n",
      "Epoch 3/11 | Train Loss: 0.3436\n",
      "Epoch 4/11 | Train Loss: 0.3078\n",
      "Epoch 5/11 | Train Loss: 0.2981\n",
      "Epoch 6/11 | Train Loss: 0.2844\n",
      "Epoch 7/11 | Train Loss: 0.2789\n",
      "Epoch 8/11 | Train Loss: 0.2675\n",
      "Epoch 9/11 | Train Loss: 0.2621\n",
      "Epoch 10/11 | Train Loss: 0.2590\n",
      "Epoch 11/11 | Train Loss: 0.2550\n",
      "ðŸ’¾ Modelo guardado: modelo_5_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_6 - lr=0.001 dropout=0.2 hidden=[256, 256] seed=2029 epochs=12\n",
      "Epoch 1/12 | Train Loss: 0.5513\n",
      "Epoch 2/12 | Train Loss: 0.3624\n",
      "Epoch 3/12 | Train Loss: 0.3228\n",
      "Epoch 4/12 | Train Loss: 0.2958\n",
      "Epoch 5/12 | Train Loss: 0.2868\n",
      "Epoch 6/12 | Train Loss: 0.2728\n",
      "Epoch 7/12 | Train Loss: 0.2644\n",
      "Epoch 8/12 | Train Loss: 0.2565\n",
      "Epoch 9/12 | Train Loss: 0.2501\n",
      "Epoch 10/12 | Train Loss: 0.2513\n",
      "Epoch 11/12 | Train Loss: 0.2449\n",
      "Epoch 12/12 | Train Loss: 0.2361\n",
      "ðŸ’¾ Modelo guardado: modelo_6_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_7 - lr=0.0005 dropout=0.1 hidden=[2048, 1024, 512, 256] seed=1009 epochs=5\n",
      "Epoch 1/5 | Train Loss: 0.4440\n",
      "Epoch 2/5 | Train Loss: 0.2941\n",
      "Epoch 3/5 | Train Loss: 0.2637\n",
      "Epoch 4/5 | Train Loss: 0.2613\n",
      "Epoch 5/5 | Train Loss: 0.2606\n",
      "ðŸ’¾ Modelo guardado: modelo_7_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_8 - lr=0.0005 dropout=0.1 hidden=[4096, 2048, 1024, 512, 256] seed=5003 epochs=9\n",
      "Epoch 1/9 | Train Loss: 0.4471\n",
      "Epoch 2/9 | Train Loss: 0.2972\n",
      "Epoch 3/9 | Train Loss: 0.2819\n",
      "Epoch 4/9 | Train Loss: 0.2709\n",
      "Epoch 5/9 | Train Loss: 0.2538\n",
      "Epoch 6/9 | Train Loss: 0.2475\n",
      "Epoch 7/9 | Train Loss: 0.2403\n",
      "Epoch 8/9 | Train Loss: 0.2360\n",
      "Epoch 9/9 | Train Loss: 0.2377\n",
      "ðŸ’¾ Modelo guardado: modelo_8_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_9 - lr=0.0005 dropout=0.2 hidden=[2048, 1024, 512, 256] seed=7777 epochs=7\n",
      "Epoch 1/7 | Train Loss: 0.5174\n",
      "Epoch 2/7 | Train Loss: 0.3327\n",
      "Epoch 3/7 | Train Loss: 0.3069\n",
      "Epoch 4/7 | Train Loss: 0.2895\n",
      "Epoch 5/7 | Train Loss: 0.2774\n",
      "Epoch 6/7 | Train Loss: 0.2664\n",
      "Epoch 7/7 | Train Loss: 0.2565\n",
      "ðŸ’¾ Modelo guardado: modelo_9_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_10 - lr=0.0005 dropout=0.1 hidden=[1024, 256, 1024] seed=1009 epochs=14\n",
      "Epoch 1/14 | Train Loss: 0.4841\n",
      "Epoch 2/14 | Train Loss: 0.3405\n",
      "Epoch 3/14 | Train Loss: 0.2967\n",
      "Epoch 4/14 | Train Loss: 0.2857\n",
      "Epoch 5/14 | Train Loss: 0.2807\n",
      "Epoch 6/14 | Train Loss: 0.2648\n",
      "Epoch 7/14 | Train Loss: 0.2532\n",
      "Epoch 8/14 | Train Loss: 0.2502\n",
      "Epoch 9/14 | Train Loss: 0.2433\n",
      "Epoch 10/14 | Train Loss: 0.2386\n",
      "Epoch 11/14 | Train Loss: 0.2338\n",
      "Epoch 12/14 | Train Loss: 0.2296\n",
      "Epoch 13/14 | Train Loss: 0.2256\n",
      "Epoch 14/14 | Train Loss: 0.2283\n",
      "ðŸ’¾ Modelo guardado: modelo_10_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_11 - lr=0.001 dropout=0.3 hidden=[1024, 512, 256] seed=101 epochs=14\n",
      "Epoch 1/14 | Train Loss: 0.5320\n",
      "Epoch 2/14 | Train Loss: 0.3418\n",
      "Epoch 3/14 | Train Loss: 0.3109\n",
      "Epoch 4/14 | Train Loss: 0.2905\n",
      "Epoch 5/14 | Train Loss: 0.2713\n",
      "Epoch 6/14 | Train Loss: 0.2652\n",
      "Epoch 7/14 | Train Loss: 0.2596\n",
      "Epoch 8/14 | Train Loss: 0.2541\n",
      "Epoch 9/14 | Train Loss: 0.2523\n",
      "Epoch 10/14 | Train Loss: 0.2457\n",
      "Epoch 11/14 | Train Loss: 0.2424\n",
      "Epoch 12/14 | Train Loss: 0.2384\n",
      "Epoch 13/14 | Train Loss: 0.2387\n",
      "Epoch 14/14 | Train Loss: 0.2387\n",
      "ðŸ’¾ Modelo guardado: modelo_11_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_12 - lr=0.001 dropout=0.4 hidden=[1024, 512, 256] seed=307 epochs=16\n",
      "Epoch 1/16 | Train Loss: 0.5995\n",
      "Epoch 2/16 | Train Loss: 0.3952\n",
      "Epoch 3/16 | Train Loss: 0.3463\n",
      "Epoch 4/16 | Train Loss: 0.3199\n",
      "Epoch 5/16 | Train Loss: 0.3014\n",
      "Epoch 6/16 | Train Loss: 0.2862\n",
      "Epoch 7/16 | Train Loss: 0.2797\n",
      "Epoch 8/16 | Train Loss: 0.2712\n",
      "Epoch 9/16 | Train Loss: 0.2674\n",
      "Epoch 10/16 | Train Loss: 0.2595\n",
      "Epoch 11/16 | Train Loss: 0.2592\n",
      "Epoch 12/16 | Train Loss: 0.2548\n",
      "Epoch 13/16 | Train Loss: 0.2519\n",
      "Epoch 14/16 | Train Loss: 0.2513\n",
      "Epoch 15/16 | Train Loss: 0.2469\n",
      "Epoch 16/16 | Train Loss: 0.2456\n",
      "ðŸ’¾ Modelo guardado: modelo_12_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_13 - lr=0.0002 dropout=0.1 hidden=[1024, 512, 256, 128] seed=2029 epochs=9\n",
      "Epoch 1/9 | Train Loss: 0.5342\n",
      "Epoch 2/9 | Train Loss: 0.3312\n",
      "Epoch 3/9 | Train Loss: 0.3019\n",
      "Epoch 4/9 | Train Loss: 0.2901\n",
      "Epoch 5/9 | Train Loss: 0.2763\n",
      "Epoch 6/9 | Train Loss: 0.2682\n",
      "Epoch 7/9 | Train Loss: 0.2602\n",
      "Epoch 8/9 | Train Loss: 0.2549\n",
      "Epoch 9/9 | Train Loss: 0.2523\n",
      "ðŸ’¾ Modelo guardado: modelo_13_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_14 - lr=0.0005 dropout=0.4 hidden=[4096, 2048, 1024, 512, 256] seed=9871 epochs=13\n",
      "Epoch 1/13 | Train Loss: 0.5969\n",
      "Epoch 2/13 | Train Loss: 0.4141\n",
      "Epoch 3/13 | Train Loss: 0.3716\n",
      "Epoch 4/13 | Train Loss: 0.3264\n",
      "Epoch 5/13 | Train Loss: 0.3108\n",
      "Epoch 6/13 | Train Loss: 0.3017\n",
      "Epoch 7/13 | Train Loss: 0.2876\n",
      "Epoch 8/13 | Train Loss: 0.2847\n",
      "Epoch 9/13 | Train Loss: 0.2719\n",
      "Epoch 10/13 | Train Loss: 0.2679\n",
      "Epoch 11/13 | Train Loss: 0.2635\n",
      "Epoch 12/13 | Train Loss: 0.2588\n",
      "Epoch 13/13 | Train Loss: 0.2534\n",
      "ðŸ’¾ Modelo guardado: modelo_14_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_15 - lr=0.001 dropout=0.1 hidden=[512, 256] seed=7777 epochs=8\n",
      "Epoch 1/8 | Train Loss: 0.4741\n",
      "Epoch 2/8 | Train Loss: 0.2883\n",
      "Epoch 3/8 | Train Loss: 0.2833\n",
      "Epoch 4/8 | Train Loss: 0.2543\n",
      "Epoch 5/8 | Train Loss: 0.2485\n",
      "Epoch 6/8 | Train Loss: 0.2372\n",
      "Epoch 7/8 | Train Loss: 0.2299\n",
      "Epoch 8/8 | Train Loss: 0.2319\n",
      "ðŸ’¾ Modelo guardado: modelo_15_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_16 - lr=0.0005 dropout=0.1 hidden=[512, 512, 256] seed=9871 epochs=10\n",
      "Epoch 1/10 | Train Loss: 0.4878\n",
      "Epoch 2/10 | Train Loss: 0.3205\n",
      "Epoch 3/10 | Train Loss: 0.2869\n",
      "Epoch 4/10 | Train Loss: 0.2732\n",
      "Epoch 5/10 | Train Loss: 0.2587\n",
      "Epoch 6/10 | Train Loss: 0.2509\n",
      "Epoch 7/10 | Train Loss: 0.2518\n",
      "Epoch 8/10 | Train Loss: 0.2453\n",
      "Epoch 9/10 | Train Loss: 0.2387\n",
      "Epoch 10/10 | Train Loss: 0.2397\n",
      "ðŸ’¾ Modelo guardado: modelo_16_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_17 - lr=0.001 dropout=0.4 hidden=[1024, 256, 1024] seed=101 epochs=15\n",
      "Epoch 1/15 | Train Loss: 0.6811\n",
      "Epoch 2/15 | Train Loss: 0.4394\n",
      "Epoch 3/15 | Train Loss: 0.3721\n",
      "Epoch 4/15 | Train Loss: 0.3406\n",
      "Epoch 5/15 | Train Loss: 0.3156\n",
      "Epoch 6/15 | Train Loss: 0.2963\n",
      "Epoch 7/15 | Train Loss: 0.2783\n",
      "Epoch 8/15 | Train Loss: 0.2808\n",
      "Epoch 9/15 | Train Loss: 0.2675\n",
      "Epoch 10/15 | Train Loss: 0.2577\n",
      "Epoch 11/15 | Train Loss: 0.2560\n",
      "Epoch 12/15 | Train Loss: 0.2548\n",
      "Epoch 13/15 | Train Loss: 0.2617\n",
      "Epoch 14/15 | Train Loss: 0.2508\n",
      "Epoch 15/15 | Train Loss: 0.2474\n",
      "ðŸ’¾ Modelo guardado: modelo_17_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_18 - lr=0.0005 dropout=0.2 hidden=[512, 256, 128] seed=7777 epochs=11\n",
      "Epoch 1/11 | Train Loss: 0.5656\n",
      "Epoch 2/11 | Train Loss: 0.3620\n",
      "Epoch 3/11 | Train Loss: 0.3204\n",
      "Epoch 4/11 | Train Loss: 0.2992\n",
      "Epoch 5/11 | Train Loss: 0.2852\n",
      "Epoch 6/11 | Train Loss: 0.2750\n",
      "Epoch 7/11 | Train Loss: 0.2698\n",
      "Epoch 8/11 | Train Loss: 0.2666\n",
      "Epoch 9/11 | Train Loss: 0.2576\n",
      "Epoch 10/11 | Train Loss: 0.2507\n",
      "Epoch 11/11 | Train Loss: 0.2497\n",
      "ðŸ’¾ Modelo guardado: modelo_18_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_19 - lr=0.001 dropout=0.2 hidden=[1024, 512, 256, 128] seed=2029 epochs=13\n",
      "Epoch 1/13 | Train Loss: 0.4847\n",
      "Epoch 2/13 | Train Loss: 0.3243\n",
      "Epoch 3/13 | Train Loss: 0.2956\n",
      "Epoch 4/13 | Train Loss: 0.2878\n",
      "Epoch 5/13 | Train Loss: 0.2654\n",
      "Epoch 6/13 | Train Loss: 0.2570\n",
      "Epoch 7/13 | Train Loss: 0.2517\n",
      "Epoch 8/13 | Train Loss: 0.2444\n",
      "Epoch 9/13 | Train Loss: 0.2422\n",
      "Epoch 10/13 | Train Loss: 0.2357\n",
      "Epoch 11/13 | Train Loss: 0.2361\n",
      "Epoch 12/13 | Train Loss: 0.2330\n",
      "Epoch 13/13 | Train Loss: 0.2293\n",
      "ðŸ’¾ Modelo guardado: modelo_19_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_20 - lr=0.001 dropout=0.3 hidden=[512, 512, 256] seed=307 epochs=13\n",
      "Epoch 1/13 | Train Loss: 0.5825\n",
      "Epoch 2/13 | Train Loss: 0.3662\n",
      "Epoch 3/13 | Train Loss: 0.3188\n",
      "Epoch 4/13 | Train Loss: 0.3082\n",
      "Epoch 5/13 | Train Loss: 0.2820\n",
      "Epoch 6/13 | Train Loss: 0.2772\n",
      "Epoch 7/13 | Train Loss: 0.2651\n",
      "Epoch 8/13 | Train Loss: 0.2612\n",
      "Epoch 9/13 | Train Loss: 0.2600\n",
      "Epoch 10/13 | Train Loss: 0.2530\n",
      "Epoch 11/13 | Train Loss: 0.2488\n",
      "Epoch 12/13 | Train Loss: 0.2481\n",
      "Epoch 13/13 | Train Loss: 0.2428\n",
      "ðŸ’¾ Modelo guardado: modelo_20_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_21 - lr=0.0002 dropout=0.1 hidden=[1024, 512, 256] seed=101 epochs=10\n",
      "Epoch 1/10 | Train Loss: 0.5060\n",
      "Epoch 2/10 | Train Loss: 0.3233\n",
      "Epoch 3/10 | Train Loss: 0.2967\n",
      "Epoch 4/10 | Train Loss: 0.2840\n",
      "Epoch 5/10 | Train Loss: 0.2705\n",
      "Epoch 6/10 | Train Loss: 0.2691\n",
      "Epoch 7/10 | Train Loss: 0.2577\n",
      "Epoch 8/10 | Train Loss: 0.2553\n",
      "Epoch 9/10 | Train Loss: 0.2566\n",
      "Epoch 10/10 | Train Loss: 0.2434\n",
      "ðŸ’¾ Modelo guardado: modelo_21_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_22 - lr=0.001 dropout=0.1 hidden=[512, 256, 128] seed=307 epochs=12\n",
      "Epoch 1/12 | Train Loss: 0.4570\n",
      "Epoch 2/12 | Train Loss: 0.2911\n",
      "Epoch 3/12 | Train Loss: 0.2708\n",
      "Epoch 4/12 | Train Loss: 0.2588\n",
      "Epoch 5/12 | Train Loss: 0.2459\n",
      "Epoch 6/12 | Train Loss: 0.2400\n",
      "Epoch 7/12 | Train Loss: 0.2337\n",
      "Epoch 8/12 | Train Loss: 0.2319\n",
      "Epoch 9/12 | Train Loss: 0.2266\n",
      "Epoch 10/12 | Train Loss: 0.2295\n",
      "Epoch 11/12 | Train Loss: 0.2225\n",
      "Epoch 12/12 | Train Loss: 0.2207\n",
      "ðŸ’¾ Modelo guardado: modelo_22_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_23 - lr=0.001 dropout=0.1 hidden=[256, 256] seed=307 epochs=9\n",
      "Epoch 1/9 | Train Loss: 0.4673\n",
      "Epoch 2/9 | Train Loss: 0.3106\n",
      "Epoch 3/9 | Train Loss: 0.2808\n",
      "Epoch 4/9 | Train Loss: 0.2629\n",
      "Epoch 5/9 | Train Loss: 0.2544\n",
      "Epoch 6/9 | Train Loss: 0.2456\n",
      "Epoch 7/9 | Train Loss: 0.2437\n",
      "Epoch 8/9 | Train Loss: 0.2361\n",
      "Epoch 9/9 | Train Loss: 0.2355\n",
      "ðŸ’¾ Modelo guardado: modelo_23_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_24 - lr=0.001 dropout=0.1 hidden=[1024, 256, 1024] seed=7777 epochs=9\n",
      "Epoch 1/9 | Train Loss: 0.5474\n",
      "Epoch 2/9 | Train Loss: 0.3140\n",
      "Epoch 3/9 | Train Loss: 0.2862\n",
      "Epoch 4/9 | Train Loss: 0.2699\n",
      "Epoch 5/9 | Train Loss: 0.2551\n",
      "Epoch 6/9 | Train Loss: 0.2532\n",
      "Epoch 7/9 | Train Loss: 0.2483\n",
      "Epoch 8/9 | Train Loss: 0.2407\n",
      "Epoch 9/9 | Train Loss: 0.2313\n",
      "ðŸ’¾ Modelo guardado: modelo_24_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_25 - lr=0.001 dropout=0.5 hidden=[1024, 512, 256] seed=307 epochs=16\n",
      "Epoch 1/16 | Train Loss: 0.6583\n",
      "Epoch 2/16 | Train Loss: 0.4346\n",
      "Epoch 3/16 | Train Loss: 0.3750\n",
      "Epoch 4/16 | Train Loss: 0.3359\n",
      "Epoch 5/16 | Train Loss: 0.3188\n",
      "Epoch 6/16 | Train Loss: 0.3039\n",
      "Epoch 7/16 | Train Loss: 0.2959\n",
      "Epoch 8/16 | Train Loss: 0.2858\n",
      "Epoch 9/16 | Train Loss: 0.2805\n",
      "Epoch 10/16 | Train Loss: 0.2753\n",
      "Epoch 11/16 | Train Loss: 0.2709\n",
      "Epoch 12/16 | Train Loss: 0.2709\n",
      "Epoch 13/16 | Train Loss: 0.2676\n",
      "Epoch 14/16 | Train Loss: 0.2639\n",
      "Epoch 15/16 | Train Loss: 0.2615\n",
      "Epoch 16/16 | Train Loss: 0.2626\n",
      "ðŸ’¾ Modelo guardado: modelo_25_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_26 - lr=0.001 dropout=0.3 hidden=[2048, 1024, 512, 256] seed=307 epochs=11\n",
      "Epoch 1/11 | Train Loss: 0.5715\n",
      "Epoch 2/11 | Train Loss: 0.3598\n",
      "Epoch 3/11 | Train Loss: 0.3253\n",
      "Epoch 4/11 | Train Loss: 0.3016\n",
      "Epoch 5/11 | Train Loss: 0.2854\n",
      "Epoch 6/11 | Train Loss: 0.2765\n",
      "Epoch 7/11 | Train Loss: 0.2659\n",
      "Epoch 8/11 | Train Loss: 0.2551\n",
      "Epoch 9/11 | Train Loss: 0.2480\n",
      "Epoch 10/11 | Train Loss: 0.2429\n",
      "Epoch 11/11 | Train Loss: 0.2381\n",
      "ðŸ’¾ Modelo guardado: modelo_26_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_27 - lr=0.001 dropout=0.4 hidden=[512, 512, 512] seed=2029 epochs=20\n",
      "Epoch 1/20 | Train Loss: 0.6730\n",
      "Epoch 2/20 | Train Loss: 0.4271\n",
      "Epoch 3/20 | Train Loss: 0.3651\n",
      "Epoch 4/20 | Train Loss: 0.3286\n",
      "Epoch 5/20 | Train Loss: 0.3071\n",
      "Epoch 6/20 | Train Loss: 0.2882\n",
      "Epoch 7/20 | Train Loss: 0.2815\n",
      "Epoch 8/20 | Train Loss: 0.2787\n",
      "Epoch 9/20 | Train Loss: 0.2676\n",
      "Epoch 10/20 | Train Loss: 0.2672\n",
      "Epoch 11/20 | Train Loss: 0.2611\n",
      "Epoch 12/20 | Train Loss: 0.2655\n",
      "Epoch 13/20 | Train Loss: 0.2568\n",
      "Epoch 14/20 | Train Loss: 0.2550\n",
      "Epoch 15/20 | Train Loss: 0.2538\n",
      "Epoch 16/20 | Train Loss: 0.2458\n",
      "Epoch 17/20 | Train Loss: 0.2456\n",
      "Epoch 18/20 | Train Loss: 0.2446\n",
      "Epoch 19/20 | Train Loss: 0.2420\n",
      "Epoch 20/20 | Train Loss: 0.2423\n",
      "ðŸ’¾ Modelo guardado: modelo_27_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_28 - lr=0.001 dropout=0.2 hidden=[1024, 256, 1024] seed=101 epochs=8\n",
      "Epoch 1/8 | Train Loss: 0.5961\n",
      "Epoch 2/8 | Train Loss: 0.3504\n",
      "Epoch 3/8 | Train Loss: 0.3143\n",
      "Epoch 4/8 | Train Loss: 0.2957\n",
      "Epoch 5/8 | Train Loss: 0.2890\n",
      "Epoch 6/8 | Train Loss: 0.2744\n",
      "Epoch 7/8 | Train Loss: 0.2593\n",
      "Epoch 8/8 | Train Loss: 0.2638\n",
      "ðŸ’¾ Modelo guardado: modelo_28_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_29 - lr=0.001 dropout=0.4 hidden=[2048, 1024, 512, 256] seed=101 epochs=10\n",
      "Epoch 1/10 | Train Loss: 0.5923\n",
      "Epoch 2/10 | Train Loss: 0.3853\n",
      "Epoch 3/10 | Train Loss: 0.3345\n",
      "Epoch 4/10 | Train Loss: 0.3076\n",
      "Epoch 5/10 | Train Loss: 0.2930\n",
      "Epoch 6/10 | Train Loss: 0.2737\n",
      "Epoch 7/10 | Train Loss: 0.2692\n",
      "Epoch 8/10 | Train Loss: 0.2621\n",
      "Epoch 9/10 | Train Loss: 0.2622\n",
      "Epoch 10/10 | Train Loss: 0.2565\n",
      "ðŸ’¾ Modelo guardado: modelo_29_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_30 - lr=0.001 dropout=0.2 hidden=[512, 256] seed=7777 epochs=8\n",
      "Epoch 1/8 | Train Loss: 0.5019\n",
      "Epoch 2/8 | Train Loss: 0.3197\n",
      "Epoch 3/8 | Train Loss: 0.3138\n",
      "Epoch 4/8 | Train Loss: 0.2768\n",
      "Epoch 5/8 | Train Loss: 0.2663\n",
      "Epoch 6/8 | Train Loss: 0.2555\n",
      "Epoch 7/8 | Train Loss: 0.2480\n",
      "Epoch 8/8 | Train Loss: 0.2465\n",
      "ðŸ’¾ Modelo guardado: modelo_30_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_31 - lr=0.0002 dropout=0.2 hidden=[4096, 2048, 1024, 512, 256] seed=1009 epochs=6\n",
      "Epoch 1/6 | Train Loss: 0.5010\n",
      "Epoch 2/6 | Train Loss: 0.3608\n",
      "Epoch 3/6 | Train Loss: 0.3303\n",
      "Epoch 4/6 | Train Loss: 0.3076\n",
      "Epoch 5/6 | Train Loss: 0.2994\n",
      "Epoch 6/6 | Train Loss: 0.2894\n",
      "ðŸ’¾ Modelo guardado: modelo_31_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_32 - lr=0.001 dropout=0.1 hidden=[256, 128] seed=1009 epochs=9\n",
      "Epoch 1/9 | Train Loss: 0.4718\n",
      "Epoch 2/9 | Train Loss: 0.3111\n",
      "Epoch 3/9 | Train Loss: 0.2856\n",
      "Epoch 4/9 | Train Loss: 0.2668\n",
      "Epoch 5/9 | Train Loss: 0.2556\n",
      "Epoch 6/9 | Train Loss: 0.2493\n",
      "Epoch 7/9 | Train Loss: 0.2451\n",
      "Epoch 8/9 | Train Loss: 0.2404\n",
      "Epoch 9/9 | Train Loss: 0.2356\n",
      "ðŸ’¾ Modelo guardado: modelo_32_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_33 - lr=0.0005 dropout=0.1 hidden=[1024, 512, 256] seed=1009 epochs=6\n",
      "Epoch 1/6 | Train Loss: 0.4937\n",
      "Epoch 2/6 | Train Loss: 0.3023\n",
      "Epoch 3/6 | Train Loss: 0.2841\n",
      "Epoch 4/6 | Train Loss: 0.2649\n",
      "Epoch 5/6 | Train Loss: 0.2565\n",
      "Epoch 6/6 | Train Loss: 0.2501\n",
      "ðŸ’¾ Modelo guardado: modelo_33_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_34 - lr=0.0005 dropout=0.1 hidden=[512, 512, 512] seed=5003 epochs=7\n",
      "Epoch 1/7 | Train Loss: 0.5005\n",
      "Epoch 2/7 | Train Loss: 0.3252\n",
      "Epoch 3/7 | Train Loss: 0.2997\n",
      "Epoch 4/7 | Train Loss: 0.2867\n",
      "Epoch 5/7 | Train Loss: 0.2700\n",
      "Epoch 6/7 | Train Loss: 0.2551\n",
      "Epoch 7/7 | Train Loss: 0.2495\n",
      "ðŸ’¾ Modelo guardado: modelo_34_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_35 - lr=0.0002 dropout=0.2 hidden=[2048, 1024, 512, 256] seed=1009 epochs=8\n",
      "Epoch 1/8 | Train Loss: 0.5069\n",
      "Epoch 2/8 | Train Loss: 0.3568\n",
      "Epoch 3/8 | Train Loss: 0.3191\n",
      "Epoch 4/8 | Train Loss: 0.3107\n",
      "Epoch 5/8 | Train Loss: 0.2952\n",
      "Epoch 6/8 | Train Loss: 0.2792\n",
      "Epoch 7/8 | Train Loss: 0.2769\n",
      "Epoch 8/8 | Train Loss: 0.2659\n",
      "ðŸ’¾ Modelo guardado: modelo_35_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_36 - lr=0.001 dropout=0.2 hidden=[512, 512, 256] seed=101 epochs=9\n",
      "Epoch 1/9 | Train Loss: 0.5191\n",
      "Epoch 2/9 | Train Loss: 0.3309\n",
      "Epoch 3/9 | Train Loss: 0.2994\n",
      "Epoch 4/9 | Train Loss: 0.2817\n",
      "Epoch 5/9 | Train Loss: 0.2681\n",
      "Epoch 6/9 | Train Loss: 0.2543\n",
      "Epoch 7/9 | Train Loss: 0.2477\n",
      "Epoch 8/9 | Train Loss: 0.2444\n",
      "Epoch 9/9 | Train Loss: 0.2427\n",
      "ðŸ’¾ Modelo guardado: modelo_36_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_37 - lr=0.001 dropout=0.1 hidden=[512, 512, 512] seed=101 epochs=9\n",
      "Epoch 1/9 | Train Loss: 0.5264\n",
      "Epoch 2/9 | Train Loss: 0.3136\n",
      "Epoch 3/9 | Train Loss: 0.2787\n",
      "Epoch 4/9 | Train Loss: 0.2708\n",
      "Epoch 5/9 | Train Loss: 0.2656\n",
      "Epoch 6/9 | Train Loss: 0.2490\n",
      "Epoch 7/9 | Train Loss: 0.2430\n",
      "Epoch 8/9 | Train Loss: 0.2385\n",
      "Epoch 9/9 | Train Loss: 0.2279\n",
      "ðŸ’¾ Modelo guardado: modelo_37_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_38 - lr=0.001 dropout=0.2 hidden=[2048, 1024, 512, 256] seed=5003 epochs=10\n",
      "Epoch 1/10 | Train Loss: 0.5199\n",
      "Epoch 2/10 | Train Loss: 0.3190\n",
      "Epoch 3/10 | Train Loss: 0.3033\n",
      "Epoch 4/10 | Train Loss: 0.2905\n",
      "Epoch 5/10 | Train Loss: 0.2687\n",
      "Epoch 6/10 | Train Loss: 0.2721\n",
      "Epoch 7/10 | Train Loss: 0.2591\n",
      "Epoch 8/10 | Train Loss: 0.2477\n",
      "Epoch 9/10 | Train Loss: 0.2468\n",
      "Epoch 10/10 | Train Loss: 0.2331\n",
      "ðŸ’¾ Modelo guardado: modelo_38_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_39 - lr=0.0005 dropout=0.2 hidden=[1024, 512, 256, 128] seed=101 epochs=13\n",
      "Epoch 1/13 | Train Loss: 0.5470\n",
      "Epoch 2/13 | Train Loss: 0.3589\n",
      "Epoch 3/13 | Train Loss: 0.3271\n",
      "Epoch 4/13 | Train Loss: 0.3096\n",
      "Epoch 5/13 | Train Loss: 0.2945\n",
      "Epoch 6/13 | Train Loss: 0.2847\n",
      "Epoch 7/13 | Train Loss: 0.2732\n",
      "Epoch 8/13 | Train Loss: 0.2629\n",
      "Epoch 9/13 | Train Loss: 0.2603\n",
      "Epoch 10/13 | Train Loss: 0.2543\n",
      "Epoch 11/13 | Train Loss: 0.2487\n",
      "Epoch 12/13 | Train Loss: 0.2440\n",
      "Epoch 13/13 | Train Loss: 0.2433\n",
      "ðŸ’¾ Modelo guardado: modelo_39_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_40 - lr=0.0002 dropout=0.3 hidden=[4096, 2048, 1024, 512, 256] seed=7777 epochs=10\n",
      "Epoch 1/10 | Train Loss: 0.5769\n",
      "Epoch 2/10 | Train Loss: 0.4274\n",
      "Epoch 3/10 | Train Loss: 0.3852\n",
      "Epoch 4/10 | Train Loss: 0.3650\n",
      "Epoch 5/10 | Train Loss: 0.3335\n",
      "Epoch 6/10 | Train Loss: 0.3340\n",
      "Epoch 7/10 | Train Loss: 0.3161\n",
      "Epoch 8/10 | Train Loss: 0.3061\n",
      "Epoch 9/10 | Train Loss: 0.2993\n",
      "Epoch 10/10 | Train Loss: 0.2913\n",
      "ðŸ’¾ Modelo guardado: modelo_40_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_41 - lr=0.0005 dropout=0.2 hidden=[1024, 256, 1024] seed=9871 epochs=8\n",
      "Epoch 1/8 | Train Loss: 0.5274\n",
      "Epoch 2/8 | Train Loss: 0.3814\n",
      "Epoch 3/8 | Train Loss: 0.3421\n",
      "Epoch 4/8 | Train Loss: 0.3269\n",
      "Epoch 5/8 | Train Loss: 0.3024\n",
      "Epoch 6/8 | Train Loss: 0.2886\n",
      "Epoch 7/8 | Train Loss: 0.2775\n",
      "Epoch 8/8 | Train Loss: 0.2779\n",
      "ðŸ’¾ Modelo guardado: modelo_41_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_42 - lr=0.0005 dropout=0.3 hidden=[1024, 256, 1024] seed=2029 epochs=15\n",
      "Epoch 1/15 | Train Loss: 0.6027\n",
      "Epoch 2/15 | Train Loss: 0.4188\n",
      "Epoch 3/15 | Train Loss: 0.3783\n",
      "Epoch 4/15 | Train Loss: 0.3571\n",
      "Epoch 5/15 | Train Loss: 0.3288\n",
      "Epoch 6/15 | Train Loss: 0.3216\n",
      "Epoch 7/15 | Train Loss: 0.3008\n",
      "Epoch 8/15 | Train Loss: 0.2870\n",
      "Epoch 9/15 | Train Loss: 0.2823\n",
      "Epoch 10/15 | Train Loss: 0.2722\n",
      "Epoch 11/15 | Train Loss: 0.2651\n",
      "Epoch 12/15 | Train Loss: 0.2591\n",
      "Epoch 13/15 | Train Loss: 0.2550\n",
      "Epoch 14/15 | Train Loss: 0.2513\n",
      "Epoch 15/15 | Train Loss: 0.2533\n",
      "ðŸ’¾ Modelo guardado: modelo_42_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_43 - lr=0.001 dropout=0.2 hidden=[512, 512, 512] seed=1009 epochs=8\n",
      "Epoch 1/8 | Train Loss: 0.6098\n",
      "Epoch 2/8 | Train Loss: 0.3564\n",
      "Epoch 3/8 | Train Loss: 0.2999\n",
      "Epoch 4/8 | Train Loss: 0.2858\n",
      "Epoch 5/8 | Train Loss: 0.2744\n",
      "Epoch 6/8 | Train Loss: 0.2639\n",
      "Epoch 7/8 | Train Loss: 0.2509\n",
      "Epoch 8/8 | Train Loss: 0.2428\n",
      "ðŸ’¾ Modelo guardado: modelo_43_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_44 - lr=0.001 dropout=0.3 hidden=[1024, 512, 256, 128] seed=307 epochs=10\n",
      "Epoch 1/10 | Train Loss: 0.5596\n",
      "Epoch 2/10 | Train Loss: 0.3596\n",
      "Epoch 3/10 | Train Loss: 0.3209\n",
      "Epoch 4/10 | Train Loss: 0.2998\n",
      "Epoch 5/10 | Train Loss: 0.2854\n",
      "Epoch 6/10 | Train Loss: 0.2733\n",
      "Epoch 7/10 | Train Loss: 0.2708\n",
      "Epoch 8/10 | Train Loss: 0.2642\n",
      "Epoch 9/10 | Train Loss: 0.2580\n",
      "Epoch 10/10 | Train Loss: 0.2564\n",
      "ðŸ’¾ Modelo guardado: modelo_44_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_45 - lr=0.0002 dropout=0.1 hidden=[1024, 256, 1024] seed=101 epochs=10\n",
      "Epoch 1/10 | Train Loss: 0.5568\n",
      "Epoch 2/10 | Train Loss: 0.3504\n",
      "Epoch 3/10 | Train Loss: 0.3158\n",
      "Epoch 4/10 | Train Loss: 0.2933\n",
      "Epoch 5/10 | Train Loss: 0.2908\n",
      "Epoch 6/10 | Train Loss: 0.2817\n",
      "Epoch 7/10 | Train Loss: 0.2705\n",
      "Epoch 8/10 | Train Loss: 0.2769\n",
      "Epoch 9/10 | Train Loss: 0.2695\n",
      "Epoch 10/10 | Train Loss: 0.2580\n",
      "ðŸ’¾ Modelo guardado: modelo_45_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_46 - lr=0.0005 dropout=0.3 hidden=[2048, 1024, 512, 256] seed=5003 epochs=11\n",
      "Epoch 1/11 | Train Loss: 0.5426\n",
      "Epoch 2/11 | Train Loss: 0.3718\n",
      "Epoch 3/11 | Train Loss: 0.3471\n",
      "Epoch 4/11 | Train Loss: 0.3183\n",
      "Epoch 5/11 | Train Loss: 0.3024\n",
      "Epoch 6/11 | Train Loss: 0.2894\n",
      "Epoch 7/11 | Train Loss: 0.2832\n",
      "Epoch 8/11 | Train Loss: 0.2674\n",
      "Epoch 9/11 | Train Loss: 0.2677\n",
      "Epoch 10/11 | Train Loss: 0.2601\n",
      "Epoch 11/11 | Train Loss: 0.2488\n",
      "ðŸ’¾ Modelo guardado: modelo_46_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_47 - lr=0.001 dropout=0.5 hidden=[2048, 1024, 512, 256] seed=7777 epochs=13\n",
      "Epoch 1/13 | Train Loss: 0.6386\n",
      "Epoch 2/13 | Train Loss: 0.4276\n",
      "Epoch 3/13 | Train Loss: 0.3603\n",
      "Epoch 4/13 | Train Loss: 0.3283\n",
      "Epoch 5/13 | Train Loss: 0.3063\n",
      "Epoch 6/13 | Train Loss: 0.2955\n",
      "Epoch 7/13 | Train Loss: 0.2876\n",
      "Epoch 8/13 | Train Loss: 0.2782\n",
      "Epoch 9/13 | Train Loss: 0.2727\n",
      "Epoch 10/13 | Train Loss: 0.2742\n",
      "Epoch 11/13 | Train Loss: 0.2668\n",
      "Epoch 12/13 | Train Loss: 0.2651\n",
      "Epoch 13/13 | Train Loss: 0.2632\n",
      "ðŸ’¾ Modelo guardado: modelo_47_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_48 - lr=0.0005 dropout=0.3 hidden=[512, 256] seed=5003 epochs=12\n",
      "Epoch 1/12 | Train Loss: 0.6000\n",
      "Epoch 2/12 | Train Loss: 0.4053\n",
      "Epoch 3/12 | Train Loss: 0.3560\n",
      "Epoch 4/12 | Train Loss: 0.3329\n",
      "Epoch 5/12 | Train Loss: 0.3126\n",
      "Epoch 6/12 | Train Loss: 0.2990\n",
      "Epoch 7/12 | Train Loss: 0.2888\n",
      "Epoch 8/12 | Train Loss: 0.2850\n",
      "Epoch 9/12 | Train Loss: 0.2747\n",
      "Epoch 10/12 | Train Loss: 0.2673\n",
      "Epoch 11/12 | Train Loss: 0.2650\n",
      "Epoch 12/12 | Train Loss: 0.2610\n",
      "ðŸ’¾ Modelo guardado: modelo_48_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_49 - lr=0.001 dropout=0.2 hidden=[512, 256, 128] seed=9871 epochs=10\n",
      "Epoch 1/10 | Train Loss: 0.5274\n",
      "Epoch 2/10 | Train Loss: 0.3477\n",
      "Epoch 3/10 | Train Loss: 0.3106\n",
      "Epoch 4/10 | Train Loss: 0.2855\n",
      "Epoch 5/10 | Train Loss: 0.2835\n",
      "Epoch 6/10 | Train Loss: 0.2681\n",
      "Epoch 7/10 | Train Loss: 0.2588\n",
      "Epoch 8/10 | Train Loss: 0.2515\n",
      "Epoch 9/10 | Train Loss: 0.2453\n",
      "Epoch 10/10 | Train Loss: 0.2429\n",
      "ðŸ’¾ Modelo guardado: modelo_49_final.pt\n",
      "\n",
      "ðŸ”§ Entrenando modelo_50 - lr=0.001 dropout=0.3 hidden=[256, 256] seed=2029 epochs=12\n",
      "Epoch 1/12 | Train Loss: 0.5883\n",
      "Epoch 2/12 | Train Loss: 0.3957\n",
      "Epoch 3/12 | Train Loss: 0.3454\n",
      "Epoch 4/12 | Train Loss: 0.3184\n",
      "Epoch 5/12 | Train Loss: 0.3054\n",
      "Epoch 6/12 | Train Loss: 0.2891\n",
      "Epoch 7/12 | Train Loss: 0.2791\n",
      "Epoch 8/12 | Train Loss: 0.2726\n",
      "Epoch 9/12 | Train Loss: 0.2614\n",
      "Epoch 10/12 | Train Loss: 0.2656\n",
      "Epoch 11/12 | Train Loss: 0.2576\n",
      "Epoch 12/12 | Train Loss: 0.2489\n",
      "ðŸ’¾ Modelo guardado: modelo_50_final.pt\n",
      "\n",
      "âœ… Â¡Entrenamiento y guardado de los 50 mejores modelos finalizado!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Entrenamiento y guardado\n",
    "# ------------------------\n",
    "\n",
    "def set_seed(seed):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "for cfg in configs:\n",
    "    print(f\"\\nðŸ”§ Entrenando {cfg['name']} - lr={cfg['lr']} dropout={cfg['dropout']} hidden={cfg['hidden_sizes']} seed={cfg['seed']} epochs={cfg['n_epochs']}\")\n",
    "    \n",
    "    set_seed(cfg['seed'])\n",
    "\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg['hidden_sizes'],\n",
    "        dropout=cfg['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrena sobre TODO el set (sin early stopping)\n",
    "    y_true, y_pred, best_epoch = train_model(\n",
    "        model,\n",
    "        train_loader_full,\n",
    "        val_loader=None,\n",
    "        n_epochs=cfg['n_epochs'],\n",
    "        lr=cfg['lr'],\n",
    "        patience=None  # No early stopping en entrenamiento final\n",
    "        # Quitar alpha y penalty_indices/coefficients si ya no los usÃ¡s\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{cfg['name']}_final.pt\")\n",
    "    print(f\"ðŸ’¾ Modelo guardado: {cfg['name']}_final.pt\")\n",
    "\n",
    "print(\"\\nâœ… Â¡Entrenamiento y guardado de los 50 mejores modelos finalizado!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18f46b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo MLP 1/50...\n",
      "Prediciendo MLP 2/50...\n",
      "Prediciendo MLP 3/50...\n",
      "Prediciendo MLP 4/50...\n",
      "Prediciendo MLP 5/50...\n",
      "Prediciendo MLP 6/50...\n",
      "Prediciendo MLP 7/50...\n",
      "Prediciendo MLP 8/50...\n",
      "Prediciendo MLP 9/50...\n",
      "Prediciendo MLP 10/50...\n",
      "Prediciendo MLP 11/50...\n",
      "Prediciendo MLP 12/50...\n",
      "Prediciendo MLP 13/50...\n",
      "Prediciendo MLP 14/50...\n",
      "Prediciendo MLP 15/50...\n",
      "Prediciendo MLP 16/50...\n",
      "Prediciendo MLP 17/50...\n",
      "Prediciendo MLP 18/50...\n",
      "Prediciendo MLP 19/50...\n",
      "Prediciendo MLP 20/50...\n",
      "Prediciendo MLP 21/50...\n",
      "Prediciendo MLP 22/50...\n",
      "Prediciendo MLP 23/50...\n",
      "Prediciendo MLP 24/50...\n",
      "Prediciendo MLP 25/50...\n",
      "Prediciendo MLP 26/50...\n",
      "Prediciendo MLP 27/50...\n",
      "Prediciendo MLP 28/50...\n",
      "Prediciendo MLP 29/50...\n",
      "Prediciendo MLP 30/50...\n",
      "Prediciendo MLP 31/50...\n",
      "Prediciendo MLP 32/50...\n",
      "Prediciendo MLP 33/50...\n",
      "Prediciendo MLP 34/50...\n",
      "Prediciendo MLP 35/50...\n",
      "Prediciendo MLP 36/50...\n",
      "Prediciendo MLP 37/50...\n",
      "Prediciendo MLP 38/50...\n",
      "Prediciendo MLP 39/50...\n",
      "Prediciendo MLP 40/50...\n",
      "Prediciendo MLP 41/50...\n",
      "Prediciendo MLP 42/50...\n",
      "Prediciendo MLP 43/50...\n",
      "Prediciendo MLP 44/50...\n",
      "Prediciendo MLP 45/50...\n",
      "Prediciendo MLP 46/50...\n",
      "Prediciendo MLP 47/50...\n",
      "Prediciendo MLP 48/50...\n",
      "Prediciendo MLP 49/50...\n",
      "Prediciendo MLP 50/50...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- PredicciÃ³n MLP (PyTorch) (50 modelos) ---\n",
    "\n",
    "# Asegurarse de que las categÃ³ricas fueron correctamente encodeadas ANTES de esto.\n",
    "\n",
    "for col in cat_cols:\n",
    "    df_pred[col] = df_pred[col].astype(np.int64)\n",
    "for col in feature_cols:\n",
    "    df_pred[col] = df_pred[col].astype(np.float32)\n",
    "\n",
    "X_cats = torch.LongTensor(df_pred[cat_cols].values)\n",
    "X_conts = torch.FloatTensor(df_pred[feature_cols].values)\n",
    "ds_pred = TensorDataset(X_cats, X_conts)\n",
    "pred_loader = DataLoader(ds_pred, batch_size=8192, shuffle=False)\n",
    "\n",
    "def predict_model(model, loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_cats, X_conts in loader:\n",
    "            X_cats = X_cats.to(device)\n",
    "            X_conts = X_conts.to(device)\n",
    "            output = model(X_cats, X_conts)\n",
    "            preds.append(output.cpu().numpy().reshape(-1))\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "# Inicializar DataFrame solo con PRODUCT_ID\n",
    "df_preds_final = pd.DataFrame({'PRODUCT_ID': df_pred['PRODUCT_ID'].values})\n",
    "\n",
    "for i, cfg in enumerate(configs):\n",
    "    print(f\"Prediciendo MLP {i+1}/50...\")\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg['hidden_sizes'],\n",
    "        dropout=cfg['dropout']\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(f\"{cfg['name']}_final.pt\", map_location=device))\n",
    "    preds = predict_model(model, pred_loader, device)\n",
    "    # Ir agregando cada predicciÃ³n como nueva columna\n",
    "    df_preds_final[f'mlp_pred_LOG1P_Z_{i+1}'] = preds\n",
    "\n",
    "# Listo, df_preds_mlp contiene solo PRODUCT_ID y las 50 columnas de predicciÃ³n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dd58b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_final.to_csv(\"predicciones_mlp_50_modelos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91161f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir Solo las columnas PRODUCT_ID, mlp_pred_LOG1P_Z_1 a mlp_pred_LOG1P_Z_10  mostrando todas las columnas dando mas ancho \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(df_preds_final[['PRODUCT_ID'] + [f'mlp_pred_LOG1P_Z_{i+1}' for i in range(len(configs))]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0221df2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-07-14 21:00:02,343] A new study created in memory with name: no-name-19783afa-3e71-42d4-ac89-52cc307caf94\n",
      "[I 2025-07-14 21:00:04,740] Trial 27 finished with value: 0.26794116396511897 and parameters: {'learning_rate': 0.29621325470950793, 'num_leaves': 172, 'max_depth': 55, 'n_estimators': 242, 'min_child_samples': 100, 'subsample': 0.8331396622968026, 'colsample_bytree': 0.8467286616297266, 'reg_alpha': 8.661124926901671, 'reg_lambda': 2.1422454083446133e-08, 'min_split_gain': 0.6717672730288906}. Best is trial 27 with value: 0.26794116396511897.\n",
      "[I 2025-07-14 21:00:06,245] Trial 24 finished with value: 0.2654017373866263 and parameters: {'learning_rate': 0.2989534871331896, 'num_leaves': 511, 'max_depth': 51, 'n_estimators': 792, 'min_child_samples': 23, 'subsample': 0.657549907644693, 'colsample_bytree': 0.5798746608577046, 'reg_alpha': 4.00216970335425e-07, 'reg_lambda': 1.137933435731566e-06, 'min_split_gain': 0.604764989454925}. Best is trial 24 with value: 0.2654017373866263.\n",
      "[I 2025-07-14 21:00:07,302] Trial 19 finished with value: 0.26791454558343947 and parameters: {'learning_rate': 0.21323535885898068, 'num_leaves': 170, 'max_depth': 61, 'n_estimators': 188, 'min_child_samples': 54, 'subsample': 0.6843198645880264, 'colsample_bytree': 0.5198890299940573, 'reg_alpha': 0.023544911104957184, 'reg_lambda': 1.3353150625920692e-07, 'min_split_gain': 0.9241243915696127}. Best is trial 24 with value: 0.2654017373866263.\n",
      "[I 2025-07-14 21:00:07,755] Trial 21 finished with value: 0.2621759733831152 and parameters: {'learning_rate': 0.18049488181702855, 'num_leaves': 163, 'max_depth': 64, 'n_estimators': 450, 'min_child_samples': 93, 'subsample': 0.9558067708970228, 'colsample_bytree': 0.8468529241931919, 'reg_alpha': 1.0147098085558429e-07, 'reg_lambda': 3.852525879920972e-08, 'min_split_gain': 0.7034294806499812}. Best is trial 21 with value: 0.2621759733831152.\n",
      "[I 2025-07-14 21:00:09,204] Trial 8 finished with value: 0.26434482144544186 and parameters: {'learning_rate': 0.14192590805146463, 'num_leaves': 864, 'max_depth': 34, 'n_estimators': 738, 'min_child_samples': 65, 'subsample': 0.5406737455415311, 'colsample_bytree': 0.8584048166088982, 'reg_alpha': 3.32835087988332e-07, 'reg_lambda': 0.001360499761731215, 'min_split_gain': 0.6214971098224719}. Best is trial 21 with value: 0.2621759733831152.\n",
      "[I 2025-07-14 21:00:16,666] Trial 17 finished with value: 0.2617473259731162 and parameters: {'learning_rate': 0.052797653462216425, 'num_leaves': 271, 'max_depth': 10, 'n_estimators': 179, 'min_child_samples': 12, 'subsample': 0.5378170397697539, 'colsample_bytree': 0.8874979743434366, 'reg_alpha': 5.852016465331253e-08, 'reg_lambda': 3.707212998856904e-07, 'min_split_gain': 0.7939289784461057}. Best is trial 17 with value: 0.2617473259731162.\n",
      "[I 2025-07-14 21:00:20,526] Trial 32 finished with value: 0.26254975493424876 and parameters: {'learning_rate': 0.06248913259103876, 'num_leaves': 765, 'max_depth': 21, 'n_estimators': 394, 'min_child_samples': 64, 'subsample': 0.6431798956083805, 'colsample_bytree': 0.8448245620301862, 'reg_alpha': 0.0006170993400920258, 'reg_lambda': 1.7868113523960851, 'min_split_gain': 0.9903933567916176}. Best is trial 17 with value: 0.2617473259731162.\n",
      "[I 2025-07-14 21:00:21,015] Trial 2 finished with value: 0.26154058161669097 and parameters: {'learning_rate': 0.0508597543246216, 'num_leaves': 575, 'max_depth': 41, 'n_estimators': 529, 'min_child_samples': 61, 'subsample': 0.9580433878855581, 'colsample_bytree': 0.8300913107156349, 'reg_alpha': 0.0036297368420082996, 'reg_lambda': 5.986432821907726e-08, 'min_split_gain': 0.5048457111639179}. Best is trial 2 with value: 0.26154058161669097.\n",
      "[I 2025-07-14 21:00:22,345] Trial 0 finished with value: 0.72641358873137 and parameters: {'learning_rate': 0.001172676020929351, 'num_leaves': 646, 'max_depth': 4, 'n_estimators': 330, 'min_child_samples': 70, 'subsample': 0.5598005071819814, 'colsample_bytree': 0.8039847686552726, 'reg_alpha': 0.001452940940071653, 'reg_lambda': 0.0014521929008433927, 'min_split_gain': 0.781156977221324}. Best is trial 2 with value: 0.26154058161669097.\n",
      "[I 2025-07-14 21:00:25,602] Trial 20 finished with value: 0.2625937916806729 and parameters: {'learning_rate': 0.030329518829576095, 'num_leaves': 753, 'max_depth': 59, 'n_estimators': 399, 'min_child_samples': 38, 'subsample': 0.9938667469526072, 'colsample_bytree': 0.632747394540961, 'reg_alpha': 0.015113531236350036, 'reg_lambda': 3.265895209653165, 'min_split_gain': 0.6706768773047443}. Best is trial 2 with value: 0.26154058161669097.\n",
      "[I 2025-07-14 21:00:31,030] Trial 1 finished with value: 0.2628107150091066 and parameters: {'learning_rate': 0.030512856893939493, 'num_leaves': 272, 'max_depth': 52, 'n_estimators': 708, 'min_child_samples': 38, 'subsample': 0.7039893135948592, 'colsample_bytree': 0.9503235951776132, 'reg_alpha': 1.7453028814227738e-08, 'reg_lambda': 1.3963984290859634e-07, 'min_split_gain': 0.8221201937995514}. Best is trial 2 with value: 0.26154058161669097.\n",
      "[I 2025-07-14 21:00:32,020] Trial 4 finished with value: 0.26147855146798565 and parameters: {'learning_rate': 0.02313963645402323, 'num_leaves': 467, 'max_depth': 37, 'n_estimators': 787, 'min_child_samples': 37, 'subsample': 0.8522873031015437, 'colsample_bytree': 0.6759269623033601, 'reg_alpha': 0.0008369093319546271, 'reg_lambda': 0.2861677278822956, 'min_split_gain': 0.7861300995970871}. Best is trial 4 with value: 0.26147855146798565.\n",
      "[I 2025-07-14 21:00:32,250] Trial 22 finished with value: 0.2604976213794196 and parameters: {'learning_rate': 0.05901104936501348, 'num_leaves': 983, 'max_depth': 34, 'n_estimators': 751, 'min_child_samples': 59, 'subsample': 0.6573020122556124, 'colsample_bytree': 0.6238918125304984, 'reg_alpha': 2.160974668482376e-08, 'reg_lambda': 0.00012175893667585027, 'min_split_gain': 0.15598104436586413}. Best is trial 22 with value: 0.2604976213794196.\n",
      "[I 2025-07-14 21:00:33,047] Trial 26 finished with value: 0.2593359782631044 and parameters: {'learning_rate': 0.020403419090133334, 'num_leaves': 722, 'max_depth': 57, 'n_estimators': 282, 'min_child_samples': 85, 'subsample': 0.6484183783771994, 'colsample_bytree': 0.539403224484557, 'reg_alpha': 0.0022924773563651837, 'reg_lambda': 0.004842602876417121, 'min_split_gain': 0.855557851828307}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:00:34,115] Trial 7 finished with value: 0.2615525734009095 and parameters: {'learning_rate': 0.018627226578607686, 'num_leaves': 920, 'max_depth': 45, 'n_estimators': 857, 'min_child_samples': 91, 'subsample': 0.9372873785159623, 'colsample_bytree': 0.6872873232235428, 'reg_alpha': 0.00015291362758833088, 'reg_lambda': 0.0007516925914601067, 'min_split_gain': 0.9629408775181594}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:00:39,043] Trial 23 finished with value: 0.26367150451483856 and parameters: {'learning_rate': 0.02115712640680629, 'num_leaves': 682, 'max_depth': 62, 'n_estimators': 247, 'min_child_samples': 22, 'subsample': 0.5102159229617265, 'colsample_bytree': 0.7472017236439521, 'reg_alpha': 5.544627315852036e-06, 'reg_lambda': 4.120840858658856e-08, 'min_split_gain': 0.70643837684416}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:00:41,033] Trial 16 finished with value: 0.5134741467139752 and parameters: {'learning_rate': 0.005717085555356093, 'num_leaves': 584, 'max_depth': 23, 'n_estimators': 142, 'min_child_samples': 13, 'subsample': 0.5225884798178906, 'colsample_bytree': 0.7646098230755407, 'reg_alpha': 0.0031321524080582678, 'reg_lambda': 1.2664120225656827, 'min_split_gain': 0.9619086221297938}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:00:42,878] Trial 18 finished with value: 0.26240186794744147 and parameters: {'learning_rate': 0.016334357441859803, 'num_leaves': 139, 'max_depth': 58, 'n_estimators': 726, 'min_child_samples': 83, 'subsample': 0.9081132044241527, 'colsample_bytree': 0.9451455036737944, 'reg_alpha': 1.5075660526296562e-05, 'reg_lambda': 1.0426249583515242e-05, 'min_split_gain': 0.777413374868667}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:00:49,540] Trial 6 finished with value: 0.26350890300263924 and parameters: {'learning_rate': 0.05723958209487874, 'num_leaves': 818, 'max_depth': 51, 'n_estimators': 159, 'min_child_samples': 36, 'subsample': 0.9907422667906844, 'colsample_bytree': 0.9577659153140813, 'reg_alpha': 0.004436703372444101, 'reg_lambda': 1.6640642220054688e-06, 'min_split_gain': 0.08989100057682964}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:00:56,772] Trial 3 finished with value: 0.26350677141254575 and parameters: {'learning_rate': 0.026765155055001266, 'num_leaves': 686, 'max_depth': 18, 'n_estimators': 426, 'min_child_samples': 32, 'subsample': 0.6937169385430749, 'colsample_bytree': 0.9747820631461984, 'reg_alpha': 0.005775182353057891, 'reg_lambda': 0.5849032833657659, 'min_split_gain': 0.23872223136839255}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:01:06,522] Trial 33 finished with value: 0.26045424529524897 and parameters: {'learning_rate': 0.011677402853105194, 'num_leaves': 267, 'max_depth': 41, 'n_estimators': 640, 'min_child_samples': 98, 'subsample': 0.8096664093169007, 'colsample_bytree': 0.9675828660880161, 'reg_alpha': 0.7871114325689905, 'reg_lambda': 4.105645576754167e-08, 'min_split_gain': 0.6586482796683288}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:01:39,355] Trial 35 finished with value: 0.26470241283247986 and parameters: {'learning_rate': 0.007426627943077428, 'num_leaves': 860, 'max_depth': 13, 'n_estimators': 657, 'min_child_samples': 42, 'subsample': 0.7176561803446242, 'colsample_bytree': 0.8277737990243867, 'reg_alpha': 1.0847816896410647e-07, 'reg_lambda': 4.396093501651907e-08, 'min_split_gain': 0.7434961678524599}. Best is trial 26 with value: 0.2593359782631044.\n",
      "[I 2025-07-14 21:01:42,457] Trial 9 finished with value: 0.257112875240953 and parameters: {'learning_rate': 0.015606970363766175, 'num_leaves': 1014, 'max_depth': 50, 'n_estimators': 450, 'min_child_samples': 22, 'subsample': 0.679385271497049, 'colsample_bytree': 0.5061037168552841, 'reg_alpha': 2.140060872700785e-07, 'reg_lambda': 1.696618763610775e-06, 'min_split_gain': 0.22692914393835917}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:01:56,931] Trial 10 finished with value: 0.36229631703940746 and parameters: {'learning_rate': 0.0025098009118667986, 'num_leaves': 292, 'max_depth': 25, 'n_estimators': 556, 'min_child_samples': 100, 'subsample': 0.7250060247151839, 'colsample_bytree': 0.7767737320282545, 'reg_alpha': 0.33294783978675413, 'reg_lambda': 0.11338377391961488, 'min_split_gain': 0.6352630991067691}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:02:12,368] Trial 28 finished with value: 0.7148199209626197 and parameters: {'learning_rate': 0.0009892241613438013, 'num_leaves': 130, 'max_depth': 23, 'n_estimators': 395, 'min_child_samples': 34, 'subsample': 0.702549734598416, 'colsample_bytree': 0.5708251214761044, 'reg_alpha': 3.077894757657271e-05, 'reg_lambda': 0.6603919994425127, 'min_split_gain': 0.6226605739103723}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:02:26,333] Trial 47 finished with value: 0.25944656050870557 and parameters: {'learning_rate': 0.008404102659483971, 'num_leaves': 1007, 'max_depth': 29, 'n_estimators': 614, 'min_child_samples': 76, 'subsample': 0.6079957823316109, 'colsample_bytree': 0.5172132093103188, 'reg_alpha': 0.1418906789346187, 'reg_lambda': 0.01224664789087416, 'min_split_gain': 0.3753643508413178}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:02:29,558] Trial 25 finished with value: 0.40077197912588325 and parameters: {'learning_rate': 0.001475239034778518, 'num_leaves': 187, 'max_depth': 31, 'n_estimators': 824, 'min_child_samples': 57, 'subsample': 0.5146509165085057, 'colsample_bytree': 0.6492031707295731, 'reg_alpha': 0.35578365241298326, 'reg_lambda': 4.688735123313394, 'min_split_gain': 0.6136847597451512}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:02:37,609] Trial 45 finished with value: 0.2621224380379704 and parameters: {'learning_rate': 0.006195157024311715, 'num_leaves': 985, 'max_depth': 29, 'n_estimators': 567, 'min_child_samples': 77, 'subsample': 0.755602063545382, 'colsample_bytree': 0.5025268297525701, 'reg_alpha': 1.1281520789896442, 'reg_lambda': 0.022583280884690568, 'min_split_gain': 0.15196365680905732}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:02:41,940] Trial 12 finished with value: 0.2644581599625169 and parameters: {'learning_rate': 0.005822643526660255, 'num_leaves': 148, 'max_depth': 11, 'n_estimators': 639, 'min_child_samples': 83, 'subsample': 0.6109178200038685, 'colsample_bytree': 0.8564048824362089, 'reg_alpha': 0.00739866255628656, 'reg_lambda': 0.022219248120579113, 'min_split_gain': 0.24539747652920674}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:02:42,184] Trial 34 finished with value: 0.273017820205875 and parameters: {'learning_rate': 0.007578405453995929, 'num_leaves': 119, 'max_depth': 25, 'n_estimators': 356, 'min_child_samples': 54, 'subsample': 0.5281961895309402, 'colsample_bytree': 0.7714988957161932, 'reg_alpha': 2.1769674056074663e-05, 'reg_lambda': 2.3816044637434672e-05, 'min_split_gain': 0.025826034058977987}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:02:51,174] Trial 43 finished with value: 0.2615738321386964 and parameters: {'learning_rate': 0.005949955784782737, 'num_leaves': 1021, 'max_depth': 22, 'n_estimators': 630, 'min_child_samples': 80, 'subsample': 0.6062545801337177, 'colsample_bytree': 0.5234412418900019, 'reg_alpha': 0.9174738129647017, 'reg_lambda': 1.907916862954382e-05, 'min_split_gain': 0.15583096344611921}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:02:59,435] Trial 36 finished with value: 0.28395950090008 and parameters: {'learning_rate': 0.003119154494871785, 'num_leaves': 710, 'max_depth': 62, 'n_estimators': 717, 'min_child_samples': 17, 'subsample': 0.8201345267533071, 'colsample_bytree': 0.6549315357679963, 'reg_alpha': 0.0029984967320820627, 'reg_lambda': 1.7453240288874073e-08, 'min_split_gain': 0.789845786507959}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:02,351] Trial 31 finished with value: 0.26639018789982877 and parameters: {'learning_rate': 0.003393105194197221, 'num_leaves': 540, 'max_depth': 49, 'n_estimators': 938, 'min_child_samples': 89, 'subsample': 0.9733238338972445, 'colsample_bytree': 0.981989569326009, 'reg_alpha': 9.401905592964215e-07, 'reg_lambda': 5.401474823300454e-08, 'min_split_gain': 0.5581068296092914}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:04,348] Trial 51 finished with value: 0.2708032339726726 and parameters: {'learning_rate': 0.005001077830570382, 'num_leaves': 30, 'max_depth': 43, 'n_estimators': 587, 'min_child_samples': 77, 'subsample': 0.7811441153743707, 'colsample_bytree': 0.5242801751147586, 'reg_alpha': 0.15256973683211517, 'reg_lambda': 0.012235461927751407, 'min_split_gain': 0.37649048332179336}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:13,614] Trial 49 finished with value: 0.25981655705207307 and parameters: {'learning_rate': 0.007422377839125867, 'num_leaves': 995, 'max_depth': 29, 'n_estimators': 588, 'min_child_samples': 78, 'subsample': 0.7858137861797504, 'colsample_bytree': 0.5046277727820561, 'reg_alpha': 0.20606158907014513, 'reg_lambda': 0.022969684520040917, 'min_split_gain': 0.40022311762252305}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:20,731] Trial 57 finished with value: 0.4719473503481417 and parameters: {'learning_rate': 0.001975764561892888, 'num_leaves': 16, 'max_depth': 46, 'n_estimators': 491, 'min_child_samples': 81, 'subsample': 0.5834195690552897, 'colsample_bytree': 0.5517586179256229, 'reg_alpha': 0.10771227932177521, 'reg_lambda': 0.011667298027070911, 'min_split_gain': 0.3886328545976782}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:24,328] Trial 39 finished with value: 0.2609523639336969 and parameters: {'learning_rate': 0.006731870436447314, 'num_leaves': 475, 'max_depth': 39, 'n_estimators': 992, 'min_child_samples': 45, 'subsample': 0.8649049754617787, 'colsample_bytree': 0.712580275762533, 'reg_alpha': 1.1086693443191827e-05, 'reg_lambda': 0.08511863148975916, 'min_split_gain': 0.3601905473872371}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:25,012] Trial 37 finished with value: 0.27238230585457934 and parameters: {'learning_rate': 0.0044473142018688116, 'num_leaves': 1011, 'max_depth': 41, 'n_estimators': 628, 'min_child_samples': 82, 'subsample': 0.8467242148138502, 'colsample_bytree': 0.9761429540306125, 'reg_alpha': 1.6807964420321293e-05, 'reg_lambda': 1.755009516557352e-05, 'min_split_gain': 0.2905103404535841}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:32,791] Trial 29 finished with value: 0.4954181194434648 and parameters: {'learning_rate': 0.0012252620259547164, 'num_leaves': 390, 'max_depth': 29, 'n_estimators': 708, 'min_child_samples': 11, 'subsample': 0.7572815126416386, 'colsample_bytree': 0.9201625705676523, 'reg_alpha': 5.455539062410948e-07, 'reg_lambda': 4.152152881299509, 'min_split_gain': 0.48992181520245137}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:32,953] Trial 48 finished with value: 0.2593642114405425 and parameters: {'learning_rate': 0.005173484554108556, 'num_leaves': 987, 'max_depth': 29, 'n_estimators': 979, 'min_child_samples': 78, 'subsample': 0.7657733685771094, 'colsample_bytree': 0.5127034922882856, 'reg_alpha': 0.20773528387963966, 'reg_lambda': 0.0261739273915774, 'min_split_gain': 0.34093481720993773}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:34,740] Trial 52 finished with value: 0.25898211231101054 and parameters: {'learning_rate': 0.00950005943228928, 'num_leaves': 418, 'max_depth': 45, 'n_estimators': 532, 'min_child_samples': 79, 'subsample': 0.599587937665071, 'colsample_bytree': 0.5036895116830686, 'reg_alpha': 0.25716168825457814, 'reg_lambda': 0.010086149730261593, 'min_split_gain': 0.34034475724766305}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:43,396] Trial 50 finished with value: 0.2738300797757756 and parameters: {'learning_rate': 0.004016127654373722, 'num_leaves': 371, 'max_depth': 45, 'n_estimators': 629, 'min_child_samples': 77, 'subsample': 0.7810915880336298, 'colsample_bytree': 0.502763253191024, 'reg_alpha': 0.3746734345402544, 'reg_lambda': 0.026732032213584455, 'min_split_gain': 0.4309090146497791}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:45,880] Trial 59 finished with value: 0.26395241546828363 and parameters: {'learning_rate': 0.010603071952767742, 'num_leaves': 27, 'max_depth': 45, 'n_estimators': 506, 'min_child_samples': 72, 'subsample': 0.7703473154539221, 'colsample_bytree': 0.5636979481867657, 'reg_alpha': 0.14360254838497083, 'reg_lambda': 0.006189346186829822, 'min_split_gain': 0.36185200215150437}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:54,057] Trial 58 finished with value: 0.2592410260193536 and parameters: {'learning_rate': 0.01113908956584953, 'num_leaves': 925, 'max_depth': 45, 'n_estimators': 941, 'min_child_samples': 74, 'subsample': 0.7808582776394354, 'colsample_bytree': 0.5640736976227567, 'reg_alpha': 0.0925080286443222, 'reg_lambda': 0.003963944931349673, 'min_split_gain': 0.3953668482798942}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:03:55,060] Trial 11 finished with value: 0.31829542368511904 and parameters: {'learning_rate': 0.002020877146115834, 'num_leaves': 247, 'max_depth': 13, 'n_estimators': 855, 'min_child_samples': 90, 'subsample': 0.6523395526180724, 'colsample_bytree': 0.5496691742192208, 'reg_alpha': 0.006064686539038464, 'reg_lambda': 3.604270898791505e-08, 'min_split_gain': 0.3835043907397868}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:03,425] Trial 61 finished with value: 0.25868800433276273 and parameters: {'learning_rate': 0.012752652360078717, 'num_leaves': 386, 'max_depth': 47, 'n_estimators': 493, 'min_child_samples': 73, 'subsample': 0.587756073485102, 'colsample_bytree': 0.5529707020009277, 'reg_alpha': 0.03851360903065504, 'reg_lambda': 0.005670314682597255, 'min_split_gain': 0.4208798853357307}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:04,436] Trial 60 finished with value: 0.2598357172554867 and parameters: {'learning_rate': 0.011495350764697267, 'num_leaves': 430, 'max_depth': 43, 'n_estimators': 476, 'min_child_samples': 74, 'subsample': 0.5803303494268813, 'colsample_bytree': 0.5593387326876544, 'reg_alpha': 0.051580780447233965, 'reg_lambda': 0.005931637521352213, 'min_split_gain': 0.3943231315818393}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:06,199] Trial 38 finished with value: 0.26349410803684226 and parameters: {'learning_rate': 0.006144341908741167, 'num_leaves': 423, 'max_depth': 38, 'n_estimators': 999, 'min_child_samples': 16, 'subsample': 0.8514041039358118, 'colsample_bytree': 0.9943548685726253, 'reg_alpha': 1.3320027209798696e-05, 'reg_lambda': 1.3172199738743393e-05, 'min_split_gain': 0.2959876623635264}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:07,044] Trial 62 finished with value: 0.2618922972525586 and parameters: {'learning_rate': 0.013156215164782168, 'num_leaves': 948, 'max_depth': 38, 'n_estimators': 490, 'min_child_samples': 69, 'subsample': 0.6014922173290111, 'colsample_bytree': 0.5651621396146087, 'reg_alpha': 0.05535259514536759, 'reg_lambda': 0.007220820904114383, 'min_split_gain': 0.4160271893812912}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:09,876] Trial 70 finished with value: 0.25965492416880137 and parameters: {'learning_rate': 0.03940878411514507, 'num_leaves': 924, 'max_depth': 55, 'n_estimators': 313, 'min_child_samples': 72, 'subsample': 0.5710869625192028, 'colsample_bytree': 0.5957603248865104, 'reg_alpha': 0.04639815529634495, 'reg_lambda': 0.00023040917033926, 'min_split_gain': 0.28523396193984873}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:10,875] Trial 46 finished with value: 0.26098459467508206 and parameters: {'learning_rate': 0.005517894352140162, 'num_leaves': 1018, 'max_depth': 29, 'n_estimators': 998, 'min_child_samples': 78, 'subsample': 0.6106954583205749, 'colsample_bytree': 0.5067592471720771, 'reg_alpha': 0.28654432448704736, 'reg_lambda': 0.015551948720705365, 'min_split_gain': 0.15453922412640983}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:13,825] Trial 64 finished with value: 0.26147314243050085 and parameters: {'learning_rate': 0.013468627833465648, 'num_leaves': 937, 'max_depth': 29, 'n_estimators': 302, 'min_child_samples': 73, 'subsample': 0.7520859119477256, 'colsample_bytree': 0.6063258625263316, 'reg_alpha': 0.00028660554840164316, 'reg_lambda': 0.004210852861137945, 'min_split_gain': 0.44399255628506695}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:17,716] Trial 63 finished with value: 0.26298039839472 and parameters: {'learning_rate': 0.011258486084130088, 'num_leaves': 915, 'max_depth': 39, 'n_estimators': 305, 'min_child_samples': 71, 'subsample': 0.6254740407725008, 'colsample_bytree': 0.599758785850369, 'reg_alpha': 0.03739125383253767, 'reg_lambda': 0.00010240211159598248, 'min_split_gain': 0.42734601039428527}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:18,565] Trial 41 finished with value: 0.2628031681558813 and parameters: {'learning_rate': 0.006158789202310695, 'num_leaves': 1001, 'max_depth': 25, 'n_estimators': 973, 'min_child_samples': 79, 'subsample': 0.7634984390148879, 'colsample_bytree': 0.7158678986161929, 'reg_alpha': 8.450223518855252e-06, 'reg_lambda': 9.271008915275312e-05, 'min_split_gain': 0.14150291493301326}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:31,715] Trial 66 finished with value: 0.26124423463113133 and parameters: {'learning_rate': 0.01133006455281553, 'num_leaves': 929, 'max_depth': 56, 'n_estimators': 491, 'min_child_samples': 71, 'subsample': 0.5718085871515151, 'colsample_bytree': 0.6059539244100732, 'reg_alpha': 0.00014550388084165646, 'reg_lambda': 0.003224272764051843, 'min_split_gain': 0.45680344558843555}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:34,154] Trial 74 finished with value: 0.2589581987428733 and parameters: {'learning_rate': 0.037485034486917805, 'num_leaves': 898, 'max_depth': 54, 'n_estimators': 306, 'min_child_samples': 68, 'subsample': 0.6269623750753062, 'colsample_bytree': 0.6048703550451288, 'reg_alpha': 9.717946571049827e-05, 'reg_lambda': 0.00018729536421543283, 'min_split_gain': 0.2275412831633271}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:34,380] Trial 53 finished with value: 0.2749677531342951 and parameters: {'learning_rate': 0.004566146369586367, 'num_leaves': 996, 'max_depth': 28, 'n_estimators': 545, 'min_child_samples': 79, 'subsample': 0.5890478475391312, 'colsample_bytree': 0.5032137557850793, 'reg_alpha': 0.10361535394270155, 'reg_lambda': 0.014426704011504013, 'min_split_gain': 0.33461537351122883}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:34,528] Trial 71 finished with value: 0.258355517700051 and parameters: {'learning_rate': 0.014801823692800885, 'num_leaves': 932, 'max_depth': 55, 'n_estimators': 889, 'min_child_samples': 70, 'subsample': 0.6773811011738075, 'colsample_bytree': 0.589405888001855, 'reg_alpha': 3.617389687332196, 'reg_lambda': 0.003751473173201726, 'min_split_gain': 0.2711201394646252}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:36,300] Trial 54 finished with value: 0.26833034754150187 and parameters: {'learning_rate': 0.00532290766236861, 'num_leaves': 1008, 'max_depth': 46, 'n_estimators': 527, 'min_child_samples': 77, 'subsample': 0.6025050865698347, 'colsample_bytree': 0.5214517131695806, 'reg_alpha': 0.00014617492462600385, 'reg_lambda': 0.02142188146839725, 'min_split_gain': 0.33142809210354623}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:38,796] Trial 42 finished with value: 0.2606908126915105 and parameters: {'learning_rate': 0.005154593279040185, 'num_leaves': 995, 'max_depth': 24, 'n_estimators': 964, 'min_child_samples': 78, 'subsample': 0.782989454759555, 'colsample_bytree': 0.5009300467425407, 'reg_alpha': 2.026982970234563e-05, 'reg_lambda': 1.8823073551979366e-05, 'min_split_gain': 0.15007169088351435}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:42,516] Trial 79 finished with value: 0.258139877062365 and parameters: {'learning_rate': 0.020356280361928047, 'num_leaves': 860, 'max_depth': 54, 'n_estimators': 898, 'min_child_samples': 88, 'subsample': 0.6822305693858977, 'colsample_bytree': 0.5366698890541651, 'reg_alpha': 4.4596965045753, 'reg_lambda': 0.0018244357627687751, 'min_split_gain': 0.21987482777800427}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:43,215] Trial 73 finished with value: 0.26122343975065926 and parameters: {'learning_rate': 0.016364051371307732, 'num_leaves': 923, 'max_depth': 55, 'n_estimators': 462, 'min_child_samples': 70, 'subsample': 0.5693946513065169, 'colsample_bytree': 0.599003561143375, 'reg_alpha': 0.0376764733146801, 'reg_lambda': 0.0028911689490914326, 'min_split_gain': 0.4634445481865768}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:43,790] Trial 86 finished with value: 0.26019150278107006 and parameters: {'learning_rate': 0.08764900383305407, 'num_leaves': 866, 'max_depth': 52, 'n_estimators': 901, 'min_child_samples': 65, 'subsample': 0.6800456993406516, 'colsample_bytree': 0.539237203028905, 'reg_alpha': 3.9602849999511816, 'reg_lambda': 0.0010925027727985786, 'min_split_gain': 0.25954016156395043}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:45,063] Trial 76 finished with value: 0.2614296700778281 and parameters: {'learning_rate': 0.015559744440587326, 'num_leaves': 899, 'max_depth': 49, 'n_estimators': 248, 'min_child_samples': 87, 'subsample': 0.5563872689130512, 'colsample_bytree': 0.5995581421267113, 'reg_alpha': 0.0013523783858920993, 'reg_lambda': 0.00047973696602780503, 'min_split_gain': 0.4801924135622876}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:45,310] Trial 83 finished with value: 0.25976250938898826 and parameters: {'learning_rate': 0.0941399379628903, 'num_leaves': 841, 'max_depth': 48, 'n_estimators': 103, 'min_child_samples': 87, 'subsample': 0.672834282544013, 'colsample_bytree': 0.5377619137341622, 'reg_alpha': 0.013292494266993192, 'reg_lambda': 0.0009912370471234077, 'min_split_gain': 0.22245672420769794}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:46,530] Trial 81 finished with value: 0.2738264485201763 and parameters: {'learning_rate': 0.02434123392049703, 'num_leaves': 334, 'max_depth': 53, 'n_estimators': 104, 'min_child_samples': 88, 'subsample': 0.6805406382640924, 'colsample_bytree': 0.5433478891541842, 'reg_alpha': 0.013276811770017967, 'reg_lambda': 0.0015077922727033965, 'min_split_gain': 0.2116657282338878}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:48,309] Trial 72 finished with value: 0.26009231510426756 and parameters: {'learning_rate': 0.016803890925846336, 'num_leaves': 915, 'max_depth': 54, 'n_estimators': 921, 'min_child_samples': 68, 'subsample': 0.7298821472661895, 'colsample_bytree': 0.5889444524559817, 'reg_alpha': 0.00015852104903227564, 'reg_lambda': 0.0030174327967438528, 'min_split_gain': 0.2840206623157994}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:49,208] Trial 68 finished with value: 0.25975259320671484 and parameters: {'learning_rate': 0.011791696310389793, 'num_leaves': 918, 'max_depth': 54, 'n_estimators': 501, 'min_child_samples': 70, 'subsample': 0.5784864630093973, 'colsample_bytree': 0.5940154631022425, 'reg_alpha': 0.06617042206033676, 'reg_lambda': 0.0029468964393977014, 'min_split_gain': 0.30367023585648895}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:49,633] Trial 78 finished with value: 0.25927893115108214 and parameters: {'learning_rate': 0.0163869566416144, 'num_leaves': 878, 'max_depth': 49, 'n_estimators': 916, 'min_child_samples': 85, 'subsample': 0.6771060614211697, 'colsample_bytree': 0.5386816619304265, 'reg_alpha': 3.928639939990446, 'reg_lambda': 0.0018953508986573218, 'min_split_gain': 0.48842650373353436}. Best is trial 9 with value: 0.257112875240953.\n",
      "[I 2025-07-14 21:04:50,679] Trial 87 finished with value: 0.2558432304289068 and parameters: {'learning_rate': 0.0782171215996599, 'num_leaves': 623, 'max_depth': 53, 'n_estimators': 901, 'min_child_samples': 66, 'subsample': 0.676274714703084, 'colsample_bytree': 0.5392137848004892, 'reg_alpha': 6.12638062330142, 'reg_lambda': 0.0007764583870457865, 'min_split_gain': 0.21918702092022535}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:04:53,406] Trial 80 finished with value: 0.2589645609596134 and parameters: {'learning_rate': 0.01774166984522665, 'num_leaves': 337, 'max_depth': 54, 'n_estimators': 892, 'min_child_samples': 85, 'subsample': 0.6801515021204729, 'colsample_bytree': 0.5383400455128199, 'reg_alpha': 3.615265195535569, 'reg_lambda': 0.0018807724681070273, 'min_split_gain': 0.22298825313322332}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:04:55,342] Trial 40 finished with value: 0.2614157528695129 and parameters: {'learning_rate': 0.00652775815708712, 'num_leaves': 411, 'max_depth': 32, 'n_estimators': 967, 'min_child_samples': 43, 'subsample': 0.824235583994931, 'colsample_bytree': 0.6600893918492812, 'reg_alpha': 1.6422571211450454e-05, 'reg_lambda': 0.02926718168615918, 'min_split_gain': 0.13789574683577954}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:04:58,452] Trial 77 finished with value: 0.25987085510851204 and parameters: {'learning_rate': 0.014811682075711857, 'num_leaves': 599, 'max_depth': 49, 'n_estimators': 531, 'min_child_samples': 85, 'subsample': 0.6312509479038565, 'colsample_bytree': 0.6065701110504004, 'reg_alpha': 0.0002158656473211852, 'reg_lambda': 0.0012982345846661379, 'min_split_gain': 0.4634993449687066}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:04:59,061] Trial 85 finished with value: 0.25765142845734007 and parameters: {'learning_rate': 0.02594440817056986, 'num_leaves': 831, 'max_depth': 53, 'n_estimators': 887, 'min_child_samples': 67, 'subsample': 0.6759360672439466, 'colsample_bytree': 0.5333521474331046, 'reg_alpha': 6.276962824414759, 'reg_lambda': 0.0009360744748592348, 'min_split_gain': 0.21813725606256462}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:01,615] Trial 89 finished with value: 0.25938425403137644 and parameters: {'learning_rate': 0.03857481894981858, 'num_leaves': 862, 'max_depth': 49, 'n_estimators': 912, 'min_child_samples': 64, 'subsample': 0.6764268101246034, 'colsample_bytree': 0.5424896140452931, 'reg_alpha': 3.7207746401744606, 'reg_lambda': 0.0014168921177430346, 'min_split_gain': 0.22256126121525888}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:03,511] Trial 44 finished with value: 0.2657519988311465 and parameters: {'learning_rate': 0.003073116709932954, 'num_leaves': 1018, 'max_depth': 30, 'n_estimators': 993, 'min_child_samples': 77, 'subsample': 0.6076776990539554, 'colsample_bytree': 0.5003867531731966, 'reg_alpha': 0.41580893145246595, 'reg_lambda': 1.9154666983555822e-05, 'min_split_gain': 0.1455214475996635}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:05,409] Trial 84 finished with value: 0.2720370302459493 and parameters: {'learning_rate': 0.02373894489605366, 'num_leaves': 865, 'max_depth': 52, 'n_estimators': 108, 'min_child_samples': 85, 'subsample': 0.6724495652983213, 'colsample_bytree': 0.5432800607523005, 'reg_alpha': 0.013869479073439182, 'reg_lambda': 0.001146348936642898, 'min_split_gain': 0.21741618274895247}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:06,278] Trial 82 finished with value: 0.2582221405358018 and parameters: {'learning_rate': 0.017047951536892838, 'num_leaves': 322, 'max_depth': 48, 'n_estimators': 908, 'min_child_samples': 86, 'subsample': 0.7263219639333184, 'colsample_bytree': 0.5401750858312357, 'reg_alpha': 5.623941369841512, 'reg_lambda': 0.001247602360698588, 'min_split_gain': 0.23174349846190045}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:09,112] Trial 90 finished with value: 0.25727388528093637 and parameters: {'learning_rate': 0.02791641473738743, 'num_leaves': 838, 'max_depth': 49, 'n_estimators': 883, 'min_child_samples': 87, 'subsample': 0.7229926436143845, 'colsample_bytree': 0.580611715718063, 'reg_alpha': 3.7352416884112873, 'reg_lambda': 0.0007686747846126362, 'min_split_gain': 0.2074800029228346}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:09,650] Trial 97 finished with value: 0.25786655559380683 and parameters: {'learning_rate': 0.03349059713057037, 'num_leaves': 811, 'max_depth': 59, 'n_estimators': 880, 'min_child_samples': 62, 'subsample': 0.6320928527156413, 'colsample_bytree': 0.628443222887089, 'reg_alpha': 9.189393599327067, 'reg_lambda': 4.270136722710824e-06, 'min_split_gain': 0.18633963950663768}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:10,053] Trial 94 finished with value: 0.258910187067948 and parameters: {'learning_rate': 0.03593903417871388, 'num_leaves': 790, 'max_depth': 59, 'n_estimators': 871, 'min_child_samples': 63, 'subsample': 0.6367529554086816, 'colsample_bytree': 0.5775101935660314, 'reg_alpha': 3.2229422149636475, 'reg_lambda': 0.06879855618268103, 'min_split_gain': 0.18713389001213215}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:11,030] Trial 88 finished with value: 0.25920055140950665 and parameters: {'learning_rate': 0.02847470722584327, 'num_leaves': 868, 'max_depth': 53, 'n_estimators': 877, 'min_child_samples': 64, 'subsample': 0.6761475248873351, 'colsample_bytree': 0.5400033878039487, 'reg_alpha': 2.265479358709613, 'reg_lambda': 0.0009515901537101741, 'min_split_gain': 0.21409206288564792}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:13,668] Trial 93 finished with value: 0.25846987401061366 and parameters: {'learning_rate': 0.030318086865211525, 'num_leaves': 797, 'max_depth': 59, 'n_estimators': 905, 'min_child_samples': 63, 'subsample': 0.7246161840854284, 'colsample_bytree': 0.5834024269521453, 'reg_alpha': 2.934391421006193, 'reg_lambda': 0.06562601487964104, 'min_split_gain': 0.1935713346020307}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:14,851] Trial 92 finished with value: 0.25977784649415775 and parameters: {'learning_rate': 0.02657997988979338, 'num_leaves': 796, 'max_depth': 52, 'n_estimators': 865, 'min_child_samples': 94, 'subsample': 0.7327052987520664, 'colsample_bytree': 0.6252552493185837, 'reg_alpha': 2.3692487273655125, 'reg_lambda': 0.1100753981851389, 'min_split_gain': 0.20444909851515763}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:16,368] Trial 91 finished with value: 0.25698643057679565 and parameters: {'learning_rate': 0.024695575970761573, 'num_leaves': 825, 'max_depth': 49, 'n_estimators': 430, 'min_child_samples': 62, 'subsample': 0.6788769685121383, 'colsample_bytree': 0.581200006937317, 'reg_alpha': 2.823476428931025, 'reg_lambda': 0.058656628346583126, 'min_split_gain': 0.23361645547762144}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:17,682] Trial 101 finished with value: 0.2598922420732378 and parameters: {'learning_rate': 0.03758077163620019, 'num_leaves': 809, 'max_depth': 61, 'n_estimators': 859, 'min_child_samples': 61, 'subsample': 0.6961140656312556, 'colsample_bytree': 0.6311248130835099, 'reg_alpha': 6.843640108024103, 'reg_lambda': 4.0214127373345756e-06, 'min_split_gain': 0.18718084703050109}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:19,352] Trial 99 finished with value: 0.25875288605155494 and parameters: {'learning_rate': 0.03578728326080606, 'num_leaves': 319, 'max_depth': 59, 'n_estimators': 861, 'min_child_samples': 62, 'subsample': 0.6342527023151597, 'colsample_bytree': 0.5794876843433907, 'reg_alpha': 1.8838183437252387, 'reg_lambda': 0.0999065297406144, 'min_split_gain': 0.1957281599375924}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:20,318] Trial 100 finished with value: 0.258824785776081 and parameters: {'learning_rate': 0.035778903881793736, 'num_leaves': 814, 'max_depth': 59, 'n_estimators': 427, 'min_child_samples': 95, 'subsample': 0.7020439935224456, 'colsample_bytree': 0.6198425181278694, 'reg_alpha': 8.512369989436955, 'reg_lambda': 0.09953593656663547, 'min_split_gain': 0.10705529517608675}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:21,135] Trial 98 finished with value: 0.2603286666193259 and parameters: {'learning_rate': 0.03640732393680962, 'num_leaves': 807, 'max_depth': 58, 'n_estimators': 866, 'min_child_samples': 94, 'subsample': 0.6311359842301411, 'colsample_bytree': 0.580745601146542, 'reg_alpha': 2.072600720263764, 'reg_lambda': 0.0004049101474192519, 'min_split_gain': 0.11204289747533011}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:23,355] Trial 102 finished with value: 0.25873937527041074 and parameters: {'learning_rate': 0.030117555012763975, 'num_leaves': 788, 'max_depth': 59, 'n_estimators': 858, 'min_child_samples': 52, 'subsample': 0.7030882278175239, 'colsample_bytree': 0.6301239322163635, 'reg_alpha': 9.69854798725963, 'reg_lambda': 4.7690173921377225e-06, 'min_split_gain': 0.18663193281145407}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:25,108] Trial 104 finished with value: 0.25938523857841056 and parameters: {'learning_rate': 0.03185642890451435, 'num_leaves': 792, 'max_depth': 59, 'n_estimators': 843, 'min_child_samples': 93, 'subsample': 0.7047458573755054, 'colsample_bytree': 0.5748285112861908, 'reg_alpha': 7.9905930340645375, 'reg_lambda': 4.789275138091674e-05, 'min_split_gain': 0.18358521207956596}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:28,273] Trial 67 finished with value: 0.8658782202756324 and parameters: {'learning_rate': 0.0006229681744705761, 'num_leaves': 906, 'max_depth': 55, 'n_estimators': 277, 'min_child_samples': 67, 'subsample': 0.6237381650273438, 'colsample_bytree': 0.5833023961575177, 'reg_alpha': 0.00021270585571401342, 'reg_lambda': 0.004103412489205746, 'min_split_gain': 0.3139427378794993}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:28,789] Trial 107 finished with value: 0.25870171915277734 and parameters: {'learning_rate': 0.03151400595102526, 'num_leaves': 788, 'max_depth': 59, 'n_estimators': 864, 'min_child_samples': 92, 'subsample': 0.7050718337080566, 'colsample_bytree': 0.580923889942234, 'reg_alpha': 9.69662398295436, 'reg_lambda': 3.987515186941689e-06, 'min_split_gain': 0.18572509363950485}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:29,971] Trial 110 finished with value: 0.258937246035779 and parameters: {'learning_rate': 0.04495691132035318, 'num_leaves': 642, 'max_depth': 61, 'n_estimators': 829, 'min_child_samples': 94, 'subsample': 0.6999613056840258, 'colsample_bytree': 0.6282872408514771, 'reg_alpha': 9.87320035540286, 'reg_lambda': 8.51661065192434e-07, 'min_split_gain': 0.10276977999672705}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:30,811] Trial 111 finished with value: 0.2596292621952207 and parameters: {'learning_rate': 0.05039730670835634, 'num_leaves': 653, 'max_depth': 59, 'n_estimators': 830, 'min_child_samples': 60, 'subsample': 0.6999549600699162, 'colsample_bytree': 0.6379412779001307, 'reg_alpha': 8.009313033165196, 'reg_lambda': 4.220434559848909e-05, 'min_split_gain': 0.09784794679870312}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:30,954] Trial 96 finished with value: 0.2574304293925539 and parameters: {'learning_rate': 0.03141361438347346, 'num_leaves': 800, 'max_depth': 59, 'n_estimators': 878, 'min_child_samples': 61, 'subsample': 0.631802843115752, 'colsample_bytree': 0.6245265051440004, 'reg_alpha': 1.844157416610244e-06, 'reg_lambda': 0.07729961073398937, 'min_split_gain': 0.19651591954458575}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:31,254] Trial 103 finished with value: 0.2593745349834543 and parameters: {'learning_rate': 0.030416378195380258, 'num_leaves': 795, 'max_depth': 58, 'n_estimators': 871, 'min_child_samples': 51, 'subsample': 0.7094002866170787, 'colsample_bytree': 0.6280619349254393, 'reg_alpha': 8.418133890903958, 'reg_lambda': 0.0005122119407132862, 'min_split_gain': 0.0973305120450548}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:33,890] Trial 106 finished with value: 0.2573245320883602 and parameters: {'learning_rate': 0.02992189455922586, 'num_leaves': 796, 'max_depth': 60, 'n_estimators': 845, 'min_child_samples': 93, 'subsample': 0.732523574808224, 'colsample_bytree': 0.6270855451156074, 'reg_alpha': 9.907036893833194, 'reg_lambda': 6.214640525360671e-05, 'min_split_gain': 0.0907408176394649}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:34,910] Trial 112 finished with value: 0.25797939217121957 and parameters: {'learning_rate': 0.047796547709578696, 'num_leaves': 669, 'max_depth': 57, 'n_estimators': 838, 'min_child_samples': 59, 'subsample': 0.6967324562802555, 'colsample_bytree': 0.6351016364733504, 'reg_alpha': 9.484987555848997, 'reg_lambda': 4.115638988129496e-06, 'min_split_gain': 0.06332464243202188}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:37,018] Trial 108 finished with value: 0.25861309090885376 and parameters: {'learning_rate': 0.031152084047131427, 'num_leaves': 812, 'max_depth': 59, 'n_estimators': 807, 'min_child_samples': 60, 'subsample': 0.699810622010472, 'colsample_bytree': 0.5822068194562797, 'reg_alpha': 7.4247437810664385, 'reg_lambda': 5.242272903338943e-07, 'min_split_gain': 0.08210793091028085}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:41,047] Trial 5 finished with value: 0.2626295978557464 and parameters: {'learning_rate': 0.009195956969009407, 'num_leaves': 643, 'max_depth': 49, 'n_estimators': 860, 'min_child_samples': 44, 'subsample': 0.6263092430858135, 'colsample_bytree': 0.7828806085157263, 'reg_alpha': 2.2261120461160754e-08, 'reg_lambda': 1.1048670243045867e-07, 'min_split_gain': 0.03796733160423249}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:43,553] Trial 119 finished with value: 0.2568042524607783 and parameters: {'learning_rate': 0.04838380902931385, 'num_leaves': 724, 'max_depth': 63, 'n_estimators': 794, 'min_child_samples': 59, 'subsample': 0.6572491333866912, 'colsample_bytree': 0.5265095943664215, 'reg_alpha': 1.1460479670627046, 'reg_lambda': 0.2658128432970753, 'min_split_gain': 0.25413343064426225}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:45,240] Trial 109 finished with value: 0.25617160005228573 and parameters: {'learning_rate': 0.021145874927949324, 'num_leaves': 809, 'max_depth': 60, 'n_estimators': 820, 'min_child_samples': 93, 'subsample': 0.7015832296577057, 'colsample_bytree': 0.5258664140477984, 'reg_alpha': 6.2711996664944465, 'reg_lambda': 2.829684431746109e-06, 'min_split_gain': 0.11270835052889819}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:51,013] Trial 113 finished with value: 0.2587406312878287 and parameters: {'learning_rate': 0.021077197898923445, 'num_leaves': 664, 'max_depth': 57, 'n_estimators': 823, 'min_child_samples': 51, 'subsample': 0.7124169126631685, 'colsample_bytree': 0.5748029209992735, 'reg_alpha': 9.472636456497192, 'reg_lambda': 4.7224391231849794e-07, 'min_split_gain': 0.0931601105191808}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:53,624] Trial 75 finished with value: 0.8601173794159319 and parameters: {'learning_rate': 0.0006029422000764905, 'num_leaves': 923, 'max_depth': 55, 'n_estimators': 299, 'min_child_samples': 70, 'subsample': 0.6749786913676824, 'colsample_bytree': 0.6004556864563272, 'reg_alpha': 0.00023969730562771916, 'reg_lambda': 0.00038062214040891263, 'min_split_gain': 0.46637592812941564}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:56,571] Trial 115 finished with value: 0.2568113710719005 and parameters: {'learning_rate': 0.04734055540143052, 'num_leaves': 963, 'max_depth': 57, 'n_estimators': 815, 'min_child_samples': 48, 'subsample': 0.6642395193797945, 'colsample_bytree': 0.527303399870592, 'reg_alpha': 0.6356293232951149, 'reg_lambda': 8.03743350077564e-07, 'min_split_gain': 0.06260034805765709}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:58,460] Trial 55 finished with value: 0.2643971232916784 and parameters: {'learning_rate': 0.0033419856646296478, 'num_leaves': 938, 'max_depth': 46, 'n_estimators': 944, 'min_child_samples': 82, 'subsample': 0.6004452025130185, 'colsample_bytree': 0.5483539514605582, 'reg_alpha': 0.06156420650353637, 'reg_lambda': 0.02036193150611384, 'min_split_gain': 0.3297569641264398}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:05:59,080] Trial 56 finished with value: 0.2646578224386909 and parameters: {'learning_rate': 0.0032492465826761295, 'num_leaves': 1022, 'max_depth': 45, 'n_estimators': 941, 'min_child_samples': 75, 'subsample': 0.5866180991450023, 'colsample_bytree': 0.5454183437675572, 'reg_alpha': 0.05295815832221209, 'reg_lambda': 8.989285663230242e-05, 'min_split_gain': 0.37730846517832084}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:05,282] Trial 126 finished with value: 0.2592623832568969 and parameters: {'learning_rate': 0.07191614126771241, 'num_leaves': 755, 'max_depth': 64, 'n_estimators': 793, 'min_child_samples': 58, 'subsample': 0.6632063776793882, 'colsample_bytree': 0.6893691765024331, 'reg_alpha': 0.6200918283073439, 'reg_lambda': 0.5793528213633945, 'min_split_gain': 0.03685614744920884}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:07,450] Trial 120 finished with value: 0.25995827863391513 and parameters: {'learning_rate': 0.047093915888450126, 'num_leaves': 667, 'max_depth': 64, 'n_estimators': 801, 'min_child_samples': 58, 'subsample': 0.7176910610086012, 'colsample_bytree': 0.644696448271772, 'reg_alpha': 0.6725676026084626, 'reg_lambda': 6.195483274931384e-07, 'min_split_gain': 0.053493678738266714}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:09,328] Trial 121 finished with value: 0.258545450559606 and parameters: {'learning_rate': 0.02018531215495165, 'num_leaves': 747, 'max_depth': 57, 'n_estimators': 790, 'min_child_samples': 48, 'subsample': 0.6621527379611951, 'colsample_bytree': 0.5230870154563788, 'reg_alpha': 0.6091017412277828, 'reg_lambda': 0.0005827476773369787, 'min_split_gain': 0.26428793999301436}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:10,632] Trial 124 finished with value: 0.2602178914915524 and parameters: {'learning_rate': 0.020188373633891046, 'num_leaves': 741, 'max_depth': 63, 'n_estimators': 792, 'min_child_samples': 56, 'subsample': 0.6554829682013501, 'colsample_bytree': 0.5291600453325482, 'reg_alpha': 0.6357041219773847, 'reg_lambda': 0.24728633225651767, 'min_split_gain': 0.2683389550072338}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:12,947] Trial 65 finished with value: 0.7662727997750086 and parameters: {'learning_rate': 0.0006464626504292881, 'num_leaves': 936, 'max_depth': 55, 'n_estimators': 480, 'min_child_samples': 70, 'subsample': 0.6272437126216873, 'colsample_bytree': 0.5996004105271397, 'reg_alpha': 0.00021376503291557807, 'reg_lambda': 0.004574923759037856, 'min_split_gain': 0.44895205185013887}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:18,444] Trial 118 finished with value: 0.2580624832111727 and parameters: {'learning_rate': 0.020768013715373602, 'num_leaves': 664, 'max_depth': 51, 'n_estimators': 791, 'min_child_samples': 58, 'subsample': 0.6602462108050331, 'colsample_bytree': 0.5202589199354385, 'reg_alpha': 0.6809178486761477, 'reg_lambda': 0.33113298076926356, 'min_split_gain': 0.2576831434098112}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:20,452] Trial 131 finished with value: 0.26403391435681495 and parameters: {'learning_rate': 0.07271728473515165, 'num_leaves': 742, 'max_depth': 64, 'n_estimators': 773, 'min_child_samples': 97, 'subsample': 0.6632031097618716, 'colsample_bytree': 0.6441838329739129, 'reg_alpha': 2.070273819987724e-06, 'reg_lambda': 0.5701915827086652, 'min_split_gain': 0.04654714724837278}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:20,696] Trial 129 finished with value: 0.2637067735029131 and parameters: {'learning_rate': 0.06854020709913305, 'num_leaves': 730, 'max_depth': 57, 'n_estimators': 787, 'min_child_samples': 57, 'subsample': 0.6593227865300848, 'colsample_bytree': 0.6698416346773092, 'reg_alpha': 1.596863870378105e-06, 'reg_lambda': 0.20570240797894557, 'min_split_gain': 0.05404864394860556}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:23,323] Trial 14 finished with value: 0.7403159513757654 and parameters: {'learning_rate': 0.0005624473709807813, 'num_leaves': 750, 'max_depth': 45, 'n_estimators': 621, 'min_child_samples': 56, 'subsample': 0.9707062514402354, 'colsample_bytree': 0.8129460150632697, 'reg_alpha': 4.999093237524059e-06, 'reg_lambda': 0.00263158548041196, 'min_split_gain': 0.10530630677931552}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:25,845] Trial 132 finished with value: 0.25933724698156874 and parameters: {'learning_rate': 0.0712154677788644, 'num_leaves': 737, 'max_depth': 64, 'n_estimators': 781, 'min_child_samples': 57, 'subsample': 0.6635842457229133, 'colsample_bytree': 0.6758665084752178, 'reg_alpha': 1.314634148661077, 'reg_lambda': 0.5538847841605228, 'min_split_gain': 0.030539135131692506}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:26,279] Trial 137 finished with value: 0.258252533448348 and parameters: {'learning_rate': 0.06477376358627947, 'num_leaves': 731, 'max_depth': 51, 'n_estimators': 740, 'min_child_samples': 28, 'subsample': 0.6506769086483365, 'colsample_bytree': 0.527766985740978, 'reg_alpha': 1.219912894336979, 'reg_lambda': 1.3887498708947364e-06, 'min_split_gain': 0.1362320621528184}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:29,227] Trial 105 finished with value: 0.6611228177336489 and parameters: {'learning_rate': 0.0005795890908263303, 'num_leaves': 801, 'max_depth': 59, 'n_estimators': 862, 'min_child_samples': 94, 'subsample': 0.7068505309273607, 'colsample_bytree': 0.6266811403157951, 'reg_alpha': 9.899836671541825, 'reg_lambda': 4.6863833602518985e-05, 'min_split_gain': 0.18891828075395323}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:33,216] Trial 127 finished with value: 0.2590916132602113 and parameters: {'learning_rate': 0.06613450435573284, 'num_leaves': 760, 'max_depth': 64, 'n_estimators': 774, 'min_child_samples': 27, 'subsample': 0.6571210621649503, 'colsample_bytree': 0.6744876566096376, 'reg_alpha': 1.3668655734937208e-06, 'reg_lambda': 0.220783606249329, 'min_split_gain': 0.036344057693690444}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:33,582] Trial 138 finished with value: 0.25998469622968917 and parameters: {'learning_rate': 0.05644980121170084, 'num_leaves': 714, 'max_depth': 62, 'n_estimators': 768, 'min_child_samples': 29, 'subsample': 0.6480785458545397, 'colsample_bytree': 0.6660396844181625, 'reg_alpha': 2.46869764398356e-06, 'reg_lambda': 1.9772431514995176e-07, 'min_split_gain': 0.24534283374682792}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:33,971] Trial 15 finished with value: 0.558842038410271 and parameters: {'learning_rate': 0.0007373646928247672, 'num_leaves': 596, 'max_depth': 60, 'n_estimators': 935, 'min_child_samples': 31, 'subsample': 0.974176035578733, 'colsample_bytree': 0.5683334982208286, 'reg_alpha': 0.004159029824132446, 'reg_lambda': 8.54470537159394e-08, 'min_split_gain': 0.4798273883797699}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:35,294] Trial 140 finished with value: 0.25902725522623093 and parameters: {'learning_rate': 0.07039501934363221, 'num_leaves': 708, 'max_depth': 62, 'n_estimators': 746, 'min_child_samples': 31, 'subsample': 0.7439506935607528, 'colsample_bytree': 0.6611828088256549, 'reg_alpha': 1.2348308768493994, 'reg_lambda': 1.3727198015131992e-06, 'min_split_gain': 0.0662936410374309}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:38,615] Trial 146 finished with value: 0.2599358075152542 and parameters: {'learning_rate': 0.11904457028786512, 'num_leaves': 690, 'max_depth': 61, 'n_estimators': 689, 'min_child_samples': 66, 'subsample': 0.6421486528343279, 'colsample_bytree': 0.558104410285838, 'reg_alpha': 1.2901461486233576e-07, 'reg_lambda': 2.2441891559975335e-06, 'min_split_gain': 0.12601283206264932}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:42,396] Trial 141 finished with value: 0.2576266564458682 and parameters: {'learning_rate': 0.0645873026977505, 'num_leaves': 705, 'max_depth': 63, 'n_estimators': 746, 'min_child_samples': 30, 'subsample': 0.6502928220226892, 'colsample_bytree': 0.6782892755233314, 'reg_alpha': 1.395253670331068, 'reg_lambda': 1.8962662878661716e-06, 'min_split_gain': 0.07239597994128985}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:42,912] Trial 134 finished with value: 0.25818201558629483 and parameters: {'learning_rate': 0.06557862534898219, 'num_leaves': 738, 'max_depth': 63, 'n_estimators': 789, 'min_child_samples': 27, 'subsample': 0.660322926487604, 'colsample_bytree': 0.6783499130769198, 'reg_alpha': 1.1408416365399379, 'reg_lambda': 0.23901763529662234, 'min_split_gain': 0.02506924436457377}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:47,645] Trial 116 finished with value: 0.26184696681942565 and parameters: {'learning_rate': 0.02097044322946002, 'num_leaves': 660, 'max_depth': 51, 'n_estimators': 813, 'min_child_samples': 52, 'subsample': 0.6599195055700371, 'colsample_bytree': 0.646013117114895, 'reg_alpha': 0.6102306227785476, 'reg_lambda': 3.052638010766698e-07, 'min_split_gain': 0.036396730661410664}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:56,260] Trial 69 finished with value: 0.780843674325343 and parameters: {'learning_rate': 0.0005765729815298744, 'num_leaves': 914, 'max_depth': 55, 'n_estimators': 501, 'min_child_samples': 71, 'subsample': 0.5730340144530375, 'colsample_bytree': 0.5977006050387474, 'reg_alpha': 0.04644686928143438, 'reg_lambda': 0.0018549079712299494, 'min_split_gain': 0.28163615225943656}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:56,553] Trial 114 finished with value: 0.654900675686654 and parameters: {'learning_rate': 0.0006154234426802832, 'num_leaves': 754, 'max_depth': 64, 'n_estimators': 828, 'min_child_samples': 49, 'subsample': 0.7131157762743416, 'colsample_bytree': 0.644118542553612, 'reg_alpha': 8.88009231544874, 'reg_lambda': 4.5687001443431534e-05, 'min_split_gain': 0.10537837658337794}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:57,148] Trial 139 finished with value: 0.2604992212085751 and parameters: {'learning_rate': 0.06461889168490713, 'num_leaves': 715, 'max_depth': 62, 'n_estimators': 755, 'min_child_samples': 25, 'subsample': 0.7405523023663452, 'colsample_bytree': 0.6841925462316154, 'reg_alpha': 1.820817031345715e-06, 'reg_lambda': 1.7657353008301114e-06, 'min_split_gain': 0.07194162301034371}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:58,712] Trial 150 finished with value: 0.25736078513215743 and parameters: {'learning_rate': 0.04321432198066321, 'num_leaves': 694, 'max_depth': 61, 'n_estimators': 816, 'min_child_samples': 54, 'subsample': 0.6922693580462816, 'colsample_bytree': 0.5553150857233725, 'reg_alpha': 5.29223208773495, 'reg_lambda': 5.92565115935237e-06, 'min_split_gain': 0.07190270099670368}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:06:59,277] Trial 148 finished with value: 0.2618842335225131 and parameters: {'learning_rate': 0.04529826444934021, 'num_leaves': 706, 'max_depth': 62, 'n_estimators': 757, 'min_child_samples': 66, 'subsample': 0.7391951656061867, 'colsample_bytree': 0.5557020737883681, 'reg_alpha': 1.9915526017106463e-07, 'reg_lambda': 2.2142329486696697e-06, 'min_split_gain': 0.24508855291295198}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:07:02,450] Trial 95 finished with value: 0.6192062013646286 and parameters: {'learning_rate': 0.0006478644696248611, 'num_leaves': 797, 'max_depth': 59, 'n_estimators': 881, 'min_child_samples': 62, 'subsample': 0.6281942719731491, 'colsample_bytree': 0.6235066822971091, 'reg_alpha': 3.544406926549561, 'reg_lambda': 0.06451113860739349, 'min_split_gain': 0.20430453878871452}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:07:03,875] Trial 143 finished with value: 0.25726374662780266 and parameters: {'learning_rate': 0.05685661535287217, 'num_leaves': 708, 'max_depth': 61, 'n_estimators': 754, 'min_child_samples': 22, 'subsample': 0.7405639451864935, 'colsample_bytree': 0.5565856807082508, 'reg_alpha': 1.1665113989682843e-08, 'reg_lambda': 2.9296887821583857e-06, 'min_split_gain': 0.16401586098403753}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:07:04,197] Trial 156 finished with value: 0.269524004581013 and parameters: {'learning_rate': 0.04375744537483813, 'num_leaves': 840, 'max_depth': 4, 'n_estimators': 839, 'min_child_samples': 40, 'subsample': 0.6886591448387146, 'colsample_bytree': 0.5128969332608254, 'reg_alpha': 1.814983669042013, 'reg_lambda': 6.356943184644695e-06, 'min_split_gain': 0.008949169252394334}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:07:10,392] Trial 142 finished with value: 0.2587353930649833 and parameters: {'learning_rate': 0.06132113874901215, 'num_leaves': 708, 'max_depth': 61, 'n_estimators': 687, 'min_child_samples': 30, 'subsample': 0.7456100020980463, 'colsample_bytree': 0.6684355111928738, 'reg_alpha': 1.9431269246000255e-07, 'reg_lambda': 2.111601426627354e-06, 'min_split_gain': 0.06798622517510883}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:07:11,406] Trial 122 finished with value: 0.25788312613293035 and parameters: {'learning_rate': 0.020038403825283244, 'num_leaves': 963, 'max_depth': 51, 'n_estimators': 755, 'min_child_samples': 29, 'subsample': 0.6621783490378055, 'colsample_bytree': 0.527691703988992, 'reg_alpha': 0.6339353968860801, 'reg_lambda': 0.0005625334949318653, 'min_split_gain': 0.06350766803707453}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:07:20,657] Trial 125 finished with value: 0.26116146042272853 and parameters: {'learning_rate': 0.020794009711932397, 'num_leaves': 745, 'max_depth': 51, 'n_estimators': 787, 'min_child_samples': 57, 'subsample': 0.6608030011719473, 'colsample_bytree': 0.6696117634131533, 'reg_alpha': 1.0258549266863391e-08, 'reg_lambda': 0.00025410074571084913, 'min_split_gain': 0.06740745661500525}. Best is trial 87 with value: 0.2558432304289068.\n",
      "[I 2025-07-14 21:07:22,485] Trial 130 finished with value: 0.25429919292904574 and parameters: {'learning_rate': 0.06930646678010728, 'num_leaves': 746, 'max_depth': 64, 'n_estimators': 770, 'min_child_samples': 57, 'subsample': 0.6645214813807224, 'colsample_bytree': 0.6918292176674755, 'reg_alpha': 1.1177566110590431, 'reg_lambda': 0.2938358715251245, 'min_split_gain': 0.0005861337905412878}. Best is trial 130 with value: 0.25429919292904574.\n",
      "[I 2025-07-14 21:07:26,231] Trial 136 finished with value: 0.2522471730040013 and parameters: {'learning_rate': 0.06608454999500171, 'num_leaves': 722, 'max_depth': 63, 'n_estimators': 780, 'min_child_samples': 27, 'subsample': 0.6592398555480282, 'colsample_bytree': 0.5263586130926626, 'reg_alpha': 1.347517240038013, 'reg_lambda': 1.9650876158139027e-07, 'min_split_gain': 0.005058946434738559}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:30,746] Trial 151 finished with value: 0.2593336085234713 and parameters: {'learning_rate': 0.025714953922290795, 'num_leaves': 835, 'max_depth': 50, 'n_estimators': 364, 'min_child_samples': 66, 'subsample': 0.6895407758208648, 'colsample_bytree': 0.5556009452678494, 'reg_alpha': 2.107256925425672e-07, 'reg_lambda': 2.5620736209297423e-06, 'min_split_gain': 0.16089563383167046}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:33,683] Trial 135 finished with value: 0.2549865847244518 and parameters: {'learning_rate': 0.0668359803366541, 'num_leaves': 750, 'max_depth': 63, 'n_estimators': 767, 'min_child_samples': 29, 'subsample': 0.6593663610733698, 'colsample_bytree': 0.6835724521537694, 'reg_alpha': 0.6209501303296405, 'reg_lambda': 0.31158483631004763, 'min_split_gain': 0.006013038663348194}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:34,878] Trial 133 finished with value: 0.25768368301627786 and parameters: {'learning_rate': 0.06269485681739104, 'num_leaves': 749, 'max_depth': 63, 'n_estimators': 762, 'min_child_samples': 30, 'subsample': 0.6608356640980908, 'colsample_bytree': 0.5261102719793678, 'reg_alpha': 0.5279536546965928, 'reg_lambda': 0.42602139656285537, 'min_split_gain': 0.00716099507316155}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:36,788] Trial 164 finished with value: 0.2613073086755231 and parameters: {'learning_rate': 0.054009445544337224, 'num_leaves': 965, 'max_depth': 56, 'n_estimators': 956, 'min_child_samples': 17, 'subsample': 0.6425608775357419, 'colsample_bytree': 0.70372439115464, 'reg_alpha': 1.4275807603522687e-08, 'reg_lambda': 8.905212159238713, 'min_split_gain': 0.1596285320432741}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:38,417] Trial 160 finished with value: 0.26069068722788735 and parameters: {'learning_rate': 0.042586485094790655, 'num_leaves': 833, 'max_depth': 56, 'n_estimators': 369, 'min_child_samples': 35, 'subsample': 0.69052400274703, 'colsample_bytree': 0.7202403270057752, 'reg_alpha': 5.3059545474632414e-08, 'reg_lambda': 7.997026283162473e-06, 'min_split_gain': 0.16412434301487575}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:41,168] Trial 165 finished with value: 0.25743793333106973 and parameters: {'learning_rate': 0.02608662904154839, 'num_leaves': 835, 'max_depth': 56, 'n_estimators': 397, 'min_child_samples': 20, 'subsample': 0.6152613905582242, 'colsample_bytree': 0.6147647587515859, 'reg_alpha': 5.2007601111599815, 'reg_lambda': 9.68446149026575, 'min_split_gain': 0.16604285736307844}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:45,307] Trial 167 finished with value: 0.2581835630856835 and parameters: {'learning_rate': 0.08527369674416073, 'num_leaves': 626, 'max_depth': 61, 'n_estimators': 729, 'min_child_samples': 21, 'subsample': 0.6899140090918267, 'colsample_bytree': 0.5149692684806457, 'reg_alpha': 7.209048210834317e-08, 'reg_lambda': 9.381112351918102e-06, 'min_split_gain': 0.155437528678955}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:47,140] Trial 170 finished with value: 0.2587373476509011 and parameters: {'learning_rate': 0.12359916066576043, 'num_leaves': 618, 'max_depth': 61, 'n_estimators': 722, 'min_child_samples': 21, 'subsample': 0.6443087318322478, 'colsample_bytree': 0.7264570903531841, 'reg_alpha': 5.9112276214780836e-08, 'reg_lambda': 1.836349392903202, 'min_split_gain': 0.15620704748017292}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:52,305] Trial 166 finished with value: 0.25935831362606226 and parameters: {'learning_rate': 0.025729582330141755, 'num_leaves': 618, 'max_depth': 56, 'n_estimators': 377, 'min_child_samples': 17, 'subsample': 0.6161811860500898, 'colsample_bytree': 0.7478115606463519, 'reg_alpha': 4.909631815396184, 'reg_lambda': 9.238092571539659e-06, 'min_split_gain': 0.1708167888538467}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:55,441] Trial 158 finished with value: 0.2586897403567982 and parameters: {'learning_rate': 0.04228515197528126, 'num_leaves': 836, 'max_depth': 50, 'n_estimators': 843, 'min_child_samples': 20, 'subsample': 0.6929911791901164, 'colsample_bytree': 0.6995143362269893, 'reg_alpha': 5.336861787497422, 'reg_lambda': 9.316307136748995e-06, 'min_split_gain': 0.004358490817058913}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:56,677] Trial 145 finished with value: 0.2574549229542652 and parameters: {'learning_rate': 0.04292702627366254, 'num_leaves': 705, 'max_depth': 62, 'n_estimators': 749, 'min_child_samples': 32, 'subsample': 0.645076394770794, 'colsample_bytree': 0.6162473243783645, 'reg_alpha': 1.409301505131954, 'reg_lambda': 2.1589118184501405e-07, 'min_split_gain': 0.012148024447287936}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:57,210] Trial 176 finished with value: 0.25883867526463855 and parameters: {'learning_rate': 0.09990084301458298, 'num_leaves': 777, 'max_depth': 60, 'n_estimators': 810, 'min_child_samples': 19, 'subsample': 0.7247056978453932, 'colsample_bytree': 0.5674222055329817, 'reg_alpha': 5.5118621040700715, 'reg_lambda': 9.347585901928256e-07, 'min_split_gain': 0.12650444888869394}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:07:59,395] Trial 177 finished with value: 0.26275586567645776 and parameters: {'learning_rate': 0.09213255911749128, 'num_leaves': 562, 'max_depth': 60, 'n_estimators': 420, 'min_child_samples': 19, 'subsample': 0.7216543444544046, 'colsample_bytree': 0.5678054058599732, 'reg_alpha': 2.2962181952003364, 'reg_lambda': 0.9218190586882449, 'min_split_gain': 0.522620816973843}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:00,167] Trial 162 finished with value: 0.2582865295353514 and parameters: {'learning_rate': 0.025835133691652523, 'num_leaves': 965, 'max_depth': 61, 'n_estimators': 355, 'min_child_samples': 16, 'subsample': 0.6876899504415965, 'colsample_bytree': 0.6134770618572606, 'reg_alpha': 1.2590277035005868e-08, 'reg_lambda': 9.267998394502309, 'min_split_gain': 0.162643273512908}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:04,267] Trial 155 finished with value: 0.2588121580600022 and parameters: {'learning_rate': 0.026031585917190524, 'num_leaves': 837, 'max_depth': 56, 'n_estimators': 353, 'min_child_samples': 20, 'subsample': 0.6158837456491276, 'colsample_bytree': 0.612421661917116, 'reg_alpha': 1.3177149561722772e-08, 'reg_lambda': 2.071967292534024e-06, 'min_split_gain': 0.16190938032236976}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:05,141] Trial 159 finished with value: 0.25813033910712085 and parameters: {'learning_rate': 0.04210890344690839, 'num_leaves': 836, 'max_depth': 61, 'n_estimators': 346, 'min_child_samples': 34, 'subsample': 0.6912136253444725, 'colsample_bytree': 0.6144372966798956, 'reg_alpha': 1.6813651792024902, 'reg_lambda': 5.881058675751007e-06, 'min_split_gain': 0.0009578493528303034}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:06,998] Trial 117 finished with value: 0.2570619329553077 and parameters: {'learning_rate': 0.019652995601449584, 'num_leaves': 735, 'max_depth': 64, 'n_estimators': 815, 'min_child_samples': 56, 'subsample': 0.6573377791816706, 'colsample_bytree': 0.7951583624660923, 'reg_alpha': 0.7045969184517, 'reg_lambda': 4.6294181298226e-05, 'min_split_gain': 0.019347727734719378}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:08,258] Trial 178 finished with value: 0.25850108770002506 and parameters: {'learning_rate': 0.09854061521785987, 'num_leaves': 771, 'max_depth': 60, 'n_estimators': 415, 'min_child_samples': 24, 'subsample': 0.7231167860633514, 'colsample_bytree': 0.7373559477805608, 'reg_alpha': 1.6383702314141813, 'reg_lambda': 0.9889937429796563, 'min_split_gain': 0.12365455167461413}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:12,788] Trial 144 finished with value: 0.26178218438019807 and parameters: {'learning_rate': 0.05658988118014788, 'num_leaves': 695, 'max_depth': 62, 'n_estimators': 370, 'min_child_samples': 26, 'subsample': 0.7435514598619057, 'colsample_bytree': 0.5133924827261171, 'reg_alpha': 3.671377780091894e-08, 'reg_lambda': 1.6253039146618904e-07, 'min_split_gain': 0.015132669492197423}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:13,489] Trial 175 finished with value: 0.2591245746481553 and parameters: {'learning_rate': 0.11443022850128921, 'num_leaves': 555, 'max_depth': 60, 'n_estimators': 430, 'min_child_samples': 18, 'subsample': 0.613235205195378, 'colsample_bytree': 0.6122501857365358, 'reg_alpha': 5.552786558632408, 'reg_lambda': 0.046535927166481646, 'min_split_gain': 0.0072826325425064675}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:14,657] Trial 163 finished with value: 0.25946603712443905 and parameters: {'learning_rate': 0.025416429586861872, 'num_leaves': 637, 'max_depth': 61, 'n_estimators': 361, 'min_child_samples': 21, 'subsample': 0.6888044429753023, 'colsample_bytree': 0.5680598811661707, 'reg_alpha': 4.1717222932940584e-08, 'reg_lambda': 8.542836270010745e-06, 'min_split_gain': 0.17030815033082522}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:20,078] Trial 128 finished with value: 0.25716579447944476 and parameters: {'learning_rate': 0.020553311612283322, 'num_leaves': 750, 'max_depth': 64, 'n_estimators': 941, 'min_child_samples': 57, 'subsample': 0.7420048023184183, 'colsample_bytree': 0.6471240694097437, 'reg_alpha': 1.3225801702394384, 'reg_lambda': 2.1578528769052514e-07, 'min_split_gain': 0.011055446397863267}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:22,265] Trial 179 finished with value: 0.2596697516036618 and parameters: {'learning_rate': 0.08178266943936269, 'num_leaves': 686, 'max_depth': 60, 'n_estimators': 438, 'min_child_samples': 33, 'subsample': 0.6203865075149284, 'colsample_bytree': 0.6159948084931836, 'reg_alpha': 5.7788287986776946e-05, 'reg_lambda': 3.5500849702555853e-07, 'min_split_gain': 0.12924932190766278}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:24,555] Trial 13 finished with value: 0.26430643256796116 and parameters: {'learning_rate': 0.003402775178454514, 'num_leaves': 663, 'max_depth': 27, 'n_estimators': 927, 'min_child_samples': 31, 'subsample': 0.9031266730370189, 'colsample_bytree': 0.8885019608037803, 'reg_alpha': 3.4058943291574237e-08, 'reg_lambda': 0.15542261984276376, 'min_split_gain': 0.12391720717629806}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:26,940] Trial 184 finished with value: 0.26089909741769907 and parameters: {'learning_rate': 0.07884083046885039, 'num_leaves': 688, 'max_depth': 63, 'n_estimators': 441, 'min_child_samples': 55, 'subsample': 0.670722220500974, 'colsample_bytree': 0.5076472201506363, 'reg_alpha': 3.8889703796592965e-05, 'reg_lambda': 1.481232648416727e-07, 'min_split_gain': 0.1232277476696912}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:31,310] Trial 186 finished with value: 0.2569850696570351 and parameters: {'learning_rate': 0.05326033015853352, 'num_leaves': 692, 'max_depth': 63, 'n_estimators': 442, 'min_child_samples': 54, 'subsample': 0.6736680956144275, 'colsample_bytree': 0.8072840883865952, 'reg_alpha': 2.2965245670803887, 'reg_lambda': 0.04880192730422352, 'min_split_gain': 0.08302867919800963}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:40,109] Trial 30 finished with value: 0.25943759464701566 and parameters: {'learning_rate': 0.006613887080420262, 'num_leaves': 645, 'max_depth': 24, 'n_estimators': 602, 'min_child_samples': 25, 'subsample': 0.720663068391378, 'colsample_bytree': 0.7310120159774189, 'reg_alpha': 0.004803788959149956, 'reg_lambda': 0.18604329463051905, 'min_split_gain': 0.0385600329314274}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:42,240] Trial 185 finished with value: 0.2576861441593396 and parameters: {'learning_rate': 0.05514077769921454, 'num_leaves': 775, 'max_depth': 63, 'n_estimators': 415, 'min_child_samples': 24, 'subsample': 0.6692312669600315, 'colsample_bytree': 0.800085953770433, 'reg_alpha': 3.5593232266617286e-08, 'reg_lambda': 2.3398893740775892, 'min_split_gain': 0.11778245224313588}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:42,961] Trial 183 finished with value: 0.25851151203984685 and parameters: {'learning_rate': 0.05129786177495519, 'num_leaves': 686, 'max_depth': 62, 'n_estimators': 707, 'min_child_samples': 24, 'subsample': 0.6392089120407499, 'colsample_bytree': 0.7333118805105201, 'reg_alpha': 2.6839006823965274e-08, 'reg_lambda': 1.7341669598374025, 'min_split_gain': 0.1129677233092883}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:46,784] Trial 190 finished with value: 0.26320475923031966 and parameters: {'learning_rate': 0.08232075007345097, 'num_leaves': 777, 'max_depth': 63, 'n_estimators': 934, 'min_child_samples': 90, 'subsample': 0.7568052270553836, 'colsample_bytree': 0.8342369989716064, 'reg_alpha': 0.3499641672132418, 'reg_lambda': 3.168335684068423e-07, 'min_split_gain': 0.04312366161587079}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:47,815] Trial 161 finished with value: 0.2543697888806863 and parameters: {'learning_rate': 0.025536080882077195, 'num_leaves': 967, 'max_depth': 56, 'n_estimators': 842, 'min_child_samples': 54, 'subsample': 0.6910960107793284, 'colsample_bytree': 0.7041599572818594, 'reg_alpha': 5.370756498186625, 'reg_lambda': 8.848439407443374, 'min_split_gain': 0.005994030130636818}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:49,674] Trial 147 finished with value: 0.2567295628623663 and parameters: {'learning_rate': 0.0451690827133005, 'num_leaves': 835, 'max_depth': 62, 'n_estimators': 759, 'min_child_samples': 66, 'subsample': 0.6911911639182676, 'colsample_bytree': 0.6167763787525822, 'reg_alpha': 8.881752355348869e-08, 'reg_lambda': 2.735329373127828e-07, 'min_split_gain': 0.0025196064653930927}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:08:55,459] Trial 173 finished with value: 0.2560755993803716 and parameters: {'learning_rate': 0.08407655439121473, 'num_leaves': 770, 'max_depth': 61, 'n_estimators': 723, 'min_child_samples': 20, 'subsample': 0.6187896314101904, 'colsample_bytree': 0.7406945533509885, 'reg_alpha': 1.8654351398340816, 'reg_lambda': 1.2028294122999643, 'min_split_gain': 0.004462804435298312}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:00,493] Trial 182 finished with value: 0.2571382992936916 and parameters: {'learning_rate': 0.05308239988048665, 'num_leaves': 770, 'max_depth': 60, 'n_estimators': 436, 'min_child_samples': 24, 'subsample': 0.612193937234986, 'colsample_bytree': 0.6113717110968868, 'reg_alpha': 1.7377604095192396, 'reg_lambda': 1.6155009518736166e-07, 'min_split_gain': 0.01764700362518675}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:01,137] Trial 187 finished with value: 0.26098187993119865 and parameters: {'learning_rate': 0.05225817601440385, 'num_leaves': 764, 'max_depth': 63, 'n_estimators': 448, 'min_child_samples': 55, 'subsample': 0.6758044083046192, 'colsample_bytree': 0.8341880899110429, 'reg_alpha': 0.28470892146907756, 'reg_lambda': 0.14103526358243854, 'min_split_gain': 0.044761574813182076}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:02,111] Trial 172 finished with value: 0.25994362826009804 and parameters: {'learning_rate': 0.0842443100904807, 'num_leaves': 606, 'max_depth': 61, 'n_estimators': 724, 'min_child_samples': 19, 'subsample': 0.6174721526930119, 'colsample_bytree': 0.7371558019855556, 'reg_alpha': 2.6363481620720795e-08, 'reg_lambda': 1.1051740417191498, 'min_split_gain': 0.012307917574443475}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:05,480] Trial 154 finished with value: 0.2554573859856607 and parameters: {'learning_rate': 0.044156978265162225, 'num_leaves': 963, 'max_depth': 56, 'n_estimators': 817, 'min_child_samples': 18, 'subsample': 0.6896503887485453, 'colsample_bytree': 0.6125154344143547, 'reg_alpha': 5.287742132952109, 'reg_lambda': 9.807146490462545, 'min_split_gain': 0.0003378691243044324}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:07,644] Trial 201 finished with value: 0.26264802488014205 and parameters: {'learning_rate': 0.17581905526411762, 'num_leaves': 891, 'max_depth': 64, 'n_estimators': 806, 'min_child_samples': 46, 'subsample': 0.7322372760989002, 'colsample_bytree': 0.7691420083319673, 'reg_alpha': 2.7944303666562402, 'reg_lambda': 6.474975429091643e-08, 'min_split_gain': 0.025369384134757526}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:10,737] Trial 123 finished with value: 0.25863671275985634 and parameters: {'learning_rate': 0.020873116197380356, 'num_leaves': 746, 'max_depth': 57, 'n_estimators': 786, 'min_child_samples': 48, 'subsample': 0.6556559521446838, 'colsample_bytree': 0.685250376448468, 'reg_alpha': 1.5794223108732896e-06, 'reg_lambda': 0.2303270486829449, 'min_split_gain': 0.024653590858615182}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:12,559] Trial 196 finished with value: 0.2568291337068283 and parameters: {'learning_rate': 0.05238406311534304, 'num_leaves': 769, 'max_depth': 64, 'n_estimators': 819, 'min_child_samples': 55, 'subsample': 0.7589580504038537, 'colsample_bytree': 0.8168068726552989, 'reg_alpha': 2.7341761682141645, 'reg_lambda': 1.0037993759834193e-07, 'min_split_gain': 0.0487407298372635}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:15,223] Trial 193 finished with value: 0.26243529113471226 and parameters: {'learning_rate': 0.05099509828499616, 'num_leaves': 773, 'max_depth': 63, 'n_estimators': 813, 'min_child_samples': 55, 'subsample': 0.7589371489849223, 'colsample_bytree': 0.8072248740042728, 'reg_alpha': 0.41098448170024765, 'reg_lambda': 6.432577768169727e-08, 'min_split_gain': 0.04496283764184418}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:16,478] Trial 189 finished with value: 0.2616181998681115 and parameters: {'learning_rate': 0.05225915277744719, 'num_leaves': 770, 'max_depth': 63, 'n_estimators': 397, 'min_child_samples': 90, 'subsample': 0.640853240508831, 'colsample_bytree': 0.7910260250331608, 'reg_alpha': 3.3763213849528006e-05, 'reg_lambda': 2.9788042932172856e-07, 'min_split_gain': 0.02671517686874886}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:19,222] Trial 171 finished with value: 0.25917349816799923 and parameters: {'learning_rate': 0.0862749809645042, 'num_leaves': 604, 'max_depth': 61, 'n_estimators': 721, 'min_child_samples': 19, 'subsample': 0.6447553029504066, 'colsample_bytree': 0.7183206695923915, 'reg_alpha': 2.4711291155988326e-08, 'reg_lambda': 1.339477541760721, 'min_split_gain': 0.0059611650188762305}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:21,119] Trial 169 finished with value: 0.257583054778313 and parameters: {'learning_rate': 0.11051938139321893, 'num_leaves': 617, 'max_depth': 61, 'n_estimators': 728, 'min_child_samples': 18, 'subsample': 0.6884134102004649, 'colsample_bytree': 0.7992754878911545, 'reg_alpha': 4.215781337655389e-08, 'reg_lambda': 8.970998190496172, 'min_split_gain': 0.003210253413930726}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:24,849] Trial 195 finished with value: 0.26170492341249374 and parameters: {'learning_rate': 0.05403014769958889, 'num_leaves': 765, 'max_depth': 63, 'n_estimators': 813, 'min_child_samples': 54, 'subsample': 0.7596367547543526, 'colsample_bytree': 0.7946353765523974, 'reg_alpha': 0.3606879224887005, 'reg_lambda': 9.004211235576815e-08, 'min_split_gain': 0.045620133286317026}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:26,776] Trial 188 finished with value: 0.2604398196328097 and parameters: {'learning_rate': 0.04870066624935672, 'num_leaves': 772, 'max_depth': 63, 'n_estimators': 822, 'min_child_samples': 22, 'subsample': 0.6398633128338767, 'colsample_bytree': 0.7879077312912063, 'reg_alpha': 0.34651474296577184, 'reg_lambda': 2.332030502953691e-07, 'min_split_gain': 0.03157641188753821}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:28,576] Trial 197 finished with value: 0.2621014608354603 and parameters: {'learning_rate': 0.051249581293973716, 'num_leaves': 767, 'max_depth': 64, 'n_estimators': 929, 'min_child_samples': 54, 'subsample': 0.7713775067518172, 'colsample_bytree': 0.8638605635261718, 'reg_alpha': 0.2997660669112804, 'reg_lambda': 8.690095995174425e-08, 'min_split_gain': 0.08688856458591852}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:30,500] Trial 149 finished with value: 0.2563983762792642 and parameters: {'learning_rate': 0.025150640043490737, 'num_leaves': 697, 'max_depth': 51, 'n_estimators': 352, 'min_child_samples': 54, 'subsample': 0.7438243538247379, 'colsample_bytree': 0.5138549374243282, 'reg_alpha': 1.666978261041374e-08, 'reg_lambda': 9.676070640583834, 'min_split_gain': 0.009733470424067026}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:38,243] Trial 180 finished with value: 0.2576146449557002 and parameters: {'learning_rate': 0.05085620096910962, 'num_leaves': 535, 'max_depth': 60, 'n_estimators': 413, 'min_child_samples': 23, 'subsample': 0.6160417602440755, 'colsample_bytree': 0.6166851638261402, 'reg_alpha': 6.945316899295646e-07, 'reg_lambda': 1.2357143872553287, 'min_split_gain': 0.02177560799235978}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:39,700] Trial 192 finished with value: 0.2552268122558176 and parameters: {'learning_rate': 0.01862241477116165, 'num_leaves': 771, 'max_depth': 63, 'n_estimators': 456, 'min_child_samples': 90, 'subsample': 0.6709307279590518, 'colsample_bytree': 0.5487152151406398, 'reg_alpha': 2.8942791151029885, 'reg_lambda': 0.043374728150970175, 'min_split_gain': 0.03492870306485875}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:44,085] Trial 152 finished with value: 0.2562417671610969 and parameters: {'learning_rate': 0.04353890554291967, 'num_leaves': 961, 'max_depth': 51, 'n_estimators': 839, 'min_child_samples': 54, 'subsample': 0.6894604418234078, 'colsample_bytree': 0.5159427783529309, 'reg_alpha': 5.177997559228846e-08, 'reg_lambda': 9.98900690975809, 'min_split_gain': 0.006009874715075157}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:50,968] Trial 199 finished with value: 0.25732333241754524 and parameters: {'learning_rate': 0.017886975456776873, 'num_leaves': 883, 'max_depth': 64, 'n_estimators': 810, 'min_child_samples': 54, 'subsample': 0.7673592022481179, 'colsample_bytree': 0.8055982005986357, 'reg_alpha': 2.604207973842122, 'reg_lambda': 0.14757331396462575, 'min_split_gain': 0.08481740577609323}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:54,908] Trial 157 finished with value: 0.25768018442749485 and parameters: {'learning_rate': 0.04653442303846148, 'num_leaves': 960, 'max_depth': 61, 'n_estimators': 959, 'min_child_samples': 20, 'subsample': 0.6886986870184425, 'colsample_bytree': 0.613292593552642, 'reg_alpha': 1.1561040195581917e-08, 'reg_lambda': 9.750046520010891e-06, 'min_split_gain': 0.008674163295744601}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:09:58,495] Trial 212 finished with value: 0.26229843357758786 and parameters: {'learning_rate': 0.05884008915760126, 'num_leaves': 987, 'max_depth': 64, 'n_estimators': 455, 'min_child_samples': 60, 'subsample': 0.6714136130954784, 'colsample_bytree': 0.8173264024223799, 'reg_alpha': 0.8436867942343832, 'reg_lambda': 8.3500158881218e-07, 'min_split_gain': 0.026161325332874838}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:02,330] Trial 208 finished with value: 0.2574945668481503 and parameters: {'learning_rate': 0.055746457812592284, 'num_leaves': 991, 'max_depth': 64, 'n_estimators': 820, 'min_child_samples': 13, 'subsample': 0.6707134579598001, 'colsample_bytree': 0.789265766014119, 'reg_alpha': 0.9951407367066929, 'reg_lambda': 8.739865292880908e-08, 'min_split_gain': 0.03452820310272578}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:02,826] Trial 198 finished with value: 0.25840029775624035 and parameters: {'learning_rate': 0.022917039927365802, 'num_leaves': 771, 'max_depth': 64, 'n_estimators': 391, 'min_child_samples': 53, 'subsample': 0.7666347830665632, 'colsample_bytree': 0.764633670408581, 'reg_alpha': 2.884195893009499, 'reg_lambda': 0.043545111378365794, 'min_split_gain': 0.025602680105949134}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:14,990] Trial 200 finished with value: 0.2584438307992439 and parameters: {'learning_rate': 0.01768953136566978, 'num_leaves': 889, 'max_depth': 64, 'n_estimators': 813, 'min_child_samples': 54, 'subsample': 0.7344566819694267, 'colsample_bytree': 0.7880625186852933, 'reg_alpha': 2.788589830990682, 'reg_lambda': 7.47887549919134e-07, 'min_split_gain': 0.03326151469650811}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:19,436] Trial 216 finished with value: 0.25865144765452547 and parameters: {'learning_rate': 0.07369584726493715, 'num_leaves': 989, 'max_depth': 53, 'n_estimators': 455, 'min_child_samples': 10, 'subsample': 0.6691280851027667, 'colsample_bytree': 0.6529572703256682, 'reg_alpha': 0.9507912595698041, 'reg_lambda': 3.808487106453542, 'min_split_gain': 0.02619436353607269}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:24,356] Trial 168 finished with value: 0.2593664911388414 and parameters: {'learning_rate': 0.09031580314839924, 'num_leaves': 837, 'max_depth': 61, 'n_estimators': 730, 'min_child_samples': 19, 'subsample': 0.6885545220844644, 'colsample_bytree': 0.7467394450731815, 'reg_alpha': 6.018863283173106e-08, 'reg_lambda': 1.0297349184969666e-05, 'min_split_gain': 0.0009302390497212495}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:24,603] Trial 174 finished with value: 0.2523797469271051 and parameters: {'learning_rate': 0.08307740933382936, 'num_leaves': 774, 'max_depth': 61, 'n_estimators': 427, 'min_child_samples': 20, 'subsample': 0.6173982523593531, 'colsample_bytree': 0.5661543389583997, 'reg_alpha': 3.926117738831247e-05, 'reg_lambda': 2.2488452007703166, 'min_split_gain': 1.2595124108996311e-05}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:27,385] Trial 223 finished with value: 0.2570018739138047 and parameters: {'learning_rate': 0.07555164742402928, 'num_leaves': 732, 'max_depth': 58, 'n_estimators': 464, 'min_child_samples': 50, 'subsample': 0.6762339035998326, 'colsample_bytree': 0.5181063501733963, 'reg_alpha': 2.4245967662871655, 'reg_lambda': 5.2097842447663805, 'min_split_gain': 0.054519469339066984}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:38,124] Trial 194 finished with value: 0.25882077642454066 and parameters: {'learning_rate': 0.0181299544885761, 'num_leaves': 767, 'max_depth': 63, 'n_estimators': 928, 'min_child_samples': 53, 'subsample': 0.7340131064105284, 'colsample_bytree': 0.5450383903216292, 'reg_alpha': 0.4090721144682991, 'reg_lambda': 7.05886329358521e-08, 'min_split_gain': 0.0453654683355319}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:44,812] Trial 203 finished with value: 0.2573948607083614 and parameters: {'learning_rate': 0.018094993343287333, 'num_leaves': 880, 'max_depth': 64, 'n_estimators': 467, 'min_child_samples': 49, 'subsample': 0.6792333775873519, 'colsample_bytree': 0.7981149786872791, 'reg_alpha': 2.681350944053993, 'reg_lambda': 5.087328708374533, 'min_split_gain': 0.027082063231717458}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:51,193] Trial 204 finished with value: 0.2575520845995862 and parameters: {'learning_rate': 0.015157417548646374, 'num_leaves': 722, 'max_depth': 35, 'n_estimators': 810, 'min_child_samples': 46, 'subsample': 0.669240018121812, 'colsample_bytree': 0.7858887657444309, 'reg_alpha': 2.6944200383662062, 'reg_lambda': 5.904826037261025e-08, 'min_split_gain': 0.026465881935554623}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:59,173] Trial 222 finished with value: 0.2574358264915181 and parameters: {'learning_rate': 0.02300060727063545, 'num_leaves': 953, 'max_depth': 53, 'n_estimators': 474, 'min_child_samples': 52, 'subsample': 0.6809425300211466, 'colsample_bytree': 0.8210940680652972, 'reg_alpha': 2.569226272877487, 'reg_lambda': 5.041520691023902, 'min_split_gain': 0.052862060253649035}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:10:59,819] Trial 230 finished with value: 0.26419619404702066 and parameters: {'learning_rate': 0.10328194748516044, 'num_leaves': 721, 'max_depth': 58, 'n_estimators': 475, 'min_child_samples': 50, 'subsample': 0.6780979089209276, 'colsample_bytree': 0.8212166270697188, 'reg_alpha': 0.0008701067449147529, 'reg_lambda': 5.466356115018521, 'min_split_gain': 0.06253424417342336}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:01,285] Trial 229 finished with value: 0.2573586165093111 and parameters: {'learning_rate': 0.07029032340248201, 'num_leaves': 726, 'max_depth': 53, 'n_estimators': 468, 'min_child_samples': 51, 'subsample': 0.8059164102976408, 'colsample_bytree': 0.5312363343205305, 'reg_alpha': 8.704210558094443e-08, 'reg_lambda': 4.343040276319607, 'min_split_gain': 0.05890504853656051}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:03,066] Trial 227 finished with value: 0.25746077028114633 and parameters: {'learning_rate': 0.03636628495384749, 'num_leaves': 723, 'max_depth': 48, 'n_estimators': 472, 'min_child_samples': 51, 'subsample': 0.6796102585468966, 'colsample_bytree': 0.8172256181790178, 'reg_alpha': 2.206052681070754, 'reg_lambda': 4.6440663267732365, 'min_split_gain': 0.06330838107634575}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:03,450] Trial 191 finished with value: 0.2600533940556664 and parameters: {'learning_rate': 0.014304916713951454, 'num_leaves': 769, 'max_depth': 63, 'n_estimators': 931, 'min_child_samples': 55, 'subsample': 0.9139997653941931, 'colsample_bytree': 0.757592209300812, 'reg_alpha': 0.32410799279640934, 'reg_lambda': 2.628671906379358, 'min_split_gain': 0.042956451845273504}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:05,481] Trial 228 finished with value: 0.25807144264215476 and parameters: {'learning_rate': 0.037672198244076756, 'num_leaves': 732, 'max_depth': 34, 'n_estimators': 850, 'min_child_samples': 49, 'subsample': 0.5978390098757453, 'colsample_bytree': 0.8179909533980337, 'reg_alpha': 2.0205176373620093, 'reg_lambda': 4.579461350568861, 'min_split_gain': 0.05453554173954716}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:06,957] Trial 233 finished with value: 0.26283715488981885 and parameters: {'learning_rate': 0.07563811248623321, 'num_leaves': 726, 'max_depth': 50, 'n_estimators': 516, 'min_child_samples': 50, 'subsample': 0.7929870939789908, 'colsample_bytree': 0.5165143773340921, 'reg_alpha': 9.937451161868607e-08, 'reg_lambda': 5.183468784577932, 'min_split_gain': 0.862879341736673}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:09,474] Trial 237 finished with value: 0.2624570199138431 and parameters: {'learning_rate': 0.07737745567238495, 'num_leaves': 1018, 'max_depth': 50, 'n_estimators': 771, 'min_child_samples': 59, 'subsample': 0.6542415167186032, 'colsample_bytree': 0.5111125188752731, 'reg_alpha': 4.377051896697354, 'reg_lambda': 3.112898675488181, 'min_split_gain': 0.8370577521580296}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:13,107] Trial 218 finished with value: 0.2598233746080173 and parameters: {'learning_rate': 0.018494601944264173, 'num_leaves': 726, 'max_depth': 53, 'n_estimators': 472, 'min_child_samples': 52, 'subsample': 0.6698793831963624, 'colsample_bytree': 0.6958302780571157, 'reg_alpha': 0.8962277950237152, 'reg_lambda': 4.192484221585756, 'min_split_gain': 0.0564159950338492}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:16,065] Trial 209 finished with value: 0.25537941119415136 and parameters: {'learning_rate': 0.05771428806754354, 'num_leaves': 977, 'max_depth': 58, 'n_estimators': 466, 'min_child_samples': 53, 'subsample': 0.7740139883671469, 'colsample_bytree': 0.7830841964326793, 'reg_alpha': 0.8825650416264788, 'reg_lambda': 1.0115929107394352e-07, 'min_split_gain': 0.0015769177715143268}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:17,822] Trial 234 finished with value: 0.2570855118520116 and parameters: {'learning_rate': 0.07634049635488634, 'num_leaves': 972, 'max_depth': 57, 'n_estimators': 845, 'min_child_samples': 38, 'subsample': 0.6513244074001658, 'colsample_bytree': 0.5131572233255315, 'reg_alpha': 3.8239579144251974, 'reg_lambda': 2.9518637565086627, 'min_split_gain': 0.0545758531924722}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:18,523] Trial 214 finished with value: 0.26181581678400206 and parameters: {'learning_rate': 0.014786824338441217, 'num_leaves': 721, 'max_depth': 35, 'n_estimators': 773, 'min_child_samples': 12, 'subsample': 0.6699344418608666, 'colsample_bytree': 0.8764477785281857, 'reg_alpha': 1.0339281636983126, 'reg_lambda': 7.271660865121829e-07, 'min_split_gain': 0.08377957976378164}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:23,033] Trial 232 finished with value: 0.2599744901571814 and parameters: {'learning_rate': 0.0711854715472757, 'num_leaves': 952, 'max_depth': 57, 'n_estimators': 478, 'min_child_samples': 59, 'subsample': 0.7084625963978521, 'colsample_bytree': 0.5004127331930379, 'reg_alpha': 6.296739118520392e-06, 'reg_lambda': 6.111068486401281, 'min_split_gain': 0.05815404910822282}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:25,494] Trial 231 finished with value: 0.25598420051183157 and parameters: {'learning_rate': 0.07554873813332656, 'num_leaves': 720, 'max_depth': 17, 'n_estimators': 509, 'min_child_samples': 14, 'subsample': 0.8016325619805306, 'colsample_bytree': 0.5183036405276626, 'reg_alpha': 0.0010359129075394452, 'reg_lambda': 6.846921827418124, 'min_split_gain': 0.05854464134684097}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:35,256] Trial 235 finished with value: 0.26297960762679107 and parameters: {'learning_rate': 0.0142102295422806, 'num_leaves': 479, 'max_depth': 58, 'n_estimators': 771, 'min_child_samples': 15, 'subsample': 0.7111032898404894, 'colsample_bytree': 0.5220786350230368, 'reg_alpha': 6.755071227755903e-06, 'reg_lambda': 6.467791194653879, 'min_split_gain': 0.9028738435107424}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:38,386] Trial 202 finished with value: 0.2603317818068814 and parameters: {'learning_rate': 0.017782915389432215, 'num_leaves': 885, 'max_depth': 64, 'n_estimators': 818, 'min_child_samples': 14, 'subsample': 0.6708645789094063, 'colsample_bytree': 0.7618635844361668, 'reg_alpha': 0.9080694787806722, 'reg_lambda': 1.1821356212754778e-07, 'min_split_gain': 0.03143787701808334}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:38,833] Trial 181 finished with value: 0.2613145354664468 and parameters: {'learning_rate': 0.05246968256756378, 'num_leaves': 769, 'max_depth': 62, 'n_estimators': 426, 'min_child_samples': 23, 'subsample': 0.6212513766081796, 'colsample_bytree': 0.8012407857183479, 'reg_alpha': 3.305929412779025e-08, 'reg_lambda': 2.477520735821378e-07, 'min_split_gain': 0.0005611399329081774}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:40,132] Trial 224 finished with value: 0.2581895437090549 and parameters: {'learning_rate': 0.014415565654766998, 'num_leaves': 952, 'max_depth': 53, 'n_estimators': 470, 'min_child_samples': 52, 'subsample': 0.679456624031208, 'colsample_bytree': 0.5188508139580038, 'reg_alpha': 1.9080029439060717, 'reg_lambda': 4.712640162335706, 'min_split_gain': 0.0518064269909887}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:45,388] Trial 153 finished with value: 0.25939570300268944 and parameters: {'learning_rate': 0.04374458938998984, 'num_leaves': 841, 'max_depth': 56, 'n_estimators': 373, 'min_child_samples': 21, 'subsample': 0.7365176118095139, 'colsample_bytree': 0.7006502794610798, 'reg_alpha': 2.8692123328156584e-08, 'reg_lambda': 7.0956999442579955e-06, 'min_split_gain': 0.0016185658862594154}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:47,883] Trial 219 finished with value: 0.25927499090343215 and parameters: {'learning_rate': 0.014630269880059098, 'num_leaves': 964, 'max_depth': 53, 'n_estimators': 516, 'min_child_samples': 50, 'subsample': 0.6780959080841049, 'colsample_bytree': 0.8130680154299471, 'reg_alpha': 0.9523933784239935, 'reg_lambda': 4.126991085108818, 'min_split_gain': 0.05294241112779408}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:52,775] Trial 226 finished with value: 0.25741291228503266 and parameters: {'learning_rate': 0.014610289910420451, 'num_leaves': 955, 'max_depth': 58, 'n_estimators': 466, 'min_child_samples': 52, 'subsample': 0.6803783567532672, 'colsample_bytree': 0.7031212280118738, 'reg_alpha': 1.9348505046720943, 'reg_lambda': 5.869370022552321, 'min_split_gain': 0.05832711478973354}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:11:57,766] Trial 213 finished with value: 0.25867104637368044 and parameters: {'learning_rate': 0.014133883739212086, 'num_leaves': 990, 'max_depth': 58, 'n_estimators': 466, 'min_child_samples': 53, 'subsample': 0.6717890921523096, 'colsample_bytree': 0.8253417871951234, 'reg_alpha': 0.9408475596655977, 'reg_lambda': 5.006629226394599, 'min_split_gain': 0.03009570434707644}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:02,938] Trial 238 finished with value: 0.25458108706754856 and parameters: {'learning_rate': 0.062057802880262004, 'num_leaves': 1019, 'max_depth': 58, 'n_estimators': 775, 'min_child_samples': 14, 'subsample': 0.6538391421757717, 'colsample_bytree': 0.5194406591888378, 'reg_alpha': 3.868966725736388, 'reg_lambda': 7.2477077421774645, 'min_split_gain': 0.0049646267611903605}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:06,552] Trial 253 finished with value: 0.262357844033923 and parameters: {'learning_rate': 0.06377765913165152, 'num_leaves': 675, 'max_depth': 6, 'n_estimators': 839, 'min_child_samples': 56, 'subsample': 0.7983666359194552, 'colsample_bytree': 0.5302883207274656, 'reg_alpha': 3.582167942469812e-07, 'reg_lambda': 9.64432438307688, 'min_split_gain': 0.0020036811435535703}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:09,696] Trial 243 finished with value: 0.25376788782271803 and parameters: {'learning_rate': 0.06296269077277583, 'num_leaves': 977, 'max_depth': 58, 'n_estimators': 843, 'min_child_samples': 38, 'subsample': 0.6505545975459903, 'colsample_bytree': 0.5219457397574739, 'reg_alpha': 3.9583203336050015, 'reg_lambda': 7.086282909711956, 'min_split_gain': 0.007188610087610697}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:11,741] Trial 225 finished with value: 0.25700971066547823 and parameters: {'learning_rate': 0.014307156599281135, 'num_leaves': 946, 'max_depth': 53, 'n_estimators': 462, 'min_child_samples': 52, 'subsample': 0.6840200509116999, 'colsample_bytree': 0.518761797380182, 'reg_alpha': 0.8977780125167173, 'reg_lambda': 4.538412807864056, 'min_split_gain': 0.05485346635143695}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:19,010] Trial 248 finished with value: 0.2554531597846243 and parameters: {'learning_rate': 0.06039358683839012, 'num_leaves': 980, 'max_depth': 11, 'n_estimators': 838, 'min_child_samples': 38, 'subsample': 0.7975619672572078, 'colsample_bytree': 0.5320484171831078, 'reg_alpha': 3.9190037715545945, 'reg_lambda': 2.1695766958446383e-08, 'min_split_gain': 0.0022789913454969862}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:20,579] Trial 220 finished with value: 0.26000278694450896 and parameters: {'learning_rate': 0.013888332705466609, 'num_leaves': 955, 'max_depth': 58, 'n_estimators': 773, 'min_child_samples': 15, 'subsample': 0.6700069765267614, 'colsample_bytree': 0.8172799747680467, 'reg_alpha': 0.9082521316917609, 'reg_lambda': 5.113685574523634, 'min_split_gain': 0.0541535818984961}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:22,221] Trial 241 finished with value: 0.25582098655789787 and parameters: {'learning_rate': 0.06289107881228599, 'num_leaves': 815, 'max_depth': 58, 'n_estimators': 835, 'min_child_samples': 57, 'subsample': 0.701015683690638, 'colsample_bytree': 0.5258462605194533, 'reg_alpha': 1.6493411520775876, 'reg_lambda': 6.528962333645348, 'min_split_gain': 0.0005713748713223055}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:30,674] Trial 221 finished with value: 0.26099662617275154 and parameters: {'learning_rate': 0.014436377632380266, 'num_leaves': 982, 'max_depth': 53, 'n_estimators': 459, 'min_child_samples': 12, 'subsample': 0.6693712144465891, 'colsample_bytree': 0.8233527727016532, 'reg_alpha': 0.9233209131001038, 'reg_lambda': 4.447648214886703, 'min_split_gain': 0.05167486942040494}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:44,193] Trial 211 finished with value: 0.2582542633311884 and parameters: {'learning_rate': 0.014144410368320894, 'num_leaves': 981, 'max_depth': 18, 'n_estimators': 476, 'min_child_samples': 53, 'subsample': 0.66949735659639, 'colsample_bytree': 0.8146264125587497, 'reg_alpha': 0.8430247620053245, 'reg_lambda': 9.705662740908424e-08, 'min_split_gain': 0.0013224232128032434}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:49,089] Trial 244 finished with value: 0.2563254227790612 and parameters: {'learning_rate': 0.06244226545876996, 'num_leaves': 971, 'max_depth': 58, 'n_estimators': 835, 'min_child_samples': 40, 'subsample': 0.6549581538492207, 'colsample_bytree': 0.5303529059442477, 'reg_alpha': 1.6174714315058476, 'reg_lambda': 2.1783168825322733e-08, 'min_split_gain': 0.0004019452088951794}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:53,159] Trial 258 finished with value: 0.25568518844110716 and parameters: {'learning_rate': 0.06130826824062453, 'num_leaves': 999, 'max_depth': 15, 'n_estimators': 797, 'min_child_samples': 44, 'subsample': 0.7775111664306633, 'colsample_bytree': 0.5324288889377844, 'reg_alpha': 4.327567097362031, 'reg_lambda': 2.4698195784930967, 'min_split_gain': 0.0029723845555121265}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:54,043] Trial 205 finished with value: 0.2590500628415392 and parameters: {'learning_rate': 0.01428290783006714, 'num_leaves': 986, 'max_depth': 64, 'n_estimators': 471, 'min_child_samples': 15, 'subsample': 0.6734200536200513, 'colsample_bytree': 0.7824301395281417, 'reg_alpha': 0.9013583412427697, 'reg_lambda': 6.891452746433099e-08, 'min_split_gain': 0.02405612533373437}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:54,697] Trial 206 finished with value: 0.257860866583012 and parameters: {'learning_rate': 0.014694527129818799, 'num_leaves': 980, 'max_depth': 64, 'n_estimators': 768, 'min_child_samples': 15, 'subsample': 0.8042691404534844, 'colsample_bytree': 0.7087820984041078, 'reg_alpha': 0.95328953040788, 'reg_lambda': 4.054115420935053, 'min_split_gain': 0.02669428239030071}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:55,912] Trial 254 finished with value: 0.2528002965804522 and parameters: {'learning_rate': 0.06294747252266736, 'num_leaves': 751, 'max_depth': 55, 'n_estimators': 848, 'min_child_samples': 57, 'subsample': 0.7996547835597259, 'colsample_bytree': 0.5316057379758604, 'reg_alpha': 3.909527534975478, 'reg_lambda': 2.9109140815516083e-08, 'min_split_gain': 0.003114868241095352}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:12:59,897] Trial 239 finished with value: 0.25660577644699095 and parameters: {'learning_rate': 0.06232930122088517, 'num_leaves': 486, 'max_depth': 57, 'n_estimators': 778, 'min_child_samples': 14, 'subsample': 0.7022983427947721, 'colsample_bytree': 0.524465036347643, 'reg_alpha': 1.5696721658275443, 'reg_lambda': 7.503063223454805, 'min_split_gain': 0.005038833729335368}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:01,947] Trial 259 finished with value: 0.25433150824976425 and parameters: {'learning_rate': 0.06140048242527979, 'num_leaves': 1001, 'max_depth': 12, 'n_estimators': 801, 'min_child_samples': 43, 'subsample': 0.8119327811067649, 'colsample_bytree': 0.5290828700940777, 'reg_alpha': 5.562053479682578, 'reg_lambda': 2.3738610214685205e-08, 'min_split_gain': 0.0056452981299347825}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:03,445] Trial 246 finished with value: 0.254941698284003 and parameters: {'learning_rate': 0.05999244077049825, 'num_leaves': 508, 'max_depth': 11, 'n_estimators': 549, 'min_child_samples': 15, 'subsample': 0.8021168621418623, 'colsample_bytree': 0.5308205837942774, 'reg_alpha': 0.006143841566280823, 'reg_lambda': 8.368360896991692, 'min_split_gain': 0.007560419627732791}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:06,675] Trial 251 finished with value: 0.2541692300370439 and parameters: {'learning_rate': 0.06186770325338062, 'num_leaves': 973, 'max_depth': 58, 'n_estimators': 511, 'min_child_samples': 42, 'subsample': 0.6527483020404953, 'colsample_bytree': 0.5321162948549512, 'reg_alpha': 3.8928600274719556, 'reg_lambda': 2.6423310473587804, 'min_split_gain': 0.001964872049664645}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:07,397] Trial 207 finished with value: 0.2588355188739597 and parameters: {'learning_rate': 0.01409318259174865, 'num_leaves': 488, 'max_depth': 58, 'n_estimators': 459, 'min_child_samples': 13, 'subsample': 0.6722423468249367, 'colsample_bytree': 0.8176238885978546, 'reg_alpha': 0.8369523240521648, 'reg_lambda': 5.42918533291738, 'min_split_gain': 0.027467311061545763}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:08,623] Trial 261 finished with value: 0.2583001737769152 and parameters: {'learning_rate': 0.061430808589474896, 'num_leaves': 1007, 'max_depth': 12, 'n_estimators': 544, 'min_child_samples': 37, 'subsample': 0.7810696267723167, 'colsample_bytree': 0.5326482314658764, 'reg_alpha': 4.237111857727213, 'reg_lambda': 3.80419249655054e-08, 'min_split_gain': 0.0014813557430701034}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:09,487] Trial 256 finished with value: 0.25621904463610323 and parameters: {'learning_rate': 0.06700614770610915, 'num_leaves': 809, 'max_depth': 14, 'n_estimators': 545, 'min_child_samples': 56, 'subsample': 0.8133015736761889, 'colsample_bytree': 0.7772727092664602, 'reg_alpha': 0.0023616872229642997, 'reg_lambda': 0.33725086496945933, 'min_split_gain': 0.0014210036428892888}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:13,265] Trial 215 finished with value: 0.2578220903249246 and parameters: {'learning_rate': 0.017996040923336284, 'num_leaves': 724, 'max_depth': 58, 'n_estimators': 468, 'min_child_samples': 52, 'subsample': 0.6729003417507777, 'colsample_bytree': 0.8195437985755369, 'reg_alpha': 0.928710312033733, 'reg_lambda': 4.318431683852076, 'min_split_gain': 0.0006468015951778955}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:14,747] Trial 242 finished with value: 0.25964173897355963 and parameters: {'learning_rate': 0.06342217225414948, 'num_leaves': 980, 'max_depth': 58, 'n_estimators': 771, 'min_child_samples': 56, 'subsample': 0.6516397918172808, 'colsample_bytree': 0.7756701263083227, 'reg_alpha': 3.891508254906631e-07, 'reg_lambda': 6.570190025238568, 'min_split_gain': 0.0010771320202457631}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:16,236] Trial 250 finished with value: 0.2627122162009392 and parameters: {'learning_rate': 0.06219692713937173, 'num_leaves': 981, 'max_depth': 58, 'n_estimators': 543, 'min_child_samples': 57, 'subsample': 0.7880495636037861, 'colsample_bytree': 0.7760449806988219, 'reg_alpha': 0.002162639591732086, 'reg_lambda': 3.543589154093917e-08, 'min_split_gain': 0.0015208414109485636}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:17,605] Trial 264 finished with value: 0.25798594242147704 and parameters: {'learning_rate': 0.0641615637132673, 'num_leaves': 1005, 'max_depth': 8, 'n_estimators': 834, 'min_child_samples': 37, 'subsample': 0.8152125279312671, 'colsample_bytree': 0.5369417409902876, 'reg_alpha': 0.008552880155283006, 'reg_lambda': 3.1734712077651944e-08, 'min_split_gain': 0.016565253508020156}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:17,828] Trial 247 finished with value: 0.259533343089188 and parameters: {'learning_rate': 0.060783583331363945, 'num_leaves': 811, 'max_depth': 58, 'n_estimators': 555, 'min_child_samples': 42, 'subsample': 0.8279343634291874, 'colsample_bytree': 0.7783262487657299, 'reg_alpha': 0.0005579817148392906, 'reg_lambda': 3.332988495731086e-08, 'min_split_gain': 0.011557274055111741}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:20,141] Trial 267 finished with value: 0.25825096420461663 and parameters: {'learning_rate': 0.06075364114995172, 'num_leaves': 1010, 'max_depth': 8, 'n_estimators': 796, 'min_child_samples': 40, 'subsample': 0.7861661512495636, 'colsample_bytree': 0.5335723570227118, 'reg_alpha': 4.961096255772762, 'reg_lambda': 1.779343748355516e-08, 'min_split_gain': 0.010450570186967564}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:22,380] Trial 240 finished with value: 0.25465515432148367 and parameters: {'learning_rate': 0.06275686043746027, 'num_leaves': 485, 'max_depth': 58, 'n_estimators': 437, 'min_child_samples': 57, 'subsample': 0.7061778895850701, 'colsample_bytree': 0.5233227298146608, 'reg_alpha': 0.9576231634785944, 'reg_lambda': 8.138994917822702, 'min_split_gain': 0.000978979823826498}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:22,950] Trial 265 finished with value: 0.2544699728404695 and parameters: {'learning_rate': 0.06937382731975342, 'num_leaves': 1011, 'max_depth': 10, 'n_estimators': 796, 'min_child_samples': 40, 'subsample': 0.8189716745075046, 'colsample_bytree': 0.5355792670942081, 'reg_alpha': 4.359871202149971, 'reg_lambda': 1.842695079476246e-08, 'min_split_gain': 0.013975411781203774}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:24,637] Trial 245 finished with value: 0.2574528456268832 and parameters: {'learning_rate': 0.0614598150910973, 'num_leaves': 966, 'max_depth': 18, 'n_estimators': 836, 'min_child_samples': 15, 'subsample': 0.7890341210062549, 'colsample_bytree': 0.5309086835371553, 'reg_alpha': 1.5293934150993485, 'reg_lambda': 7.173272612283899, 'min_split_gain': 0.0039539341609183395}. Best is trial 136 with value: 0.2522471730040013.\n",
      "[I 2025-07-14 21:13:29,036] Trial 249 finished with value: 0.2497732745396593 and parameters: {'learning_rate': 0.06231644339727185, 'num_leaves': 973, 'max_depth': 58, 'n_estimators': 844, 'min_child_samples': 58, 'subsample': 0.7971670807732923, 'colsample_bytree': 0.5333434673612824, 'reg_alpha': 3.7371206169171867, 'reg_lambda': 7.172199174507102, 'min_split_gain': 0.002062115270766521}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:32,800] Trial 274 finished with value: 0.2555442565172351 and parameters: {'learning_rate': 0.06278992126878533, 'num_leaves': 495, 'max_depth': 10, 'n_estimators': 574, 'min_child_samples': 43, 'subsample': 0.8230219282776963, 'colsample_bytree': 0.5459903327715205, 'reg_alpha': 5.969218022860304, 'reg_lambda': 2.4767421847607912e-08, 'min_split_gain': 0.00594215655598813}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:33,295] Trial 255 finished with value: 0.2601696158080137 and parameters: {'learning_rate': 0.06090247278468034, 'num_leaves': 749, 'max_depth': 27, 'n_estimators': 576, 'min_child_samples': 56, 'subsample': 0.6588998990769629, 'colsample_bytree': 0.5324023356115818, 'reg_alpha': 0.0004369399927255526, 'reg_lambda': 3.0720558835376655e-08, 'min_split_gain': 0.0030595138996897717}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:40,107] Trial 273 finished with value: 0.2589764487353691 and parameters: {'learning_rate': 0.060032332828502906, 'num_leaves': 437, 'max_depth': 14, 'n_estimators': 855, 'min_child_samples': 44, 'subsample': 0.8238396169996862, 'colsample_bytree': 0.5400832101230091, 'reg_alpha': 4.551842360415911, 'reg_lambda': 1.0412649341237013e-08, 'min_split_gain': 0.0023520685466828367}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:46,335] Trial 271 finished with value: 0.25399570653689757 and parameters: {'learning_rate': 0.0614259841649408, 'num_leaves': 512, 'max_depth': 11, 'n_estimators': 852, 'min_child_samples': 41, 'subsample': 0.819297742560509, 'colsample_bytree': 0.5372537816985189, 'reg_alpha': 4.737149435858425, 'reg_lambda': 2.3615577974339394e-08, 'min_split_gain': 0.0011063581923695803}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:47,799] Trial 287 finished with value: 0.25988168928890115 and parameters: {'learning_rate': 0.08974463820877844, 'num_leaves': 453, 'max_depth': 10, 'n_estimators': 503, 'min_child_samples': 43, 'subsample': 0.8376277221231844, 'colsample_bytree': 0.547817067505887, 'reg_alpha': 5.693610378181576, 'reg_lambda': 1.042641884274562e-08, 'min_split_gain': 0.025851309189741413}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:51,923] Trial 286 finished with value: 0.2566134568913302 and parameters: {'learning_rate': 0.0859675942048267, 'num_leaves': 509, 'max_depth': 14, 'n_estimators': 600, 'min_child_samples': 44, 'subsample': 0.8316809772558177, 'colsample_bytree': 0.5462392488285615, 'reg_alpha': 5.26913458724328, 'reg_lambda': 2.1230223934600683e-08, 'min_split_gain': 0.02557142057525115}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:52,105] Trial 277 finished with value: 0.2579891697208797 and parameters: {'learning_rate': 0.0653825866089676, 'num_leaves': 1006, 'max_depth': 10, 'n_estimators': 565, 'min_child_samples': 41, 'subsample': 0.8295628442628249, 'colsample_bytree': 0.5448393533425728, 'reg_alpha': 0.00044600483838044743, 'reg_lambda': 1.2456373773377638e-08, 'min_split_gain': 0.017624200617750885}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:54,023] Trial 270 finished with value: 0.25743849242454814 and parameters: {'learning_rate': 0.06892241205222618, 'num_leaves': 492, 'max_depth': 12, 'n_estimators': 841, 'min_child_samples': 41, 'subsample': 0.812281393946333, 'colsample_bytree': 0.5309452811459948, 'reg_alpha': 4.6085972559204205, 'reg_lambda': 8.04989438681364, 'min_split_gain': 0.0004832653747739238}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:54,312] Trial 281 finished with value: 0.25968282105903057 and parameters: {'learning_rate': 0.08841950933655209, 'num_leaves': 1008, 'max_depth': 14, 'n_estimators': 522, 'min_child_samples': 40, 'subsample': 0.822076504980603, 'colsample_bytree': 0.5476811061985256, 'reg_alpha': 0.003562962046624279, 'reg_lambda': 2.058787593606594, 'min_split_gain': 0.02197494148502665}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:55,041] Trial 288 finished with value: 0.25887654925165837 and parameters: {'learning_rate': 0.08436101291771633, 'num_leaves': 495, 'max_depth': 10, 'n_estimators': 518, 'min_child_samples': 39, 'subsample': 0.8351320951618196, 'colsample_bytree': 0.5429648261614699, 'reg_alpha': 6.685349900599294, 'reg_lambda': 2.2496091956463543e-08, 'min_split_gain': 0.029776712403779906}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:55,429] Trial 252 finished with value: 0.25542961484840637 and parameters: {'learning_rate': 0.060863166490282214, 'num_leaves': 812, 'max_depth': 13, 'n_estimators': 553, 'min_child_samples': 40, 'subsample': 0.6551932712436355, 'colsample_bytree': 0.5354583794246764, 'reg_alpha': 3.2379288108800406e-06, 'reg_lambda': 2.628285963093241e-08, 'min_split_gain': 3.276082848724469e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:57,832] Trial 268 finished with value: 0.25303876613587556 and parameters: {'learning_rate': 0.06250326217665085, 'num_leaves': 1003, 'max_depth': 16, 'n_estimators': 838, 'min_child_samples': 38, 'subsample': 0.8165113226526663, 'colsample_bytree': 0.5339307278612591, 'reg_alpha': 5.645226369052753, 'reg_lambda': 2.484907119994454e-08, 'min_split_gain': 0.0009012914030467334}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:13:58,253] Trial 280 finished with value: 0.2596889548790039 and parameters: {'learning_rate': 0.0894055582310998, 'num_leaves': 1017, 'max_depth': 13, 'n_estimators': 579, 'min_child_samples': 44, 'subsample': 0.8103641231016772, 'colsample_bytree': 0.5440880164141675, 'reg_alpha': 0.003289821622512749, 'reg_lambda': 1.9368041979267014e-08, 'min_split_gain': 0.02108088206574303}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:02,093] Trial 289 finished with value: 0.256539058157765 and parameters: {'learning_rate': 0.08710752620649491, 'num_leaves': 528, 'max_depth': 9, 'n_estimators': 866, 'min_child_samples': 43, 'subsample': 0.8445814838510507, 'colsample_bytree': 0.5451022653124835, 'reg_alpha': 5.650606057143461, 'reg_lambda': 1.6934914363425533e-08, 'min_split_gain': 0.026659960036396008}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:02,282] Trial 284 finished with value: 0.2572054465482593 and parameters: {'learning_rate': 0.08457849223526305, 'num_leaves': 936, 'max_depth': 14, 'n_estimators': 568, 'min_child_samples': 45, 'subsample': 0.8342203553202046, 'colsample_bytree': 0.5469259740777913, 'reg_alpha': 0.0024760834116675985, 'reg_lambda': 2.0437325403954016, 'min_split_gain': 0.023494020114588787}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:02,751] Trial 279 finished with value: 0.25734800072796044 and parameters: {'learning_rate': 0.08633282590446247, 'num_leaves': 1023, 'max_depth': 14, 'n_estimators': 589, 'min_child_samples': 41, 'subsample': 0.8212233501421317, 'colsample_bytree': 0.5466108893292163, 'reg_alpha': 0.0029745814545025263, 'reg_lambda': 2.046060729141398e-08, 'min_split_gain': 0.018621906699351536}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:04,072] Trial 260 finished with value: 0.25427830435453563 and parameters: {'learning_rate': 0.06110352285786931, 'num_leaves': 997, 'max_depth': 17, 'n_estimators': 787, 'min_child_samples': 41, 'subsample': 0.8350902382739971, 'colsample_bytree': 0.5321034053535272, 'reg_alpha': 4.209802892881353, 'reg_lambda': 8.673883641774388, 'min_split_gain': 0.0006181229618564578}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:04,496] Trial 283 finished with value: 0.2569449672556312 and parameters: {'learning_rate': 0.08887773775819197, 'num_leaves': 459, 'max_depth': 15, 'n_estimators': 584, 'min_child_samples': 44, 'subsample': 0.808312384000521, 'colsample_bytree': 0.5324477805041476, 'reg_alpha': 0.0004099519627122672, 'reg_lambda': 1.754030736209479e-08, 'min_split_gain': 0.023533190453746973}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:06,238] Trial 290 finished with value: 0.25678775854556574 and parameters: {'learning_rate': 0.08146246238787373, 'num_leaves': 1012, 'max_depth': 11, 'n_estimators': 607, 'min_child_samples': 40, 'subsample': 0.8413167330573249, 'colsample_bytree': 0.5443149770343647, 'reg_alpha': 6.3201511507343735, 'reg_lambda': 1.706832243635523e-08, 'min_split_gain': 0.02331729401433964}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:06,677] Trial 266 finished with value: 0.2594667395922606 and parameters: {'learning_rate': 0.06080057481335417, 'num_leaves': 998, 'max_depth': 16, 'n_estimators': 793, 'min_child_samples': 41, 'subsample': 0.7821221173108164, 'colsample_bytree': 0.5331179212455741, 'reg_alpha': 0.008883387178804053, 'reg_lambda': 4.0347056082420616e-08, 'min_split_gain': 0.002636001865570596}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:07,138] Trial 263 finished with value: 0.25291141365523395 and parameters: {'learning_rate': 0.05705818952220585, 'num_leaves': 1006, 'max_depth': 55, 'n_estimators': 560, 'min_child_samples': 41, 'subsample': 0.8237618841159182, 'colsample_bytree': 0.5469554096755713, 'reg_alpha': 4.071507720230382, 'reg_lambda': 1.0282478539297453e-08, 'min_split_gain': 0.001354998068104385}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:08,844] Trial 282 finished with value: 0.25654373065612657 and parameters: {'learning_rate': 0.08816164794052352, 'num_leaves': 1012, 'max_depth': 15, 'n_estimators': 519, 'min_child_samples': 43, 'subsample': 0.8197035234841027, 'colsample_bytree': 0.5458128836264622, 'reg_alpha': 0.0004793351589263057, 'reg_lambda': 1.9712725760331943e-08, 'min_split_gain': 0.02097202688057758}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:09,627] Trial 293 finished with value: 0.2556831221459502 and parameters: {'learning_rate': 0.0814547006698412, 'num_leaves': 527, 'max_depth': 10, 'n_estimators': 523, 'min_child_samples': 45, 'subsample': 0.8061878674337752, 'colsample_bytree': 0.5497804714355484, 'reg_alpha': 6.398192952498723, 'reg_lambda': 2.339255985684975, 'min_split_gain': 0.03461978308060886}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:10,201] Trial 285 finished with value: 0.26134747106623607 and parameters: {'learning_rate': 0.0897333395262294, 'num_leaves': 1019, 'max_depth': 15, 'n_estimators': 565, 'min_child_samples': 43, 'subsample': 0.8136927108698037, 'colsample_bytree': 0.5485700272532255, 'reg_alpha': 0.0029975996090197818, 'reg_lambda': 1.1553655504356712e-08, 'min_split_gain': 0.025470385960130428}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:11,650] Trial 294 finished with value: 0.2572752414917986 and parameters: {'learning_rate': 0.07553007273517269, 'num_leaves': 512, 'max_depth': 12, 'n_estimators': 860, 'min_child_samples': 38, 'subsample': 0.7987910729434797, 'colsample_bytree': 0.548003343170408, 'reg_alpha': 6.211755740577941, 'reg_lambda': 2.377960554805157e-08, 'min_split_gain': 0.03699797737466024}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:12,360] Trial 302 finished with value: 0.2637771722038483 and parameters: {'learning_rate': 0.07218985538852533, 'num_leaves': 515, 'max_depth': 16, 'n_estimators': 800, 'min_child_samples': 36, 'subsample': 0.7981033060384414, 'colsample_bytree': 0.5358458862920586, 'reg_alpha': 3.552355700437326, 'reg_lambda': 4.5131639353651725e-08, 'min_split_gain': 0.5946639616418751}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:13,329] Trial 292 finished with value: 0.25842027453842953 and parameters: {'learning_rate': 0.07852775414504325, 'num_leaves': 514, 'max_depth': 10, 'n_estimators': 859, 'min_child_samples': 40, 'subsample': 0.7986038808456534, 'colsample_bytree': 0.5533529915661405, 'reg_alpha': 3.695258372723754, 'reg_lambda': 2.0818469550214957, 'min_split_gain': 0.03076386821914732}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:13,933] Trial 272 finished with value: 0.25385698164756326 and parameters: {'learning_rate': 0.062022528022454546, 'num_leaves': 510, 'max_depth': 11, 'n_estimators': 551, 'min_child_samples': 43, 'subsample': 0.8258003533385521, 'colsample_bytree': 0.5348325242058233, 'reg_alpha': 0.0019440299759088237, 'reg_lambda': 1.1830517832613103e-08, 'min_split_gain': 0.002727575455203168}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:14,766] Trial 269 finished with value: 0.2566489042164182 and parameters: {'learning_rate': 0.06080754234163924, 'num_leaves': 465, 'max_depth': 13, 'n_estimators': 834, 'min_child_samples': 44, 'subsample': 0.8224736228940276, 'colsample_bytree': 0.5338109580901321, 'reg_alpha': 0.006308466508902711, 'reg_lambda': 1.727009669467873e-08, 'min_split_gain': 0.00030874312138158815}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:14,944] Trial 304 finished with value: 0.26266860361040273 and parameters: {'learning_rate': 0.07085650567337315, 'num_leaves': 515, 'max_depth': 12, 'n_estimators': 800, 'min_child_samples': 36, 'subsample': 0.7988787050019489, 'colsample_bytree': 0.5545823913143693, 'reg_alpha': 3.9858346402991285, 'reg_lambda': 4.52024857614205e-08, 'min_split_gain': 0.5828005845792554}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:15,544] Trial 291 finished with value: 0.25525651167276053 and parameters: {'learning_rate': 0.07527494825802401, 'num_leaves': 1022, 'max_depth': 10, 'n_estimators': 858, 'min_child_samples': 41, 'subsample': 0.8002302704160733, 'colsample_bytree': 0.5495823759421705, 'reg_alpha': 3.7065883607469496, 'reg_lambda': 2.210073396547225, 'min_split_gain': 0.02825115548902611}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:15,927] Trial 305 finished with value: 0.2602417988622306 and parameters: {'learning_rate': 0.07161076349677122, 'num_leaves': 522, 'max_depth': 12, 'n_estimators': 795, 'min_child_samples': 36, 'subsample': 0.7978181492915691, 'colsample_bytree': 0.5586946517414018, 'reg_alpha': 3.5097888947973095, 'reg_lambda': 2.942124051035111e-08, 'min_split_gain': 0.5836682692916929}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:16,937] Trial 278 finished with value: 0.25730021716682927 and parameters: {'learning_rate': 0.06933402068478806, 'num_leaves': 1023, 'max_depth': 15, 'n_estimators': 594, 'min_child_samples': 42, 'subsample': 0.8187734337259709, 'colsample_bytree': 0.5454230487184484, 'reg_alpha': 0.007223411462644147, 'reg_lambda': 2.103281300362196e-08, 'min_split_gain': 0.018544723178640983}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:20,611] Trial 298 finished with value: 0.2573187425377977 and parameters: {'learning_rate': 0.07405627484787677, 'num_leaves': 531, 'max_depth': 11, 'n_estimators': 877, 'min_child_samples': 38, 'subsample': 0.8489455478942723, 'colsample_bytree': 0.5578904037320641, 'reg_alpha': 3.6023126510627717, 'reg_lambda': 2.3737482487794304e-08, 'min_split_gain': 0.030995946156304608}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:22,023] Trial 262 finished with value: 0.2575511437916909 and parameters: {'learning_rate': 0.060111825386074846, 'num_leaves': 1019, 'max_depth': 55, 'n_estimators': 543, 'min_child_samples': 57, 'subsample': 0.8119274702946915, 'colsample_bytree': 0.535823910496077, 'reg_alpha': 3.6437032281647642e-06, 'reg_lambda': 2.945523827054979e-08, 'min_split_gain': 0.0035897758100711663}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:22,461] Trial 297 finished with value: 0.2583233759036622 and parameters: {'learning_rate': 0.07176971561437737, 'num_leaves': 520, 'max_depth': 12, 'n_estimators': 874, 'min_child_samples': 36, 'subsample': 0.8028969634792964, 'colsample_bytree': 0.5499999442839973, 'reg_alpha': 3.5713511198117818, 'reg_lambda': 1.3531249348323473e-08, 'min_split_gain': 0.03025847418933403}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:23,976] Trial 295 finished with value: 0.25545717046388977 and parameters: {'learning_rate': 0.07598873477818331, 'num_leaves': 1022, 'max_depth': 12, 'n_estimators': 869, 'min_child_samples': 46, 'subsample': 0.8017500296473618, 'colsample_bytree': 0.5546550995672395, 'reg_alpha': 3.507004164162688, 'reg_lambda': 1.54360739935762e-08, 'min_split_gain': 0.036574281647038395}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:26,179] Trial 299 finished with value: 0.2565462983732054 and parameters: {'learning_rate': 0.07192208613571612, 'num_leaves': 466, 'max_depth': 12, 'n_estimators': 874, 'min_child_samples': 38, 'subsample': 0.8023026900709229, 'colsample_bytree': 0.5374547026016312, 'reg_alpha': 3.514883500972648, 'reg_lambda': 1.4130486803248636e-08, 'min_split_gain': 0.03373684218875232}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:26,406] Trial 210 finished with value: 0.25832496315756986 and parameters: {'learning_rate': 0.0178937448379816, 'num_leaves': 984, 'max_depth': 64, 'n_estimators': 818, 'min_child_samples': 14, 'subsample': 0.7692607424578493, 'colsample_bytree': 0.8219898130065256, 'reg_alpha': 0.9495754775104734, 'reg_lambda': 9.451478686399129e-08, 'min_split_gain': 0.001099251491011776}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:26,888] Trial 300 finished with value: 0.26012440796538355 and parameters: {'learning_rate': 0.07191175879036814, 'num_leaves': 1017, 'max_depth': 15, 'n_estimators': 655, 'min_child_samples': 37, 'subsample': 0.8163439956009376, 'colsample_bytree': 0.5582161226504794, 'reg_alpha': 3.375717592680725, 'reg_lambda': 4.579853485937813e-08, 'min_split_gain': 0.037537804921855225}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:27,028] Trial 301 finished with value: 0.256238666766625 and parameters: {'learning_rate': 0.07061175119945881, 'num_leaves': 450, 'max_depth': 12, 'n_estimators': 878, 'min_child_samples': 38, 'subsample': 0.7949499747636505, 'colsample_bytree': 0.5588861240759952, 'reg_alpha': 3.6028307964783406, 'reg_lambda': 4.96447259873127e-08, 'min_split_gain': 0.03635779057893197}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:30,330] Trial 309 finished with value: 0.2575299067234198 and parameters: {'learning_rate': 0.07265718982642216, 'num_leaves': 511, 'max_depth': 12, 'n_estimators': 655, 'min_child_samples': 36, 'subsample': 0.7990881144395158, 'colsample_bytree': 0.56332368586934, 'reg_alpha': 3.490937732216464, 'reg_lambda': 3.2246229813224825e-08, 'min_split_gain': 0.043538918120358094}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:31,556] Trial 308 finished with value: 0.25451779531349644 and parameters: {'learning_rate': 0.07410702037331396, 'num_leaves': 506, 'max_depth': 12, 'n_estimators': 656, 'min_child_samples': 34, 'subsample': 0.8633978346579879, 'colsample_bytree': 0.5585222136517849, 'reg_alpha': 3.615648888525779, 'reg_lambda': 1.3072220735702059e-08, 'min_split_gain': 0.03747382267715384}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:33,308] Trial 307 finished with value: 0.2576213880515081 and parameters: {'learning_rate': 0.07192317617344975, 'num_leaves': 529, 'max_depth': 11, 'n_estimators': 800, 'min_child_samples': 38, 'subsample': 0.797863293295813, 'colsample_bytree': 0.5616396958616616, 'reg_alpha': 3.4541556910120437, 'reg_lambda': 1.2389285030514702e-08, 'min_split_gain': 0.03701586031027991}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:34,466] Trial 310 finished with value: 0.25693819961150755 and parameters: {'learning_rate': 0.07237249417262896, 'num_leaves': 541, 'max_depth': 11, 'n_estimators': 664, 'min_child_samples': 37, 'subsample': 0.8697109095355222, 'colsample_bytree': 0.5657907652954055, 'reg_alpha': 3.4035220207911703, 'reg_lambda': 4.215248411068426e-08, 'min_split_gain': 0.03657223845093725}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:35,781] Trial 311 finished with value: 0.25760063249110554 and parameters: {'learning_rate': 0.07062237690072316, 'num_leaves': 223, 'max_depth': 11, 'n_estimators': 538, 'min_child_samples': 38, 'subsample': 0.804228043088826, 'colsample_bytree': 0.558138306753673, 'reg_alpha': 3.3940598682003715, 'reg_lambda': 3.054998063789876e-08, 'min_split_gain': 0.03885624405803425}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:36,191] Trial 306 finished with value: 0.2570184194399524 and parameters: {'learning_rate': 0.05462694160644049, 'num_leaves': 508, 'max_depth': 12, 'n_estimators': 856, 'min_child_samples': 38, 'subsample': 0.8568402063685858, 'colsample_bytree': 0.5578571836592365, 'reg_alpha': 3.599202832653774, 'reg_lambda': 1.2064972231073192e-08, 'min_split_gain': 0.031242497272295965}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:39,136] Trial 296 finished with value: 0.2567500013832034 and parameters: {'learning_rate': 0.07322042249957451, 'num_leaves': 527, 'max_depth': 16, 'n_estimators': 863, 'min_child_samples': 46, 'subsample': 0.8000625280525793, 'colsample_bytree': 0.5534953788346916, 'reg_alpha': 3.319479989364995e-06, 'reg_lambda': 2.0014827695934454e-08, 'min_split_gain': 0.034721151336485685}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:39,681] Trial 314 finished with value: 0.25584168867788093 and parameters: {'learning_rate': 0.054106797668107436, 'num_leaves': 559, 'max_depth': 8, 'n_estimators': 540, 'min_child_samples': 36, 'subsample': 0.8753243635153698, 'colsample_bytree': 0.5613679061089137, 'reg_alpha': 3.5066415001818214, 'reg_lambda': 1.3096550756859413e-08, 'min_split_gain': 0.037503148790570845}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:40,612] Trial 321 finished with value: 0.25907615097222386 and parameters: {'learning_rate': 0.054337932002683845, 'num_leaves': 548, 'max_depth': 8, 'n_estimators': 658, 'min_child_samples': 38, 'subsample': 0.8595994363344812, 'colsample_bytree': 0.5095508422175756, 'reg_alpha': 9.985051974656285, 'reg_lambda': 1.146656052210081e-08, 'min_split_gain': 0.0418124852803154}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:40,891] Trial 312 finished with value: 0.25722560915617165 and parameters: {'learning_rate': 0.054538833518952674, 'num_leaves': 574, 'max_depth': 11, 'n_estimators': 855, 'min_child_samples': 38, 'subsample': 0.8479949547882604, 'colsample_bytree': 0.5601677392023061, 'reg_alpha': 3.6168282297821195, 'reg_lambda': 2.8526092045978116e-08, 'min_split_gain': 0.03598929714226956}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:41,870] Trial 316 finished with value: 0.2579512395172067 and parameters: {'learning_rate': 0.05515728503535351, 'num_leaves': 1023, 'max_depth': 8, 'n_estimators': 875, 'min_child_samples': 34, 'subsample': 0.8652992850338653, 'colsample_bytree': 0.5654910732847627, 'reg_alpha': 2.803988933697271, 'reg_lambda': 1.0080440447613766e-08, 'min_split_gain': 0.03885539721543805}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:42,788] Trial 313 finished with value: 0.25643467531716485 and parameters: {'learning_rate': 0.05578010314405596, 'num_leaves': 547, 'max_depth': 12, 'n_estimators': 548, 'min_child_samples': 35, 'subsample': 0.8653728983436614, 'colsample_bytree': 0.565402160612048, 'reg_alpha': 3.3344280601795573, 'reg_lambda': 1.4082519746744122e-08, 'min_split_gain': 0.03903693739305644}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:43,955] Trial 315 finished with value: 0.2597063118206214 and parameters: {'learning_rate': 0.054514456888782085, 'num_leaves': 576, 'max_depth': 11, 'n_estimators': 543, 'min_child_samples': 39, 'subsample': 0.8279841656378405, 'colsample_bytree': 0.5580447779291456, 'reg_alpha': 3.4726409405981604, 'reg_lambda': 1.0233298976221639e-08, 'min_split_gain': 0.03728066313967324}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:46,556] Trial 303 finished with value: 0.25918265033318444 and parameters: {'learning_rate': 0.07132758435232917, 'num_leaves': 513, 'max_depth': 11, 'n_estimators': 795, 'min_child_samples': 35, 'subsample': 0.7962991033950078, 'colsample_bytree': 0.5613658060913251, 'reg_alpha': 3.28555663241493e-06, 'reg_lambda': 4.380699403837572e-08, 'min_split_gain': 0.03733305988284276}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:52,048] Trial 335 finished with value: 0.2646652176139415 and parameters: {'learning_rate': 0.047944735337786266, 'num_leaves': 579, 'max_depth': 9, 'n_estimators': 623, 'min_child_samples': 34, 'subsample': 0.8315189106012439, 'colsample_bytree': 0.5046938620103374, 'reg_alpha': 2.281840874601357, 'reg_lambda': 0.6283864053455653, 'min_split_gain': 0.7461966128971987}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:53,276] Trial 322 finished with value: 0.26051087037026793 and parameters: {'learning_rate': 0.05425399856682163, 'num_leaves': 979, 'max_depth': 8, 'n_estimators': 897, 'min_child_samples': 39, 'subsample': 0.8586899458157008, 'colsample_bytree': 0.5044284147268183, 'reg_alpha': 9.668118109196116e-05, 'reg_lambda': 1.0768601421352436e-08, 'min_split_gain': 0.04165083551328104}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:54,120] Trial 332 finished with value: 0.2598650492503157 and parameters: {'learning_rate': 0.054739715325367436, 'num_leaves': 933, 'max_depth': 21, 'n_estimators': 892, 'min_child_samples': 33, 'subsample': 0.8777221830653398, 'colsample_bytree': 0.5071088405268378, 'reg_alpha': 9.80192709137516, 'reg_lambda': 1.0371200325354207e-08, 'min_split_gain': 0.07661222746163632}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:54,520] Trial 329 finished with value: 0.2570543151888269 and parameters: {'learning_rate': 0.05482416179108887, 'num_leaves': 994, 'max_depth': 9, 'n_estimators': 850, 'min_child_samples': 35, 'subsample': 0.8567141805457462, 'colsample_bytree': 0.5087879169366402, 'reg_alpha': 9.675377010824334, 'reg_lambda': 0.7363484974337152, 'min_split_gain': 0.0422365360368628}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:54,865] Trial 326 finished with value: 0.2542820761423119 and parameters: {'learning_rate': 0.05436929986512142, 'num_leaves': 555, 'max_depth': 8, 'n_estimators': 856, 'min_child_samples': 46, 'subsample': 0.8541022971753481, 'colsample_bytree': 0.507204403010447, 'reg_alpha': 2.3344986924372866, 'reg_lambda': 1.1021298448907602e-08, 'min_split_gain': 0.041239844502334644}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:55,717] Trial 317 finished with value: 0.2587872559140612 and parameters: {'learning_rate': 0.05271406782668901, 'num_leaves': 993, 'max_depth': 8, 'n_estimators': 648, 'min_child_samples': 38, 'subsample': 0.8496396843049419, 'colsample_bytree': 0.5608987021475982, 'reg_alpha': 3.0501646396476035e-06, 'reg_lambda': 1.3710385276018982e-08, 'min_split_gain': 0.0434618194037265}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:56,779] Trial 323 finished with value: 0.25854180804592203 and parameters: {'learning_rate': 0.05446089062576388, 'num_leaves': 993, 'max_depth': 9, 'n_estimators': 855, 'min_child_samples': 33, 'subsample': 0.790524479204446, 'colsample_bytree': 0.5647038279281547, 'reg_alpha': 2.702102566443237, 'reg_lambda': 1.1841537643608751e-08, 'min_split_gain': 0.04148139208847616}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:57,619] Trial 257 finished with value: 0.25645633797015577 and parameters: {'learning_rate': 0.06050203623921704, 'num_leaves': 815, 'max_depth': 55, 'n_estimators': 787, 'min_child_samples': 14, 'subsample': 0.7822706108924564, 'colsample_bytree': 0.5322511887516117, 'reg_alpha': 0.007096388787426819, 'reg_lambda': 7.129174894117441, 'min_split_gain': 0.006718170812888008}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:14:57,886] Trial 333 finished with value: 0.2577519784187223 and parameters: {'learning_rate': 0.0549407105651307, 'num_leaves': 995, 'max_depth': 8, 'n_estimators': 632, 'min_child_samples': 31, 'subsample': 0.8289808927445825, 'colsample_bytree': 0.505828847772961, 'reg_alpha': 8.956752830621248, 'reg_lambda': 2.6849532486544327, 'min_split_gain': 0.07579109248010388}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:00,968] Trial 341 finished with value: 0.25963901555036184 and parameters: {'learning_rate': 0.13527417202388578, 'num_leaves': 993, 'max_depth': 19, 'n_estimators': 744, 'min_child_samples': 41, 'subsample': 0.8783228874013921, 'colsample_bytree': 0.947875109928305, 'reg_alpha': 7.224889864543473, 'reg_lambda': 3.14654769731869, 'min_split_gain': 0.07654603111397792}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:01,242] Trial 338 finished with value: 0.2579454516916183 and parameters: {'learning_rate': 0.050113761181416056, 'num_leaves': 984, 'max_depth': 6, 'n_estimators': 493, 'min_child_samples': 32, 'subsample': 0.8848226940361168, 'colsample_bytree': 0.5241994767278362, 'reg_alpha': 2.0847087456146247, 'reg_lambda': 1.522107656075299, 'min_split_gain': 0.016987196763902743}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:01,483] Trial 276 finished with value: 0.2531772214063649 and parameters: {'learning_rate': 0.0626641903464264, 'num_leaves': 999, 'max_depth': 14, 'n_estimators': 560, 'min_child_samples': 41, 'subsample': 0.8259030175077385, 'colsample_bytree': 0.5489579014360508, 'reg_alpha': 0.0027576677000292114, 'reg_lambda': 2.2652980994914373e-08, 'min_split_gain': 0.00015595773162556045}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:01,971] Trial 340 finished with value: 0.25649404396414205 and parameters: {'learning_rate': 0.13543293143389695, 'num_leaves': 983, 'max_depth': 19, 'n_estimators': 894, 'min_child_samples': 42, 'subsample': 0.8398958055040019, 'colsample_bytree': 0.5220058670418894, 'reg_alpha': 7.416998853603204, 'reg_lambda': 0.7561024703402136, 'min_split_gain': 0.01646777889989254}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:03,279] Trial 330 finished with value: 0.2541726578359606 and parameters: {'learning_rate': 0.05406046227039908, 'num_leaves': 987, 'max_depth': 8, 'n_estimators': 896, 'min_child_samples': 33, 'subsample': 0.8800102432405001, 'colsample_bytree': 0.5052042817320727, 'reg_alpha': 7.318597899004416, 'reg_lambda': 1.0026855085519703e-08, 'min_split_gain': 0.01787144735564928}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:03,538] Trial 345 finished with value: 0.26894806248012465 and parameters: {'learning_rate': 0.04764577205969751, 'num_leaves': 999, 'max_depth': 4, 'n_estimators': 490, 'min_child_samples': 41, 'subsample': 0.8413587388560722, 'colsample_bytree': 0.5210909931977945, 'reg_alpha': 2.223002362809886, 'reg_lambda': 2.6922943217705444, 'min_split_gain': 0.07486844106877408}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:03,889] Trial 339 finished with value: 0.26049878553236666 and parameters: {'learning_rate': 0.046572541821058945, 'num_leaves': 993, 'max_depth': 6, 'n_estimators': 621, 'min_child_samples': 42, 'subsample': 0.8371757608834491, 'colsample_bytree': 0.5214281142357728, 'reg_alpha': 2.037008648568709, 'reg_lambda': 9.739239778289383, 'min_split_gain': 0.07397396506766774}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:04,761] Trial 346 finished with value: 0.26109879073767367 and parameters: {'learning_rate': 0.13666681936881087, 'num_leaves': 994, 'max_depth': 6, 'n_estimators': 497, 'min_child_samples': 41, 'subsample': 0.840004664121828, 'colsample_bytree': 0.520096362816504, 'reg_alpha': 1.593463355464806, 'reg_lambda': 1.3954457069934394, 'min_split_gain': 0.016552719769884948}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:05,741] Trial 327 finished with value: 0.2597737999233386 and parameters: {'learning_rate': 0.053902154820732656, 'num_leaves': 991, 'max_depth': 8, 'n_estimators': 888, 'min_child_samples': 47, 'subsample': 0.7888301638739782, 'colsample_bytree': 0.5200541388055362, 'reg_alpha': 5.742323578248744e-05, 'reg_lambda': 1.1327972273692429e-08, 'min_split_gain': 0.04255025491133121}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:06,765] Trial 342 finished with value: 0.2607718272200489 and parameters: {'learning_rate': 0.15421648237416144, 'num_leaves': 994, 'max_depth': 9, 'n_estimators': 499, 'min_child_samples': 32, 'subsample': 0.8134980257177231, 'colsample_bytree': 0.5203193017693134, 'reg_alpha': 7.165603517610967, 'reg_lambda': 9.404097589375713, 'min_split_gain': 0.016852434061680896}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:07,004] Trial 350 finished with value: 0.26689271058489383 and parameters: {'learning_rate': 0.10678938243983273, 'num_leaves': 484, 'max_depth': 4, 'n_estimators': 492, 'min_child_samples': 42, 'subsample': 0.6328334602456356, 'colsample_bytree': 0.5210651019476662, 'reg_alpha': 1.5769024541789873, 'reg_lambda': 9.856606874567355, 'min_split_gain': 0.017260300312448094}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:07,448] Trial 337 finished with value: 0.262369956147387 and parameters: {'learning_rate': 0.10579842291500764, 'num_leaves': 994, 'max_depth': 21, 'n_estimators': 621, 'min_child_samples': 33, 'subsample': 0.8916163813748605, 'colsample_bytree': 0.9068848037755651, 'reg_alpha': 2.1403010695962482e-05, 'reg_lambda': 3.1810989019292104, 'min_split_gain': 0.07684096537957662}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:10,325] Trial 325 finished with value: 0.2594365581206484 and parameters: {'learning_rate': 0.05427391968354452, 'num_leaves': 564, 'max_depth': 8, 'n_estimators': 888, 'min_child_samples': 47, 'subsample': 0.8288368474716465, 'colsample_bytree': 0.9147634649859007, 'reg_alpha': 1.0641197164023e-05, 'reg_lambda': 1.330197299719125e-08, 'min_split_gain': 0.04169820921436942}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:11,092] Trial 318 finished with value: 0.2537533093940172 and parameters: {'learning_rate': 0.0548762620493003, 'num_leaves': 988, 'max_depth': 11, 'n_estimators': 654, 'min_child_samples': 37, 'subsample': 0.8662512617682999, 'colsample_bytree': 0.5610748320252611, 'reg_alpha': 0.02684956820659224, 'reg_lambda': 1.3194157682622007e-08, 'min_split_gain': 0.03745545099386669}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:11,453] Trial 275 finished with value: 0.2551290235379471 and parameters: {'learning_rate': 0.06405041451143408, 'num_leaves': 1024, 'max_depth': 16, 'n_estimators': 538, 'min_child_samples': 42, 'subsample': 0.8199141118303034, 'colsample_bytree': 0.5472556849626223, 'reg_alpha': 0.0023384621786222103, 'reg_lambda': 2.2753052449661028e-08, 'min_split_gain': 0.0010455465949253976}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:13,397] Trial 352 finished with value: 0.26121185402320984 and parameters: {'learning_rate': 0.09577045787115415, 'num_leaves': 482, 'max_depth': 6, 'n_estimators': 754, 'min_child_samples': 42, 'subsample': 0.8414160099045901, 'colsample_bytree': 0.5207475898443169, 'reg_alpha': 1.5474220259917861, 'reg_lambda': 9.2810914892735, 'min_split_gain': 0.018061917075483384}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:13,604] Trial 328 finished with value: 0.2571539645770328 and parameters: {'learning_rate': 0.05213713168412892, 'num_leaves': 558, 'max_depth': 21, 'n_estimators': 851, 'min_child_samples': 34, 'subsample': 0.8561467011376955, 'colsample_bytree': 0.5235586715974454, 'reg_alpha': 2.2120447152164413, 'reg_lambda': 1.017261156337791e-08, 'min_split_gain': 0.0397120340219489}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:13,915] Trial 353 finished with value: 0.2569163791858542 and parameters: {'learning_rate': 0.09811499514680087, 'num_leaves': 938, 'max_depth': 6, 'n_estimators': 433, 'min_child_samples': 28, 'subsample': 0.8171219764743398, 'colsample_bytree': 0.5186010151947184, 'reg_alpha': 1.8592360112059978, 'reg_lambda': 9.39344512791701, 'min_split_gain': 0.018283391049024055}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:14,131] Trial 360 finished with value: 0.26200412303683246 and parameters: {'learning_rate': 0.10569302970872331, 'num_leaves': 962, 'max_depth': 55, 'n_estimators': 912, 'min_child_samples': 10, 'subsample': 0.8208490231058054, 'colsample_bytree': 0.5134481243404231, 'reg_alpha': 0.0017996120349750724, 'reg_lambda': 3.303655853014357, 'min_split_gain': 0.9984517690736414}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:14,422] Trial 319 finished with value: 0.258747178595197 and parameters: {'learning_rate': 0.049606308253085644, 'num_leaves': 569, 'max_depth': 9, 'n_estimators': 896, 'min_child_samples': 33, 'subsample': 0.8112801508729861, 'colsample_bytree': 0.9290912612157056, 'reg_alpha': 8.335603273227268e-05, 'reg_lambda': 1.168301666834948e-08, 'min_split_gain': 0.03756105122498195}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:15,764] Trial 347 finished with value: 0.2596753031535065 and parameters: {'learning_rate': 0.04869524000304126, 'num_leaves': 475, 'max_depth': 6, 'n_estimators': 500, 'min_child_samples': 41, 'subsample': 0.8360072493071696, 'colsample_bytree': 0.5194230547521552, 'reg_alpha': 1.555388421048765, 'reg_lambda': 3.0399767608253416, 'min_split_gain': 0.015576473334069684}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:18,261] Trial 324 finished with value: 0.2587264327605404 and parameters: {'learning_rate': 0.05465384861338495, 'num_leaves': 996, 'max_depth': 11, 'n_estimators': 659, 'min_child_samples': 39, 'subsample': 0.870748388565345, 'colsample_bytree': 0.5059181887278695, 'reg_alpha': 9.29769086369816e-05, 'reg_lambda': 1.372323663948106e-08, 'min_split_gain': 0.04306028674324188}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:19,836] Trial 354 finished with value: 0.25816231788145566 and parameters: {'learning_rate': 0.10119495693477855, 'num_leaves': 487, 'max_depth': 7, 'n_estimators': 828, 'min_child_samples': 47, 'subsample': 0.8396871077369841, 'colsample_bytree': 0.9140845223492076, 'reg_alpha': 1.5668284451909875, 'reg_lambda': 8.61468993338465, 'min_split_gain': 0.019322060808535147}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:20,897] Trial 348 finished with value: 0.25676953066962743 and parameters: {'learning_rate': 0.10506331386189134, 'num_leaves': 998, 'max_depth': 56, 'n_estimators': 500, 'min_child_samples': 42, 'subsample': 0.8911430251304995, 'colsample_bytree': 0.521873874758231, 'reg_alpha': 1.06082559142602e-05, 'reg_lambda': 2.5535796960677395, 'min_split_gain': 0.07619105334312798}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:22,853] Trial 336 finished with value: 0.25649734093943155 and parameters: {'learning_rate': 0.04950677834666943, 'num_leaves': 991, 'max_depth': 19, 'n_estimators': 895, 'min_child_samples': 31, 'subsample': 0.7748212122810596, 'colsample_bytree': 0.5173223424324354, 'reg_alpha': 7.2871903078796905, 'reg_lambda': 2.8698579832063777, 'min_split_gain': 0.01592068446291148}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:24,137] Trial 355 finished with value: 0.2612895165239471 and parameters: {'learning_rate': 0.04680759072514296, 'num_leaves': 483, 'max_depth': 6, 'n_estimators': 916, 'min_child_samples': 46, 'subsample': 0.8177723331941408, 'colsample_bytree': 0.5221376023989305, 'reg_alpha': 1.5386062140789376, 'reg_lambda': 8.918935954049772, 'min_split_gain': 0.017567548590092415}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:27,358] Trial 343 finished with value: 0.2580754810213828 and parameters: {'learning_rate': 0.10293744216922075, 'num_leaves': 990, 'max_depth': 9, 'n_estimators': 496, 'min_child_samples': 41, 'subsample': 0.8992143729070885, 'colsample_bytree': 0.5215944008831751, 'reg_alpha': 0.027216822672622813, 'reg_lambda': 3.04037737020262, 'min_split_gain': 0.01165442899984696}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:29,620] Trial 320 finished with value: 0.2599368772233034 and parameters: {'learning_rate': 0.05520324094168799, 'num_leaves': 988, 'max_depth': 21, 'n_estimators': 859, 'min_child_samples': 38, 'subsample': 0.8666685663680881, 'colsample_bytree': 0.5198307814174435, 'reg_alpha': 0.00010800230128476023, 'reg_lambda': 1.3109883667473603e-08, 'min_split_gain': 0.035291464340843764}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:31,221] Trial 363 finished with value: 0.260529427283854 and parameters: {'learning_rate': 0.04332833371444544, 'num_leaves': 932, 'max_depth': 56, 'n_estimators': 693, 'min_child_samples': 29, 'subsample': 0.8664838839586992, 'colsample_bytree': 0.511611623418351, 'reg_alpha': 0.01754578720764476, 'reg_lambda': 1.9093549301782216e-08, 'min_split_gain': 0.5135797217627656}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:32,155] Trial 362 finished with value: 0.2579840371713211 and parameters: {'learning_rate': 0.0995366308018771, 'num_leaves': 965, 'max_depth': 56, 'n_estimators': 824, 'min_child_samples': 28, 'subsample': 0.9072473200930742, 'colsample_bytree': 0.5008122930106774, 'reg_alpha': 5.872401788987995, 'reg_lambda': 6.861574712339422, 'min_split_gain': 0.01789770570036824}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:35,676] Trial 371 finished with value: 0.2615308106314697 and parameters: {'learning_rate': 0.0408453114491058, 'num_leaves': 397, 'max_depth': 17, 'n_estimators': 686, 'min_child_samples': 61, 'subsample': 0.8521154374709335, 'colsample_bytree': 0.6883691597417472, 'reg_alpha': 0.001673046786634761, 'reg_lambda': 1.9935358702727542e-08, 'min_split_gain': 0.512582794014719}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:38,671] Trial 356 finished with value: 0.2569936688510354 and parameters: {'learning_rate': 0.03990514480369847, 'num_leaves': 954, 'max_depth': 7, 'n_estimators': 759, 'min_child_samples': 28, 'subsample': 0.9046948866715553, 'colsample_bytree': 0.5172469782260947, 'reg_alpha': 6.409338831056314, 'reg_lambda': 3.191232067337147, 'min_split_gain': 0.015068000989650318}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:39,299] Trial 358 finished with value: 0.2574722828202524 and parameters: {'learning_rate': 0.04175517379959369, 'num_leaves': 476, 'max_depth': 7, 'n_estimators': 697, 'min_child_samples': 27, 'subsample': 0.8821988493642755, 'colsample_bytree': 0.5003477657987405, 'reg_alpha': 0.02623754889797771, 'reg_lambda': 8.967696018942009, 'min_split_gain': 0.01853070521866145}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:40,884] Trial 344 finished with value: 0.25945023315353655 and parameters: {'learning_rate': 0.1052124261118713, 'num_leaves': 997, 'max_depth': 19, 'n_estimators': 489, 'min_child_samples': 41, 'subsample': 0.7865724128899184, 'colsample_bytree': 0.5220392443760413, 'reg_alpha': 0.029004076481980744, 'reg_lambda': 3.0026778409410935, 'min_split_gain': 0.00038568469475432077}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:41,998] Trial 366 finished with value: 0.25820025036426036 and parameters: {'learning_rate': 0.2614442655884774, 'num_leaves': 942, 'max_depth': 56, 'n_estimators': 696, 'min_child_samples': 27, 'subsample': 0.8940788370477094, 'colsample_bytree': 0.5117834703917151, 'reg_alpha': 5.149673447698502, 'reg_lambda': 1.8927097143807473e-08, 'min_split_gain': 0.0007879583248002178}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:44,292] Trial 334 finished with value: 0.25525916237080953 and parameters: {'learning_rate': 0.04864915163856593, 'num_leaves': 993, 'max_depth': 9, 'n_estimators': 496, 'min_child_samples': 33, 'subsample': 0.8890158677392968, 'colsample_bytree': 0.5083839770726395, 'reg_alpha': 6.972811572517001e-05, 'reg_lambda': 1.5594829941252297, 'min_split_gain': 0.01566448775982844}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:47,476] Trial 351 finished with value: 0.2619014244804345 and parameters: {'learning_rate': 0.10401770805791367, 'num_leaves': 484, 'max_depth': 56, 'n_estimators': 679, 'min_child_samples': 42, 'subsample': 0.8422953927923558, 'colsample_bytree': 0.5206998962606983, 'reg_alpha': 0.030483429141098205, 'reg_lambda': 8.05863737731057, 'min_split_gain': 0.016923709066265457}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:55,524] Trial 331 finished with value: 0.256828869993126 and parameters: {'learning_rate': 0.05381106011794769, 'num_leaves': 996, 'max_depth': 19, 'n_estimators': 496, 'min_child_samples': 33, 'subsample': 0.8678180837042409, 'colsample_bytree': 0.5104593974054235, 'reg_alpha': 0.00010197422468663832, 'reg_lambda': 3.038840588006175, 'min_split_gain': 0.019985289214075108}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:58,331] Trial 370 finished with value: 0.26101916566152 and parameters: {'learning_rate': 0.06457554380629968, 'num_leaves': 113, 'max_depth': 17, 'n_estimators': 683, 'min_child_samples': 29, 'subsample': 0.8942490997995561, 'colsample_bytree': 0.533792817876467, 'reg_alpha': 0.01589516134947424, 'reg_lambda': 1.7873722866578063e-08, 'min_split_gain': 0.0034270805400444613}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:15:59,920] Trial 349 finished with value: 0.2555085421983858 and parameters: {'learning_rate': 0.046841705383544034, 'num_leaves': 476, 'max_depth': 43, 'n_estimators': 758, 'min_child_samples': 42, 'subsample': 0.8433283536642601, 'colsample_bytree': 0.9209976720426516, 'reg_alpha': 1.851320174881533, 'reg_lambda': 3.1852547818406722, 'min_split_gain': 0.01856669657357798}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:00,511] Trial 372 finished with value: 0.2609256356489408 and parameters: {'learning_rate': 0.26609940229917706, 'num_leaves': 965, 'max_depth': 56, 'n_estimators': 675, 'min_child_samples': 61, 'subsample': 0.8498104251073505, 'colsample_bytree': 0.5379355184559301, 'reg_alpha': 0.004755837872037068, 'reg_lambda': 1.9895833816723638e-08, 'min_split_gain': 0.0009023610327604271}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:01,027] Trial 368 finished with value: 0.2617013378140873 and parameters: {'learning_rate': 0.238842528216307, 'num_leaves': 964, 'max_depth': 17, 'n_estimators': 702, 'min_child_samples': 61, 'subsample': 0.903330245904705, 'colsample_bytree': 0.537944332441771, 'reg_alpha': 0.0012670421661322671, 'reg_lambda': 1.7394556714941285e-08, 'min_split_gain': 0.0026771984205371255}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:14,215] Trial 384 finished with value: 0.2617418225081101 and parameters: {'learning_rate': 0.06472230340882525, 'num_leaves': 968, 'max_depth': 14, 'n_estimators': 784, 'min_child_samples': 60, 'subsample': 0.8261477872690141, 'colsample_bytree': 0.540201696300979, 'reg_alpha': 0.0045843479796452095, 'reg_lambda': 2.8246020091773892e-08, 'min_split_gain': 0.06446880884461707}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:16,430] Trial 383 finished with value: 0.2591239562351774 and parameters: {'learning_rate': 0.06487591963283117, 'num_leaves': 970, 'max_depth': 31, 'n_estimators': 203, 'min_child_samples': 60, 'subsample': 0.826069975932894, 'colsample_bytree': 0.5339347298682755, 'reg_alpha': 0.0011799838950411394, 'reg_lambda': 2.6052190416938943e-08, 'min_split_gain': 0.0643820566866215}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:16,960] Trial 378 finished with value: 0.25540257642050207 and parameters: {'learning_rate': 0.06502678901781443, 'num_leaves': 962, 'max_depth': 16, 'n_estimators': 779, 'min_child_samples': 26, 'subsample': 0.851061983847423, 'colsample_bytree': 0.5377655275050535, 'reg_alpha': 5.574240391314015, 'reg_lambda': 2.6726751100063498e-08, 'min_split_gain': 0.005713673048784202}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:33,566] Trial 390 finished with value: 0.25792714029175406 and parameters: {'learning_rate': 0.06456670773539343, 'num_leaves': 1023, 'max_depth': 13, 'n_estimators': 560, 'min_child_samples': 59, 'subsample': 0.8255529464057464, 'colsample_bytree': 0.5385426902471394, 'reg_alpha': 0.001290913372260669, 'reg_lambda': 3.119590690458198e-08, 'min_split_gain': 0.06113581061213451}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:35,271] Trial 389 finished with value: 0.2575137139269489 and parameters: {'learning_rate': 0.06452887209265076, 'num_leaves': 912, 'max_depth': 32, 'n_estimators': 778, 'min_child_samples': 58, 'subsample': 0.9544049161863016, 'colsample_bytree': 0.5352846971213183, 'reg_alpha': 0.0018474093829681218, 'reg_lambda': 2.7716739392897337e-08, 'min_split_gain': 0.06372338538575051}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:35,581] Trial 385 finished with value: 0.25508915654583153 and parameters: {'learning_rate': 0.06581048844268662, 'num_leaves': 909, 'max_depth': 40, 'n_estimators': 203, 'min_child_samples': 58, 'subsample': 0.8242323283767268, 'colsample_bytree': 0.5748043936436908, 'reg_alpha': 4.745830496843751, 'reg_lambda': 3.1470119742648695e-08, 'min_split_gain': 0.0002212039617394924}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:36,342] Trial 357 finished with value: 0.2576750426527744 and parameters: {'learning_rate': 0.009006112104063423, 'num_leaves': 474, 'max_depth': 7, 'n_estimators': 824, 'min_child_samples': 29, 'subsample': 0.8987932734923103, 'colsample_bytree': 0.5146032535904174, 'reg_alpha': 6.255661891303204, 'reg_lambda': 3.3847882883087426, 'min_split_gain': 0.013898075863899805}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:37,853] Trial 391 finished with value: 0.2574812237363378 and parameters: {'learning_rate': 0.06346808268957772, 'num_leaves': 971, 'max_depth': 13, 'n_estimators': 564, 'min_child_samples': 44, 'subsample': 0.8270114660186153, 'colsample_bytree': 0.5347802561646421, 'reg_alpha': 0.005355603334142001, 'reg_lambda': 3.289678131552927e-08, 'min_split_gain': 0.06014248573612398}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:41,043] Trial 377 finished with value: 0.2533235411237842 and parameters: {'learning_rate': 0.04146776008874663, 'num_leaves': 911, 'max_depth': 13, 'n_estimators': 693, 'min_child_samples': 59, 'subsample': 0.6492311467459078, 'colsample_bytree': 0.5734274437889891, 'reg_alpha': 5.6430435653710145, 'reg_lambda': 2.22060897184505e-08, 'min_split_gain': 0.0024960224279475273}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:42,057] Trial 379 finished with value: 0.2588820429169607 and parameters: {'learning_rate': 0.0637261604995218, 'num_leaves': 909, 'max_depth': 17, 'n_estimators': 559, 'min_child_samples': 62, 'subsample': 0.8483057596131487, 'colsample_bytree': 0.5376908825913997, 'reg_alpha': 0.004378537250545174, 'reg_lambda': 2.609010591608225e-08, 'min_split_gain': 0.00310675291171761}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:42,292] Trial 387 finished with value: 0.2607868441665197 and parameters: {'learning_rate': 0.06548759954558468, 'num_leaves': 963, 'max_depth': 13, 'n_estimators': 562, 'min_child_samples': 44, 'subsample': 0.8228373637038506, 'colsample_bytree': 0.5382364980449571, 'reg_alpha': 0.0016931959704628975, 'reg_lambda': 2.925466152227276e-08, 'min_split_gain': 0.002361038160097574}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:44,354] Trial 361 finished with value: 0.42446754855124524 and parameters: {'learning_rate': 0.0013662914174317079, 'num_leaves': 941, 'max_depth': 7, 'n_estimators': 829, 'min_child_samples': 28, 'subsample': 0.5529967243693903, 'colsample_bytree': 0.500642115230043, 'reg_alpha': 5.091005136031104, 'reg_lambda': 3.394113918258213, 'min_split_gain': 0.018125052711245038}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:46,691] Trial 359 finished with value: 0.2576933867473606 and parameters: {'learning_rate': 0.0655554180967655, 'num_leaves': 944, 'max_depth': 56, 'n_estimators': 753, 'min_child_samples': 29, 'subsample': 0.8220906145110494, 'colsample_bytree': 0.5009518254501768, 'reg_alpha': 0.0014324714531794144, 'reg_lambda': 3.5760185630723034, 'min_split_gain': 0.017909773673786183}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:49,299] Trial 392 finished with value: 0.2612273419362914 and parameters: {'learning_rate': 0.06428750691467185, 'num_leaves': 1019, 'max_depth': 13, 'n_estimators': 607, 'min_child_samples': 58, 'subsample': 0.8257442032998504, 'colsample_bytree': 0.5716086747683651, 'reg_alpha': 0.004766424686654841, 'reg_lambda': 3.4959969885152815e-08, 'min_split_gain': 0.05200469708099308}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:50,593] Trial 398 finished with value: 0.2574058326598847 and parameters: {'learning_rate': 0.08037984077361868, 'num_leaves': 435, 'max_depth': 38, 'n_estimators': 141, 'min_child_samples': 45, 'subsample': 0.6416888701832189, 'colsample_bytree': 0.5736893172498921, 'reg_alpha': 5.163423102222442, 'reg_lambda': 5.669731951055337e-08, 'min_split_gain': 0.06105495043047365}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:52,308] Trial 399 finished with value: 0.25638550131766374 and parameters: {'learning_rate': 0.07909951109218119, 'num_leaves': 440, 'max_depth': 59, 'n_estimators': 209, 'min_child_samples': 45, 'subsample': 0.6478437471848485, 'colsample_bytree': 0.5736854842697161, 'reg_alpha': 4.983377236976989, 'reg_lambda': 6.610632713515217e-08, 'min_split_gain': 0.053794178561609195}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:55,900] Trial 403 finished with value: 0.2608624008167734 and parameters: {'learning_rate': 0.08037494854623968, 'num_leaves': 1009, 'max_depth': 13, 'n_estimators': 736, 'min_child_samples': 40, 'subsample': 0.6434772037581683, 'colsample_bytree': 0.5286920667070927, 'reg_alpha': 9.7504526976384, 'reg_lambda': 0.00016795636094166805, 'min_split_gain': 0.05752381644153321}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:56,258] Trial 386 finished with value: 0.2599236826228367 and parameters: {'learning_rate': 0.06631150832420016, 'num_leaves': 1023, 'max_depth': 13, 'n_estimators': 565, 'min_child_samples': 45, 'subsample': 0.8244805553308185, 'colsample_bytree': 0.538614849481526, 'reg_alpha': 0.07894991280261959, 'reg_lambda': 3.139569454677504e-08, 'min_split_gain': 0.0002412884018619453}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:16:58,031] Trial 402 finished with value: 0.25794225805496657 and parameters: {'learning_rate': 0.07728415141738819, 'num_leaves': 936, 'max_depth': 60, 'n_estimators': 741, 'min_child_samples': 58, 'subsample': 0.6432308141976335, 'colsample_bytree': 0.5714154538575508, 'reg_alpha': 4.946329262217378, 'reg_lambda': 5.808808955583705, 'min_split_gain': 0.05519391559649181}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:02,233] Trial 404 finished with value: 0.2586943387640622 and parameters: {'learning_rate': 0.0789664328780183, 'num_leaves': 359, 'max_depth': 10, 'n_estimators': 738, 'min_child_samples': 17, 'subsample': 0.6467870619132164, 'colsample_bytree': 0.5879657566465423, 'reg_alpha': 7.206201017094652, 'reg_lambda': 5.755982450123137e-08, 'min_split_gain': 0.055251351361779985}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:07,868] Trial 369 finished with value: 0.25909520472646114 and parameters: {'learning_rate': 0.0421541392983136, 'num_leaves': 949, 'max_depth': 40, 'n_estimators': 706, 'min_child_samples': 61, 'subsample': 0.9211873546139386, 'colsample_bytree': 0.5024029415074479, 'reg_alpha': 0.0039222030855471475, 'reg_lambda': 2.2796286528465023e-08, 'min_split_gain': 0.0008253557834099721}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:09,719] Trial 407 finished with value: 0.2596277962262342 and parameters: {'learning_rate': 0.078506378165609, 'num_leaves': 923, 'max_depth': 10, 'n_estimators': 635, 'min_child_samples': 39, 'subsample': 0.6458652289159673, 'colsample_bytree': 0.5292731083333995, 'reg_alpha': 8.305950262344338, 'reg_lambda': 2.6244585787354287e-05, 'min_split_gain': 0.02341608980674738}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:11,849] Trial 380 finished with value: 0.25362168488621945 and parameters: {'learning_rate': 0.06485765146168458, 'num_leaves': 949, 'max_depth': 17, 'n_estimators': 529, 'min_child_samples': 45, 'subsample': 0.8241762628011553, 'colsample_bytree': 0.5382514372015093, 'reg_alpha': 0.0011925585369433828, 'reg_lambda': 2.7941705870565744e-08, 'min_split_gain': 0.00012999848271384013}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:14,542] Trial 395 finished with value: 0.259338162624364 and parameters: {'learning_rate': 0.06630507443974643, 'num_leaves': 440, 'max_depth': 13, 'n_estimators': 530, 'min_child_samples': 44, 'subsample': 0.9221783237145158, 'colsample_bytree': 0.5721910462736546, 'reg_alpha': 0.06380532305033335, 'reg_lambda': 6.368837722883482e-08, 'min_split_gain': 0.048085424332374636}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:18,977] Trial 405 finished with value: 0.2598942496393698 and parameters: {'learning_rate': 0.07888685999640856, 'num_leaves': 1006, 'max_depth': 10, 'n_estimators': 733, 'min_child_samples': 36, 'subsample': 0.6402295817020919, 'colsample_bytree': 0.5789319588621759, 'reg_alpha': 0.0849147371905595, 'reg_lambda': 6.275594831558127e-08, 'min_split_gain': 0.05328806808824044}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:27,803] Trial 217 finished with value: 0.25829162625609475 and parameters: {'learning_rate': 0.013302547713099702, 'num_leaves': 971, 'max_depth': 58, 'n_estimators': 467, 'min_child_samples': 13, 'subsample': 0.6717238992027457, 'colsample_bytree': 0.6984383923926029, 'reg_alpha': 0.9779737770190966, 'reg_lambda': 4.41799285098889, 'min_split_gain': 0.0006292881156732266}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:32,041] Trial 373 finished with value: 0.259949826988843 and parameters: {'learning_rate': 0.039842736062207576, 'num_leaves': 943, 'max_depth': 32, 'n_estimators': 782, 'min_child_samples': 60, 'subsample': 0.8950284939088337, 'colsample_bytree': 0.5011097282467332, 'reg_alpha': 0.02263481577565435, 'reg_lambda': 2.752846096205884e-08, 'min_split_gain': 0.002909989664531219}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:32,626] Trial 397 finished with value: 0.258681214699146 and parameters: {'learning_rate': 0.08059969869947997, 'num_leaves': 907, 'max_depth': 13, 'n_estimators': 740, 'min_child_samples': 45, 'subsample': 0.646724674844807, 'colsample_bytree': 0.5296810429390256, 'reg_alpha': 0.08252698410270314, 'reg_lambda': 6.555804520452949e-08, 'min_split_gain': 0.0005456252338039004}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:35,625] Trial 409 finished with value: 0.257408780642292 and parameters: {'learning_rate': 0.0594319703455886, 'num_leaves': 500, 'max_depth': 10, 'n_estimators': 804, 'min_child_samples': 36, 'subsample': 0.6509232547819409, 'colsample_bytree': 0.5872662740649031, 'reg_alpha': 0.0008079574437397136, 'reg_lambda': 5.894444060284087, 'min_split_gain': 0.026178532626121756}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:36,862] Trial 393 finished with value: 0.25570134883459256 and parameters: {'learning_rate': 0.06597685280497186, 'num_leaves': 921, 'max_depth': 59, 'n_estimators': 561, 'min_child_samples': 44, 'subsample': 0.6411762010098766, 'colsample_bytree': 0.5390690445343603, 'reg_alpha': 0.15126795069017335, 'reg_lambda': 3.575502360635643e-08, 'min_split_gain': 0.0003091087444556579}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:37,605] Trial 396 finished with value: 0.2519259294657501 and parameters: {'learning_rate': 0.06664545236488376, 'num_leaves': 1023, 'max_depth': 13, 'n_estimators': 565, 'min_child_samples': 44, 'subsample': 0.6431620370080372, 'colsample_bytree': 0.5279342059410572, 'reg_alpha': 0.09846810404695876, 'reg_lambda': 5.0183028248891753e-08, 'min_split_gain': 0.0011621208896599555}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:42,293] Trial 401 finished with value: 0.25439854854613847 and parameters: {'learning_rate': 0.0804951879608293, 'num_leaves': 430, 'max_depth': 10, 'n_estimators': 599, 'min_child_samples': 17, 'subsample': 0.6370493960508898, 'colsample_bytree': 0.5774081242875927, 'reg_alpha': 0.07996725620386552, 'reg_lambda': 5.86919746500196, 'min_split_gain': 3.076843623990317e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:43,822] Trial 400 finished with value: 0.404029401937782 and parameters: {'learning_rate': 0.002067403858140759, 'num_leaves': 1023, 'max_depth': 26, 'n_estimators': 597, 'min_child_samples': 63, 'subsample': 0.6316734219900993, 'colsample_bytree': 0.713665054314716, 'reg_alpha': 5.5287615444624025, 'reg_lambda': 5.5120627833707205, 'min_split_gain': 0.6850438489295982}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:44,570] Trial 365 finished with value: 0.2583950999010152 and parameters: {'learning_rate': 0.04261780475316485, 'num_leaves': 961, 'max_depth': 17, 'n_estimators': 692, 'min_child_samples': 27, 'subsample': 0.8994345757742187, 'colsample_bytree': 0.508890565188904, 'reg_alpha': 0.004990972174958796, 'reg_lambda': 1.7188392228009594e-08, 'min_split_gain': 0.01640681983138311}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:55,018] Trial 367 finished with value: 0.2577630992339455 and parameters: {'learning_rate': 0.04120212716559377, 'num_leaves': 970, 'max_depth': 56, 'n_estimators': 562, 'min_child_samples': 60, 'subsample': 0.8496776326725407, 'colsample_bytree': 0.5010969044788592, 'reg_alpha': 0.023736855921093314, 'reg_lambda': 2.261702237344413e-08, 'min_split_gain': 0.000935274933843588}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:57,952] Trial 416 finished with value: 0.25724480566713354 and parameters: {'learning_rate': 0.058671343476037904, 'num_leaves': 972, 'max_depth': 15, 'n_estimators': 805, 'min_child_samples': 40, 'subsample': 0.6263728388828178, 'colsample_bytree': 0.5490106732535461, 'reg_alpha': 2.3129861324157033, 'reg_lambda': 1.4393548616538914e-08, 'min_split_gain': 0.02433418551603495}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:17:58,337] Trial 364 finished with value: 0.2568994653199516 and parameters: {'learning_rate': 0.04067792218460331, 'num_leaves': 952, 'max_depth': 17, 'n_estimators': 756, 'min_child_samples': 27, 'subsample': 0.8901503594250546, 'colsample_bytree': 0.5000580181016884, 'reg_alpha': 0.01788592796784014, 'reg_lambda': 2.138101553072083e-08, 'min_split_gain': 0.0008649532930336336}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:01,642] Trial 374 finished with value: 0.2575652162061187 and parameters: {'learning_rate': 0.03971481738025296, 'num_leaves': 931, 'max_depth': 42, 'n_estimators': 759, 'min_child_samples': 45, 'subsample': 0.9048867735761708, 'colsample_bytree': 0.538010478207148, 'reg_alpha': 0.0014008305227950863, 'reg_lambda': 1.943483348033635e-08, 'min_split_gain': 0.005145723571062379}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:02,045] Trial 411 finished with value: 0.2576429291198547 and parameters: {'learning_rate': 0.05927479136952842, 'num_leaves': 498, 'max_depth': 14, 'n_estimators': 586, 'min_child_samples': 36, 'subsample': 0.6327234636629454, 'colsample_bytree': 0.5273235539966096, 'reg_alpha': 0.15677888052542588, 'reg_lambda': 1.0082399043543388e-08, 'min_split_gain': 0.02622585796786877}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:07,546] Trial 408 finished with value: 0.2563532393289688 and parameters: {'learning_rate': 0.05716319662649204, 'num_leaves': 974, 'max_depth': 10, 'n_estimators': 594, 'min_child_samples': 12, 'subsample': 0.6398082726484969, 'colsample_bytree': 0.5275983225648666, 'reg_alpha': 2.7282567882643063, 'reg_lambda': 1.6053624802599702e-08, 'min_split_gain': 0.0005443531864098591}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:11,758] Trial 412 finished with value: 0.2583455881882294 and parameters: {'learning_rate': 0.057177681296135464, 'num_leaves': 980, 'max_depth': 10, 'n_estimators': 810, 'min_child_samples': 11, 'subsample': 0.6306927217566284, 'colsample_bytree': 0.7241370422793705, 'reg_alpha': 0.01085140151197864, 'reg_lambda': 0.42649451334385174, 'min_split_gain': 0.02502615555242195}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:17,362] Trial 421 finished with value: 0.25634998998101705 and parameters: {'learning_rate': 0.046283099827150134, 'num_leaves': 970, 'max_depth': 15, 'n_estimators': 588, 'min_child_samples': 40, 'subsample': 0.8123051658072354, 'colsample_bytree': 0.5282628180904548, 'reg_alpha': 2.397752136202319, 'reg_lambda': 1.0178482517088432e-08, 'min_split_gain': 0.028618753943931142}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:18,065] Trial 413 finished with value: 0.2571451281474258 and parameters: {'learning_rate': 0.03541730484263544, 'num_leaves': 505, 'max_depth': 59, 'n_estimators': 810, 'min_child_samples': 12, 'subsample': 0.8574955006555456, 'colsample_bytree': 0.5289893874525077, 'reg_alpha': 2.5045564984239244, 'reg_lambda': 1.5739814738514985e-08, 'min_split_gain': 0.027238078540757216}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:20,813] Trial 420 finished with value: 0.257992052747345 and parameters: {'learning_rate': 0.04640131242389393, 'num_leaves': 971, 'max_depth': 54, 'n_estimators': 589, 'min_child_samples': 48, 'subsample': 0.806559201478726, 'colsample_bytree': 0.5494422300189495, 'reg_alpha': 2.4263009165822274, 'reg_lambda': 1.010575225660074e-08, 'min_split_gain': 0.02606658189082651}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:22,977] Trial 375 finished with value: 0.2553708411213392 and parameters: {'learning_rate': 0.04119104932764163, 'num_leaves': 946, 'max_depth': 31, 'n_estimators': 780, 'min_child_samples': 28, 'subsample': 0.8504578574877186, 'colsample_bytree': 0.5384187871684807, 'reg_alpha': 0.02735518638065194, 'reg_lambda': 1.9525440162714502e-08, 'min_split_gain': 0.00456115476283489}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:27,892] Trial 376 finished with value: 0.25488217003571334 and parameters: {'learning_rate': 0.03972414845322773, 'num_leaves': 365, 'max_depth': 17, 'n_estimators': 561, 'min_child_samples': 44, 'subsample': 0.8516603351617431, 'colsample_bytree': 0.6916490097818526, 'reg_alpha': 0.001543394662338456, 'reg_lambda': 2.1266447031383964e-08, 'min_split_gain': 0.0011849036283963954}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:32,336] Trial 418 finished with value: 0.25996196510342695 and parameters: {'learning_rate': 0.05848592740906326, 'num_leaves': 501, 'max_depth': 54, 'n_estimators': 590, 'min_child_samples': 39, 'subsample': 0.6273827019449305, 'colsample_bytree': 0.5276563085348398, 'reg_alpha': 0.16604821094935204, 'reg_lambda': 1.650184573038643e-08, 'min_split_gain': 0.027594965777942522}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:33,230] Trial 427 finished with value: 0.2562357754819514 and parameters: {'learning_rate': 0.049486744672171463, 'num_leaves': 981, 'max_depth': 23, 'n_estimators': 601, 'min_child_samples': 43, 'subsample': 0.8787248406217563, 'colsample_bytree': 0.5515552885632006, 'reg_alpha': 2.9878039992459353, 'reg_lambda': 1.5902266466252913e-08, 'min_split_gain': 0.03214441343081286}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:33,501] Trial 425 finished with value: 0.2583959048459145 and parameters: {'learning_rate': 0.0594500185995502, 'num_leaves': 976, 'max_depth': 15, 'n_estimators': 589, 'min_child_samples': 47, 'subsample': 0.6374879925875089, 'colsample_bytree': 0.5510864983216665, 'reg_alpha': 0.15032909406980102, 'reg_lambda': 1.5643206601584272e-08, 'min_split_gain': 0.031058131499206974}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:37,432] Trial 428 finished with value: 0.25926440307886606 and parameters: {'learning_rate': 0.04918905942154783, 'num_leaves': 410, 'max_depth': 14, 'n_estimators': 327, 'min_child_samples': 48, 'subsample': 0.6292914510015596, 'colsample_bytree': 0.5499617356884103, 'reg_alpha': 0.04012135342089506, 'reg_lambda': 1.0190940883391894e-08, 'min_split_gain': 0.09680222425006657}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:37,582] Trial 436 finished with value: 0.2588056436241412 and parameters: {'learning_rate': 0.07167422361274027, 'num_leaves': 416, 'max_depth': 12, 'n_estimators': 329, 'min_child_samples': 48, 'subsample': 0.627860488694584, 'colsample_bytree': 0.552142969439867, 'reg_alpha': 9.752017766558462, 'reg_lambda': 3.839424558905815e-08, 'min_split_gain': 0.09459509220327601}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:37,999] Trial 382 finished with value: 0.2543252352639651 and parameters: {'learning_rate': 0.06491134416836604, 'num_leaves': 952, 'max_depth': 40, 'n_estimators': 781, 'min_child_samples': 59, 'subsample': 0.8263553097655647, 'colsample_bytree': 0.5376668391199554, 'reg_alpha': 0.005110127848385234, 'reg_lambda': 2.6693920282912546e-08, 'min_split_gain': 0.0020592345202201006}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:39,622] Trial 417 finished with value: 0.26159555574061005 and parameters: {'learning_rate': 0.058713390174029316, 'num_leaves': 974, 'max_depth': 26, 'n_estimators': 593, 'min_child_samples': 39, 'subsample': 0.8100427675934843, 'colsample_bytree': 0.5511830357185017, 'reg_alpha': 0.0008808291718484402, 'reg_lambda': 1.611519756641238e-08, 'min_split_gain': 0.02707997712255038}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:40,580] Trial 410 finished with value: 0.25977370389722343 and parameters: {'learning_rate': 0.03407587153034964, 'num_leaves': 504, 'max_depth': 59, 'n_estimators': 642, 'min_child_samples': 64, 'subsample': 0.6560120263549435, 'colsample_bytree': 0.5900048135383473, 'reg_alpha': 0.17810408894978164, 'reg_lambda': 1.6307661208170854e-08, 'min_split_gain': 0.023315809351860797}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:42,119] Trial 440 finished with value: 0.26038149117216214 and parameters: {'learning_rate': 0.09066201007894678, 'num_leaves': 1012, 'max_depth': 11, 'n_estimators': 842, 'min_child_samples': 43, 'subsample': 0.8348011056095166, 'colsample_bytree': 0.5531119552175604, 'reg_alpha': 0.000650680293724137, 'reg_lambda': 3.9896388923747146e-08, 'min_split_gain': 0.6341361699084498}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:44,200] Trial 424 finished with value: 0.25543103108049425 and parameters: {'learning_rate': 0.03480034590031458, 'num_leaves': 979, 'max_depth': 14, 'n_estimators': 590, 'min_child_samples': 40, 'subsample': 0.627654195484658, 'colsample_bytree': 0.5918760560247497, 'reg_alpha': 2.5921167404596805, 'reg_lambda': 4.5136252249069326e-08, 'min_split_gain': 0.024081354215518463}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:44,653] Trial 437 finished with value: 0.25605936919068795 and parameters: {'learning_rate': 0.0715042859876187, 'num_leaves': 387, 'max_depth': 11, 'n_estimators': 527, 'min_child_samples': 43, 'subsample': 0.8766968387628963, 'colsample_bytree': 0.5526960761106462, 'reg_alpha': 7.088042084653504, 'reg_lambda': 4.568628274840665e-08, 'min_split_gain': 0.039859214973475075}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:46,407] Trial 445 finished with value: 0.2637526884831267 and parameters: {'learning_rate': 0.0886385680360798, 'num_leaves': 1005, 'max_depth': 11, 'n_estimators': 837, 'min_child_samples': 43, 'subsample': 0.8323944547228939, 'colsample_bytree': 0.5646745529765264, 'reg_alpha': 6.5919279408516465, 'reg_lambda': 4.813935769112873e-08, 'min_split_gain': 0.6481595235733429}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:46,680] Trial 435 finished with value: 0.256471056811466 and parameters: {'learning_rate': 0.09097594703174987, 'num_leaves': 597, 'max_depth': 11, 'n_estimators': 638, 'min_child_samples': 47, 'subsample': 0.606233887105042, 'colsample_bytree': 0.59159498096998, 'reg_alpha': 0.18429331769807705, 'reg_lambda': 4.695319967149492e-08, 'min_split_gain': 0.038234598947622295}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:48,910] Trial 432 finished with value: 0.2590474328261549 and parameters: {'learning_rate': 0.048944155646974755, 'num_leaves': 419, 'max_depth': 15, 'n_estimators': 643, 'min_child_samples': 48, 'subsample': 0.5993560822667731, 'colsample_bytree': 0.5921440891727423, 'reg_alpha': 0.19331072440833108, 'reg_lambda': 1.0111057936365357e-08, 'min_split_gain': 0.09754098510267115}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:51,987] Trial 422 finished with value: 0.2601905192167841 and parameters: {'learning_rate': 0.04629426424222463, 'num_leaves': 979, 'max_depth': 14, 'n_estimators': 591, 'min_child_samples': 40, 'subsample': 0.6292553880064299, 'colsample_bytree': 0.5514805186201587, 'reg_alpha': 0.0007633027717626578, 'reg_lambda': 1.7301608774851667e-08, 'min_split_gain': 0.029807777245364347}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:52,711] Trial 426 finished with value: 0.25988177281244806 and parameters: {'learning_rate': 0.03322081700380945, 'num_leaves': 389, 'max_depth': 14, 'n_estimators': 525, 'min_child_samples': 43, 'subsample': 0.6262083166686327, 'colsample_bytree': 0.542577779848201, 'reg_alpha': 0.12694801718860194, 'reg_lambda': 1.595209212511952e-08, 'min_split_gain': 0.09669971553660191}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:53,945] Trial 442 finished with value: 0.2621118841556824 and parameters: {'learning_rate': 0.08956772473805832, 'num_leaves': 314, 'max_depth': 11, 'n_estimators': 635, 'min_child_samples': 40, 'subsample': 0.6031707269999468, 'colsample_bytree': 0.5838042261605815, 'reg_alpha': 4.745881252119041, 'reg_lambda': 4.973551355843113e-08, 'min_split_gain': 0.044947539190172456}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:54,832] Trial 431 finished with value: 0.2587576988015738 and parameters: {'learning_rate': 0.04789827029986761, 'num_leaves': 1007, 'max_depth': 15, 'n_estimators': 637, 'min_child_samples': 40, 'subsample': 0.6233230801749932, 'colsample_bytree': 0.5912978420327087, 'reg_alpha': 0.0002998601399903646, 'reg_lambda': 4.140387232410388e-08, 'min_split_gain': 0.09136503089555474}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:56,693] Trial 423 finished with value: 0.2607771442329137 and parameters: {'learning_rate': 0.047820760325386236, 'num_leaves': 967, 'max_depth': 26, 'n_estimators': 586, 'min_child_samples': 40, 'subsample': 0.6266357385156471, 'colsample_bytree': 0.5525311229942417, 'reg_alpha': 0.1611207029473475, 'reg_lambda': 1.7060560920891234e-08, 'min_split_gain': 0.029059962636016585}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:58,432] Trial 415 finished with value: 0.25935435976665905 and parameters: {'learning_rate': 0.05818620463065378, 'num_leaves': 970, 'max_depth': 54, 'n_estimators': 331, 'min_child_samples': 12, 'subsample': 0.630043722640839, 'colsample_bytree': 0.5284808588132645, 'reg_alpha': 0.0007009292487973611, 'reg_lambda': 1.59067545301806e-08, 'min_split_gain': 0.027655746150647163}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:18:59,573] Trial 447 finished with value: 0.2622199609396521 and parameters: {'learning_rate': 0.07078162969227164, 'num_leaves': 947, 'max_depth': 12, 'n_estimators': 529, 'min_child_samples': 42, 'subsample': 0.8367092853354815, 'colsample_bytree': 0.5435376608658063, 'reg_alpha': 0.00022501321530110036, 'reg_lambda': 3.298089295172687e-08, 'min_split_gain': 0.5507547777427597}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:00,442] Trial 429 finished with value: 0.25771769572581593 and parameters: {'learning_rate': 0.047328287241793804, 'num_leaves': 1008, 'max_depth': 14, 'n_estimators': 841, 'min_child_samples': 48, 'subsample': 0.8101550516189333, 'colsample_bytree': 0.5509343233427142, 'reg_alpha': 0.18460036624647697, 'reg_lambda': 4.5550244575967216e-08, 'min_split_gain': 0.02804136045080731}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:05,948] Trial 439 finished with value: 0.259580633440404 and parameters: {'learning_rate': 0.07005180558470436, 'num_leaves': 1011, 'max_depth': 12, 'n_estimators': 638, 'min_child_samples': 17, 'subsample': 0.6081026189059131, 'colsample_bytree': 0.586560946559768, 'reg_alpha': 0.11507056405452931, 'reg_lambda': 4.578058684556814e-08, 'min_split_gain': 0.09469543955331242}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:06,333] Trial 438 finished with value: 0.2578401006665401 and parameters: {'learning_rate': 0.07191752390747153, 'num_leaves': 1008, 'max_depth': 12, 'n_estimators': 612, 'min_child_samples': 48, 'subsample': 0.6035379454916432, 'colsample_bytree': 0.5506277675916094, 'reg_alpha': 0.21025180883610928, 'reg_lambda': 4.480812035533922e-08, 'min_split_gain': 0.04149914047410574}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:06,693] Trial 444 finished with value: 0.25958984577474414 and parameters: {'learning_rate': 0.0904178893833625, 'num_leaves': 1006, 'max_depth': 11, 'n_estimators': 646, 'min_child_samples': 43, 'subsample': 0.5010599338780749, 'colsample_bytree': 0.5925365902056434, 'reg_alpha': 0.2454074254809239, 'reg_lambda': 4.756776335002009e-08, 'min_split_gain': 0.0462532662138974}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:09,615] Trial 453 finished with value: 0.2571683403822026 and parameters: {'learning_rate': 0.07543588392730491, 'num_leaves': 948, 'max_depth': 9, 'n_estimators': 613, 'min_child_samples': 37, 'subsample': 0.8315992054764005, 'colsample_bytree': 0.5791187290013795, 'reg_alpha': 4.762542522689081, 'reg_lambda': 1.1102689877932328e-07, 'min_split_gain': 0.04529342442337358}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:10,416] Trial 443 finished with value: 0.2597875257694629 and parameters: {'learning_rate': 0.08804368720065021, 'num_leaves': 1002, 'max_depth': 11, 'n_estimators': 836, 'min_child_samples': 40, 'subsample': 0.8346274014340291, 'colsample_bytree': 0.5859068550015387, 'reg_alpha': 0.0003073925978004655, 'reg_lambda': 4.614925240947239e-08, 'min_split_gain': 0.04413293107934314}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:11,385] Trial 454 finished with value: 0.25658689048187583 and parameters: {'learning_rate': 0.07485120228241722, 'num_leaves': 943, 'max_depth': 9, 'n_estimators': 844, 'min_child_samples': 36, 'subsample': 0.8355857242764587, 'colsample_bytree': 0.5705529435489912, 'reg_alpha': 4.3923956202461945, 'reg_lambda': 2.459269751064494e-08, 'min_split_gain': 0.04930435536394041}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:13,077] Trial 452 finished with value: 0.2609586473696899 and parameters: {'learning_rate': 0.07193662495693778, 'num_leaves': 306, 'max_depth': 47, 'n_estimators': 823, 'min_child_samples': 37, 'subsample': 0.8168989958676833, 'colsample_bytree': 0.5459480824355336, 'reg_alpha': 4.564401615897878, 'reg_lambda': 2.6714068950199213e-08, 'min_split_gain': 0.0465744837609345}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:13,913] Trial 448 finished with value: 0.25910075392071014 and parameters: {'learning_rate': 0.08946452865055404, 'num_leaves': 1004, 'max_depth': 12, 'n_estimators': 528, 'min_child_samples': 56, 'subsample': 0.8350187972466648, 'colsample_bytree': 0.5701214929975535, 'reg_alpha': 0.046205312148686506, 'reg_lambda': 4.078307056373827e-08, 'min_split_gain': 0.047279972555943565}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:14,644] Trial 450 finished with value: 0.26195273679171477 and parameters: {'learning_rate': 0.07152697109281021, 'num_leaves': 925, 'max_depth': 9, 'n_estimators': 831, 'min_child_samples': 56, 'subsample': 0.8123618768332165, 'colsample_bytree': 0.5775382146658949, 'reg_alpha': 0.04553800032629827, 'reg_lambda': 1.0135174030931893e-08, 'min_split_gain': 0.052431379376400224}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:16,040] Trial 455 finished with value: 0.25559613809772647 and parameters: {'learning_rate': 0.07177664480918906, 'num_leaves': 946, 'max_depth': 47, 'n_estimators': 840, 'min_child_samples': 37, 'subsample': 0.8125337569653667, 'colsample_bytree': 0.570522720272581, 'reg_alpha': 4.607819264657903, 'reg_lambda': 2.614680053181504e-08, 'min_split_gain': 0.04966319047543916}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:16,353] Trial 446 finished with value: 0.26091327266635433 and parameters: {'learning_rate': 0.07642878834197414, 'num_leaves': 600, 'max_depth': 11, 'n_estimators': 527, 'min_child_samples': 42, 'subsample': 0.60133216511273, 'colsample_bytree': 0.5818835668259831, 'reg_alpha': 0.056316984946058454, 'reg_lambda': 4.03586891008745e-08, 'min_split_gain': 0.046658064867473044}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:18,925] Trial 434 finished with value: 0.2577729773005735 and parameters: {'learning_rate': 0.04807131817170054, 'num_leaves': 355, 'max_depth': 54, 'n_estimators': 839, 'min_child_samples': 48, 'subsample': 0.6603754561917077, 'colsample_bytree': 0.550566238215872, 'reg_alpha': 0.14178450810912863, 'reg_lambda': 4.5293574470707774e-08, 'min_split_gain': 0.04190996873462538}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:19,979] Trial 451 finished with value: 0.25842765356531283 and parameters: {'learning_rate': 0.07123964590500935, 'num_leaves': 951, 'max_depth': 9, 'n_estimators': 823, 'min_child_samples': 40, 'subsample': 0.8118357748060987, 'colsample_bytree': 0.5418517329522045, 'reg_alpha': 0.05559956766565624, 'reg_lambda': 2.6059300635612837e-08, 'min_split_gain': 0.04895758005183075}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:20,401] Trial 466 finished with value: 0.26290075167215127 and parameters: {'learning_rate': 0.0564805631554081, 'num_leaves': 922, 'max_depth': 9, 'n_estimators': 858, 'min_child_samples': 56, 'subsample': 0.8095593519078486, 'colsample_bytree': 0.5140534875081144, 'reg_alpha': 7.121422563929898, 'reg_lambda': 2.5272186628195983e-08, 'min_split_gain': 0.9558544725800682}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:21,098] Trial 458 finished with value: 0.2560883187726934 and parameters: {'learning_rate': 0.07187900694763635, 'num_leaves': 927, 'max_depth': 47, 'n_estimators': 825, 'min_child_samples': 56, 'subsample': 0.8131110031500064, 'colsample_bytree': 0.5698879279920651, 'reg_alpha': 4.903187179251747, 'reg_lambda': 2.6686905966358494e-08, 'min_split_gain': 0.04322456413820601}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:23,002] Trial 441 finished with value: 0.2587191662967199 and parameters: {'learning_rate': 0.07552100784772613, 'num_leaves': 1006, 'max_depth': 11, 'n_estimators': 841, 'min_child_samples': 17, 'subsample': 0.6588809177766771, 'colsample_bytree': 0.5973243281790322, 'reg_alpha': 0.000649822520363632, 'reg_lambda': 4.26543792893534e-08, 'min_split_gain': 0.0479076678326091}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:24,008] Trial 462 finished with value: 0.255864873130443 and parameters: {'learning_rate': 0.07831436181181407, 'num_leaves': 949, 'max_depth': 9, 'n_estimators': 824, 'min_child_samples': 35, 'subsample': 0.8132587014978595, 'colsample_bytree': 0.5682051168946091, 'reg_alpha': 4.776032606077252, 'reg_lambda': 1.1208212603525043e-07, 'min_split_gain': 0.04759398669237779}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:24,566] Trial 449 finished with value: 0.26316284215905483 and parameters: {'learning_rate': 0.07064100248034862, 'num_leaves': 949, 'max_depth': 12, 'n_estimators': 825, 'min_child_samples': 56, 'subsample': 0.8133920489143631, 'colsample_bytree': 0.961508159560249, 'reg_alpha': 0.0028305119816254727, 'reg_lambda': 4.257698611400046e-08, 'min_split_gain': 0.04612004600300884}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:25,693] Trial 459 finished with value: 0.25622343205843195 and parameters: {'learning_rate': 0.07706870725965405, 'num_leaves': 946, 'max_depth': 44, 'n_estimators': 826, 'min_child_samples': 56, 'subsample': 0.8129329643414859, 'colsample_bytree': 0.5644685434898652, 'reg_alpha': 4.6913386603248455, 'reg_lambda': 1.0296860033920901e-07, 'min_split_gain': 0.05207579563453382}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:26,318] Trial 456 finished with value: 0.2589098135478487 and parameters: {'learning_rate': 0.07133438255890637, 'num_leaves': 949, 'max_depth': 9, 'n_estimators': 825, 'min_child_samples': 56, 'subsample': 0.8337038391972783, 'colsample_bytree': 0.5711599568843572, 'reg_alpha': 0.0461212411396515, 'reg_lambda': 2.591280294558227e-08, 'min_split_gain': 0.04784609610897622}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:28,621] Trial 457 finished with value: 0.26237231704120084 and parameters: {'learning_rate': 0.07245553964977862, 'num_leaves': 997, 'max_depth': 9, 'n_estimators': 616, 'min_child_samples': 58, 'subsample': 0.8110141200226808, 'colsample_bytree': 0.5654093686823372, 'reg_alpha': 0.0025749012688104232, 'reg_lambda': 2.6747074021796396e-08, 'min_split_gain': 0.051226543767551314}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:35,220] Trial 460 finished with value: 0.2608302871057854 and parameters: {'learning_rate': 0.0776410929229853, 'num_leaves': 949, 'max_depth': 9, 'n_estimators': 669, 'min_child_samples': 35, 'subsample': 0.8350638910106837, 'colsample_bytree': 0.9682761356562637, 'reg_alpha': 0.05423266920544129, 'reg_lambda': 2.3197221753359733e-08, 'min_split_gain': 0.05040759881016393}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:38,982] Trial 474 finished with value: 0.26101416705709385 and parameters: {'learning_rate': 0.05749863568117985, 'num_leaves': 990, 'max_depth': 34, 'n_estimators': 668, 'min_child_samples': 58, 'subsample': 0.8662731587970274, 'colsample_bytree': 0.5287575337481731, 'reg_alpha': 0.0029652440511699677, 'reg_lambda': 8.15407563378719e-08, 'min_split_gain': 0.36046942610931865}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:43,241] Trial 433 finished with value: 0.2583710367019338 and parameters: {'learning_rate': 0.0337261583558199, 'num_leaves': 295, 'max_depth': 14, 'n_estimators': 634, 'min_child_samples': 48, 'subsample': 0.624471636200232, 'colsample_bytree': 0.5475103866725892, 'reg_alpha': 0.20762288284251582, 'reg_lambda': 1.6385872276545733e-08, 'min_split_gain': 0.03242504710296414}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:45,967] Trial 463 finished with value: 0.2593829905326844 and parameters: {'learning_rate': 0.07962477660577398, 'num_leaves': 643, 'max_depth': 46, 'n_estimators': 818, 'min_child_samples': 35, 'subsample': 0.8146638793153795, 'colsample_bytree': 0.5702446474527353, 'reg_alpha': 0.011118510364020057, 'reg_lambda': 2.4844391661336144e-08, 'min_split_gain': 0.06481154591465467}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:46,277] Trial 470 finished with value: 0.25651491560754053 and parameters: {'learning_rate': 0.05576130951814945, 'num_leaves': 647, 'max_depth': 18, 'n_estimators': 858, 'min_child_samples': 59, 'subsample': 0.856379145635742, 'colsample_bytree': 0.5129116812592222, 'reg_alpha': 6.871901497694164, 'reg_lambda': 2.411418668643835e-08, 'min_split_gain': 0.018574338800275682}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:47,243] Trial 414 finished with value: 0.6342787858913224 and parameters: {'learning_rate': 0.0010072784053172497, 'num_leaves': 972, 'max_depth': 11, 'n_estimators': 534, 'min_child_samples': 40, 'subsample': 0.8105889647935897, 'colsample_bytree': 0.5286606842306161, 'reg_alpha': 2.472369261145319, 'reg_lambda': 1.695910425029207e-08, 'min_split_gain': 0.02702910253484272}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:51,187] Trial 477 finished with value: 0.25663606924414106 and parameters: {'learning_rate': 0.059375909503471894, 'num_leaves': 989, 'max_depth': 62, 'n_estimators': 883, 'min_child_samples': 58, 'subsample': 0.8723224544022465, 'colsample_bytree': 0.5301881083260449, 'reg_alpha': 7.391109066624518, 'reg_lambda': 1.4594573910613903e-08, 'min_split_gain': 0.018571830336218058}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:53,964] Trial 394 finished with value: 0.2550925003347958 and parameters: {'learning_rate': 0.06825296194942096, 'num_leaves': 443, 'max_depth': 59, 'n_estimators': 579, 'min_child_samples': 44, 'subsample': 0.6399626001257805, 'colsample_bytree': 0.712914096731591, 'reg_alpha': 0.0008671713437663686, 'reg_lambda': 3.267839882853572e-08, 'min_split_gain': 0.0017520116721110217}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:19:54,839] Trial 480 finished with value: 0.25791857495685927 and parameters: {'learning_rate': 0.05720207402888508, 'num_leaves': 893, 'max_depth': 62, 'n_estimators': 796, 'min_child_samples': 46, 'subsample': 0.8447834893243835, 'colsample_bytree': 0.51228990060021, 'reg_alpha': 9.284007417121208, 'reg_lambda': 1.4656066813708901e-08, 'min_split_gain': 0.017733320280761914}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:07,892] Trial 485 finished with value: 0.2557595853066373 and parameters: {'learning_rate': 0.1187754251285057, 'num_leaves': 893, 'max_depth': 37, 'n_estimators': 876, 'min_child_samples': 63, 'subsample': 0.8689370446543335, 'colsample_bytree': 0.5374335409521471, 'reg_alpha': 2.6263884953527596, 'reg_lambda': 1.5501736778651283e-08, 'min_split_gain': 0.020380710774848265}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:08,658] Trial 464 finished with value: 0.2535433102023894 and parameters: {'learning_rate': 0.07607350535939446, 'num_leaves': 899, 'max_depth': 47, 'n_estimators': 875, 'min_child_samples': 35, 'subsample': 0.8643211209068412, 'colsample_bytree': 0.5662130806326541, 'reg_alpha': 4.856657815276274, 'reg_lambda': 2.6230232531911523e-08, 'min_split_gain': 0.0006845067851606668}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:11,088] Trial 476 finished with value: 0.2555098684937005 and parameters: {'learning_rate': 0.05707159265949357, 'num_leaves': 90, 'max_depth': 37, 'n_estimators': 813, 'min_child_samples': 59, 'subsample': 0.8659154601543076, 'colsample_bytree': 0.5310770573662351, 'reg_alpha': 0.00227841958526565, 'reg_lambda': 1.5123839324232958e-08, 'min_split_gain': 0.019016574968395233}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:13,970] Trial 467 finished with value: 0.2555740280487529 and parameters: {'learning_rate': 0.05686211178219794, 'num_leaves': 950, 'max_depth': 9, 'n_estimators': 867, 'min_child_samples': 58, 'subsample': 0.8118637303549816, 'colsample_bytree': 0.9859682805896539, 'reg_alpha': 0.0029586958885132572, 'reg_lambda': 2.245247928178406e-08, 'min_split_gain': 0.01709238422872762}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:18,368] Trial 430 finished with value: 0.6121770395214425 and parameters: {'learning_rate': 0.0009548125716118488, 'num_leaves': 980, 'max_depth': 15, 'n_estimators': 610, 'min_child_samples': 43, 'subsample': 0.5964688186360586, 'colsample_bytree': 0.55205887493599, 'reg_alpha': 2.4580332829701432, 'reg_lambda': 4.4687563701078674e-08, 'min_split_gain': 0.0981699073848293}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:30,236] Trial 488 finished with value: 0.25563024740547613 and parameters: {'learning_rate': 0.05485036403162892, 'num_leaves': 904, 'max_depth': 16, 'n_estimators': 800, 'min_child_samples': 63, 'subsample': 0.858143920641872, 'colsample_bytree': 0.5390074416083479, 'reg_alpha': 9.523543037464696, 'reg_lambda': 1.0476527595234637e-08, 'min_split_gain': 0.0007849891193058133}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:30,668] Trial 468 finished with value: 0.2588091959141369 and parameters: {'learning_rate': 0.05740553786774546, 'num_leaves': 645, 'max_depth': 13, 'n_estimators': 671, 'min_child_samples': 34, 'subsample': 0.8623841146583904, 'colsample_bytree': 0.541488730142591, 'reg_alpha': 0.002613244085227328, 'reg_lambda': 2.4858768163913485e-08, 'min_split_gain': 0.018925181364976493}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:32,842] Trial 482 finished with value: 0.25538922842114525 and parameters: {'learning_rate': 0.05821370713785859, 'num_leaves': 895, 'max_depth': 16, 'n_estimators': 879, 'min_child_samples': 45, 'subsample': 0.847036810340352, 'colsample_bytree': 0.5123488524856966, 'reg_alpha': 2.7898599867977163, 'reg_lambda': 1.4685837350979076e-08, 'min_split_gain': 0.000684537897812012}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:33,066] Trial 486 finished with value: 0.2555756119696271 and parameters: {'learning_rate': 0.05893673815152215, 'num_leaves': 855, 'max_depth': 62, 'n_estimators': 793, 'min_child_samples': 46, 'subsample': 0.8595359908845642, 'colsample_bytree': 0.5384314597948701, 'reg_alpha': 2.9567472349495123, 'reg_lambda': 1.5335088919967847e-08, 'min_split_gain': 0.017190270328349386}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:34,224] Trial 469 finished with value: 0.2570071251087481 and parameters: {'learning_rate': 0.056908305973287845, 'num_leaves': 894, 'max_depth': 16, 'n_estimators': 879, 'min_child_samples': 46, 'subsample': 0.859934341362224, 'colsample_bytree': 0.5121539223296617, 'reg_alpha': 0.0025790278017264537, 'reg_lambda': 2.484711965331779e-08, 'min_split_gain': 0.018218655412145915}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:36,197] Trial 475 finished with value: 0.26008018255934456 and parameters: {'learning_rate': 0.057530887866072404, 'num_leaves': 1024, 'max_depth': 16, 'n_estimators': 870, 'min_child_samples': 45, 'subsample': 0.8647103230038298, 'colsample_bytree': 0.5137483703588717, 'reg_alpha': 0.0024009311570663766, 'reg_lambda': 8.546744021568253e-08, 'min_split_gain': 0.0171853863275226}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:37,608] Trial 481 finished with value: 0.25336186000445954 and parameters: {'learning_rate': 0.05747186247763484, 'num_leaves': 987, 'max_depth': 16, 'n_estimators': 874, 'min_child_samples': 59, 'subsample': 0.8636918845819727, 'colsample_bytree': 0.5274895673020713, 'reg_alpha': 9.388961921189143, 'reg_lambda': 1.0055346054976132e-08, 'min_split_gain': 0.00018311472541075247}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:41,604] Trial 484 finished with value: 0.2547873047020653 and parameters: {'learning_rate': 0.057509096363573854, 'num_leaves': 976, 'max_depth': 18, 'n_estimators': 867, 'min_child_samples': 45, 'subsample': 0.8545785571099798, 'colsample_bytree': 0.5273647533480232, 'reg_alpha': 9.932275556832327, 'reg_lambda': 1.4330274862552854e-08, 'min_split_gain': 0.0004549393512387782}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:42,527] Trial 473 finished with value: 0.258286661627494 and parameters: {'learning_rate': 0.05928216202099926, 'num_leaves': 890, 'max_depth': 34, 'n_estimators': 877, 'min_child_samples': 59, 'subsample': 0.9835978254582642, 'colsample_bytree': 0.5280711400497594, 'reg_alpha': 0.0024910666262991927, 'reg_lambda': 0.009020294243154156, 'min_split_gain': 0.01711963072377332}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:44,831] Trial 471 finished with value: 0.2555649917496408 and parameters: {'learning_rate': 0.05731595592231312, 'num_leaves': 241, 'max_depth': 36, 'n_estimators': 809, 'min_child_samples': 58, 'subsample': 0.864061899734014, 'colsample_bytree': 0.5151058977476595, 'reg_alpha': 0.0031835153363658887, 'reg_lambda': 2.1919984251573363e-08, 'min_split_gain': 0.015008448780393114}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:49,874] Trial 478 finished with value: 0.25608152321003314 and parameters: {'learning_rate': 0.057672640228038675, 'num_leaves': 981, 'max_depth': 62, 'n_estimators': 883, 'min_child_samples': 45, 'subsample': 0.855764176728962, 'colsample_bytree': 0.5143573436034019, 'reg_alpha': 0.010029980798685102, 'reg_lambda': 0.007673380052237488, 'min_split_gain': 0.020388538807254084}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:53,089] Trial 487 finished with value: 0.25540962417691326 and parameters: {'learning_rate': 0.05279887917380721, 'num_leaves': 1024, 'max_depth': 57, 'n_estimators': 795, 'min_child_samples': 46, 'subsample': 0.8446564109210656, 'colsample_bytree': 0.5365255447670092, 'reg_alpha': 9.965827788494693, 'reg_lambda': 1.022755966742808e-08, 'min_split_gain': 0.0011987034692522436}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:54,996] Trial 493 finished with value: 0.2583944193862758 and parameters: {'learning_rate': 0.06430563632957793, 'num_leaves': 906, 'max_depth': 42, 'n_estimators': 875, 'min_child_samples': 46, 'subsample': 0.8269768341266486, 'colsample_bytree': 0.5401853689927743, 'reg_alpha': 9.504876409597468, 'reg_lambda': 1.6838744062362037, 'min_split_gain': 0.0012571663662917145}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:20:58,661] Trial 479 finished with value: 0.2547406347342503 and parameters: {'learning_rate': 0.058087318409578204, 'num_leaves': 655, 'max_depth': 62, 'n_estimators': 670, 'min_child_samples': 59, 'subsample': 0.8660766225976775, 'colsample_bytree': 0.5289947491915022, 'reg_alpha': 2.798620285983241, 'reg_lambda': 1.5397870353195995e-08, 'min_split_gain': 9.078860073362654e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:09,776] Trial 483 finished with value: 0.2599848172263732 and parameters: {'learning_rate': 0.05572462520831292, 'num_leaves': 655, 'max_depth': 60, 'n_estimators': 882, 'min_child_samples': 45, 'subsample': 0.859345354219295, 'colsample_bytree': 0.5143128512068397, 'reg_alpha': 0.011573229502692963, 'reg_lambda': 1.316851327274388e-08, 'min_split_gain': 0.019249222499287448}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:10,050] Trial 465 finished with value: 0.25971430951756086 and parameters: {'learning_rate': 0.06971220722673345, 'num_leaves': 937, 'max_depth': 47, 'n_estimators': 820, 'min_child_samples': 57, 'subsample': 0.8612240590318527, 'colsample_bytree': 0.5420884868702863, 'reg_alpha': 0.051012719917388774, 'reg_lambda': 2.5336880485030804e-08, 'min_split_gain': 0.0006413290892512136}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:16,754] Trial 499 finished with value: 0.25721241482519525 and parameters: {'learning_rate': 0.05068851325802763, 'num_leaves': 857, 'max_depth': 60, 'n_estimators': 894, 'min_child_samples': 39, 'subsample': 0.8253172211293534, 'colsample_bytree': 0.5238747319471205, 'reg_alpha': 9.967277010382949, 'reg_lambda': 4.828269389835845, 'min_split_gain': 0.001372888681081217}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:26,556] Trial 494 finished with value: 0.2546227787664252 and parameters: {'learning_rate': 0.06357893507003007, 'num_leaves': 263, 'max_depth': 57, 'n_estimators': 903, 'min_child_samples': 38, 'subsample': 0.65031533024445, 'colsample_bytree': 0.5409213906855594, 'reg_alpha': 7.468215473089654, 'reg_lambda': 4.419900291832421, 'min_split_gain': 0.00023497085583904062}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:29,331] Trial 461 finished with value: 0.5272882036598666 and parameters: {'learning_rate': 0.0009561676701504027, 'num_leaves': 919, 'max_depth': 10, 'n_estimators': 822, 'min_child_samples': 37, 'subsample': 0.8167798509081452, 'colsample_bytree': 0.5632322949155137, 'reg_alpha': 4.633908777399683, 'reg_lambda': 2.593561070120723e-08, 'min_split_gain': 0.051716457653570476}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:30,020] Trial 502 finished with value: 0.2563127195559424 and parameters: {'learning_rate': 0.048632235471576066, 'num_leaves': 868, 'max_depth': 43, 'n_estimators': 914, 'min_child_samples': 60, 'subsample': 0.8847592583371096, 'colsample_bytree': 0.5247240224052979, 'reg_alpha': 6.918796632399927, 'reg_lambda': 5.19931442948691, 'min_split_gain': 0.0013892661800603734}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:34,528] Trial 505 finished with value: 0.25697332131217476 and parameters: {'learning_rate': 0.050027927348447576, 'num_leaves': 919, 'max_depth': 60, 'n_estimators': 899, 'min_child_samples': 42, 'subsample': 0.8836494840273764, 'colsample_bytree': 0.524824318420535, 'reg_alpha': 6.562411661810417, 'reg_lambda': 6.120992572170009, 'min_split_gain': 0.0009591189944114346}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:35,549] Trial 388 finished with value: 0.4950041854737221 and parameters: {'learning_rate': 0.0015053584483714182, 'num_leaves': 1024, 'max_depth': 42, 'n_estimators': 567, 'min_child_samples': 60, 'subsample': 0.8287574298527136, 'colsample_bytree': 0.5769190961624895, 'reg_alpha': 0.0013761713964579108, 'reg_lambda': 2.798706216235939e-08, 'min_split_gain': 0.05948381380296306}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:38,624] Trial 498 finished with value: 0.2546932732064606 and parameters: {'learning_rate': 0.06416538074750203, 'num_leaves': 1023, 'max_depth': 42, 'n_estimators': 871, 'min_child_samples': 39, 'subsample': 0.823141774083246, 'colsample_bytree': 0.6543981943423658, 'reg_alpha': 6.486157598242371, 'reg_lambda': 4.588337528601768, 'min_split_gain': 0.0006505775881835557}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:43,328] Trial 514 finished with value: 0.2579951359721885 and parameters: {'learning_rate': 0.08391710911829503, 'num_leaves': 791, 'max_depth': 39, 'n_estimators': 853, 'min_child_samples': 24, 'subsample': 0.6607622732870826, 'colsample_bytree': 0.6641608768036675, 'reg_alpha': 5.961459360217456, 'reg_lambda': 6.883058540468442, 'min_split_gain': 0.07113054962780543}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:48,980] Trial 381 finished with value: 0.45878640345339505 and parameters: {'learning_rate': 0.001720408876570831, 'num_leaves': 911, 'max_depth': 41, 'n_estimators': 559, 'min_child_samples': 59, 'subsample': 0.8250970017403569, 'colsample_bytree': 0.5369224111516657, 'reg_alpha': 0.0012632331226562776, 'reg_lambda': 2.547995037470093e-08, 'min_split_gain': 0.0003012617108143333}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:49,218] Trial 517 finished with value: 0.2646078215003243 and parameters: {'learning_rate': 0.043012150127310245, 'num_leaves': 989, 'max_depth': 55, 'n_estimators': 545, 'min_child_samples': 62, 'subsample': 0.8454954120586212, 'colsample_bytree': 0.5216097268693778, 'reg_alpha': 1.9081484498967887, 'reg_lambda': 1.0300035407690357e-08, 'min_split_gain': 0.7605021227597881}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:51,842] Trial 489 finished with value: 0.25594495154004887 and parameters: {'learning_rate': 0.05263129187009207, 'num_leaves': 855, 'max_depth': 37, 'n_estimators': 874, 'min_child_samples': 42, 'subsample': 0.8579927754167425, 'colsample_bytree': 0.540514108214213, 'reg_alpha': 2.7256963379513772, 'reg_lambda': 1.0174050102139938e-08, 'min_split_gain': 0.00014901776896707652}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:53,803] Trial 501 finished with value: 0.254899637843885 and parameters: {'learning_rate': 0.049384348236194836, 'num_leaves': 921, 'max_depth': 57, 'n_estimators': 917, 'min_child_samples': 38, 'subsample': 0.8804180498665473, 'colsample_bytree': 0.5252470589740521, 'reg_alpha': 7.305022591866648, 'reg_lambda': 4.507259995834896, 'min_split_gain': 0.00048448104308098446}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:55,307] Trial 516 finished with value: 0.2567615363355094 and parameters: {'learning_rate': 0.08465868508445018, 'num_leaves': 987, 'max_depth': 52, 'n_estimators': 545, 'min_child_samples': 25, 'subsample': 0.6573131176253747, 'colsample_bytree': 0.5603510412979538, 'reg_alpha': 1.3917694273226193, 'reg_lambda': 6.919302724891119, 'min_split_gain': 0.07421334369517552}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:56,144] Trial 491 finished with value: 0.2544862126515347 and parameters: {'learning_rate': 0.052402878028066396, 'num_leaves': 1023, 'max_depth': 55, 'n_estimators': 864, 'min_child_samples': 38, 'subsample': 0.8583000763882022, 'colsample_bytree': 0.5132504929253319, 'reg_alpha': 1.3540226053343267, 'reg_lambda': 5.485506382664927, 'min_split_gain': 0.00043493647925944514}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:21:59,827] Trial 492 finished with value: 0.25363164236737545 and parameters: {'learning_rate': 0.052868747521113424, 'num_leaves': 255, 'max_depth': 16, 'n_estimators': 917, 'min_child_samples': 39, 'subsample': 0.8260818285626025, 'colsample_bytree': 0.5134479789943638, 'reg_alpha': 1.2952057659379401, 'reg_lambda': 5.741846404719004, 'min_split_gain': 0.0013914607022652622}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:01,922] Trial 507 finished with value: 0.2527355031898406 and parameters: {'learning_rate': 0.04529656326760325, 'num_leaves': 921, 'max_depth': 60, 'n_estimators': 903, 'min_child_samples': 60, 'subsample': 0.8750056601466312, 'colsample_bytree': 0.5288676624045511, 'reg_alpha': 6.228148217856839, 'reg_lambda': 5.172915599780887, 'min_split_gain': 0.0011123802855812737}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:02,937] Trial 506 finished with value: 0.2536414409797629 and parameters: {'learning_rate': 0.0491994589897865, 'num_leaves': 920, 'max_depth': 40, 'n_estimators': 919, 'min_child_samples': 62, 'subsample': 0.8833365529300939, 'colsample_bytree': 0.6571585170261566, 'reg_alpha': 6.096876205550838, 'reg_lambda': 5.378757906139428, 'min_split_gain': 0.0015392986861338489}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:04,515] Trial 515 finished with value: 0.2556557762144353 and parameters: {'learning_rate': 0.06502735810916789, 'num_leaves': 986, 'max_depth': 57, 'n_estimators': 552, 'min_child_samples': 41, 'subsample': 0.6611628439151443, 'colsample_bytree': 0.8571056763967375, 'reg_alpha': 3.8673335440658527, 'reg_lambda': 1.2506889433372343, 'min_split_gain': 0.02622062902151883}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:06,175] Trial 523 finished with value: 0.2637835944926524 and parameters: {'learning_rate': 0.06616420356068088, 'num_leaves': 962, 'max_depth': 13, 'n_estimators': 846, 'min_child_samples': 31, 'subsample': 0.6521268406916634, 'colsample_bytree': 0.5146069976648436, 'reg_alpha': 1.2517127170399145, 'reg_lambda': 8.596774364036242e-05, 'min_split_gain': 0.7981587623836313}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:06,503] Trial 495 finished with value: 0.2541225692602752 and parameters: {'learning_rate': 0.06452452494129036, 'num_leaves': 252, 'max_depth': 62, 'n_estimators': 915, 'min_child_samples': 38, 'subsample': 0.8239664603995633, 'colsample_bytree': 0.5408610427273256, 'reg_alpha': 1.6008250035630476, 'reg_lambda': 8.07928320117674e-08, 'min_split_gain': 0.002155993970237708}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:08,526] Trial 503 finished with value: 0.2519456172438984 and parameters: {'learning_rate': 0.05024988063426686, 'num_leaves': 925, 'max_depth': 55, 'n_estimators': 855, 'min_child_samples': 61, 'subsample': 0.8789663711022501, 'colsample_bytree': 0.52073707473068, 'reg_alpha': 6.8661976852896975, 'reg_lambda': 5.205975807553854, 'min_split_gain': 4.544312058760646e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:09,194] Trial 513 finished with value: 0.26140227806648014 and parameters: {'learning_rate': 0.045849360466175035, 'num_leaves': 923, 'max_depth': 55, 'n_estimators': 855, 'min_child_samples': 62, 'subsample': 0.8778814519260385, 'colsample_bytree': 0.5219649610340028, 'reg_alpha': 0.4841861057795688, 'reg_lambda': 6.329054104574841, 'min_split_gain': 0.07002178442137727}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:13,765] Trial 511 finished with value: 0.2573156695590838 and parameters: {'learning_rate': 0.04450118999710645, 'num_leaves': 917, 'max_depth': 44, 'n_estimators': 909, 'min_child_samples': 61, 'subsample': 0.8803054640389911, 'colsample_bytree': 0.6589093208553822, 'reg_alpha': 1.4949369816129454, 'reg_lambda': 5.321700764892023, 'min_split_gain': 0.021528385228573407}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:16,074] Trial 519 finished with value: 0.2596739438153801 and parameters: {'learning_rate': 0.08434688148859219, 'num_leaves': 962, 'max_depth': 18, 'n_estimators': 548, 'min_child_samples': 61, 'subsample': 0.6511155214337474, 'colsample_bytree': 0.5562192097642326, 'reg_alpha': 1.5451528762057485, 'reg_lambda': 1.5156950932612552e-07, 'min_split_gain': 0.02550626436748834}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:19,699] Trial 524 finished with value: 0.256186036782963 and parameters: {'learning_rate': 0.06516137861067013, 'num_leaves': 751, 'max_depth': 18, 'n_estimators': 848, 'min_child_samples': 54, 'subsample': 0.6508823783870014, 'colsample_bytree': 0.5586588790324383, 'reg_alpha': 3.931149994610644, 'reg_lambda': 7.374953598691851e-08, 'min_split_gain': 0.031228603191048164}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:20,168] Trial 504 finished with value: 0.25338393348625116 and parameters: {'learning_rate': 0.050751953832934, 'num_leaves': 919, 'max_depth': 42, 'n_estimators': 860, 'min_child_samples': 22, 'subsample': 0.8842348488140355, 'colsample_bytree': 0.5243321100850568, 'reg_alpha': 6.318712692486671, 'reg_lambda': 6.16351360697893, 'min_split_gain': 0.0004742717300235638}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:22,673] Trial 530 finished with value: 0.2610164769821819 and parameters: {'learning_rate': 0.03946352221733064, 'num_leaves': 756, 'max_depth': 18, 'n_estimators': 947, 'min_child_samples': 64, 'subsample': 0.8430735047842557, 'colsample_bytree': 0.5185308281130254, 'reg_alpha': 0.4017611006822696, 'reg_lambda': 3.718823908591744, 'min_split_gain': 0.40363509642915}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:23,866] Trial 472 finished with value: 0.2567933458590104 and parameters: {'learning_rate': 0.057426647643061095, 'num_leaves': 633, 'max_depth': 36, 'n_estimators': 671, 'min_child_samples': 59, 'subsample': 0.8572854024718499, 'colsample_bytree': 0.5149142085688867, 'reg_alpha': 0.009448004536204098, 'reg_lambda': 1.317600785029021e-07, 'min_split_gain': 0.00017850135044794602}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:25,896] Trial 522 finished with value: 0.25846460188297227 and parameters: {'learning_rate': 0.08375207862467095, 'num_leaves': 960, 'max_depth': 18, 'n_estimators': 848, 'min_child_samples': 61, 'subsample': 0.6158750184094036, 'colsample_bytree': 0.5601647141417223, 'reg_alpha': 0.5238955389209424, 'reg_lambda': 1.6537075670172294, 'min_split_gain': 0.02580601449743162}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:29,153] Trial 521 finished with value: 0.2589809163418655 and parameters: {'learning_rate': 0.06521177679072829, 'num_leaves': 972, 'max_depth': 20, 'n_estimators': 847, 'min_child_samples': 54, 'subsample': 0.66104671585364, 'colsample_bytree': 0.5594709890408309, 'reg_alpha': 1.3555066619460556, 'reg_lambda': 6.1874865258859995, 'min_split_gain': 0.025198327430221226}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:33,317] Trial 518 finished with value: 0.2592123560663928 and parameters: {'learning_rate': 0.046086599944046854, 'num_leaves': 988, 'max_depth': 18, 'n_estimators': 550, 'min_child_samples': 54, 'subsample': 0.8754015847353198, 'colsample_bytree': 0.5592371539234781, 'reg_alpha': 1.4180933237166446, 'reg_lambda': 1.5349544230920287e-07, 'min_split_gain': 0.026109038848941297}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:35,837] Trial 512 finished with value: 0.25509789087622325 and parameters: {'learning_rate': 0.04375200476312474, 'num_leaves': 923, 'max_depth': 55, 'n_estimators': 907, 'min_child_samples': 62, 'subsample': 0.6628597291312158, 'colsample_bytree': 0.5600249073109472, 'reg_alpha': 5.996628608375579, 'reg_lambda': 5.309737006467105, 'min_split_gain': 0.0006424258865308114}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:42,688] Trial 520 finished with value: 0.25748729122722885 and parameters: {'learning_rate': 0.04990123310388524, 'num_leaves': 745, 'max_depth': 18, 'n_estimators': 914, 'min_child_samples': 42, 'subsample': 0.6534952372223204, 'colsample_bytree': 0.5575507956261919, 'reg_alpha': 1.3881085364387893, 'reg_lambda': 9.873887332557414, 'min_split_gain': 0.024566093892256727}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:44,901] Trial 537 finished with value: 0.25977955719914986 and parameters: {'learning_rate': 0.04320457426672052, 'num_leaves': 215, 'max_depth': 45, 'n_estimators': 929, 'min_child_samples': 20, 'subsample': 0.8768189281367202, 'colsample_bytree': 0.5308130226536382, 'reg_alpha': 3.742357289920306e-05, 'reg_lambda': 2.3925222612872377, 'min_split_gain': 0.31671266104829676}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:46,146] Trial 532 finished with value: 0.25405562896255396 and parameters: {'learning_rate': 0.042326658386225985, 'num_leaves': 234, 'max_depth': 60, 'n_estimators': 960, 'min_child_samples': 64, 'subsample': 0.8775627910959276, 'colsample_bytree': 0.5107115586858405, 'reg_alpha': 3.7517170594561198, 'reg_lambda': 1.7547586743064663, 'min_split_gain': 0.026636622031896016}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:46,955] Trial 527 finished with value: 0.2608141623494313 and parameters: {'learning_rate': 0.044595746105362925, 'num_leaves': 62, 'max_depth': 18, 'n_estimators': 898, 'min_child_samples': 63, 'subsample': 0.8757521983296144, 'colsample_bytree': 0.6369987476069418, 'reg_alpha': 0.5604352241115409, 'reg_lambda': 1.0013460777924053, 'min_split_gain': 0.026688018872895347}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:48,383] Trial 529 finished with value: 0.2572908188405069 and parameters: {'learning_rate': 0.05224960991610906, 'num_leaves': 882, 'max_depth': 44, 'n_estimators': 931, 'min_child_samples': 66, 'subsample': 0.8781816113108696, 'colsample_bytree': 0.5059962585941322, 'reg_alpha': 1.4158098927862488, 'reg_lambda': 2.0286235128331813, 'min_split_gain': 0.027177901423654352}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:49,013] Trial 500 finished with value: 0.25371085001675864 and parameters: {'learning_rate': 0.06464752625994903, 'num_leaves': 858, 'max_depth': 42, 'n_estimators': 856, 'min_child_samples': 38, 'subsample': 0.824677358947841, 'colsample_bytree': 0.524357444597027, 'reg_alpha': 1.3975085450630336, 'reg_lambda': 6.624803112950276, 'min_split_gain': 0.0017409454022606031}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:49,554] Trial 526 finished with value: 0.2578931512422262 and parameters: {'learning_rate': 0.045210680258631265, 'num_leaves': 931, 'max_depth': 16, 'n_estimators': 942, 'min_child_samples': 65, 'subsample': 0.8731659289727687, 'colsample_bytree': 0.5201250442578298, 'reg_alpha': 0.54194587227906, 'reg_lambda': 1.0058112755704476e-08, 'min_split_gain': 0.026490262137979975}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:52,524] Trial 528 finished with value: 0.254446026039341 and parameters: {'learning_rate': 0.05025115675018292, 'num_leaves': 175, 'max_depth': 45, 'n_estimators': 909, 'min_child_samples': 36, 'subsample': 0.8797783844373241, 'colsample_bytree': 0.5074905410565567, 'reg_alpha': 1.7085322315237441, 'reg_lambda': 1.070980343568273, 'min_split_gain': 0.02513853982250338}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:54,956] Trial 509 finished with value: 0.2567669461689893 and parameters: {'learning_rate': 0.05018877099838974, 'num_leaves': 924, 'max_depth': 55, 'n_estimators': 913, 'min_child_samples': 61, 'subsample': 0.8755472547583123, 'colsample_bytree': 0.5600655335204442, 'reg_alpha': 1.3840792862338709, 'reg_lambda': 6.029129194273722, 'min_split_gain': 0.0022779162634818557}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:55,931] Trial 538 finished with value: 0.25611898643805187 and parameters: {'learning_rate': 0.043564804266094215, 'num_leaves': 216, 'max_depth': 20, 'n_estimators': 917, 'min_child_samples': 66, 'subsample': 0.8868368976784192, 'colsample_bytree': 0.5295982989401936, 'reg_alpha': 5.690949082360098, 'reg_lambda': 1.567989295441438, 'min_split_gain': 0.026700604213790807}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:57,562] Trial 490 finished with value: 0.25626770657159337 and parameters: {'learning_rate': 0.05176701673952877, 'num_leaves': 986, 'max_depth': 16, 'n_estimators': 789, 'min_child_samples': 42, 'subsample': 0.8582148695397714, 'colsample_bytree': 0.5400003234797408, 'reg_alpha': 1.2621008957733386, 'reg_lambda': 1.0079358269799808e-08, 'min_split_gain': 0.0004172585644359397}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:22:58,394] Trial 525 finished with value: 0.25685905912603857 and parameters: {'learning_rate': 0.046214194523250166, 'num_leaves': 747, 'max_depth': 18, 'n_estimators': 933, 'min_child_samples': 41, 'subsample': 0.8391042384829046, 'colsample_bytree': 0.5080949231516967, 'reg_alpha': 0.44142312989522614, 'reg_lambda': 1.7399048608389777e-07, 'min_split_gain': 0.02639830848815855}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:04,347] Trial 539 finished with value: 0.25598784288564197 and parameters: {'learning_rate': 0.04280344633098858, 'num_leaves': 873, 'max_depth': 20, 'n_estimators': 970, 'min_child_samples': 22, 'subsample': 0.8838019577370869, 'colsample_bytree': 0.5334108688843323, 'reg_alpha': 6.580248427855617, 'reg_lambda': 2.5811965358980866, 'min_split_gain': 0.02440587738464714}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:05,300] Trial 536 finished with value: 0.25704976964054627 and parameters: {'learning_rate': 0.04099595420758208, 'num_leaves': 71, 'max_depth': 45, 'n_estimators': 946, 'min_child_samples': 62, 'subsample': 0.5805142343240084, 'colsample_bytree': 0.5301007215953293, 'reg_alpha': 6.201860708813347, 'reg_lambda': 2.2871313883623703, 'min_split_gain': 0.02785814194112498}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:09,461] Trial 540 finished with value: 0.2550910218819577 and parameters: {'learning_rate': 0.04319110373225321, 'num_leaves': 904, 'max_depth': 16, 'n_estimators': 970, 'min_child_samples': 21, 'subsample': 0.8877386443608511, 'colsample_bytree': 0.5316719698288139, 'reg_alpha': 5.981755427489136, 'reg_lambda': 3.622581754272727, 'min_split_gain': 0.023739245922329177}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:09,957] Trial 531 finished with value: 0.25748876983619223 and parameters: {'learning_rate': 0.04463900513938159, 'num_leaves': 281, 'max_depth': 60, 'n_estimators': 960, 'min_child_samples': 61, 'subsample': 0.8747700527842024, 'colsample_bytree': 0.5069478928192207, 'reg_alpha': 0.48051424382207625, 'reg_lambda': 4.07303749185284, 'min_split_gain': 0.027608157512983608}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:14,002] Trial 556 finished with value: 0.2619286275119262 and parameters: {'learning_rate': 0.04985209569590546, 'num_leaves': 880, 'max_depth': 40, 'n_estimators': 980, 'min_child_samples': 67, 'subsample': 0.9120086410770794, 'colsample_bytree': 0.54513395275439, 'reg_alpha': 3.4220407820440584, 'reg_lambda': 3.815507833752854, 'min_split_gain': 0.7103733968528133}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:15,539] Trial 510 finished with value: 0.25431516346650956 and parameters: {'learning_rate': 0.04901987221974165, 'num_leaves': 922, 'max_depth': 55, 'n_estimators': 915, 'min_child_samples': 62, 'subsample': 0.8764057415970632, 'colsample_bytree': 0.5223677855383478, 'reg_alpha': 1.5393248494825444, 'reg_lambda': 5.756947982725528, 'min_split_gain': 0.00046057596823575}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:15,804] Trial 542 finished with value: 0.2549450521890376 and parameters: {'learning_rate': 0.039668680499923815, 'num_leaves': 228, 'max_depth': 61, 'n_estimators': 932, 'min_child_samples': 65, 'subsample': 0.884057613248235, 'colsample_bytree': 0.5302631460122799, 'reg_alpha': 4.401702043463234, 'reg_lambda': 2.3772311400585453, 'min_split_gain': 0.02335747584780361}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:27,414] Trial 548 finished with value: 0.2542013893506151 and parameters: {'learning_rate': 0.044824881413258896, 'num_leaves': 202, 'max_depth': 41, 'n_estimators': 931, 'min_child_samples': 63, 'subsample': 0.8918094701255761, 'colsample_bytree': 0.5239305906869026, 'reg_alpha': 3.893628735868221, 'reg_lambda': 2.0067197363047127, 'min_split_gain': 0.02368343900544296}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:28,713] Trial 533 finished with value: 0.25572347361554304 and parameters: {'learning_rate': 0.04003705697774058, 'num_leaves': 214, 'max_depth': 18, 'n_estimators': 917, 'min_child_samples': 62, 'subsample': 0.8746152561912828, 'colsample_bytree': 0.5064130304908558, 'reg_alpha': 4.160630235634386e-05, 'reg_lambda': 1.4717305252623962e-07, 'min_split_gain': 0.02721456425242945}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:29,491] Trial 545 finished with value: 0.2548320668765632 and parameters: {'learning_rate': 0.039349102558885345, 'num_leaves': 197, 'max_depth': 60, 'n_estimators': 959, 'min_child_samples': 65, 'subsample': 0.885714388150116, 'colsample_bytree': 0.5002260900867442, 'reg_alpha': 4.04603666747208, 'reg_lambda': 1.789553545224474, 'min_split_gain': 0.023481411752267103}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:31,529] Trial 546 finished with value: 0.2557467584251281 and parameters: {'learning_rate': 0.037625269985876, 'num_leaves': 196, 'max_depth': 60, 'n_estimators': 983, 'min_child_samples': 66, 'subsample': 0.9145252417404649, 'colsample_bytree': 0.5014199851164477, 'reg_alpha': 4.140265682202263, 'reg_lambda': 2.003235970712827, 'min_split_gain': 0.02462877662784256}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:31,716] Trial 557 finished with value: 0.2547598183464871 and parameters: {'learning_rate': 0.04931469685879518, 'num_leaves': 269, 'max_depth': 61, 'n_estimators': 935, 'min_child_samples': 63, 'subsample': 0.8975341685329602, 'colsample_bytree': 0.5442757484135884, 'reg_alpha': 3.569577215298768, 'reg_lambda': 3.543910808445743, 'min_split_gain': 0.07346367299987516}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:33,211] Trial 549 finished with value: 0.2557504800760503 and parameters: {'learning_rate': 0.038205568429597526, 'num_leaves': 179, 'max_depth': 61, 'n_estimators': 950, 'min_child_samples': 67, 'subsample': 0.8925543134961074, 'colsample_bytree': 0.5318309072475922, 'reg_alpha': 4.2224085164099705, 'reg_lambda': 3.558723271497799, 'min_split_gain': 0.024734102099313697}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:35,962] Trial 559 finished with value: 0.2570749464731466 and parameters: {'learning_rate': 0.04955832219032994, 'num_leaves': 287, 'max_depth': 39, 'n_estimators': 929, 'min_child_samples': 63, 'subsample': 0.8956070253336774, 'colsample_bytree': 0.5416004539902278, 'reg_alpha': 3.3390089758566583, 'reg_lambda': 2.0251081348714526, 'min_split_gain': 0.07156241082290937}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:36,304] Trial 497 finished with value: 0.2536397353874358 and parameters: {'learning_rate': 0.06505464016439833, 'num_leaves': 873, 'max_depth': 55, 'n_estimators': 870, 'min_child_samples': 38, 'subsample': 0.6519018633891417, 'colsample_bytree': 0.5229945121367208, 'reg_alpha': 0.4640205452483775, 'reg_lambda': 4.204173089834077, 'min_split_gain': 0.00020180054827743607}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:37,536] Trial 547 finished with value: 0.2551708953580283 and parameters: {'learning_rate': 0.036173972353687495, 'num_leaves': 224, 'max_depth': 61, 'n_estimators': 958, 'min_child_samples': 62, 'subsample': 0.8862654679056674, 'colsample_bytree': 0.5230487569087283, 'reg_alpha': 3.8361645734485403, 'reg_lambda': 1.785533693973868, 'min_split_gain': 0.02133230108997038}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:39,557] Trial 534 finished with value: 0.25731770745609206 and parameters: {'learning_rate': 0.03852368839736986, 'num_leaves': 878, 'max_depth': 60, 'n_estimators': 923, 'min_child_samples': 64, 'subsample': 0.5284771071858807, 'colsample_bytree': 0.5094245583065183, 'reg_alpha': 3.3228664919884866e-05, 'reg_lambda': 1.6219585847678752, 'min_split_gain': 0.027830576680091}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:40,288] Trial 551 finished with value: 0.255987557359645 and parameters: {'learning_rate': 0.03460456733360704, 'num_leaves': 215, 'max_depth': 41, 'n_estimators': 963, 'min_child_samples': 65, 'subsample': 0.9025948530063364, 'colsample_bytree': 0.5308619098799116, 'reg_alpha': 3.526873479397211, 'reg_lambda': 1.9754808197973956, 'min_split_gain': 0.022741504661982076}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:42,621] Trial 563 finished with value: 0.2652611181386746 and parameters: {'learning_rate': 0.034910967693632625, 'num_leaves': 255, 'max_depth': 60, 'n_estimators': 947, 'min_child_samples': 38, 'subsample': 0.8966255384675059, 'colsample_bytree': 0.5439095738974393, 'reg_alpha': 0.0001372385869331357, 'reg_lambda': 2.9807347889189675, 'min_split_gain': 0.8802732883558682}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:47,480] Trial 564 finished with value: 0.2550844849091145 and parameters: {'learning_rate': 0.06445657925262291, 'num_leaves': 255, 'max_depth': 61, 'n_estimators': 986, 'min_child_samples': 69, 'subsample': 0.915850461983039, 'colsample_bytree': 0.5450561365546681, 'reg_alpha': 2.9936642069597363, 'reg_lambda': 3.332337402834304, 'min_split_gain': 0.06653717041881234}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:50,255] Trial 561 finished with value: 0.2596688028311334 and parameters: {'learning_rate': 0.03797328008025775, 'num_leaves': 271, 'max_depth': 41, 'n_estimators': 941, 'min_child_samples': 23, 'subsample': 0.5403924937044849, 'colsample_bytree': 0.5440523797881017, 'reg_alpha': 2.6859369150438748, 'reg_lambda': 3.486436972445378, 'min_split_gain': 0.06298895920838357}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:50,616] Trial 535 finished with value: 0.2576239600266792 and parameters: {'learning_rate': 0.044147368008906535, 'num_leaves': 202, 'max_depth': 61, 'n_estimators': 931, 'min_child_samples': 63, 'subsample': 0.8753351418879511, 'colsample_bytree': 0.5073674772149954, 'reg_alpha': 3.0170857381344948e-05, 'reg_lambda': 4.0261933993172665, 'min_split_gain': 0.025759872140951608}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:54,022] Trial 555 finished with value: 0.2560426966031339 and parameters: {'learning_rate': 0.036499068141605986, 'num_leaves': 245, 'max_depth': 60, 'n_estimators': 985, 'min_child_samples': 64, 'subsample': 0.8978512636897263, 'colsample_bytree': 0.5443408665592662, 'reg_alpha': 4.014348177191371, 'reg_lambda': 3.454953907712632, 'min_split_gain': 0.018849285109976854}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:54,983] Trial 553 finished with value: 0.2552107840448291 and parameters: {'learning_rate': 0.03960150163618968, 'num_leaves': 271, 'max_depth': 40, 'n_estimators': 931, 'min_child_samples': 21, 'subsample': 0.8874182880481222, 'colsample_bytree': 0.5442016001909047, 'reg_alpha': 3.532768612687097, 'reg_lambda': 2.4934355777777872, 'min_split_gain': 0.01991059671479852}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:23:56,707] Trial 541 finished with value: 0.25713586838300573 and parameters: {'learning_rate': 0.04371123869589282, 'num_leaves': 182, 'max_depth': 16, 'n_estimators': 935, 'min_child_samples': 64, 'subsample': 0.8870355612674522, 'colsample_bytree': 0.533197029839534, 'reg_alpha': 4.5868494991906244e-05, 'reg_lambda': 4.1232938797663286, 'min_split_gain': 0.020739558021962035}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:03,359] Trial 552 finished with value: 0.2548063281223796 and parameters: {'learning_rate': 0.04042466024666577, 'num_leaves': 204, 'max_depth': 40, 'n_estimators': 931, 'min_child_samples': 38, 'subsample': 0.8923836156973739, 'colsample_bytree': 0.5326232381296588, 'reg_alpha': 3.839553909406695, 'reg_lambda': 3.3491708970275043, 'min_split_gain': 0.019465902548048735}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:04,504] Trial 554 finished with value: 0.2582612318916183 and parameters: {'learning_rate': 0.031106422409121138, 'num_leaves': 200, 'max_depth': 41, 'n_estimators': 937, 'min_child_samples': 22, 'subsample': 0.9142820584569498, 'colsample_bytree': 0.5310918473739747, 'reg_alpha': 3.4698445775526783, 'reg_lambda': 2.163582131670953, 'min_split_gain': 0.01918679444114156}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:05,145] Trial 571 finished with value: 0.2582295538940754 and parameters: {'learning_rate': 0.06606595637534056, 'num_leaves': 264, 'max_depth': 41, 'n_estimators': 898, 'min_child_samples': 35, 'subsample': 0.7957465460578544, 'colsample_bytree': 0.5174072323160224, 'reg_alpha': 2.470754471929574, 'reg_lambda': 4.394370323738726, 'min_split_gain': 0.04725448088921498}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:06,006] Trial 566 finished with value: 0.2563842054025169 and parameters: {'learning_rate': 0.03685517713050342, 'num_leaves': 238, 'max_depth': 41, 'n_estimators': 894, 'min_child_samples': 37, 'subsample': 0.7832009527215562, 'colsample_bytree': 0.543017379096258, 'reg_alpha': 2.4865787238335937, 'reg_lambda': 3.6824548446222, 'min_split_gain': 0.07046786080439792}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:08,378] Trial 570 finished with value: 0.25614487090814075 and parameters: {'learning_rate': 0.06437349719911424, 'num_leaves': 869, 'max_depth': 41, 'n_estimators': 278, 'min_child_samples': 23, 'subsample': 0.7792788332684757, 'colsample_bytree': 0.5228089630683238, 'reg_alpha': 2.6808824918224268, 'reg_lambda': 7.717000583569562e-08, 'min_split_gain': 0.0419384410126607}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:08,844] Trial 573 finished with value: 0.25767862905183925 and parameters: {'learning_rate': 0.06703035510543529, 'num_leaves': 855, 'max_depth': 49, 'n_estimators': 889, 'min_child_samples': 37, 'subsample': 0.791695860140062, 'colsample_bytree': 0.5181180245106495, 'reg_alpha': 2.107819717213161, 'reg_lambda': 8.425507549726076, 'min_split_gain': 0.04365144211103923}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:09,673] Trial 560 finished with value: 0.26102091593037247 and parameters: {'learning_rate': 0.03624982068145842, 'num_leaves': 877, 'max_depth': 61, 'n_estimators': 985, 'min_child_samples': 64, 'subsample': 0.5422083260886059, 'colsample_bytree': 0.543923241446683, 'reg_alpha': 0.00014745014840250375, 'reg_lambda': 1.7028974560964483, 'min_split_gain': 0.07220532983321473}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:12,480] Trial 572 finished with value: 0.25655526669507406 and parameters: {'learning_rate': 0.0656966953669188, 'num_leaves': 878, 'max_depth': 43, 'n_estimators': 878, 'min_child_samples': 37, 'subsample': 0.9288059279823756, 'colsample_bytree': 0.5185032814421785, 'reg_alpha': 0.680567326366791, 'reg_lambda': 6.990874678444566e-08, 'min_split_gain': 0.0600185764896843}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:12,992] Trial 578 finished with value: 0.2583647918342158 and parameters: {'learning_rate': 0.06165132555931607, 'num_leaves': 858, 'max_depth': 52, 'n_estimators': 894, 'min_child_samples': 37, 'subsample': 0.7812128229228376, 'colsample_bytree': 0.5186996945436293, 'reg_alpha': 9.75114302931965, 'reg_lambda': 9.390687680657, 'min_split_gain': 0.04405316464337272}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:13,543] Trial 544 finished with value: 0.2581651995188632 and parameters: {'learning_rate': 0.0398414791102234, 'num_leaves': 259, 'max_depth': 60, 'n_estimators': 933, 'min_child_samples': 67, 'subsample': 0.8858488729865273, 'colsample_bytree': 0.5281149314181581, 'reg_alpha': 0.00017976979302011836, 'reg_lambda': 2.2678860068705347, 'min_split_gain': 0.02252544218558846}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:19,001] Trial 543 finished with value: 0.25771180327288223 and parameters: {'learning_rate': 0.03862338382704435, 'num_leaves': 232, 'max_depth': 60, 'n_estimators': 952, 'min_child_samples': 22, 'subsample': 0.8794336480221364, 'colsample_bytree': 0.5292224075389867, 'reg_alpha': 0.0001499013973711653, 'reg_lambda': 1.9864833951856713, 'min_split_gain': 0.023559120481659753}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:20,291] Trial 550 finished with value: 0.2657729567048745 and parameters: {'learning_rate': 0.004218560529303713, 'num_leaves': 272, 'max_depth': 40, 'n_estimators': 989, 'min_child_samples': 22, 'subsample': 0.9163926499374083, 'colsample_bytree': 0.5428596333425444, 'reg_alpha': 3.267573356606371, 'reg_lambda': 3.553140891884783, 'min_split_gain': 0.7154076335891923}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:21,051] Trial 574 finished with value: 0.2567796922604225 and parameters: {'learning_rate': 0.06480106038456825, 'num_leaves': 856, 'max_depth': 52, 'n_estimators': 890, 'min_child_samples': 37, 'subsample': 0.7864593040087247, 'colsample_bytree': 0.5180941902817064, 'reg_alpha': 0.7915975062265967, 'reg_lambda': 9.664415718609584, 'min_split_gain': 0.043274192841925845}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:24,653] Trial 496 finished with value: 0.26347038537294204 and parameters: {'learning_rate': 0.0041474756482557055, 'num_leaves': 857, 'max_depth': 60, 'n_estimators': 886, 'min_child_samples': 46, 'subsample': 0.656883992968124, 'colsample_bytree': 0.6611596431273049, 'reg_alpha': 3.097038277142682, 'reg_lambda': 5.034897165057386, 'min_split_gain': 0.00023429528419122213}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:44,948] Trial 583 finished with value: 0.2570224151319796 and parameters: {'learning_rate': 0.06590371615263887, 'num_leaves': 853, 'max_depth': 52, 'n_estimators': 887, 'min_child_samples': 37, 'subsample': 0.8031611285939574, 'colsample_bytree': 0.5183306354685857, 'reg_alpha': 0.7563354771081252, 'reg_lambda': 7.595503909182588e-08, 'min_split_gain': 0.04233396993854942}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:49,724] Trial 592 finished with value: 0.2556503971562102 and parameters: {'learning_rate': 0.09775633265390597, 'num_leaves': 897, 'max_depth': 43, 'n_estimators': 866, 'min_child_samples': 34, 'subsample': 0.8243514742017665, 'colsample_bytree': 0.515344219558456, 'reg_alpha': 7.453892868006898, 'reg_lambda': 5.791856034270583, 'min_split_gain': 0.0006820838346141025}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:24:57,389] Trial 406 finished with value: 0.33238575546054894 and parameters: {'learning_rate': 0.00215351678305821, 'num_leaves': 297, 'max_depth': 60, 'n_estimators': 733, 'min_child_samples': 40, 'subsample': 0.6342472844778773, 'colsample_bytree': 0.52968600324869, 'reg_alpha': 0.07977104437725192, 'reg_lambda': 1.0188586222864746e-08, 'min_split_gain': 0.0527224816642616}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:00,146] Trial 577 finished with value: 0.2560087387851106 and parameters: {'learning_rate': 0.030271918416746823, 'num_leaves': 891, 'max_depth': 53, 'n_estimators': 891, 'min_child_samples': 37, 'subsample': 0.7996226028484161, 'colsample_bytree': 0.5188875482594725, 'reg_alpha': 2.3120226564385398, 'reg_lambda': 7.087489034452177, 'min_split_gain': 0.039752320559637414}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:01,230] Trial 593 finished with value: 0.2561097855145176 and parameters: {'learning_rate': 0.09634603858838155, 'num_leaves': 893, 'max_depth': 54, 'n_estimators': 864, 'min_child_samples': 39, 'subsample': 0.8216085592755608, 'colsample_bytree': 0.5140129180300372, 'reg_alpha': 9.77326060184971, 'reg_lambda': 7.27067526199796e-08, 'min_split_gain': 0.000514261147829943}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:04,068] Trial 587 finished with value: 0.2565301742296378 and parameters: {'learning_rate': 0.05258217803853556, 'num_leaves': 901, 'max_depth': 54, 'n_estimators': 865, 'min_child_samples': 39, 'subsample': 0.8302258807406259, 'colsample_bytree': 0.5184979744657675, 'reg_alpha': 0.8283206885700413, 'reg_lambda': 6.49805132152486, 'min_split_gain': 0.042062287317284695}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:08,104] Trial 575 finished with value: 0.2534046018251995 and parameters: {'learning_rate': 0.0651217726542331, 'num_leaves': 148, 'max_depth': 54, 'n_estimators': 893, 'min_child_samples': 37, 'subsample': 0.5890446207595045, 'colsample_bytree': 0.5171023196306238, 'reg_alpha': 2.2173454314246306, 'reg_lambda': 9.401273383442003, 'min_split_gain': 0.0013309480321743146}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:21,504] Trial 558 finished with value: 0.2587728943759922 and parameters: {'learning_rate': 0.035235464152118576, 'num_leaves': 242, 'max_depth': 61, 'n_estimators': 928, 'min_child_samples': 64, 'subsample': 0.9184455668425027, 'colsample_bytree': 0.543108398466644, 'reg_alpha': 0.00014070206687807264, 'reg_lambda': 3.429428665694099, 'min_split_gain': 0.018974799899900616}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:28,793] Trial 581 finished with value: 0.2556251083558336 and parameters: {'learning_rate': 0.0664072739327136, 'num_leaves': 884, 'max_depth': 53, 'n_estimators': 893, 'min_child_samples': 36, 'subsample': 0.790038781150267, 'colsample_bytree': 0.5181857723179609, 'reg_alpha': 1.9538522764734751, 'reg_lambda': 6.407691275160881, 'min_split_gain': 0.0025859931522090743}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:29,706] Trial 594 finished with value: 0.2570823415992038 and parameters: {'learning_rate': 0.09540342795070615, 'num_leaves': 334, 'max_depth': 54, 'n_estimators': 866, 'min_child_samples': 39, 'subsample': 0.8223397224577679, 'colsample_bytree': 0.5521946866356311, 'reg_alpha': 9.58650994785374, 'reg_lambda': 6.838434288669452, 'min_split_gain': 0.0011971534385495626}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:32,070] Trial 596 finished with value: 0.2579614598674168 and parameters: {'learning_rate': 0.0807722909883728, 'num_leaves': 131, 'max_depth': 54, 'n_estimators': 862, 'min_child_samples': 39, 'subsample': 0.563600041720844, 'colsample_bytree': 0.5540438272368178, 'reg_alpha': 9.958248540874743, 'reg_lambda': 0.01583592321660129, 'min_split_gain': 0.0015829104419429478}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:38,394] Trial 580 finished with value: 0.25665663199671646 and parameters: {'learning_rate': 0.06689489753705079, 'num_leaves': 846, 'max_depth': 43, 'n_estimators': 887, 'min_child_samples': 37, 'subsample': 0.7871842990907494, 'colsample_bytree': 0.5200175708072079, 'reg_alpha': 0.7047437450124696, 'reg_lambda': 7.0297566259709665, 'min_split_gain': 0.0006418446733176015}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:47,013] Trial 584 finished with value: 0.2595317979060026 and parameters: {'learning_rate': 0.06665914274466125, 'num_leaves': 853, 'max_depth': 52, 'n_estimators': 884, 'min_child_samples': 39, 'subsample': 0.5889173644093078, 'colsample_bytree': 0.5172347201969403, 'reg_alpha': 0.7024711339524677, 'reg_lambda': 0.00030793380416594155, 'min_split_gain': 0.0007266917271083994}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:48,398] Trial 588 finished with value: 0.24990691697399328 and parameters: {'learning_rate': 0.09415473889227101, 'num_leaves': 901, 'max_depth': 59, 'n_estimators': 867, 'min_child_samples': 39, 'subsample': 0.8212381838938069, 'colsample_bytree': 0.5228280861398271, 'reg_alpha': 5.897861800674065, 'reg_lambda': 7.021842519723051, 'min_split_gain': 0.0002401704454612645}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:55,723] Trial 595 finished with value: 0.25173027762216815 and parameters: {'learning_rate': 0.0536866881073233, 'num_leaves': 900, 'max_depth': 57, 'n_estimators': 864, 'min_child_samples': 39, 'subsample': 0.8413796611852522, 'colsample_bytree': 0.5244855751042465, 'reg_alpha': 6.744423458134775, 'reg_lambda': 5.983870832252938e-08, 'min_split_gain': 0.002391951110936324}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:25:57,571] Trial 562 finished with value: 0.2573451858687177 and parameters: {'learning_rate': 0.0073916739187511816, 'num_leaves': 282, 'max_depth': 60, 'n_estimators': 892, 'min_child_samples': 64, 'subsample': 0.7894610911894137, 'colsample_bytree': 0.544017406790114, 'reg_alpha': 2.9463954371544068, 'reg_lambda': 3.5700176721803585, 'min_split_gain': 0.0689928379341265}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:00,494] Trial 569 finished with value: 0.25377294588846355 and parameters: {'learning_rate': 0.06583820424797662, 'num_leaves': 866, 'max_depth': 43, 'n_estimators': 897, 'min_child_samples': 37, 'subsample': 0.790366390238638, 'colsample_bytree': 0.5174896842757467, 'reg_alpha': 2.0123962491897442, 'reg_lambda': 7.094437788685885, 'min_split_gain': 1.879194569541975e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:02,640] Trial 600 finished with value: 0.2550354945501827 and parameters: {'learning_rate': 0.08046205854268347, 'num_leaves': 141, 'max_depth': 57, 'n_estimators': 856, 'min_child_samples': 19, 'subsample': 0.5863338353795029, 'colsample_bytree': 0.5108940365982837, 'reg_alpha': 6.114380912649636, 'reg_lambda': 9.44202198617454, 'min_split_gain': 0.0020992965862720384}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:05,938] Trial 604 finished with value: 0.25655903806586183 and parameters: {'learning_rate': 0.052450452043526746, 'num_leaves': 911, 'max_depth': 57, 'n_estimators': 876, 'min_child_samples': 35, 'subsample': 0.868022832765937, 'colsample_bytree': 0.509085202973358, 'reg_alpha': 6.132259726510926, 'reg_lambda': 4.4269554631881844e-07, 'min_split_gain': 0.018193553787600136}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:08,137] Trial 606 finished with value: 0.2557399159852609 and parameters: {'learning_rate': 0.05409893432768027, 'num_leaves': 908, 'max_depth': 58, 'n_estimators': 852, 'min_child_samples': 100, 'subsample': 0.5796856195974498, 'colsample_bytree': 0.5089422060743302, 'reg_alpha': 6.0373740740570625, 'reg_lambda': 7.893477841964814, 'min_split_gain': 0.03951311957382824}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:10,392] Trial 603 finished with value: 0.25741110770930564 and parameters: {'learning_rate': 0.05355423628254283, 'num_leaves': 128, 'max_depth': 57, 'n_estimators': 256, 'min_child_samples': 31, 'subsample': 0.5747901526106084, 'colsample_bytree': 0.5107092659716537, 'reg_alpha': 5.761874485372594, 'reg_lambda': 0.015727082616741953, 'min_split_gain': 0.013027636416579278}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:12,051] Trial 582 finished with value: 0.2566513445889657 and parameters: {'learning_rate': 0.06586328446838956, 'num_leaves': 858, 'max_depth': 49, 'n_estimators': 894, 'min_child_samples': 37, 'subsample': 0.8302262289564256, 'colsample_bytree': 0.5201096588173947, 'reg_alpha': 0.8058023257497561, 'reg_lambda': 5.964437264936473, 'min_split_gain': 0.0003044187915353032}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:12,573] Trial 597 finished with value: 0.2545777970328507 and parameters: {'learning_rate': 0.05247245339978339, 'num_leaves': 915, 'max_depth': 58, 'n_estimators': 862, 'min_child_samples': 39, 'subsample': 0.8312295324632282, 'colsample_bytree': 0.5485318907306103, 'reg_alpha': 6.344502069409766, 'reg_lambda': 6.498469176038593, 'min_split_gain': 0.0019977710125899345}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:13,797] Trial 607 finished with value: 0.2556438237532908 and parameters: {'learning_rate': 0.08075723816094336, 'num_leaves': 154, 'max_depth': 57, 'n_estimators': 848, 'min_child_samples': 34, 'subsample': 0.94389357912637, 'colsample_bytree': 0.5339101148974171, 'reg_alpha': 5.874109584584665, 'reg_lambda': 9.742096275410296, 'min_split_gain': 0.01521230295982745}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:15,405] Trial 608 finished with value: 0.2564870604489483 and parameters: {'learning_rate': 0.10981452456251255, 'num_leaves': 902, 'max_depth': 57, 'n_estimators': 856, 'min_child_samples': 35, 'subsample': 0.8422425450901431, 'colsample_bytree': 0.5272313973815237, 'reg_alpha': 5.907318743334982, 'reg_lambda': 7.88448040095248, 'min_split_gain': 0.015263724148402736}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:16,666] Trial 599 finished with value: 0.25243108869433195 and parameters: {'learning_rate': 0.0530796710691807, 'num_leaves': 897, 'max_depth': 58, 'n_estimators': 859, 'min_child_samples': 60, 'subsample': 0.8444778511435669, 'colsample_bytree': 0.553424879475853, 'reg_alpha': 6.188965950597846, 'reg_lambda': 6.521984699562199, 'min_split_gain': 0.0014546701178401727}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:17,120] Trial 598 finished with value: 0.25392327439733553 and parameters: {'learning_rate': 0.05208394526168392, 'num_leaves': 913, 'max_depth': 57, 'n_estimators': 864, 'min_child_samples': 39, 'subsample': 0.8355942134179831, 'colsample_bytree': 0.5360274466244409, 'reg_alpha': 6.148118414446696, 'reg_lambda': 2.7620802671232896e-05, 'min_split_gain': 0.001837314412825495}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:17,999] Trial 611 finished with value: 0.2578885084019517 and parameters: {'learning_rate': 0.1273086716096057, 'num_leaves': 902, 'max_depth': 57, 'n_estimators': 906, 'min_child_samples': 34, 'subsample': 0.5720127301127895, 'colsample_bytree': 0.5247424093794552, 'reg_alpha': 6.08810229495457, 'reg_lambda': 9.569857506647383, 'min_split_gain': 0.018437385728841613}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:19,254] Trial 615 finished with value: 0.2618626884910361 and parameters: {'learning_rate': 0.08554929083231765, 'num_leaves': 907, 'max_depth': 56, 'n_estimators': 870, 'min_child_samples': 34, 'subsample': 0.8466410860330149, 'colsample_bytree': 0.5286059302053481, 'reg_alpha': 6.2328041305805115, 'reg_lambda': 9.386417392510303, 'min_split_gain': 0.4803459501437935}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:20,327] Trial 616 finished with value: 0.2604799013610602 and parameters: {'learning_rate': 0.09388640167797134, 'num_leaves': 900, 'max_depth': 56, 'n_estimators': 909, 'min_child_samples': 34, 'subsample': 0.7669014856826081, 'colsample_bytree': 0.5252148210222983, 'reg_alpha': 1.9149058055479034, 'reg_lambda': 9.668230642974935, 'min_split_gain': 0.4329082021606369}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:21,577] Trial 612 finished with value: 0.2616951579612711 and parameters: {'learning_rate': 0.09667240924778985, 'num_leaves': 831, 'max_depth': 57, 'n_estimators': 909, 'min_child_samples': 32, 'subsample': 0.8460164089642619, 'colsample_bytree': 0.5255780408214797, 'reg_alpha': 6.234524724073203, 'reg_lambda': 9.108218992807508, 'min_split_gain': 0.03914141493065987}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:21,825] Trial 576 finished with value: 0.2524823604095851 and parameters: {'learning_rate': 0.06629188163033072, 'num_leaves': 898, 'max_depth': 53, 'n_estimators': 902, 'min_child_samples': 38, 'subsample': 0.9395097036303188, 'colsample_bytree': 0.5171367042944774, 'reg_alpha': 0.7033360600649953, 'reg_lambda': 9.961275823403197, 'min_split_gain': 0.0005351836303724878}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:22,902] Trial 609 finished with value: 0.2543639651565927 and parameters: {'learning_rate': 0.08147995244992445, 'num_leaves': 148, 'max_depth': 57, 'n_estimators': 851, 'min_child_samples': 36, 'subsample': 0.8447658800371964, 'colsample_bytree': 0.5259016909601386, 'reg_alpha': 5.7275650378267295, 'reg_lambda': 8.911217658447269, 'min_split_gain': 0.018469905013100512}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:25,644] Trial 618 finished with value: 0.25836753079838604 and parameters: {'learning_rate': 0.08780698185832528, 'num_leaves': 880, 'max_depth': 56, 'n_estimators': 910, 'min_child_samples': 32, 'subsample': 0.5523269810740035, 'colsample_bytree': 0.5259152021723598, 'reg_alpha': 1.3660150500574357e-05, 'reg_lambda': 6.224287297842932, 'min_split_gain': 0.4270520030132349}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:28,028] Trial 620 finished with value: 0.2616212886819301 and parameters: {'learning_rate': 0.0958044880237495, 'num_leaves': 23, 'max_depth': 56, 'n_estimators': 874, 'min_child_samples': 57, 'subsample': 0.561746547334894, 'colsample_bytree': 0.5261531289046946, 'reg_alpha': 6.55002509612164, 'reg_lambda': 5.342078998150321, 'min_split_gain': 0.030880050650620827}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:29,066] Trial 621 finished with value: 0.2585184106342902 and parameters: {'learning_rate': 0.11293368457440586, 'num_leaves': 933, 'max_depth': 56, 'n_estimators': 872, 'min_child_samples': 60, 'subsample': 0.8407863180048557, 'colsample_bytree': 0.5535073746025587, 'reg_alpha': 9.778019366828618, 'reg_lambda': 5.148992372934446, 'min_split_gain': 0.034638565330543686}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:29,616] Trial 585 finished with value: 0.2542945746742414 and parameters: {'learning_rate': 0.0974246992110879, 'num_leaves': 893, 'max_depth': 44, 'n_estimators': 887, 'min_child_samples': 35, 'subsample': 0.5708543629865046, 'colsample_bytree': 0.5162520384196806, 'reg_alpha': 0.7242347327018999, 'reg_lambda': 7.210497726209354, 'min_split_gain': 0.00014984288279877177}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:30,861] Trial 619 finished with value: 0.2580944549407806 and parameters: {'learning_rate': 0.09084543229481977, 'num_leaves': 831, 'max_depth': 56, 'n_estimators': 872, 'min_child_samples': 60, 'subsample': 0.8457456943084658, 'colsample_bytree': 0.5268508386503906, 'reg_alpha': 7.655063220654077, 'reg_lambda': 5.0360783545543955, 'min_split_gain': 0.040742755519743004}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:31,585] Trial 624 finished with value: 0.26133261063460844 and parameters: {'learning_rate': 0.11547984665478876, 'num_leaves': 41, 'max_depth': 55, 'n_estimators': 842, 'min_child_samples': 59, 'subsample': 0.8508821902575459, 'colsample_bytree': 0.5650072605250479, 'reg_alpha': 9.73598651303397, 'reg_lambda': 5.208155534291759, 'min_split_gain': 0.042305488607443376}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:32,428] Trial 622 finished with value: 0.2568450184867539 and parameters: {'learning_rate': 0.09381754758438456, 'num_leaves': 931, 'max_depth': 55, 'n_estimators': 840, 'min_child_samples': 60, 'subsample': 0.5922531862658031, 'colsample_bytree': 0.5003394229777293, 'reg_alpha': 8.396030104368307, 'reg_lambda': 7.397524346071104, 'min_split_gain': 0.03777213713537134}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:34,526] Trial 623 finished with value: 0.2577186170859142 and parameters: {'learning_rate': 0.07685443684742806, 'num_leaves': 933, 'max_depth': 55, 'n_estimators': 845, 'min_child_samples': 56, 'subsample': 0.8446022118103914, 'colsample_bytree': 0.5673790374838176, 'reg_alpha': 9.974334988251774, 'reg_lambda': 5.504149229168184, 'min_split_gain': 0.047265446088317634}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:35,313] Trial 610 finished with value: 0.25848608616429547 and parameters: {'learning_rate': 0.08231246345230249, 'num_leaves': 145, 'max_depth': 57, 'n_estimators': 862, 'min_child_samples': 32, 'subsample': 0.846753294093384, 'colsample_bytree': 0.5232188931930538, 'reg_alpha': 1.491926710446404e-05, 'reg_lambda': 8.434694005340537, 'min_split_gain': 0.03899026429404118}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:35,821] Trial 617 finished with value: 0.25687175963161774 and parameters: {'learning_rate': 0.09532395717096187, 'num_leaves': 831, 'max_depth': 56, 'n_estimators': 905, 'min_child_samples': 35, 'subsample': 0.8517783696736586, 'colsample_bytree': 0.5265703581942578, 'reg_alpha': 2.284499686031537, 'reg_lambda': 5.7048413194755065, 'min_split_gain': 0.04137339984247221}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:40,699] Trial 627 finished with value: 0.2594522319612793 and parameters: {'learning_rate': 0.07656185635005354, 'num_leaves': 879, 'max_depth': 51, 'n_estimators': 880, 'min_child_samples': 60, 'subsample': 0.9987133582679716, 'colsample_bytree': 0.5698082777568863, 'reg_alpha': 9.655643821229313, 'reg_lambda': 4.628079665572929, 'min_split_gain': 0.03849223729931132}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:43,207] Trial 508 finished with value: 0.2623296487745986 and parameters: {'learning_rate': 0.004130941006980407, 'num_leaves': 859, 'max_depth': 60, 'n_estimators': 906, 'min_child_samples': 61, 'subsample': 0.8731274892527421, 'colsample_bytree': 0.5225203318636428, 'reg_alpha': 0.4620071357011029, 'reg_lambda': 5.2086997598460165, 'min_split_gain': 2.8775541318689073e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:44,006] Trial 633 finished with value: 0.2643627162309786 and parameters: {'learning_rate': 0.0765167112517795, 'num_leaves': 892, 'max_depth': 50, 'n_estimators': 901, 'min_child_samples': 59, 'subsample': 0.8668419995579218, 'colsample_bytree': 0.5372679464846486, 'reg_alpha': 0.39555781230237463, 'reg_lambda': 4.6868851122478965, 'min_split_gain': 0.5404384880377308}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:44,317] Trial 601 finished with value: 0.25297366139550825 and parameters: {'learning_rate': 0.0527079567981938, 'num_leaves': 911, 'max_depth': 57, 'n_estimators': 861, 'min_child_samples': 35, 'subsample': 0.6158019477381236, 'colsample_bytree': 0.5127799479410909, 'reg_alpha': 5.713135546008621, 'reg_lambda': 6.9226372765761495, 'min_split_gain': 0.0006870290245753682}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:44,440] Trial 579 finished with value: 0.25801344638490303 and parameters: {'learning_rate': 0.031386573658288244, 'num_leaves': 863, 'max_depth': 48, 'n_estimators': 888, 'min_child_samples': 37, 'subsample': 0.8028302451566315, 'colsample_bytree': 0.5225943284160475, 'reg_alpha': 2.034581135775823, 'reg_lambda': 7.07170723248369, 'min_split_gain': 0.0007469718317659136}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:45,561] Trial 629 finished with value: 0.25791553287161734 and parameters: {'learning_rate': 0.07408430119460584, 'num_leaves': 887, 'max_depth': 50, 'n_estimators': 394, 'min_child_samples': 60, 'subsample': 0.984494448971083, 'colsample_bytree': 0.6051859690801129, 'reg_alpha': 9.502633053513678, 'reg_lambda': 4.889933964131334, 'min_split_gain': 0.041154007457042875}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:45,745] Trial 626 finished with value: 0.26028853692004866 and parameters: {'learning_rate': 0.12249405657244777, 'num_leaves': 882, 'max_depth': 55, 'n_estimators': 875, 'min_child_samples': 57, 'subsample': 0.5928153209680815, 'colsample_bytree': 0.5664993684049962, 'reg_alpha': 0.27683792230322735, 'reg_lambda': 5.24125037510169, 'min_split_gain': 0.0389182627046468}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:46,170] Trial 614 finished with value: 0.2591580162870431 and parameters: {'learning_rate': 0.11526591217635816, 'num_leaves': 828, 'max_depth': 44, 'n_estimators': 904, 'min_child_samples': 34, 'subsample': 0.8436843547131906, 'colsample_bytree': 0.527226130296599, 'reg_alpha': 1.0690008820257582, 'reg_lambda': 9.977630664437472, 'min_split_gain': 0.015903458896795283}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:48,602] Trial 631 finished with value: 0.2588668245715284 and parameters: {'learning_rate': 0.16057623261695453, 'num_leaves': 886, 'max_depth': 54, 'n_estimators': 837, 'min_child_samples': 60, 'subsample': 0.866926537426544, 'colsample_bytree': 0.5677045394411402, 'reg_alpha': 0.31964490481303504, 'reg_lambda': 4.57168366292604, 'min_split_gain': 0.040126092348074835}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:49,216] Trial 625 finished with value: 0.25620715790981546 and parameters: {'learning_rate': 0.11229458610329776, 'num_leaves': 879, 'max_depth': 54, 'n_estimators': 876, 'min_child_samples': 60, 'subsample': 0.9394107276872626, 'colsample_bytree': 0.565167471980785, 'reg_alpha': 1.0241311802408768, 'reg_lambda': 5.1923331055120405, 'min_split_gain': 0.04177119823461482}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:50,613] Trial 628 finished with value: 0.2608135351732319 and parameters: {'learning_rate': 0.11817986074703689, 'num_leaves': 879, 'max_depth': 55, 'n_estimators': 881, 'min_child_samples': 61, 'subsample': 0.8519145078334553, 'colsample_bytree': 0.5007362044846938, 'reg_alpha': 0.38038774358894173, 'reg_lambda': 4.8393867470716065, 'min_split_gain': 0.04459553780084935}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:26:54,094] Trial 613 finished with value: 0.2531286650106953 and parameters: {'learning_rate': 0.11624073358794774, 'num_leaves': 826, 'max_depth': 56, 'n_estimators': 907, 'min_child_samples': 34, 'subsample': 0.5655919731583523, 'colsample_bytree': 0.5253437623905174, 'reg_alpha': 6.070400921210787, 'reg_lambda': 9.562918991669171, 'min_split_gain': 0.0007370784825092033}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:00,780] Trial 630 finished with value: 0.2558023617556119 and parameters: {'learning_rate': 0.07395326076339488, 'num_leaves': 877, 'max_depth': 55, 'n_estimators': 841, 'min_child_samples': 61, 'subsample': 0.9582796306231931, 'colsample_bytree': 0.5653188452593664, 'reg_alpha': 1.047679133679005, 'reg_lambda': 4.799584495890616, 'min_split_gain': 0.038062778905538726}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:02,857] Trial 638 finished with value: 0.2579132750315322 and parameters: {'learning_rate': 0.15653974677216093, 'num_leaves': 94, 'max_depth': 50, 'n_estimators': 878, 'min_child_samples': 41, 'subsample': 0.8650565903497752, 'colsample_bytree': 0.5369894487968109, 'reg_alpha': 0.9717666944827499, 'reg_lambda': 5.751393671052756e-08, 'min_split_gain': 0.018487115652849512}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:04,744] Trial 602 finished with value: 0.25203113436026847 and parameters: {'learning_rate': 0.051832888582275125, 'num_leaves': 915, 'max_depth': 57, 'n_estimators': 871, 'min_child_samples': 32, 'subsample': 0.5554532515110827, 'colsample_bytree': 0.5100048586740744, 'reg_alpha': 5.380537942380326, 'reg_lambda': 6.5627636208748825, 'min_split_gain': 0.0010483765362046236}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:07,642] Trial 589 finished with value: 0.2703130820936287 and parameters: {'learning_rate': 0.0038198799663313058, 'num_leaves': 154, 'max_depth': 51, 'n_estimators': 861, 'min_child_samples': 39, 'subsample': 0.5680084533326959, 'colsample_bytree': 0.5160705482906017, 'reg_alpha': 6.92359108988808, 'reg_lambda': 6.232181774607633, 'min_split_gain': 0.002434126593280281}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:07,926] Trial 568 finished with value: 0.2525996512680123 and parameters: {'learning_rate': 0.0320766972063381, 'num_leaves': 252, 'max_depth': 43, 'n_estimators': 891, 'min_child_samples': 22, 'subsample': 0.9376659754907131, 'colsample_bytree': 0.5197223453981258, 'reg_alpha': 0.00017584654512779244, 'reg_lambda': 9.4739503826325, 'min_split_gain': 0.0009978275278068693}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:08,746] Trial 648 finished with value: 0.25518067854847626 and parameters: {'learning_rate': 0.13906006658201245, 'num_leaves': 792, 'max_depth': 58, 'n_estimators': 916, 'min_child_samples': 58, 'subsample': 0.548004550321129, 'colsample_bytree': 0.5096640184375691, 'reg_alpha': 4.96847661510118, 'reg_lambda': 3.359090889569092, 'min_split_gain': 0.020104113687871412}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:13,458] Trial 641 finished with value: 0.25951346917481094 and parameters: {'learning_rate': 0.14774763086718187, 'num_leaves': 885, 'max_depth': 54, 'n_estimators': 872, 'min_child_samples': 58, 'subsample': 0.9705501993654442, 'colsample_bytree': 0.5083170796850394, 'reg_alpha': 0.30231779429158123, 'reg_lambda': 4.358078709499226, 'min_split_gain': 0.01952978428292249}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:17,290] Trial 643 finished with value: 0.257147791784394 and parameters: {'learning_rate': 0.05764099073249397, 'num_leaves': 108, 'max_depth': 53, 'n_estimators': 856, 'min_child_samples': 58, 'subsample': 0.9522541864618101, 'colsample_bytree': 0.5023122538486094, 'reg_alpha': 4.63650197772148, 'reg_lambda': 3.9029908026812095, 'min_split_gain': 0.017682482348863923}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:19,135] Trial 590 finished with value: 0.2586717582123126 and parameters: {'learning_rate': 0.00811111359395747, 'num_leaves': 905, 'max_depth': 54, 'n_estimators': 865, 'min_child_samples': 35, 'subsample': 0.5866593275238, 'colsample_bytree': 0.5233837098101258, 'reg_alpha': 9.87376637029138, 'reg_lambda': 5.986945274799155, 'min_split_gain': 0.0017829318047496872}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:21,109] Trial 649 finished with value: 0.25416286164907875 and parameters: {'learning_rate': 0.12977723144255365, 'num_leaves': 926, 'max_depth': 58, 'n_estimators': 894, 'min_child_samples': 30, 'subsample': 0.6103439355943108, 'colsample_bytree': 0.5093109826338528, 'reg_alpha': 4.357712482195373, 'reg_lambda': 9.365958457521518, 'min_split_gain': 0.016794986595835754}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:24,325] Trial 646 finished with value: 0.25742600394336085 and parameters: {'learning_rate': 0.05841734648824562, 'num_leaves': 922, 'max_depth': 59, 'n_estimators': 861, 'min_child_samples': 40, 'subsample': 0.6092599270414693, 'colsample_bytree': 0.5350211047754986, 'reg_alpha': 4.639545438824087, 'reg_lambda': 3.0747895162430092, 'min_split_gain': 0.013966553194544321}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:25,151] Trial 652 finished with value: 0.2574118033657355 and parameters: {'learning_rate': 0.1437567826344385, 'num_leaves': 913, 'max_depth': 58, 'n_estimators': 412, 'min_child_samples': 30, 'subsample': 0.5507069323334651, 'colsample_bytree': 0.5018365958773138, 'reg_alpha': 4.912149147035651, 'reg_lambda': 0.02947711533563482, 'min_split_gain': 0.015102869133787233}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:26,624] Trial 419 finished with value: 0.5302954797084046 and parameters: {'learning_rate': 0.0009350225546315234, 'num_leaves': 975, 'max_depth': 11, 'n_estimators': 810, 'min_child_samples': 40, 'subsample': 0.808759648122978, 'colsample_bytree': 0.5509003270900978, 'reg_alpha': 0.000743041665189744, 'reg_lambda': 1.6514387099465943e-08, 'min_split_gain': 0.02622988480244405}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:35,115] Trial 632 finished with value: 0.2572980345548689 and parameters: {'learning_rate': 0.05736207905260189, 'num_leaves': 107, 'max_depth': 51, 'n_estimators': 902, 'min_child_samples': 25, 'subsample': 0.9863862083172062, 'colsample_bytree': 0.5019658915433872, 'reg_alpha': 0.3164733227329422, 'reg_lambda': 4.4765491476292505, 'min_split_gain': 0.015243760235324023}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:38,036] Trial 636 finished with value: 0.25745145696754773 and parameters: {'learning_rate': 0.05644756105122312, 'num_leaves': 927, 'max_depth': 54, 'n_estimators': 385, 'min_child_samples': 58, 'subsample': 0.9479028438843998, 'colsample_bytree': 0.5377205485514814, 'reg_alpha': 1.1252969464695195, 'reg_lambda': 4.455331010259795, 'min_split_gain': 0.01625777788376886}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:40,188] Trial 639 finished with value: 0.2562983898996992 and parameters: {'learning_rate': 0.056777171042908, 'num_leaves': 882, 'max_depth': 54, 'n_estimators': 880, 'min_child_samples': 58, 'subsample': 0.9795596111585388, 'colsample_bytree': 0.5547440950772641, 'reg_alpha': 1.1549333438757394, 'reg_lambda': 3.8463600590872478, 'min_split_gain': 0.019801464548846844}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:41,970] Trial 637 finished with value: 0.2583472542740053 and parameters: {'learning_rate': 0.058215309989583026, 'num_leaves': 884, 'max_depth': 53, 'n_estimators': 394, 'min_child_samples': 58, 'subsample': 0.8336773997120888, 'colsample_bytree': 0.552242984255383, 'reg_alpha': 1.1115607316899918, 'reg_lambda': 4.891361598299038, 'min_split_gain': 0.014911362799333028}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:43,594] Trial 647 finished with value: 0.2569444660258143 and parameters: {'learning_rate': 0.057524373951263266, 'num_leaves': 921, 'max_depth': 59, 'n_estimators': 856, 'min_child_samples': 25, 'subsample': 0.6194359432647886, 'colsample_bytree': 0.51120499595365, 'reg_alpha': 4.655146635026788, 'reg_lambda': 9.705390815547682, 'min_split_gain': 0.015653796079421235}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:46,153] Trial 644 finished with value: 0.2579211200507587 and parameters: {'learning_rate': 0.05879120284511922, 'num_leaves': 917, 'max_depth': 59, 'n_estimators': 835, 'min_child_samples': 58, 'subsample': 0.6123692470092089, 'colsample_bytree': 0.5340939450650778, 'reg_alpha': 1.0075285554791127, 'reg_lambda': 3.887458261840007, 'min_split_gain': 0.017637932502237778}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:49,140] Trial 658 finished with value: 0.2547949631851872 and parameters: {'learning_rate': 0.04755723738054138, 'num_leaves': 931, 'max_depth': 58, 'n_estimators': 902, 'min_child_samples': 25, 'subsample': 0.5306918770635383, 'colsample_bytree': 0.5088904615974448, 'reg_alpha': 4.97250914085888, 'reg_lambda': 7.118962958023273, 'min_split_gain': 0.06551591916897569}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:52,545] Trial 640 finished with value: 0.2565719295768163 and parameters: {'learning_rate': 0.05783870787870535, 'num_leaves': 909, 'max_depth': 54, 'n_estimators': 881, 'min_child_samples': 25, 'subsample': 0.953286682371227, 'colsample_bytree': 0.5030378559550017, 'reg_alpha': 1.20164027922974, 'reg_lambda': 4.189823096509088, 'min_split_gain': 0.0175511498867732}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:27:57,226] Trial 642 finished with value: 0.2577990280254816 and parameters: {'learning_rate': 0.05771793537447419, 'num_leaves': 101, 'max_depth': 53, 'n_estimators': 877, 'min_child_samples': 25, 'subsample': 0.9310688839551464, 'colsample_bytree': 0.501001400921615, 'reg_alpha': 1.0534994514950942, 'reg_lambda': 9.45513188389973, 'min_split_gain': 0.016591867407448738}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:09,678] Trial 664 finished with value: 0.2552587026699154 and parameters: {'learning_rate': 0.04750644168595455, 'num_leaves': 943, 'max_depth': 58, 'n_estimators': 898, 'min_child_samples': 19, 'subsample': 0.9270228464455246, 'colsample_bytree': 0.5114467757310424, 'reg_alpha': 2.7986625079047998, 'reg_lambda': 9.651999501727861, 'min_split_gain': 0.06414737422521499}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:11,169] Trial 669 finished with value: 0.2597247183348158 and parameters: {'learning_rate': 0.04676201984157691, 'num_leaves': 802, 'max_depth': 59, 'n_estimators': 892, 'min_child_samples': 33, 'subsample': 0.5183847077505117, 'colsample_bytree': 0.5101549133310045, 'reg_alpha': 0.0003097816500613953, 'reg_lambda': 9.697622084345106, 'min_split_gain': 0.2878425261612034}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:13,671] Trial 657 finished with value: 0.25796808595130327 and parameters: {'learning_rate': 0.0499149212650944, 'num_leaves': 926, 'max_depth': 59, 'n_estimators': 893, 'min_child_samples': 25, 'subsample': 0.5513549841020826, 'colsample_bytree': 0.500058487102856, 'reg_alpha': 2.1771823071498424e-05, 'reg_lambda': 9.623093864141003, 'min_split_gain': 0.06246736023758023}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:15,791] Trial 660 finished with value: 0.2566904590538636 and parameters: {'learning_rate': 0.04814019697224832, 'num_leaves': 936, 'max_depth': 59, 'n_estimators': 902, 'min_child_samples': 24, 'subsample': 0.5393048344484812, 'colsample_bytree': 0.505568250366674, 'reg_alpha': 0.00020453751486459605, 'reg_lambda': 8.899781704234096, 'min_split_gain': 0.07075435824254457}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:21,501] Trial 567 finished with value: 0.2834220559202702 and parameters: {'learning_rate': 0.0026506134942746178, 'num_leaves': 905, 'max_depth': 58, 'n_estimators': 885, 'min_child_samples': 38, 'subsample': 0.9300169832235942, 'colsample_bytree': 0.543213623865551, 'reg_alpha': 2.392420411436053, 'reg_lambda': 7.724763143730871e-08, 'min_split_gain': 0.06685191784665355}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:24,123] Trial 650 finished with value: 0.25424285932003404 and parameters: {'learning_rate': 0.055774654447341525, 'num_leaves': 904, 'max_depth': 53, 'n_estimators': 911, 'min_child_samples': 30, 'subsample': 0.5548732554948527, 'colsample_bytree': 0.5095114224805513, 'reg_alpha': 5.106263337127592, 'reg_lambda': 3.3269251021859243, 'min_split_gain': 0.00011228151214306265}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:28,051] Trial 663 finished with value: 0.25791619554336276 and parameters: {'learning_rate': 0.04697860702274431, 'num_leaves': 908, 'max_depth': 59, 'n_estimators': 885, 'min_child_samples': 25, 'subsample': 0.5662510742879131, 'colsample_bytree': 0.5093674710168434, 'reg_alpha': 6.663651566687826e-05, 'reg_lambda': 9.329896037166652, 'min_split_gain': 0.0675498493079842}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:28,378] Trial 634 finished with value: 0.25711625953116624 and parameters: {'learning_rate': 0.05443415406228224, 'num_leaves': 878, 'max_depth': 54, 'n_estimators': 900, 'min_child_samples': 60, 'subsample': 0.9910576466449725, 'colsample_bytree': 0.5373357572278109, 'reg_alpha': 0.9910156473690149, 'reg_lambda': 5.488511549048499, 'min_split_gain': 8.572922600864159e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:28,626] Trial 651 finished with value: 0.2535164273056722 and parameters: {'learning_rate': 0.056021162776750164, 'num_leaves': 929, 'max_depth': 58, 'n_estimators': 857, 'min_child_samples': 25, 'subsample': 0.5518706500556674, 'colsample_bytree': 0.5090457160353795, 'reg_alpha': 4.721628673068319, 'reg_lambda': 9.971101043925685, 'min_split_gain': 8.171038390110798e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:29,325] Trial 662 finished with value: 0.25759825972791456 and parameters: {'learning_rate': 0.04822596648297365, 'num_leaves': 935, 'max_depth': 58, 'n_estimators': 885, 'min_child_samples': 19, 'subsample': 0.5523158687069437, 'colsample_bytree': 0.5119688494156927, 'reg_alpha': 0.0003612569714835114, 'reg_lambda': 9.764844633033679, 'min_split_gain': 0.06271560086872519}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:37,586] Trial 668 finished with value: 0.25917457153006196 and parameters: {'learning_rate': 0.04713828999618654, 'num_leaves': 945, 'max_depth': 58, 'n_estimators': 893, 'min_child_samples': 31, 'subsample': 0.5644019528313372, 'colsample_bytree': 0.5152034349981802, 'reg_alpha': 0.00021414645022727776, 'reg_lambda': 9.773262770342043, 'min_split_gain': 0.06087004725704274}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:38,114] Trial 666 finished with value: 0.2589166153640968 and parameters: {'learning_rate': 0.046586722791872134, 'num_leaves': 907, 'max_depth': 59, 'n_estimators': 898, 'min_child_samples': 19, 'subsample': 0.9388208786505705, 'colsample_bytree': 0.5113974720031755, 'reg_alpha': 5.61183196115514e-06, 'reg_lambda': 8.630742518728717, 'min_split_gain': 0.06295782321542007}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:38,264] Trial 673 finished with value: 0.25402161062827167 and parameters: {'learning_rate': 0.04923617101344266, 'num_leaves': 942, 'max_depth': 56, 'n_estimators': 916, 'min_child_samples': 31, 'subsample': 0.5651459965765341, 'colsample_bytree': 0.5354938103195109, 'reg_alpha': 3.027759153152374, 'reg_lambda': 9.756187320255563, 'min_split_gain': 0.05448945513030579}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:42,609] Trial 670 finished with value: 0.25786553898914877 and parameters: {'learning_rate': 0.04805048813208242, 'num_leaves': 942, 'max_depth': 59, 'n_estimators': 896, 'min_child_samples': 18, 'subsample': 0.9264063237506983, 'colsample_bytree': 0.5116776423287338, 'reg_alpha': 0.0005184897085410944, 'reg_lambda': 9.532743555554504, 'min_split_gain': 0.07565196014025403}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:46,516] Trial 681 finished with value: 0.256932102725772 and parameters: {'learning_rate': 0.07290810973362821, 'num_leaves': 947, 'max_depth': 56, 'n_estimators': 830, 'min_child_samples': 23, 'subsample': 0.5382045713402401, 'colsample_bytree': 0.5366416875290492, 'reg_alpha': 3.3207102991764805, 'reg_lambda': 6.00886216221468e-08, 'min_split_gain': 0.34047909232969453}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:50,934] Trial 591 finished with value: 0.25326933095324305 and parameters: {'learning_rate': 0.05256095267899987, 'num_leaves': 876, 'max_depth': 54, 'n_estimators': 869, 'min_child_samples': 39, 'subsample': 0.8034048781493957, 'colsample_bytree': 0.5137815829921706, 'reg_alpha': 0.9278939724816249, 'reg_lambda': 6.568998549968091e-08, 'min_split_gain': 0.0002714187085013415}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:55,249] Trial 655 finished with value: 0.2561004778179444 and parameters: {'learning_rate': 0.010762987645448772, 'num_leaves': 931, 'max_depth': 59, 'n_estimators': 889, 'min_child_samples': 30, 'subsample': 0.6115250864040188, 'colsample_bytree': 0.5082059532768631, 'reg_alpha': 4.032187362885048, 'reg_lambda': 9.200770252337286, 'min_split_gain': 0.06261324187006238}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:28:58,947] Trial 686 finished with value: 0.2587826354241626 and parameters: {'learning_rate': 0.21018456993559012, 'num_leaves': 813, 'max_depth': 56, 'n_estimators': 848, 'min_child_samples': 27, 'subsample': 0.5806792172499844, 'colsample_bytree': 0.5184721218479247, 'reg_alpha': 6.842316476380863, 'reg_lambda': 9.859952673874475e-08, 'min_split_gain': 0.03265755914490358}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:01,369] Trial 683 finished with value: 0.2582103942208373 and parameters: {'learning_rate': 0.07048902289044874, 'num_leaves': 947, 'max_depth': 56, 'n_estimators': 840, 'min_child_samples': 21, 'subsample': 0.9650465410953809, 'colsample_bytree': 0.8946065362836706, 'reg_alpha': 6.44804952695323, 'reg_lambda': 5.584711181588291e-08, 'min_split_gain': 0.0338039678376942}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:07,670] Trial 678 finished with value: 0.25654178309628345 and parameters: {'learning_rate': 0.05115972347000633, 'num_leaves': 788, 'max_depth': 56, 'n_estimators': 832, 'min_child_samples': 32, 'subsample': 0.9596548503391735, 'colsample_bytree': 0.5342218564857271, 'reg_alpha': 2.697012454586953, 'reg_lambda': 3.482588439340245e-08, 'min_split_gain': 0.03099714194683496}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:14,737] Trial 682 finished with value: 0.25541106062132846 and parameters: {'learning_rate': 0.10474023748777614, 'num_leaves': 945, 'max_depth': 56, 'n_estimators': 834, 'min_child_samples': 21, 'subsample': 0.9605668270539719, 'colsample_bytree': 0.5325315876142331, 'reg_alpha': 7.0260703538714315, 'reg_lambda': 1.4353299710460092e-05, 'min_split_gain': 0.0007880680755425781}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:27,106] Trial 687 finished with value: 0.25854277380914736 and parameters: {'learning_rate': 0.043274361599925724, 'num_leaves': 899, 'max_depth': 57, 'n_estimators': 844, 'min_child_samples': 21, 'subsample': 0.54769377764956, 'colsample_bytree': 0.5193195972812478, 'reg_alpha': 6.764951808328119, 'reg_lambda': 6.11534426447763e-08, 'min_split_gain': 0.03310133123162851}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:29,933] Trial 688 finished with value: 0.25867774691946155 and parameters: {'learning_rate': 0.04089224504832954, 'num_leaves': 840, 'max_depth': 57, 'n_estimators': 835, 'min_child_samples': 22, 'subsample': 0.970170285680139, 'colsample_bytree': 0.5214787503913813, 'reg_alpha': 7.2691360146508055, 'reg_lambda': 2.2017822600112745e-07, 'min_split_gain': 0.03311307723326882}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:32,408] Trial 679 finished with value: 0.2568062844344041 and parameters: {'learning_rate': 0.027598751260202115, 'num_leaves': 937, 'max_depth': 56, 'n_estimators': 835, 'min_child_samples': 31, 'subsample': 0.5576132332665731, 'colsample_bytree': 0.5346135587119089, 'reg_alpha': 3.134018163066535, 'reg_lambda': 6.793846678317935, 'min_split_gain': 0.033063598571068636}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:39,797] Trial 605 finished with value: 0.25744602542633155 and parameters: {'learning_rate': 0.008122110199483893, 'num_leaves': 156, 'max_depth': 51, 'n_estimators': 866, 'min_child_samples': 34, 'subsample': 0.5888144145995154, 'colsample_bytree': 0.5263658911170368, 'reg_alpha': 5.709933026627798, 'reg_lambda': 6.848456702527537, 'min_split_gain': 0.00040329489271894435}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:54,335] Trial 645 finished with value: 0.2558299569644714 and parameters: {'learning_rate': 0.009985486045844203, 'num_leaves': 915, 'max_depth': 53, 'n_estimators': 857, 'min_child_samples': 40, 'subsample': 0.9783355832590371, 'colsample_bytree': 0.5002537012301824, 'reg_alpha': 4.920446110339948, 'reg_lambda': 9.874759133406855, 'min_split_gain': 0.011985137928114432}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:29:55,217] Trial 684 finished with value: 0.2570872409266338 and parameters: {'learning_rate': 0.07409829701152798, 'num_leaves': 954, 'max_depth': 56, 'n_estimators': 832, 'min_child_samples': 21, 'subsample': 0.5380584706685727, 'colsample_bytree': 0.9992329554771797, 'reg_alpha': 7.1110260022721645, 'reg_lambda': 3.714834012354039e-08, 'min_split_gain': 0.00047407073781595855}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:01,570] Trial 696 finished with value: 0.26161755729910374 and parameters: {'learning_rate': 0.05123802995277611, 'num_leaves': 821, 'max_depth': 57, 'n_estimators': 853, 'min_child_samples': 28, 'subsample': 0.5310476624282587, 'colsample_bytree': 0.5166127252029653, 'reg_alpha': 3.964095137498454, 'reg_lambda': 9.567253039966708e-08, 'min_split_gain': 0.9322498172292353}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:03,254] Trial 565 finished with value: 0.2618094848772056 and parameters: {'learning_rate': 0.004572424475359175, 'num_leaves': 245, 'max_depth': 61, 'n_estimators': 888, 'min_child_samples': 69, 'subsample': 0.792362726447271, 'colsample_bytree': 0.5450566486545363, 'reg_alpha': 1.6189768357603942e-05, 'reg_lambda': 3.6547487669997283, 'min_split_gain': 0.07318826058373736}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:04,889] Trial 671 finished with value: 0.2539209194327109 and parameters: {'learning_rate': 0.04869518479748474, 'num_leaves': 940, 'max_depth': 59, 'n_estimators': 914, 'min_child_samples': 30, 'subsample': 0.9374153182092377, 'colsample_bytree': 0.5140366119656865, 'reg_alpha': 3.0669657783368343, 'reg_lambda': 8.80357484636984, 'min_split_gain': 0.00023903498460117647}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:07,352] Trial 692 finished with value: 0.25719288495774634 and parameters: {'learning_rate': 0.05315160586385655, 'num_leaves': 846, 'max_depth': 52, 'n_estimators': 857, 'min_child_samples': 22, 'subsample': 0.5376215496407173, 'colsample_bytree': 0.5196893071425335, 'reg_alpha': 9.935951661673117, 'reg_lambda': 1.25809632795381e-07, 'min_split_gain': 0.0013564644830963278}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:09,040] Trial 693 finished with value: 0.25654727796372684 and parameters: {'learning_rate': 0.051792598233584286, 'num_leaves': 899, 'max_depth': 52, 'n_estimators': 859, 'min_child_samples': 23, 'subsample': 0.521219166199701, 'colsample_bytree': 0.5202819165219401, 'reg_alpha': 4.182198620809589, 'reg_lambda': 6.830616743055265, 'min_split_gain': 0.01983742419797689}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:09,770] Trial 691 finished with value: 0.25843737536917705 and parameters: {'learning_rate': 0.04404352956107288, 'num_leaves': 901, 'max_depth': 57, 'n_estimators': 852, 'min_child_samples': 33, 'subsample': 0.545323952004049, 'colsample_bytree': 0.5180955078590416, 'reg_alpha': 4.232511902789214, 'reg_lambda': 6.5487217113566265, 'min_split_gain': 0.0003591695229949832}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:10,367] Trial 680 finished with value: 0.2520312017147178 and parameters: {'learning_rate': 0.0730711750787527, 'num_leaves': 952, 'max_depth': 56, 'n_estimators': 834, 'min_child_samples': 21, 'subsample': 0.5758946629908313, 'colsample_bytree': 0.5363730331732026, 'reg_alpha': 3.2860641461944105, 'reg_lambda': 6.840765700209149, 'min_split_gain': 0.0011454809788292032}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:12,114] Trial 690 finished with value: 0.2551210353126918 and parameters: {'learning_rate': 0.050826922568862135, 'num_leaves': 899, 'max_depth': 57, 'n_estimators': 855, 'min_child_samples': 22, 'subsample': 0.5559823131323914, 'colsample_bytree': 0.5211723980249637, 'reg_alpha': 4.756097181414386, 'reg_lambda': 0.0028263321110164152, 'min_split_gain': 0.0003113661005469184}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:12,406] Trial 685 finished with value: 0.25573835474750045 and parameters: {'learning_rate': 0.07201441414605675, 'num_leaves': 785, 'max_depth': 57, 'n_estimators': 846, 'min_child_samples': 23, 'subsample': 0.9607243983655375, 'colsample_bytree': 0.5339622670042513, 'reg_alpha': 6.69878974262851, 'reg_lambda': 3.728223886823579e-08, 'min_split_gain': 0.00016891931323681616}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:14,523] Trial 586 finished with value: 0.2644038005624708 and parameters: {'learning_rate': 0.0037291808987580176, 'num_leaves': 902, 'max_depth': 53, 'n_estimators': 887, 'min_child_samples': 39, 'subsample': 0.9369710151503011, 'colsample_bytree': 0.5186681409853233, 'reg_alpha': 0.7208598838755133, 'reg_lambda': 9.42846683529275, 'min_split_gain': 0.00027644939429727084}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:18,537] Trial 689 finished with value: 0.2569907442161944 and parameters: {'learning_rate': 0.028018044287155514, 'num_leaves': 793, 'max_depth': 57, 'n_estimators': 853, 'min_child_samples': 21, 'subsample': 0.5586211046395665, 'colsample_bytree': 0.5204891758353203, 'reg_alpha': 2.3745118909273923, 'reg_lambda': 4.219356041597672e-08, 'min_split_gain': 0.019850286470626706}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:20,747] Trial 704 finished with value: 0.26352701101618886 and parameters: {'learning_rate': 0.060256162499199364, 'num_leaves': 69, 'max_depth': 55, 'n_estimators': 817, 'min_child_samples': 55, 'subsample': 0.5947576839965718, 'colsample_bytree': 0.5287593275545859, 'reg_alpha': 2.133567340353327, 'reg_lambda': 2.676602498143642, 'min_split_gain': 0.8148598792963765}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:24,041] Trial 659 finished with value: 0.2560396876633593 and parameters: {'learning_rate': 0.04745728484808954, 'num_leaves': 930, 'max_depth': 59, 'n_estimators': 894, 'min_child_samples': 22, 'subsample': 0.94509351540303, 'colsample_bytree': 0.5003545984438691, 'reg_alpha': 6.614273668799016e-05, 'reg_lambda': 9.492853816878206, 'min_split_gain': 0.00031155352581918334}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:25,277] Trial 674 finished with value: 0.25540651393738056 and parameters: {'learning_rate': 0.050663753858607136, 'num_leaves': 940, 'max_depth': 56, 'n_estimators': 840, 'min_child_samples': 31, 'subsample': 0.9348644890260266, 'colsample_bytree': 0.534979242108625, 'reg_alpha': 2.8694895459601732, 'reg_lambda': 1.432127026292654e-05, 'min_split_gain': 0.0012415068515213264}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:29,107] Trial 700 finished with value: 0.25777482443302796 and parameters: {'learning_rate': 0.060847232439728206, 'num_leaves': 895, 'max_depth': 55, 'n_estimators': 864, 'min_child_samples': 27, 'subsample': 0.5768920345680694, 'colsample_bytree': 0.5258124691511459, 'reg_alpha': 9.99071539048456, 'reg_lambda': 0.0006833000940029585, 'min_split_gain': 0.01856553653051297}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:31,034] Trial 654 finished with value: 0.2810527920476374 and parameters: {'learning_rate': 0.0027014246996857386, 'num_leaves': 929, 'max_depth': 58, 'n_estimators': 893, 'min_child_samples': 26, 'subsample': 0.9538404994113749, 'colsample_bytree': 0.5079595690769168, 'reg_alpha': 0.00034740464012913687, 'reg_lambda': 7.865527050162079, 'min_split_gain': 0.2952945313795348}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:32,207] Trial 236 finished with value: 0.25562850622943245 and parameters: {'learning_rate': 0.014046227646109343, 'num_leaves': 955, 'max_depth': 57, 'n_estimators': 768, 'min_child_samples': 14, 'subsample': 0.8909714538286395, 'colsample_bytree': 0.5165864068885854, 'reg_alpha': 1.2812711942456515e-07, 'reg_lambda': 2.443388319667786, 'min_split_gain': 0.0022050560108319087}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:38,035] Trial 698 finished with value: 0.2570540390138556 and parameters: {'learning_rate': 0.060763488254917356, 'num_leaves': 895, 'max_depth': 55, 'n_estimators': 864, 'min_child_samples': 33, 'subsample': 0.5224044858652894, 'colsample_bytree': 0.5214829737619604, 'reg_alpha': 2.159232711870564, 'reg_lambda': 1.209382496226495e-07, 'min_split_gain': 0.01981961120594215}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:40,570] Trial 708 finished with value: 0.25466818453970147 and parameters: {'learning_rate': 0.06170120539848045, 'num_leaves': 52, 'max_depth': 29, 'n_estimators': 817, 'min_child_samples': 26, 'subsample': 0.5836946773567736, 'colsample_bytree': 0.5310693680218226, 'reg_alpha': 2.1225123813675784, 'reg_lambda': 2.929705803444513, 'min_split_gain': 0.033633502345418176}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:46,913] Trial 702 finished with value: 0.26045467156417856 and parameters: {'learning_rate': 0.06072407649455252, 'num_leaves': 871, 'max_depth': 55, 'n_estimators': 869, 'min_child_samples': 33, 'subsample': 0.5721361360876146, 'colsample_bytree': 0.5285482472985292, 'reg_alpha': 1.9410564550513245, 'reg_lambda': 6.403963910234172, 'min_split_gain': 0.022457373055685223}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:47,690] Trial 707 finished with value: 0.25460816320759394 and parameters: {'learning_rate': 0.06151931080184628, 'num_leaves': 867, 'max_depth': 55, 'n_estimators': 814, 'min_child_samples': 18, 'subsample': 0.5996233029371504, 'colsample_bytree': 0.5293576057598879, 'reg_alpha': 2.5697200106087923, 'reg_lambda': 6.02450292696654, 'min_split_gain': 0.033087806186594494}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:49,680] Trial 703 finished with value: 0.2564321889415973 and parameters: {'learning_rate': 0.0626010977121727, 'num_leaves': 866, 'max_depth': 28, 'n_estimators': 868, 'min_child_samples': 24, 'subsample': 0.5534041472594624, 'colsample_bytree': 0.5333493686995024, 'reg_alpha': 2.0656203328786225, 'reg_lambda': 5.866216912819095e-08, 'min_split_gain': 0.022680275003104076}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:51,874] Trial 709 finished with value: 0.25703921022559 and parameters: {'learning_rate': 0.0616413501064225, 'num_leaves': 864, 'max_depth': 29, 'n_estimators': 298, 'min_child_samples': 27, 'subsample': 0.5689482700913481, 'colsample_bytree': 0.5299298311817396, 'reg_alpha': 2.058260854700351, 'reg_lambda': 6.054203080254263, 'min_split_gain': 0.03538534156545033}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:53,776] Trial 705 finished with value: 0.2583493846354596 and parameters: {'learning_rate': 0.061476780851214705, 'num_leaves': 919, 'max_depth': 55, 'n_estimators': 816, 'min_child_samples': 24, 'subsample': 0.5950948604695436, 'colsample_bytree': 0.5293925966546027, 'reg_alpha': 2.01388126430007, 'reg_lambda': 2.631553574046772, 'min_split_gain': 0.02201321782269062}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:53,917] Trial 706 finished with value: 0.25416982739624305 and parameters: {'learning_rate': 0.061324673683701425, 'num_leaves': 961, 'max_depth': 23, 'n_estimators': 815, 'min_child_samples': 24, 'subsample': 0.5923628409547447, 'colsample_bytree': 0.8726211617179462, 'reg_alpha': 2.070929326225867, 'reg_lambda': 2.7214164351909904, 'min_split_gain': 0.02378620964322745}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:55,636] Trial 718 finished with value: 0.26012979869771924 and parameters: {'learning_rate': 0.18691358368011807, 'num_leaves': 961, 'max_depth': 58, 'n_estimators': 870, 'min_child_samples': 20, 'subsample': 0.5619608756350906, 'colsample_bytree': 0.5485615124662913, 'reg_alpha': 4.275671196484598, 'reg_lambda': 6.016636165608729, 'min_split_gain': 0.048013980041465246}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:58,685] Trial 676 finished with value: 0.7042466811764084 and parameters: {'learning_rate': 0.0005023697025639569, 'num_leaves': 948, 'max_depth': 56, 'n_estimators': 836, 'min_child_samples': 32, 'subsample': 0.9637943632142337, 'colsample_bytree': 0.5349807329378977, 'reg_alpha': 3.062337528154487, 'reg_lambda': 9.498180372876948, 'min_split_gain': 0.001249104176946545}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:59,081] Trial 711 finished with value: 0.25606257533562365 and parameters: {'learning_rate': 0.06252893411198385, 'num_leaves': 957, 'max_depth': 55, 'n_estimators': 295, 'min_child_samples': 24, 'subsample': 0.5753834368802121, 'colsample_bytree': 0.5459833756434828, 'reg_alpha': 1.9802490291141146, 'reg_lambda': 2.762441289724789, 'min_split_gain': 0.03144191689509023}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:30:59,773] Trial 710 finished with value: 0.2577759652992648 and parameters: {'learning_rate': 0.06216149233109403, 'num_leaves': 922, 'max_depth': 55, 'n_estimators': 867, 'min_child_samples': 20, 'subsample': 0.5903430945650564, 'colsample_bytree': 0.531019835151705, 'reg_alpha': 1.8755077699666676, 'reg_lambda': 6.340834799467648, 'min_split_gain': 0.0320357154964376}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:01,435] Trial 713 finished with value: 0.25774853834571676 and parameters: {'learning_rate': 0.06071215581523311, 'num_leaves': 966, 'max_depth': 28, 'n_estimators': 868, 'min_child_samples': 24, 'subsample': 0.5699633858847811, 'colsample_bytree': 0.5491809239383577, 'reg_alpha': 2.2043472155855293, 'reg_lambda': 5.955051206067858, 'min_split_gain': 0.0365151642753904}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:03,361] Trial 701 finished with value: 0.25477398147067154 and parameters: {'learning_rate': 0.04338229927286903, 'num_leaves': 901, 'max_depth': 55, 'n_estimators': 867, 'min_child_samples': 33, 'subsample': 0.5681085161265714, 'colsample_bytree': 0.5288229598312557, 'reg_alpha': 2.0039223710840948, 'reg_lambda': 0.004446034131058479, 'min_split_gain': 0.021926650570856288}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:04,567] Trial 714 finished with value: 0.2561433133207193 and parameters: {'learning_rate': 0.06253795811489064, 'num_leaves': 861, 'max_depth': 29, 'n_estimators': 813, 'min_child_samples': 19, 'subsample': 0.5529909133121149, 'colsample_bytree': 0.5459664117925664, 'reg_alpha': 2.0920872124289325, 'reg_lambda': 0.00014462044135989723, 'min_split_gain': 0.03598598264769186}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:09,672] Trial 716 finished with value: 0.25689891007138344 and parameters: {'learning_rate': 0.06952299378221269, 'num_leaves': 961, 'max_depth': 55, 'n_estimators': 813, 'min_child_samples': 18, 'subsample': 0.5667294816514487, 'colsample_bytree': 0.5448959328697986, 'reg_alpha': 1.897879734280003, 'reg_lambda': 3.1664917776831357e-07, 'min_split_gain': 0.03748847031298839}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:11,173] Trial 727 finished with value: 0.26201841025601225 and parameters: {'learning_rate': 0.04199570744024521, 'num_leaves': 818, 'max_depth': 62, 'n_estimators': 830, 'min_child_samples': 83, 'subsample': 0.5820655146665112, 'colsample_bytree': 0.5413330761151907, 'reg_alpha': 4.5835755941683916, 'reg_lambda': 4.534225809935492, 'min_split_gain': 0.6117977206579303}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:17,620] Trial 729 finished with value: 0.255703487239612 and parameters: {'learning_rate': 0.08461738156962692, 'num_leaves': 921, 'max_depth': 62, 'n_estimators': 843, 'min_child_samples': 55, 'subsample': 0.579622458159323, 'colsample_bytree': 0.5117208811422836, 'reg_alpha': 4.337761503184765, 'reg_lambda': 4.561312820784974, 'min_split_gain': 0.0481013074143694}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:18,994] Trial 728 finished with value: 0.25906397731898123 and parameters: {'learning_rate': 0.08225178903787125, 'num_leaves': 842, 'max_depth': 58, 'n_estimators': 172, 'min_child_samples': 17, 'subsample': 0.6020558999160804, 'colsample_bytree': 0.5435421021760326, 'reg_alpha': 4.3603698243819915, 'reg_lambda': 3.446873067664963e-07, 'min_split_gain': 0.04759386630287784}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:19,340] Trial 719 finished with value: 0.2542607065094226 and parameters: {'learning_rate': 0.041988638342455864, 'num_leaves': 962, 'max_depth': 24, 'n_estimators': 228, 'min_child_samples': 55, 'subsample': 0.8042982224457943, 'colsample_bytree': 0.5449311744899604, 'reg_alpha': 4.018753599486366, 'reg_lambda': 5.908016904469788, 'min_split_gain': 0.04781945316109704}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:20,258] Trial 723 finished with value: 0.2569512892638405 and parameters: {'learning_rate': 0.04273879730118851, 'num_leaves': 920, 'max_depth': 54, 'n_estimators': 832, 'min_child_samples': 19, 'subsample': 0.806510126083385, 'colsample_bytree': 0.5442117280705111, 'reg_alpha': 9.82379010319909, 'reg_lambda': 0.00012127422003800819, 'min_split_gain': 0.050200829197688375}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:20,568] Trial 635 finished with value: 0.25692194530854073 and parameters: {'learning_rate': 0.010681509985952032, 'num_leaves': 873, 'max_depth': 53, 'n_estimators': 899, 'min_child_samples': 40, 'subsample': 0.9515405930399589, 'colsample_bytree': 0.5384976256502145, 'reg_alpha': 0.41087227404538657, 'reg_lambda': 5.079580966866801, 'min_split_gain': 0.017123547492789763}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:21,327] Trial 694 finished with value: 0.25418812323962664 and parameters: {'learning_rate': 0.05088426500483628, 'num_leaves': 899, 'max_depth': 52, 'n_estimators': 858, 'min_child_samples': 23, 'subsample': 0.5229439123209034, 'colsample_bytree': 0.5219001461050196, 'reg_alpha': 4.561048760706844, 'reg_lambda': 1.258933185224171e-07, 'min_split_gain': 0.0006016173206387396}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:22,301] Trial 730 finished with value: 0.2578804058201544 and parameters: {'learning_rate': 0.0834822825229854, 'num_leaves': 811, 'max_depth': 62, 'n_estimators': 847, 'min_child_samples': 55, 'subsample': 0.6179030050452861, 'colsample_bytree': 0.5148394875337066, 'reg_alpha': 9.616857902988826, 'reg_lambda': 4.037753526726394, 'min_split_gain': 0.05101832138743371}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:24,068] Trial 712 finished with value: 0.2572484442065251 and parameters: {'learning_rate': 0.033512851273293114, 'num_leaves': 963, 'max_depth': 29, 'n_estimators': 827, 'min_child_samples': 19, 'subsample': 0.5791579913997484, 'colsample_bytree': 0.547000501907723, 'reg_alpha': 1.9399242840130326, 'reg_lambda': 5.576220884001116, 'min_split_gain': 0.04138007023778492}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:24,399] Trial 731 finished with value: 0.2614919322059198 and parameters: {'learning_rate': 0.08232002132124425, 'num_leaves': 925, 'max_depth': 58, 'n_estimators': 849, 'min_child_samples': 55, 'subsample': 0.58276750009255, 'colsample_bytree': 0.5137590240158406, 'reg_alpha': 7.379104660250525, 'reg_lambda': 4.095944240646791, 'min_split_gain': 0.050913724121499526}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:25,794] Trial 695 finished with value: 0.25364210126027403 and parameters: {'learning_rate': 0.0506142549500622, 'num_leaves': 892, 'max_depth': 52, 'n_estimators': 862, 'min_child_samples': 26, 'subsample': 0.5776560640457273, 'colsample_bytree': 0.5187301986780853, 'reg_alpha': 4.515348971897004, 'reg_lambda': 9.724891739174708e-08, 'min_split_gain': 0.0005452897700963357}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:27,554] Trial 726 finished with value: 0.26232685321139837 and parameters: {'learning_rate': 0.08348106863911245, 'num_leaves': 706, 'max_depth': 58, 'n_estimators': 174, 'min_child_samples': 55, 'subsample': 0.5739773477463876, 'colsample_bytree': 0.5495219080105281, 'reg_alpha': 1.1622511832634746e-06, 'reg_lambda': 4.573070301360816, 'min_split_gain': 0.04797589113677399}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:29,101] Trial 722 finished with value: 0.25553585104106347 and parameters: {'learning_rate': 0.04241610756030292, 'num_leaves': 965, 'max_depth': 62, 'n_estimators': 831, 'min_child_samples': 18, 'subsample': 0.5782922991004265, 'colsample_bytree': 0.5484021513099012, 'reg_alpha': 3.6463628535511554, 'reg_lambda': 5.920078604346037, 'min_split_gain': 0.04864115511914659}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:31,845] Trial 697 finished with value: 0.253760187433335 and parameters: {'learning_rate': 0.05157093318843381, 'num_leaves': 899, 'max_depth': 52, 'n_estimators': 862, 'min_child_samples': 33, 'subsample': 0.6027735263203841, 'colsample_bytree': 0.5171621304591494, 'reg_alpha': 2.1683197116446515, 'reg_lambda': 1.1616189920655818e-07, 'min_split_gain': 0.0006477383068134491}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:32,584] Trial 734 finished with value: 0.25733143380257617 and parameters: {'learning_rate': 0.10164220781732251, 'num_leaves': 1006, 'max_depth': 58, 'n_estimators': 848, 'min_child_samples': 36, 'subsample': 0.5765923625695578, 'colsample_bytree': 0.513553438801124, 'reg_alpha': 9.67977313654077, 'reg_lambda': 4.200261087014061, 'min_split_gain': 0.019765183068388037}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:33,401] Trial 721 finished with value: 0.256280367586727 and parameters: {'learning_rate': 0.03300436568963119, 'num_leaves': 959, 'max_depth': 62, 'n_estimators': 823, 'min_child_samples': 24, 'subsample': 0.7510406605901265, 'colsample_bytree': 0.5488191864194483, 'reg_alpha': 4.471079846059387, 'reg_lambda': 0.005351622189066431, 'min_split_gain': 0.050577113383214474}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:33,811] Trial 720 finished with value: 0.2580315343919817 and parameters: {'learning_rate': 0.03335096156760226, 'num_leaves': 962, 'max_depth': 62, 'n_estimators': 825, 'min_child_samples': 24, 'subsample': 0.5735673484809259, 'colsample_bytree': 0.5454051725843221, 'reg_alpha': 4.085397985470296, 'reg_lambda': 5.990097575330425, 'min_split_gain': 0.05021524181666018}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:34,561] Trial 737 finished with value: 0.2570072927938895 and parameters: {'learning_rate': 0.12603030357148337, 'num_leaves': 919, 'max_depth': 58, 'n_estimators': 846, 'min_child_samples': 35, 'subsample': 0.5809436054086866, 'colsample_bytree': 0.5119951422407192, 'reg_alpha': 7.287570161805006, 'reg_lambda': 3.5824149653679096, 'min_split_gain': 0.020412412462947817}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:35,620] Trial 738 finished with value: 0.2604437507772363 and parameters: {'learning_rate': 0.1106620339095002, 'num_leaves': 1003, 'max_depth': 58, 'n_estimators': 872, 'min_child_samples': 36, 'subsample': 0.5835925118470423, 'colsample_bytree': 0.51176871078875, 'reg_alpha': 6.734409109316812, 'reg_lambda': 1.3782969951638246e-06, 'min_split_gain': 0.022008913091622763}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:36,054] Trial 725 finished with value: 0.2578986082818949 and parameters: {'learning_rate': 0.08370587774509183, 'num_leaves': 922, 'max_depth': 62, 'n_estimators': 875, 'min_child_samples': 19, 'subsample': 0.5683104462224701, 'colsample_bytree': 0.5437501519913581, 'reg_alpha': 8.040495052181307e-06, 'reg_lambda': 5.987473641942242, 'min_split_gain': 0.0486790297890513}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:38,614] Trial 675 finished with value: 0.25858207600897865 and parameters: {'learning_rate': 0.04818474237390128, 'num_leaves': 945, 'max_depth': 46, 'n_estimators': 831, 'min_child_samples': 31, 'subsample': 0.935833215844115, 'colsample_bytree': 0.5370353882366763, 'reg_alpha': 6.5378627210900806e-06, 'reg_lambda': 0.004510526332631576, 'min_split_gain': 8.627549009221423e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:39,170] Trial 743 finished with value: 0.26066675993618893 and parameters: {'learning_rate': 0.10255269489463369, 'num_leaves': 763, 'max_depth': 57, 'n_estimators': 880, 'min_child_samples': 35, 'subsample': 0.5640511422808308, 'colsample_bytree': 0.5090982791324418, 'reg_alpha': 6.365127061549935, 'reg_lambda': 6.408655414441835, 'min_split_gain': 0.08824469609444842}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:40,534] Trial 739 finished with value: 0.25698213088822625 and parameters: {'learning_rate': 0.10203824899668773, 'num_leaves': 773, 'max_depth': 58, 'n_estimators': 575, 'min_child_samples': 36, 'subsample': 0.7529178394159267, 'colsample_bytree': 0.5128876196217225, 'reg_alpha': 6.822632704863524, 'reg_lambda': 6.782928616539699, 'min_split_gain': 0.020269018491033252}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:40,905] Trial 740 finished with value: 0.25627934004701053 and parameters: {'learning_rate': 0.12123348221936617, 'num_leaves': 768, 'max_depth': 58, 'n_estimators': 576, 'min_child_samples': 35, 'subsample': 0.5604797186635428, 'colsample_bytree': 0.5111406799038486, 'reg_alpha': 6.928061344805177, 'reg_lambda': 6.756962275338158, 'min_split_gain': 0.017965803910473012}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:41,666] Trial 715 finished with value: 0.2577826699382395 and parameters: {'learning_rate': 0.032091625547444484, 'num_leaves': 963, 'max_depth': 55, 'n_estimators': 871, 'min_child_samples': 20, 'subsample': 0.5882187781777329, 'colsample_bytree': 0.5282886448430547, 'reg_alpha': 1.981685775402597, 'reg_lambda': 5.8679153097385734, 'min_split_gain': 0.03750505762198982}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:42,856] Trial 741 finished with value: 0.2553160324909863 and parameters: {'learning_rate': 0.10654576888688437, 'num_leaves': 1002, 'max_depth': 58, 'n_estimators': 575, 'min_child_samples': 36, 'subsample': 0.748907212209963, 'colsample_bytree': 0.5044574424059025, 'reg_alpha': 6.581077704426337, 'reg_lambda': 6.783381653010696, 'min_split_gain': 0.019388327286817764}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:46,213] Trial 735 finished with value: 0.2556130456660911 and parameters: {'learning_rate': 0.07119030511504852, 'num_leaves': 1006, 'max_depth': 58, 'n_estimators': 851, 'min_child_samples': 36, 'subsample': 0.5609175713482014, 'colsample_bytree': 0.5157156548327769, 'reg_alpha': 6.930928815106728, 'reg_lambda': 0.11918949082106021, 'min_split_gain': 0.016896347231662556}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:47,935] Trial 724 finished with value: 0.256955710915934 and parameters: {'learning_rate': 0.03290396139966197, 'num_leaves': 921, 'max_depth': 62, 'n_estimators': 875, 'min_child_samples': 83, 'subsample': 0.575006584000493, 'colsample_bytree': 0.5439976864251168, 'reg_alpha': 4.339347772521871, 'reg_lambda': 6.321053333494258, 'min_split_gain': 0.04652462306011061}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:48,075] Trial 748 finished with value: 0.2572602494907442 and parameters: {'learning_rate': 0.06958581330357261, 'num_leaves': 747, 'max_depth': 33, 'n_estimators': 874, 'min_child_samples': 28, 'subsample': 0.5557379852847613, 'colsample_bytree': 0.5255126289371486, 'reg_alpha': 7.429850066909919, 'reg_lambda': 6.806654337982405, 'min_split_gain': 0.09195580796758265}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:49,339] Trial 750 finished with value: 0.2590937174761175 and parameters: {'learning_rate': 0.07089094913638955, 'num_leaves': 998, 'max_depth': 57, 'n_estimators': 920, 'min_child_samples': 57, 'subsample': 0.5609048122602294, 'colsample_bytree': 0.5249918632911035, 'reg_alpha': 5.737707554033695, 'reg_lambda': 9.717003866939148, 'min_split_gain': 0.08359151831256903}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:51,281] Trial 742 finished with value: 0.2561628613920868 and parameters: {'learning_rate': 0.06997882596734395, 'num_leaves': 918, 'max_depth': 59, 'n_estimators': 878, 'min_child_samples': 36, 'subsample': 0.7675658621408153, 'colsample_bytree': 0.5007005890763099, 'reg_alpha': 6.775291504144592, 'reg_lambda': 6.878226553775436, 'min_split_gain': 0.018223577726783402}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:52,102] Trial 746 finished with value: 0.2564586946011067 and parameters: {'learning_rate': 0.05471044934543979, 'num_leaves': 1001, 'max_depth': 57, 'n_estimators': 575, 'min_child_samples': 28, 'subsample': 0.5635396648302867, 'colsample_bytree': 0.510855036703255, 'reg_alpha': 5.887216502972452, 'reg_lambda': 6.238318323431995, 'min_split_gain': 0.08754299254409956}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:55,375] Trial 757 finished with value: 0.2619052486688685 and parameters: {'learning_rate': 0.07143248591673432, 'num_leaves': 885, 'max_depth': 32, 'n_estimators': 922, 'min_child_samples': 29, 'subsample': 0.5383241711890984, 'colsample_bytree': 0.5236236346156348, 'reg_alpha': 3.0953974302010234, 'reg_lambda': 9.930011451797979, 'min_split_gain': 0.3800415810512032}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:56,952] Trial 749 finished with value: 0.2570551121379576 and parameters: {'learning_rate': 0.07231176125279212, 'num_leaves': 771, 'max_depth': 57, 'n_estimators': 571, 'min_child_samples': 57, 'subsample': 0.5596590353272703, 'colsample_bytree': 0.8472884521342254, 'reg_alpha': 7.034726256086836, 'reg_lambda': 0.09218713465094541, 'min_split_gain': 0.019110851467984795}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:57,383] Trial 732 finished with value: 0.25760265088007817 and parameters: {'learning_rate': 0.0324920805676983, 'num_leaves': 917, 'max_depth': 58, 'n_estimators': 850, 'min_child_samples': 56, 'subsample': 0.5589645752392144, 'colsample_bytree': 0.5001120198458393, 'reg_alpha': 9.974196073055166, 'reg_lambda': 4.016789634457429, 'min_split_gain': 0.018888136041681537}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:31:58,123] Trial 745 finished with value: 0.25637644246654767 and parameters: {'learning_rate': 0.07132174694056528, 'num_leaves': 781, 'max_depth': 54, 'n_estimators': 879, 'min_child_samples': 35, 'subsample': 0.7764168787265079, 'colsample_bytree': 0.5022638900957418, 'reg_alpha': 6.379368674448195, 'reg_lambda': 6.677940537697108, 'min_split_gain': 0.019530150098872823}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:02,200] Trial 717 finished with value: 0.2569721037875726 and parameters: {'learning_rate': 0.04311430373461342, 'num_leaves': 964, 'max_depth': 62, 'n_estimators': 820, 'min_child_samples': 19, 'subsample': 0.5589339318049048, 'colsample_bytree': 0.5478986857715705, 'reg_alpha': 8.619084494830417e-06, 'reg_lambda': 5.9700181038657645, 'min_split_gain': 0.04554148859527712}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:03,984] Trial 751 finished with value: 0.2583829820982554 and parameters: {'learning_rate': 0.07175852727130132, 'num_leaves': 774, 'max_depth': 54, 'n_estimators': 918, 'min_child_samples': 21, 'subsample': 0.5569733684707079, 'colsample_bytree': 0.5011106910211525, 'reg_alpha': 6.448592998929309, 'reg_lambda': 0.13912916957813232, 'min_split_gain': 0.018985483290731006}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:05,133] Trial 733 finished with value: 0.25684340760451935 and parameters: {'learning_rate': 0.031369842632975445, 'num_leaves': 765, 'max_depth': 32, 'n_estimators': 844, 'min_child_samples': 36, 'subsample': 0.8041011497565143, 'colsample_bytree': 0.5163488627598992, 'reg_alpha': 9.658757540054454, 'reg_lambda': 6.235980316868788, 'min_split_gain': 0.019302761610669295}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:05,288] Trial 744 finished with value: 0.2571061739758818 and parameters: {'learning_rate': 0.10398425861408526, 'num_leaves': 1007, 'max_depth': 35, 'n_estimators': 878, 'min_child_samples': 57, 'subsample': 0.5565122579502934, 'colsample_bytree': 0.5093295259578017, 'reg_alpha': 2.768281290571441e-05, 'reg_lambda': 6.181906476801385, 'min_split_gain': 0.01905282586794092}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:07,491] Trial 753 finished with value: 0.2572105535572266 and parameters: {'learning_rate': 0.07273797898004697, 'num_leaves': 742, 'max_depth': 60, 'n_estimators': 915, 'min_child_samples': 75, 'subsample': 0.7745956373650988, 'colsample_bytree': 0.5269523478548358, 'reg_alpha': 3.250144413403263, 'reg_lambda': 9.854118109944483, 'min_split_gain': 0.018458394544222424}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:08,008] Trial 747 finished with value: 0.2578365630179878 and parameters: {'learning_rate': 0.053339022615800986, 'num_leaves': 771, 'max_depth': 59, 'n_estimators': 576, 'min_child_samples': 36, 'subsample': 0.5562703226419086, 'colsample_bytree': 0.5083858505627016, 'reg_alpha': 6.915720186049708, 'reg_lambda': 2.722271802540342, 'min_split_gain': 0.016356227756842794}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:09,380] Trial 736 finished with value: 0.2566605357515899 and parameters: {'learning_rate': 0.1019091857279326, 'num_leaves': 770, 'max_depth': 58, 'n_estimators': 849, 'min_child_samples': 20, 'subsample': 0.6172107167051109, 'colsample_bytree': 0.5105593621239662, 'reg_alpha': 7.402896091985187, 'reg_lambda': 6.410720103809083, 'min_split_gain': 0.00021727053161645684}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:12,765] Trial 754 finished with value: 0.25477911857341506 and parameters: {'learning_rate': 0.07453831781139515, 'num_leaves': 733, 'max_depth': 54, 'n_estimators': 887, 'min_child_samples': 28, 'subsample': 0.5568331034095386, 'colsample_bytree': 0.5244735547101249, 'reg_alpha': 3.303315511485681, 'reg_lambda': 6.722174267605748e-05, 'min_split_gain': 0.021548662653884068}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:15,294] Trial 756 finished with value: 0.254777039334895 and parameters: {'learning_rate': 0.07125140989135711, 'num_leaves': 843, 'max_depth': 53, 'n_estimators': 711, 'min_child_samples': 41, 'subsample': 0.6162253174433908, 'colsample_bytree': 0.5265830862341625, 'reg_alpha': 3.033838007296555, 'reg_lambda': 0.0014994347271819505, 'min_split_gain': 0.019381750858231725}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:16,158] Trial 752 finished with value: 0.25834551445493853 and parameters: {'learning_rate': 0.07145279707027599, 'num_leaves': 1001, 'max_depth': 54, 'n_estimators': 576, 'min_child_samples': 16, 'subsample': 0.7758125963994792, 'colsample_bytree': 0.5264384255022296, 'reg_alpha': 3.213773551303783, 'reg_lambda': 9.743389930591702, 'min_split_gain': 0.019524871211414288}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:24,153] Trial 755 finished with value: 0.25708799805040883 and parameters: {'learning_rate': 0.06979776311540505, 'num_leaves': 987, 'max_depth': 60, 'n_estimators': 920, 'min_child_samples': 57, 'subsample': 0.6219395917684092, 'colsample_bytree': 0.5022324093007456, 'reg_alpha': 3.1682826923476783, 'reg_lambda': 9.935632074747915, 'min_split_gain': 0.02016755625028449}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:25,002] Trial 759 finished with value: 0.25306022825360536 and parameters: {'learning_rate': 0.06971566919923479, 'num_leaves': 990, 'max_depth': 60, 'n_estimators': 917, 'min_child_samples': 41, 'subsample': 0.7993865050344471, 'colsample_bytree': 0.5263420847108201, 'reg_alpha': 3.028265832597941, 'reg_lambda': 9.796501906635623, 'min_split_gain': 0.017551215701030676}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:28,666] Trial 667 finished with value: 0.25354765876769525 and parameters: {'learning_rate': 0.04730570093310139, 'num_leaves': 906, 'max_depth': 58, 'n_estimators': 897, 'min_child_samples': 19, 'subsample': 0.560751907344925, 'colsample_bytree': 0.5133435092942498, 'reg_alpha': 8.005729679980057e-05, 'reg_lambda': 9.629022574840093, 'min_split_gain': 0.0004719682308533021}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:34,251] Trial 765 finished with value: 0.2585588725174269 and parameters: {'learning_rate': 0.051271611927284584, 'num_leaves': 734, 'max_depth': 35, 'n_estimators': 911, 'min_child_samples': 41, 'subsample': 0.8054714246282301, 'colsample_bytree': 0.5342456755411167, 'reg_alpha': 3.910170529902092, 'reg_lambda': 3.0788096708361223, 'min_split_gain': 0.01940429882656334}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:45,449] Trial 767 finished with value: 0.25212920266186 and parameters: {'learning_rate': 0.053610650571659695, 'num_leaves': 835, 'max_depth': 54, 'n_estimators': 912, 'min_child_samples': 41, 'subsample': 0.6184519354038941, 'colsample_bytree': 0.5331911568260371, 'reg_alpha': 3.169452186302612, 'reg_lambda': 2.638298207081307, 'min_split_gain': 0.019385420828904897}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:51,788] Trial 771 finished with value: 0.2552728426069029 and parameters: {'learning_rate': 0.054825374435749925, 'num_leaves': 839, 'max_depth': 14, 'n_estimators': 896, 'min_child_samples': 41, 'subsample': 0.6106241672761364, 'colsample_bytree': 0.5386662393167814, 'reg_alpha': 3.277420703258213, 'reg_lambda': 2.8369465891969, 'min_split_gain': 0.016837273805477764}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:52,951] Trial 770 finished with value: 0.2571766707529184 and parameters: {'learning_rate': 0.05463786077425103, 'num_leaves': 876, 'max_depth': 56, 'n_estimators': 914, 'min_child_samples': 22, 'subsample': 0.5448797917463452, 'colsample_bytree': 0.5348338479299292, 'reg_alpha': 3.2740661536723166, 'reg_lambda': 2.970095218182729, 'min_split_gain': 0.018779900715896147}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:55,329] Trial 763 finished with value: 0.255922487168232 and parameters: {'learning_rate': 0.05470912332746259, 'num_leaves': 831, 'max_depth': 38, 'n_estimators': 711, 'min_child_samples': 41, 'subsample': 0.5467300100001203, 'colsample_bytree': 0.500650893236749, 'reg_alpha': 3.2203979674437506, 'reg_lambda': 2.2673680166973902e-07, 'min_split_gain': 0.01816586302589338}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:57,278] Trial 778 finished with value: 0.25688403573036317 and parameters: {'learning_rate': 0.08598602707361407, 'num_leaves': 1023, 'max_depth': 56, 'n_estimators': 915, 'min_child_samples': 41, 'subsample': 0.8018861711199954, 'colsample_bytree': 0.5369880919255392, 'reg_alpha': 3.0614167802506764, 'reg_lambda': 2.5425541915897933, 'min_split_gain': 0.03382476199970027}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:32:59,479] Trial 677 finished with value: 0.25728827894288386 and parameters: {'learning_rate': 0.009588639558681717, 'num_leaves': 943, 'max_depth': 56, 'n_estimators': 831, 'min_child_samples': 21, 'subsample': 0.9441024011150329, 'colsample_bytree': 0.5351752145775405, 'reg_alpha': 3.1081661252575388, 'reg_lambda': 1.425922865683265e-05, 'min_split_gain': 0.0002731093180374356}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:00,574] Trial 781 finished with value: 0.2602867911736734 and parameters: {'learning_rate': 0.0913557103530294, 'num_leaves': 853, 'max_depth': 14, 'n_estimators': 903, 'min_child_samples': 41, 'subsample': 0.7957772051269063, 'colsample_bytree': 0.5375318387125553, 'reg_alpha': 3.1305402972741634, 'reg_lambda': 1.0827686047605756, 'min_split_gain': 0.06308519603329778}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:08,480] Trial 661 finished with value: 0.2767945685965562 and parameters: {'learning_rate': 0.002581475808826559, 'num_leaves': 946, 'max_depth': 59, 'n_estimators': 902, 'min_child_samples': 26, 'subsample': 0.9266734797254405, 'colsample_bytree': 0.510531454484738, 'reg_alpha': 0.0003346679146869494, 'reg_lambda': 0.00014551954835718164, 'min_split_gain': 0.29705438426051795}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:15,281] Trial 780 finished with value: 0.2615880712440068 and parameters: {'learning_rate': 0.08233495209823641, 'num_leaves': 979, 'max_depth': 14, 'n_estimators': 939, 'min_child_samples': 42, 'subsample': 0.7967271655531607, 'colsample_bytree': 0.5553255930889056, 'reg_alpha': 5.1793271265959894e-05, 'reg_lambda': 1.0134775325152336, 'min_split_gain': 0.030106438341238103}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:24,431] Trial 783 finished with value: 0.25658103736622584 and parameters: {'learning_rate': 0.038889862284626375, 'num_leaves': 842, 'max_depth': 60, 'n_estimators': 952, 'min_child_samples': 43, 'subsample': 0.7910561795399853, 'colsample_bytree': 0.5559237914178811, 'reg_alpha': 4.789242305922819, 'reg_lambda': 6.161763983220805e-08, 'min_split_gain': 0.06248396927452861}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:25,295] Trial 784 finished with value: 0.2576117779757709 and parameters: {'learning_rate': 0.03969580314374343, 'num_leaves': 802, 'max_depth': 60, 'n_estimators': 938, 'min_child_samples': 43, 'subsample': 0.7927604660624427, 'colsample_bytree': 0.5588556903886177, 'reg_alpha': 4.710127260635278, 'reg_lambda': 4.2846723227183336e-08, 'min_split_gain': 0.06047119882698273}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:25,702] Trial 769 finished with value: 0.25572448325772057 and parameters: {'learning_rate': 0.05386683626881502, 'num_leaves': 845, 'max_depth': 60, 'n_estimators': 719, 'min_child_samples': 97, 'subsample': 0.6187559536183794, 'colsample_bytree': 0.9354108451788286, 'reg_alpha': 3.1463314705992067, 'reg_lambda': 2.8629090193261817, 'min_split_gain': 0.0007700180081262991}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:30,839] Trial 656 finished with value: 0.2560967012127936 and parameters: {'learning_rate': 0.04830283622488863, 'num_leaves': 930, 'max_depth': 59, 'n_estimators': 892, 'min_child_samples': 19, 'subsample': 0.5578657432784317, 'colsample_bytree': 0.5014438660381646, 'reg_alpha': 0.00033822069395537724, 'reg_lambda': 8.080340348094616, 'min_split_gain': 4.740398869334725e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:32,382] Trial 672 finished with value: 0.25671825791003905 and parameters: {'learning_rate': 0.049007981668209376, 'num_leaves': 931, 'max_depth': 58, 'n_estimators': 831, 'min_child_samples': 20, 'subsample': 0.5604503040429105, 'colsample_bytree': 0.8984864966508377, 'reg_alpha': 0.0004524731743190858, 'reg_lambda': 9.242077198491762, 'min_split_gain': 0.00035169313482075263}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:32,767] Trial 699 finished with value: 0.25739206337280074 and parameters: {'learning_rate': 0.032469726309324945, 'num_leaves': 893, 'max_depth': 55, 'n_estimators': 866, 'min_child_samples': 27, 'subsample': 0.5751700926995774, 'colsample_bytree': 0.5177362530682575, 'reg_alpha': 7.067319803641035e-07, 'reg_lambda': 1.4635831184165035e-07, 'min_split_gain': 0.018784289790003513}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:35,266] Trial 665 finished with value: 0.7008043603043996 and parameters: {'learning_rate': 0.0005048309993489733, 'num_leaves': 935, 'max_depth': 59, 'n_estimators': 836, 'min_child_samples': 25, 'subsample': 0.5334939731238387, 'colsample_bytree': 0.5121531872053938, 'reg_alpha': 0.0003652456643885555, 'reg_lambda': 9.711804079832207, 'min_split_gain': 0.061078035436458414}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:37,101] Trial 787 finished with value: 0.2581463517217126 and parameters: {'learning_rate': 0.039961405447819856, 'num_leaves': 812, 'max_depth': 63, 'n_estimators': 938, 'min_child_samples': 43, 'subsample': 0.6341995903637576, 'colsample_bytree': 0.5574443017722173, 'reg_alpha': 4.8363013402978545, 'reg_lambda': 5.403000547584797e-08, 'min_split_gain': 0.03691724246385802}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:37,683] Trial 779 finished with value: 0.25576947626702456 and parameters: {'learning_rate': 0.08609934067597028, 'num_leaves': 983, 'max_depth': 60, 'n_estimators': 909, 'min_child_samples': 41, 'subsample': 0.7960540808948785, 'colsample_bytree': 0.537864860461583, 'reg_alpha': 3.0962768059472667, 'reg_lambda': 4.302091655233455e-08, 'min_split_gain': 8.86588016672232e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:38,479] Trial 768 finished with value: 0.25649015150682314 and parameters: {'learning_rate': 0.05451089435887209, 'num_leaves': 832, 'max_depth': 60, 'n_estimators': 889, 'min_child_samples': 26, 'subsample': 0.6191681625039528, 'colsample_bytree': 0.5311471094617302, 'reg_alpha': 2.965832699831055, 'reg_lambda': 9.912977595669172, 'min_split_gain': 0.0013275294546968024}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:38,737] Trial 776 finished with value: 0.2562093550761045 and parameters: {'learning_rate': 0.056358614732793196, 'num_leaves': 878, 'max_depth': 14, 'n_estimators': 901, 'min_child_samples': 41, 'subsample': 0.8151027444091575, 'colsample_bytree': 0.5357060349050944, 'reg_alpha': 8.839589472944542e-05, 'reg_lambda': 1.9886056190536243e-07, 'min_split_gain': 0.0014854087625772107}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:39,772] Trial 782 finished with value: 0.255559018591969 and parameters: {'learning_rate': 0.05317542697022503, 'num_leaves': 830, 'max_depth': 60, 'n_estimators': 948, 'min_child_samples': 42, 'subsample': 0.7977521923401332, 'colsample_bytree': 0.556849930201532, 'reg_alpha': 3.048741858622178, 'reg_lambda': 1.270318693351119, 'min_split_gain': 0.0359155996663727}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:42,898] Trial 775 finished with value: 0.25676856108173857 and parameters: {'learning_rate': 0.05262936232155751, 'num_leaves': 979, 'max_depth': 56, 'n_estimators': 900, 'min_child_samples': 41, 'subsample': 0.8183910518105597, 'colsample_bytree': 0.5369016358513223, 'reg_alpha': 3.0732051562144744, 'reg_lambda': 2.6508465567397446, 'min_split_gain': 0.0006923587257420265}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:46,451] Trial 799 finished with value: 0.26384349550150843 and parameters: {'learning_rate': 0.04644864604550779, 'num_leaves': 877, 'max_depth': 53, 'n_estimators': 880, 'min_child_samples': 38, 'subsample': 0.8121784230888084, 'colsample_bytree': 0.5573799177822703, 'reg_alpha': 9.719869328907842, 'reg_lambda': 1.9322155605556544, 'min_split_gain': 0.9766609107477271}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:52,984] Trial 773 finished with value: 0.25532406503653327 and parameters: {'learning_rate': 0.054683902705554885, 'num_leaves': 872, 'max_depth': 56, 'n_estimators': 909, 'min_child_samples': 41, 'subsample': 0.8162570757128308, 'colsample_bytree': 0.9317408819870929, 'reg_alpha': 3.1569106704458756, 'reg_lambda': 5.543444670810922e-08, 'min_split_gain': 0.0022102386897877705}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:55,927] Trial 791 finished with value: 0.25739388662324175 and parameters: {'learning_rate': 0.045013083877978165, 'num_leaves': 818, 'max_depth': 63, 'n_estimators': 924, 'min_child_samples': 39, 'subsample': 0.6365379603644844, 'colsample_bytree': 0.5538669875028162, 'reg_alpha': 4.9539369445565, 'reg_lambda': 1.512511594042163, 'min_split_gain': 0.03768587218190918}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:58,528] Trial 796 finished with value: 0.25912133495851014 and parameters: {'learning_rate': 0.04311050535029021, 'num_leaves': 823, 'max_depth': 63, 'n_estimators': 927, 'min_child_samples': 39, 'subsample': 0.6373821014547207, 'colsample_bytree': 0.5371010990369348, 'reg_alpha': 9.858163270968413, 'reg_lambda': 5.943039836662185e-07, 'min_split_gain': 0.03571718781033325}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:58,917] Trial 793 finished with value: 0.25486284815030624 and parameters: {'learning_rate': 0.045270088511157366, 'num_leaves': 981, 'max_depth': 63, 'n_estimators': 925, 'min_child_samples': 39, 'subsample': 0.6364703924817631, 'colsample_bytree': 0.5578692821399982, 'reg_alpha': 5.065674275071511, 'reg_lambda': 3.390894819082187e-08, 'min_split_gain': 0.04030228786354856}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:33:59,334] Trial 762 finished with value: 0.25490644960732883 and parameters: {'learning_rate': 0.05257539041543651, 'num_leaves': 847, 'max_depth': 54, 'n_estimators': 923, 'min_child_samples': 41, 'subsample': 0.807426934359553, 'colsample_bytree': 0.534254832160535, 'reg_alpha': 3.180852008621714, 'reg_lambda': 1.9032717345337311e-07, 'min_split_gain': 0.0007265441880388112}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:00,226] Trial 798 finished with value: 0.2597145193099691 and parameters: {'learning_rate': 0.04534092253739607, 'num_leaves': 1024, 'max_depth': 64, 'n_estimators': 929, 'min_child_samples': 40, 'subsample': 0.813498415973923, 'colsample_bytree': 0.5345978655693496, 'reg_alpha': 9.874935849543242, 'reg_lambda': 2.6349633321069974, 'min_split_gain': 0.03560091857211753}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:01,526] Trial 777 finished with value: 0.25731678253781487 and parameters: {'learning_rate': 0.05573936959301218, 'num_leaves': 886, 'max_depth': 14, 'n_estimators': 904, 'min_child_samples': 22, 'subsample': 0.8168354767832551, 'colsample_bytree': 0.5591029239390285, 'reg_alpha': 4.121806223482299, 'reg_lambda': 5.522582480404277e-08, 'min_split_gain': 0.0009800026242528135}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:01,930] Trial 800 finished with value: 0.2595319429282097 and parameters: {'learning_rate': 0.04468672784658778, 'num_leaves': 680, 'max_depth': 64, 'n_estimators': 933, 'min_child_samples': 39, 'subsample': 0.8163459110972159, 'colsample_bytree': 0.7493469082135257, 'reg_alpha': 9.782578143274025, 'reg_lambda': 3.088646020344323e-05, 'min_split_gain': 0.037615473785013585}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:04,260] Trial 760 finished with value: 0.2566465355794987 and parameters: {'learning_rate': 0.05366734550226811, 'num_leaves': 878, 'max_depth': 54, 'n_estimators': 910, 'min_child_samples': 41, 'subsample': 0.5456022734460323, 'colsample_bytree': 0.501082345345376, 'reg_alpha': 2.5291869354616563e-05, 'reg_lambda': 3.5676344814466594, 'min_split_gain': 0.00031356560432574854}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:04,803] Trial 774 finished with value: 0.25241548675990494 and parameters: {'learning_rate': 0.05423393026710959, 'num_leaves': 680, 'max_depth': 14, 'n_estimators': 908, 'min_child_samples': 40, 'subsample': 0.8021089589068968, 'colsample_bytree': 0.5376297972854727, 'reg_alpha': 3.211659919094692, 'reg_lambda': 2.61025137615618, 'min_split_gain': 0.00017198225572109122}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:05,233] Trial 802 finished with value: 0.2575966539651204 and parameters: {'learning_rate': 0.044457002564491865, 'num_leaves': 862, 'max_depth': 53, 'n_estimators': 918, 'min_child_samples': 39, 'subsample': 0.5048433786687945, 'colsample_bytree': 0.555180931847837, 'reg_alpha': 9.389997526069685, 'reg_lambda': 1.3636897176423646, 'min_split_gain': 0.0374969957100221}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:12,562] Trial 806 finished with value: 0.26008762197459995 and parameters: {'learning_rate': 0.03772946026336081, 'num_leaves': 982, 'max_depth': 61, 'n_estimators': 879, 'min_child_samples': 59, 'subsample': 0.8374999479933629, 'colsample_bytree': 0.5267655524036436, 'reg_alpha': 1.405022630749775, 'reg_lambda': 3.3052740201855947e-08, 'min_split_gain': 0.4636445828583359}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:14,632] Trial 795 finished with value: 0.25632984130420416 and parameters: {'learning_rate': 0.04376234018213895, 'num_leaves': 814, 'max_depth': 63, 'n_estimators': 928, 'min_child_samples': 39, 'subsample': 0.8144333725959378, 'colsample_bytree': 0.5565612042908991, 'reg_alpha': 1.5112089167081109, 'reg_lambda': 1.2944956040826927, 'min_split_gain': 0.037420886124667326}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:18,610] Trial 805 finished with value: 0.25864416671715706 and parameters: {'learning_rate': 0.0380883080378454, 'num_leaves': 862, 'max_depth': 61, 'n_estimators': 924, 'min_child_samples': 60, 'subsample': 0.8355774023672211, 'colsample_bytree': 0.7509594773295688, 'reg_alpha': 9.86374301504609, 'reg_lambda': 3.4915982253833984e-08, 'min_split_gain': 0.03801315494464778}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:20,618] Trial 766 finished with value: 0.25399207798160794 and parameters: {'learning_rate': 0.05394395527397925, 'num_leaves': 841, 'max_depth': 14, 'n_estimators': 710, 'min_child_samples': 16, 'subsample': 0.6145528137417875, 'colsample_bytree': 0.5571772389601457, 'reg_alpha': 3.3386275284999454, 'reg_lambda': 1.981107218954781e-07, 'min_split_gain': 0.0003946390264542805}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:20,919] Trial 801 finished with value: 0.2569062491162841 and parameters: {'learning_rate': 0.04515274545744611, 'num_leaves': 984, 'max_depth': 56, 'n_estimators': 932, 'min_child_samples': 38, 'subsample': 0.8148870191824606, 'colsample_bytree': 0.5545187255401411, 'reg_alpha': 1.4044670348049153, 'reg_lambda': 1.5336339370829648, 'min_split_gain': 0.035628593027125906}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:21,635] Trial 812 finished with value: 0.25733836577490754 and parameters: {'learning_rate': 0.06384168077389818, 'num_leaves': 988, 'max_depth': 61, 'n_estimators': 879, 'min_child_samples': 59, 'subsample': 0.7859441517354526, 'colsample_bytree': 0.5247711231523199, 'reg_alpha': 5.722579748507984, 'reg_lambda': 3.197898805640966e-08, 'min_split_gain': 0.03420652298513872}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:24,997] Trial 761 finished with value: 0.253280692452422 and parameters: {'learning_rate': 0.053611754990371865, 'num_leaves': 844, 'max_depth': 53, 'n_estimators': 918, 'min_child_samples': 22, 'subsample': 0.6168484022365105, 'colsample_bytree': 0.5335797020973163, 'reg_alpha': 3.3694109994877466, 'reg_lambda': 9.851018150594369, 'min_split_gain': 0.0003097151868348969}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:25,537] Trial 803 finished with value: 0.25535221919969714 and parameters: {'learning_rate': 0.043621458475466855, 'num_leaves': 862, 'max_depth': 64, 'n_estimators': 920, 'min_child_samples': 39, 'subsample': 0.6368867479126595, 'colsample_bytree': 0.5234875543811024, 'reg_alpha': 5.02129684520445, 'reg_lambda': 3.446635273302434e-08, 'min_split_gain': 0.03730815095971177}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:26,101] Trial 814 finished with value: 0.25609935512276033 and parameters: {'learning_rate': 0.06454452863850219, 'num_leaves': 717, 'max_depth': 13, 'n_estimators': 803, 'min_child_samples': 53, 'subsample': 0.7865554345084069, 'colsample_bytree': 0.5431637252471203, 'reg_alpha': 1.5214425529305424, 'reg_lambda': 1.6977435286036244, 'min_split_gain': 0.055043161935188455}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:28,165] Trial 786 finished with value: 0.2585502908107419 and parameters: {'learning_rate': 0.03830312980964739, 'num_leaves': 827, 'max_depth': 60, 'n_estimators': 923, 'min_child_samples': 39, 'subsample': 0.7931795793833457, 'colsample_bytree': 0.5568171665865275, 'reg_alpha': 0.00011408428035298723, 'reg_lambda': 4.135950178066653e-08, 'min_split_gain': 0.03737161658202098}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:28,652] Trial 808 finished with value: 0.2562424039797708 and parameters: {'learning_rate': 0.06440346421529747, 'num_leaves': 1017, 'max_depth': 61, 'n_estimators': 874, 'min_child_samples': 38, 'subsample': 0.8351989163404303, 'colsample_bytree': 0.5240558381264288, 'reg_alpha': 1.4476262376289175, 'reg_lambda': 2.0387073729127502e-08, 'min_split_gain': 0.034444927375724195}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:30,160] Trial 758 finished with value: 0.2524235486023523 and parameters: {'learning_rate': 0.07231400295849735, 'num_leaves': 880, 'max_depth': 54, 'n_estimators': 921, 'min_child_samples': 21, 'subsample': 0.5418605095929512, 'colsample_bytree': 0.5265249993861638, 'reg_alpha': 3.0519525713915003, 'reg_lambda': 9.862570457464951, 'min_split_gain': 0.00010375851941188527}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:31,312] Trial 817 finished with value: 0.26138543677155096 and parameters: {'learning_rate': 0.0626362306201697, 'num_leaves': 687, 'max_depth': 15, 'n_estimators': 885, 'min_child_samples': 38, 'subsample': 0.7875302963586581, 'colsample_bytree': 0.5430077186297764, 'reg_alpha': 1.4827269475083837, 'reg_lambda': 3.578057573863287, 'min_split_gain': 0.2452298571612069}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:31,810] Trial 811 finished with value: 0.2561760015796897 and parameters: {'learning_rate': 0.06505842928107597, 'num_leaves': 1022, 'max_depth': 61, 'n_estimators': 880, 'min_child_samples': 59, 'subsample': 0.8047553717681832, 'colsample_bytree': 0.5254576715513568, 'reg_alpha': 1.4308794753488485, 'reg_lambda': 3.9042063599374957, 'min_split_gain': 0.03283773073106822}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:31,811] Trial 785 finished with value: 0.25511182212708916 and parameters: {'learning_rate': 0.04040132044101159, 'num_leaves': 800, 'max_depth': 60, 'n_estimators': 937, 'min_child_samples': 43, 'subsample': 0.7946712935548508, 'colsample_bytree': 0.557155301295255, 'reg_alpha': 4.591517691488135, 'reg_lambda': 4.1085346246522684e-08, 'min_split_gain': 0.0002926439869257996}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:32,945] Trial 813 finished with value: 0.25958246571399074 and parameters: {'learning_rate': 0.06398184077400582, 'num_leaves': 739, 'max_depth': 61, 'n_estimators': 880, 'min_child_samples': 38, 'subsample': 0.8345926800200355, 'colsample_bytree': 0.5437217128703551, 'reg_alpha': 1.5832034070512362, 'reg_lambda': 2.1513007087486388e-08, 'min_split_gain': 0.03367567339681001}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:35,718] Trial 821 finished with value: 0.26478074102397114 and parameters: {'learning_rate': 0.06446595649061597, 'num_leaves': 724, 'max_depth': 51, 'n_estimators': 341, 'min_child_samples': 38, 'subsample': 0.8051331986213772, 'colsample_bytree': 0.5424064501337643, 'reg_alpha': 1.6592948527083338, 'reg_lambda': 3.846844412121923, 'min_split_gain': 0.6657527201069997}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:36,084] Trial 792 finished with value: 0.260162700621404 and parameters: {'learning_rate': 0.043657351017038964, 'num_leaves': 817, 'max_depth': 63, 'n_estimators': 919, 'min_child_samples': 39, 'subsample': 0.634076350931242, 'colsample_bytree': 0.5537758832124309, 'reg_alpha': 2.181442828095759e-05, 'reg_lambda': 3.467152245406903e-08, 'min_split_gain': 0.03856770161133191}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:37,741] Trial 804 finished with value: 0.25678464964948206 and parameters: {'learning_rate': 0.06393785060089018, 'num_leaves': 983, 'max_depth': 63, 'n_estimators': 926, 'min_child_samples': 39, 'subsample': 0.8320347384479356, 'colsample_bytree': 0.526876073045114, 'reg_alpha': 2.354058258121707e-05, 'reg_lambda': 3.1370888045607166, 'min_split_gain': 0.03540574461435323}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:38,013] Trial 816 finished with value: 0.25474834735452295 and parameters: {'learning_rate': 0.059467657973908954, 'num_leaves': 1016, 'max_depth': 15, 'n_estimators': 886, 'min_child_samples': 43, 'subsample': 0.8288890001355815, 'colsample_bytree': 0.5434104027548482, 'reg_alpha': 1.4827477671837406, 'reg_lambda': 2.1775430425599147, 'min_split_gain': 0.06315996579980446}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:38,915] Trial 794 finished with value: 0.25890572111651183 and parameters: {'learning_rate': 0.043884935196541366, 'num_leaves': 806, 'max_depth': 63, 'n_estimators': 922, 'min_child_samples': 39, 'subsample': 0.5031289745207462, 'colsample_bytree': 0.5356771637987456, 'reg_alpha': 2.280063379488594e-05, 'reg_lambda': 1.8819534015942816, 'min_split_gain': 0.036185095946475954}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:39,924] Trial 824 finished with value: 0.26164382411305204 and parameters: {'learning_rate': 0.0919974093870026, 'num_leaves': 717, 'max_depth': 51, 'n_estimators': 353, 'min_child_samples': 43, 'subsample': 0.6079138604273048, 'colsample_bytree': 0.5765929861165933, 'reg_alpha': 1.8481144550423663, 'reg_lambda': 3.660353494555555, 'min_split_gain': 0.11562565034213455}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:41,569] Trial 815 finished with value: 0.2582028252652387 and parameters: {'learning_rate': 0.06432902842056128, 'num_leaves': 750, 'max_depth': 13, 'n_estimators': 803, 'min_child_samples': 53, 'subsample': 0.782624182926105, 'colsample_bytree': 0.5422451545780234, 'reg_alpha': 1.219831674690262, 'reg_lambda': 0.5145110210429881, 'min_split_gain': 0.03323396101315132}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:41,952] Trial 797 finished with value: 0.25715104464580135 and parameters: {'learning_rate': 0.045116829060117915, 'num_leaves': 868, 'max_depth': 61, 'n_estimators': 922, 'min_child_samples': 39, 'subsample': 0.8155160351143943, 'colsample_bytree': 0.5361308231299512, 'reg_alpha': 9.681251807595045e-05, 'reg_lambda': 1.6889676076663929, 'min_split_gain': 0.03691578063332996}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:43,286] Trial 819 finished with value: 0.25738588677858854 and parameters: {'learning_rate': 0.06626819014763644, 'num_leaves': 796, 'max_depth': 15, 'n_estimators': 884, 'min_child_samples': 53, 'subsample': 0.779879522814279, 'colsample_bytree': 0.5422610753666467, 'reg_alpha': 1.7373469435703057, 'reg_lambda': 2.092471272886261e-08, 'min_split_gain': 0.05928720994858793}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:43,965] Trial 823 finished with value: 0.25973899674189366 and parameters: {'learning_rate': 0.08014907371381966, 'num_leaves': 699, 'max_depth': 51, 'n_estimators': 952, 'min_child_samples': 37, 'subsample': 0.6046413370440531, 'colsample_bytree': 0.5425451156734544, 'reg_alpha': 1.5991042838100251, 'reg_lambda': 3.9499096123293995, 'min_split_gain': 0.07804672494312656}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:44,772] Trial 820 finished with value: 0.2583714108711535 and parameters: {'learning_rate': 0.06499616556751621, 'num_leaves': 750, 'max_depth': 16, 'n_estimators': 946, 'min_child_samples': 62, 'subsample': 0.7903782187931212, 'colsample_bytree': 0.5419641494295954, 'reg_alpha': 1.676579375439514, 'reg_lambda': 0.6887489791372473, 'min_split_gain': 0.054027238845441236}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:45,838] Trial 818 finished with value: 0.2567078995848159 and parameters: {'learning_rate': 0.0632101867827651, 'num_leaves': 702, 'max_depth': 15, 'n_estimators': 883, 'min_child_samples': 62, 'subsample': 0.7879600161036395, 'colsample_bytree': 0.5429623512751909, 'reg_alpha': 1.4618916920646714, 'reg_lambda': 0.5582906107493821, 'min_split_gain': 0.058845273994804576}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:46,525] Trial 807 finished with value: 0.2578425841474779 and parameters: {'learning_rate': 0.03710770103832148, 'num_leaves': 694, 'max_depth': 61, 'n_estimators': 808, 'min_child_samples': 59, 'subsample': 0.8299449514769404, 'colsample_bytree': 0.5255102820867346, 'reg_alpha': 1.3125835127671737, 'reg_lambda': 3.9225491504979697, 'min_split_gain': 0.037858440478651084}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:48,237] Trial 825 finished with value: 0.2594299618739739 and parameters: {'learning_rate': 0.07989692722333613, 'num_leaves': 800, 'max_depth': 51, 'n_estimators': 958, 'min_child_samples': 43, 'subsample': 0.6262315912287292, 'colsample_bytree': 0.577323949692491, 'reg_alpha': 1.6855667286880105, 'reg_lambda': 4.088104606390051e-06, 'min_split_gain': 0.06351264220825924}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:48,240] Trial 822 finished with value: 0.25750476754263846 and parameters: {'learning_rate': 0.06364909139362679, 'num_leaves': 819, 'max_depth': 53, 'n_estimators': 806, 'min_child_samples': 43, 'subsample': 0.6244235489590915, 'colsample_bytree': 0.54032425046702, 'reg_alpha': 1.53091961042828, 'reg_lambda': 3.925274794400095, 'min_split_gain': 0.07871988987649657}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:48,764] Trial 809 finished with value: 0.25680167440934454 and parameters: {'learning_rate': 0.036986186070365946, 'num_leaves': 798, 'max_depth': 61, 'n_estimators': 880, 'min_child_samples': 61, 'subsample': 0.8384795281890866, 'colsample_bytree': 0.5246222306377092, 'reg_alpha': 1.3760832967842207, 'reg_lambda': 3.4023024702723485e-05, 'min_split_gain': 0.035278290574112504}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:49,730] Trial 828 finished with value: 0.2566229784691289 and parameters: {'learning_rate': 0.08144798090219434, 'num_leaves': 706, 'max_depth': 51, 'n_estimators': 966, 'min_child_samples': 42, 'subsample': 0.8036540275929951, 'colsample_bytree': 0.5315327185395098, 'reg_alpha': 2.073972144678716, 'reg_lambda': 4.116505358867488, 'min_split_gain': 0.07477619423263551}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:51,592] Trial 827 finished with value: 0.26134314599291547 and parameters: {'learning_rate': 0.0830079704406103, 'num_leaves': 797, 'max_depth': 51, 'n_estimators': 350, 'min_child_samples': 43, 'subsample': 0.804282886420823, 'colsample_bytree': 0.5396062482829099, 'reg_alpha': 1.4568640247135685, 'reg_lambda': 2.712433729111503, 'min_split_gain': 0.05934145389769543}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:53,478] Trial 772 finished with value: 0.581465987789761 and parameters: {'learning_rate': 0.0007246896756181404, 'num_leaves': 880, 'max_depth': 14, 'n_estimators': 903, 'min_child_samples': 40, 'subsample': 0.6110119748556275, 'colsample_bytree': 0.531754374520335, 'reg_alpha': 3.129962344977003, 'reg_lambda': 3.125603084059551, 'min_split_gain': 0.03209021317402413}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:54,539] Trial 829 finished with value: 0.25473353433706125 and parameters: {'learning_rate': 0.09210579153721311, 'num_leaves': 795, 'max_depth': 51, 'n_estimators': 941, 'min_child_samples': 43, 'subsample': 0.6019511145363567, 'colsample_bytree': 0.5373697521465521, 'reg_alpha': 2.1498868807105045, 'reg_lambda': 4.0608908547253435, 'min_split_gain': 0.06103072854253641}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:55,127] Trial 789 finished with value: 0.25354389720107307 and parameters: {'learning_rate': 0.03856522502093635, 'num_leaves': 819, 'max_depth': 63, 'n_estimators': 945, 'min_child_samples': 39, 'subsample': 0.5092849515126023, 'colsample_bytree': 0.55780632074205, 'reg_alpha': 4.973060653833003, 'reg_lambda': 1.7882703713508816, 'min_split_gain': 0.00024764772628177575}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:56,272] Trial 835 finished with value: 0.25954284440847275 and parameters: {'learning_rate': 0.08098253477602477, 'num_leaves': 707, 'max_depth': 52, 'n_estimators': 941, 'min_child_samples': 43, 'subsample': 0.7607891895124288, 'colsample_bytree': 0.5315770922518683, 'reg_alpha': 0.6668507573197275, 'reg_lambda': 4.329911075208225, 'min_split_gain': 0.11716124537187982}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:56,757] Trial 832 finished with value: 0.2586786960111073 and parameters: {'learning_rate': 0.07913946986442713, 'num_leaves': 695, 'max_depth': 53, 'n_estimators': 954, 'min_child_samples': 43, 'subsample': 0.6110211118584579, 'colsample_bytree': 0.5340501770195122, 'reg_alpha': 0.6650318925172393, 'reg_lambda': 2.027325139719466, 'min_split_gain': 0.07687412831096063}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:57,094] Trial 831 finished with value: 0.2591108093964237 and parameters: {'learning_rate': 0.0791646542998718, 'num_leaves': 795, 'max_depth': 52, 'n_estimators': 966, 'min_child_samples': 42, 'subsample': 0.6100999047528034, 'colsample_bytree': 0.535731827035678, 'reg_alpha': 0.6687707360918789, 'reg_lambda': 2.2060670899908374, 'min_split_gain': 0.07586855189861237}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:58,040] Trial 826 finished with value: 0.25784128155124164 and parameters: {'learning_rate': 0.0916972018343163, 'num_leaves': 720, 'max_depth': 53, 'n_estimators': 962, 'min_child_samples': 43, 'subsample': 0.8042108587148732, 'colsample_bytree': 0.539132444797173, 'reg_alpha': 1.8738904386269057, 'reg_lambda': 4.122605097271321, 'min_split_gain': 0.02111734372118782}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:58,733] Trial 836 finished with value: 0.25708186307780767 and parameters: {'learning_rate': 0.07668473553940307, 'num_leaves': 692, 'max_depth': 52, 'n_estimators': 902, 'min_child_samples': 17, 'subsample': 0.7794543641919749, 'colsample_bytree': 0.5354514749667006, 'reg_alpha': 2.524402270720945, 'reg_lambda': 4.516772727030301, 'min_split_gain': 0.08616571230658206}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:34:59,010] Trial 834 finished with value: 0.2607040908697864 and parameters: {'learning_rate': 0.08979528496225765, 'num_leaves': 709, 'max_depth': 52, 'n_estimators': 904, 'min_child_samples': 43, 'subsample': 0.7733263518347695, 'colsample_bytree': 0.5348583298218927, 'reg_alpha': 2.305046314667377, 'reg_lambda': 4.170141497675459, 'min_split_gain': 0.08066753469810982}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:02,633] Trial 830 finished with value: 0.25466882870750873 and parameters: {'learning_rate': 0.08111639028749516, 'num_leaves': 803, 'max_depth': 52, 'n_estimators': 956, 'min_child_samples': 43, 'subsample': 0.6003749114248306, 'colsample_bytree': 0.5373100689836641, 'reg_alpha': 2.4677734167491283, 'reg_lambda': 0.46102957366563896, 'min_split_gain': 0.019601702073357426}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:03,410] Trial 839 finished with value: 0.2571564620465753 and parameters: {'learning_rate': 0.07929831465901742, 'num_leaves': 705, 'max_depth': 53, 'n_estimators': 955, 'min_child_samples': 44, 'subsample': 0.6238703507503786, 'colsample_bytree': 0.5297947189729681, 'reg_alpha': 0.5740543103817561, 'reg_lambda': 4.356754435395352e-06, 'min_split_gain': 0.08615953329014508}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:03,880] Trial 833 finished with value: 0.26197689904812327 and parameters: {'learning_rate': 0.09209905454194013, 'num_leaves': 665, 'max_depth': 54, 'n_estimators': 937, 'min_child_samples': 16, 'subsample': 0.6115671524473916, 'colsample_bytree': 0.5348493188126735, 'reg_alpha': 1.0579922732039317, 'reg_lambda': 2.291334220124591, 'min_split_gain': 0.08461712808292186}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:12,032] Trial 840 finished with value: 0.2587490799756281 and parameters: {'learning_rate': 0.09150536748094994, 'num_leaves': 860, 'max_depth': 52, 'n_estimators': 905, 'min_child_samples': 42, 'subsample': 0.5272138773190905, 'colsample_bytree': 0.5285775136169797, 'reg_alpha': 2.4096601985112875, 'reg_lambda': 4.378767453252041, 'min_split_gain': 0.017989669024125857}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:15,811] Trial 857 finished with value: 0.26111369407849444 and parameters: {'learning_rate': 0.12104794792981685, 'num_leaves': 847, 'max_depth': 55, 'n_estimators': 912, 'min_child_samples': 23, 'subsample': 0.524017499855784, 'colsample_bytree': 0.5252379801853436, 'reg_alpha': 4.12461058804196, 'reg_lambda': 6.382894106599032, 'min_split_gain': 0.01668650799143367}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:16,646] Trial 810 finished with value: 0.26810804359202356 and parameters: {'learning_rate': 0.006314902427005086, 'num_leaves': 989, 'max_depth': 53, 'n_estimators': 512, 'min_child_samples': 61, 'subsample': 0.9093489500637125, 'colsample_bytree': 0.5247142896604551, 'reg_alpha': 5.4409245500943495, 'reg_lambda': 2.0581889258958115e-08, 'min_split_gain': 0.03513517485346884}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:18,445] Trial 847 finished with value: 0.2559696836704645 and parameters: {'learning_rate': 0.09248832414176514, 'num_leaves': 866, 'max_depth': 53, 'n_estimators': 148, 'min_child_samples': 17, 'subsample': 0.5283875732004321, 'colsample_bytree': 0.5199752250476521, 'reg_alpha': 2.6380222355157024, 'reg_lambda': 4.7460879056660366, 'min_split_gain': 0.018195024595747657}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:20,009] Trial 859 finished with value: 0.25493288868519093 and parameters: {'learning_rate': 0.12060784668066897, 'num_leaves': 864, 'max_depth': 55, 'n_estimators': 259, 'min_child_samples': 34, 'subsample': 0.8228029851340258, 'colsample_bytree': 0.5193113239555116, 'reg_alpha': 4.186098652158714, 'reg_lambda': 9.97278806512819, 'min_split_gain': 0.01882721565382074}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:20,472] Trial 856 finished with value: 0.2560806177235923 and parameters: {'learning_rate': 0.12233317586890323, 'num_leaves': 852, 'max_depth': 56, 'n_estimators': 909, 'min_child_samples': 16, 'subsample': 0.8215861679611157, 'colsample_bytree': 0.5199356681868209, 'reg_alpha': 2.572810764436477, 'reg_lambda': 0.03416194520760597, 'min_split_gain': 0.018905406879639517}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:21,137] Trial 844 finished with value: 0.25479396073722627 and parameters: {'learning_rate': 0.07592857861900099, 'num_leaves': 846, 'max_depth': 53, 'n_estimators': 941, 'min_child_samples': 23, 'subsample': 0.8046596699028014, 'colsample_bytree': 0.521621588872143, 'reg_alpha': 2.655789025180916, 'reg_lambda': 4.265783662488828, 'min_split_gain': 0.01504932092602485}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:23,786] Trial 790 finished with value: 0.2537405214819573 and parameters: {'learning_rate': 0.03781693047614655, 'num_leaves': 868, 'max_depth': 61, 'n_estimators': 925, 'min_child_samples': 39, 'subsample': 0.8135118096917727, 'colsample_bytree': 0.5541097905278327, 'reg_alpha': 4.76462247442329, 'reg_lambda': 3.9848758927378546e-08, 'min_split_gain': 0.0003435666112336545}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:24,152] Trial 838 finished with value: 0.2565621949259982 and parameters: {'learning_rate': 0.07724983686718909, 'num_leaves': 701, 'max_depth': 52, 'n_estimators': 947, 'min_child_samples': 43, 'subsample': 0.6252058137229588, 'colsample_bytree': 0.5287816258509548, 'reg_alpha': 0.6313253168335281, 'reg_lambda': 4.693117262441437, 'min_split_gain': 0.019006687077058767}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:26,962] Trial 837 finished with value: 0.26155548917567223 and parameters: {'learning_rate': 0.07763973972981138, 'num_leaves': 692, 'max_depth': 53, 'n_estimators': 941, 'min_child_samples': 44, 'subsample': 0.6035952316612594, 'colsample_bytree': 0.5311288705531855, 'reg_alpha': 0.5801719761001215, 'reg_lambda': 0.8742334166108763, 'min_split_gain': 0.018199363494822356}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:28,006] Trial 653 finished with value: 0.25628045173948916 and parameters: {'learning_rate': 0.01021331152056532, 'num_leaves': 799, 'max_depth': 58, 'n_estimators': 903, 'min_child_samples': 24, 'subsample': 0.9359219347790675, 'colsample_bytree': 0.5073291081877581, 'reg_alpha': 0.0002910611276326538, 'reg_lambda': 7.619038575665874, 'min_split_gain': 0.017306916385130874}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:28,735] Trial 861 finished with value: 0.2584125132556617 and parameters: {'learning_rate': 0.17315827727579564, 'num_leaves': 852, 'max_depth': 55, 'n_estimators': 905, 'min_child_samples': 37, 'subsample': 0.8190601185126046, 'colsample_bytree': 0.5192990598096651, 'reg_alpha': 5.004766657230322, 'reg_lambda': 6.885516736453585, 'min_split_gain': 0.0174783219809834}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:29,225] Trial 854 finished with value: 0.258025112554945 and parameters: {'learning_rate': 0.09532292518150451, 'num_leaves': 850, 'max_depth': 55, 'n_estimators': 904, 'min_child_samples': 17, 'subsample': 0.8030901551769905, 'colsample_bytree': 0.5191401588374143, 'reg_alpha': 2.8104324508031517, 'reg_lambda': 9.957264677501177, 'min_split_gain': 0.013899284903677946}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:29,866] Trial 843 finished with value: 0.2587359074763299 and parameters: {'learning_rate': 0.0784376840394635, 'num_leaves': 856, 'max_depth': 52, 'n_estimators': 966, 'min_child_samples': 44, 'subsample': 0.8059253770124352, 'colsample_bytree': 0.5308582810468763, 'reg_alpha': 0.7992003492294347, 'reg_lambda': 4.527095463990917, 'min_split_gain': 0.01794027840041399}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:32,866] Trial 858 finished with value: 0.2570562980794774 and parameters: {'learning_rate': 0.07088630537947327, 'num_leaves': 856, 'max_depth': 55, 'n_estimators': 906, 'min_child_samples': 17, 'subsample': 0.8190815248599049, 'colsample_bytree': 0.5199712253101663, 'reg_alpha': 4.786632103856918, 'reg_lambda': 6.61568915527423, 'min_split_gain': 0.017371220895503206}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:33,189] Trial 841 finished with value: 0.2601882920402454 and parameters: {'learning_rate': 0.08039263600246344, 'num_leaves': 859, 'max_depth': 53, 'n_estimators': 950, 'min_child_samples': 17, 'subsample': 0.8028214568583912, 'colsample_bytree': 0.5217758175772855, 'reg_alpha': 0.5895794217552626, 'reg_lambda': 4.043052020118149, 'min_split_gain': 0.021665913192362645}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:34,346] Trial 845 finished with value: 0.2569143770908127 and parameters: {'learning_rate': 0.08790457765016399, 'num_leaves': 851, 'max_depth': 52, 'n_estimators': 974, 'min_child_samples': 16, 'subsample': 0.5146891882320916, 'colsample_bytree': 0.5222828771492347, 'reg_alpha': 0.606230946413252, 'reg_lambda': 4.5052388566941035, 'min_split_gain': 0.020737580967502178}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:35,403] Trial 864 finished with value: 0.256921009465001 and parameters: {'learning_rate': 0.13662731822011775, 'num_leaves': 850, 'max_depth': 55, 'n_estimators': 903, 'min_child_samples': 37, 'subsample': 0.8196199757558829, 'colsample_bytree': 0.5187505402421586, 'reg_alpha': 4.292409898234553, 'reg_lambda': 0.0004798704846724941, 'min_split_gain': 0.018761755195298813}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:37,133] Trial 852 finished with value: 0.2558278614294685 and parameters: {'learning_rate': 0.07399533213805819, 'num_leaves': 856, 'max_depth': 55, 'n_estimators': 906, 'min_child_samples': 17, 'subsample': 0.8212312998840965, 'colsample_bytree': 0.5251708779560005, 'reg_alpha': 2.7175140781007863, 'reg_lambda': 9.994146350807865, 'min_split_gain': 0.017080768989146115}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:37,956] Trial 855 finished with value: 0.25809699059229896 and parameters: {'learning_rate': 0.07424399374228041, 'num_leaves': 847, 'max_depth': 55, 'n_estimators': 252, 'min_child_samples': 23, 'subsample': 0.525815428866512, 'colsample_bytree': 0.5197627282083932, 'reg_alpha': 2.3597765109529, 'reg_lambda': 9.860937798483695, 'min_split_gain': 0.017533465862831908}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:38,325] Trial 853 finished with value: 0.2570897022079024 and parameters: {'learning_rate': 0.07431877725137188, 'num_leaves': 851, 'max_depth': 54, 'n_estimators': 318, 'min_child_samples': 17, 'subsample': 0.8226254222258691, 'colsample_bytree': 0.5202245048683097, 'reg_alpha': 2.4136974317954327, 'reg_lambda': 6.115084214353223, 'min_split_gain': 0.018355784928247958}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:39,490] Trial 849 finished with value: 0.2538308635673588 and parameters: {'learning_rate': 0.09331856806870102, 'num_leaves': 668, 'max_depth': 53, 'n_estimators': 955, 'min_child_samples': 17, 'subsample': 0.5965096197784401, 'colsample_bytree': 0.5230048958866556, 'reg_alpha': 0.6668182763570776, 'reg_lambda': 4.8947228234743845, 'min_split_gain': 0.017559259680683448}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:40,589] Trial 850 finished with value: 0.256819208913174 and parameters: {'learning_rate': 0.09106790936862229, 'num_leaves': 855, 'max_depth': 53, 'n_estimators': 944, 'min_child_samples': 23, 'subsample': 0.8231471543979995, 'colsample_bytree': 0.5210942305502517, 'reg_alpha': 0.5694472733136038, 'reg_lambda': 4.82452345541508, 'min_split_gain': 0.017183240456296045}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:43,059] Trial 860 finished with value: 0.2571791117489271 and parameters: {'learning_rate': 0.07270985647854537, 'num_leaves': 847, 'max_depth': 56, 'n_estimators': 236, 'min_child_samples': 23, 'subsample': 0.908983296279475, 'colsample_bytree': 0.5182010895999772, 'reg_alpha': 4.420013389730979, 'reg_lambda': 6.776864785174769, 'min_split_gain': 0.020597413179600142}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:44,956] Trial 851 finished with value: 0.2569614172752502 and parameters: {'learning_rate': 0.07559470314639277, 'num_leaves': 675, 'max_depth': 53, 'n_estimators': 973, 'min_child_samples': 22, 'subsample': 0.763027661606895, 'colsample_bytree': 0.5215658188295206, 'reg_alpha': 0.5709071231207236, 'reg_lambda': 5.568290336576685, 'min_split_gain': 0.020768177539954972}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:45,259] Trial 862 finished with value: 0.2579842762651313 and parameters: {'learning_rate': 0.07144288453837533, 'num_leaves': 889, 'max_depth': 55, 'n_estimators': 118, 'min_child_samples': 21, 'subsample': 0.8220503877672634, 'colsample_bytree': 0.5189774212002333, 'reg_alpha': 4.592445118544584, 'reg_lambda': 9.720582684497783, 'min_split_gain': 0.01756449387776581}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:46,676] Trial 863 finished with value: 0.25935265886344017 and parameters: {'learning_rate': 0.07195346009963356, 'num_leaves': 845, 'max_depth': 56, 'n_estimators': 901, 'min_child_samples': 34, 'subsample': 0.8256905040968163, 'colsample_bytree': 0.5162253126171907, 'reg_alpha': 4.369118957082736, 'reg_lambda': 7.377789662423767, 'min_split_gain': 0.017428832874339297}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:47,062] Trial 879 finished with value: 0.26028869560500284 and parameters: {'learning_rate': 0.059605947033912615, 'num_leaves': 892, 'max_depth': 57, 'n_estimators': 543, 'min_child_samples': 34, 'subsample': 0.5380671335801649, 'colsample_bytree': 0.5143427246504452, 'reg_alpha': 4.132261184835829, 'reg_lambda': 6.382932770422144, 'min_split_gain': 0.4928371734013081}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:51,394] Trial 846 finished with value: 0.26091362696964315 and parameters: {'learning_rate': 0.08580535368607362, 'num_leaves': 861, 'max_depth': 53, 'n_estimators': 974, 'min_child_samples': 23, 'subsample': 0.8001738648160458, 'colsample_bytree': 0.5305132072819861, 'reg_alpha': 0.6684681003361054, 'reg_lambda': 4.427792774042413, 'min_split_gain': 0.01675377097144186}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:52,304] Trial 842 finished with value: 0.25851448568204494 and parameters: {'learning_rate': 0.07915445836673315, 'num_leaves': 846, 'max_depth': 53, 'n_estimators': 957, 'min_child_samples': 23, 'subsample': 0.6232918256390698, 'colsample_bytree': 0.5292365761652263, 'reg_alpha': 0.78629265419389, 'reg_lambda': 4.33198108479179, 'min_split_gain': 0.01734766926774613}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:53,245] Trial 866 finished with value: 0.253341561578679 and parameters: {'learning_rate': 0.07227095334093186, 'num_leaves': 893, 'max_depth': 55, 'n_estimators': 858, 'min_child_samples': 21, 'subsample': 0.8204594741901514, 'colsample_bytree': 0.5190312316093904, 'reg_alpha': 4.215311373248343, 'reg_lambda': 6.335716545696073, 'min_split_gain': 0.016851299017158358}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:55,875] Trial 865 finished with value: 0.2534897024854637 and parameters: {'learning_rate': 0.07236682920446781, 'num_leaves': 897, 'max_depth': 56, 'n_estimators': 899, 'min_child_samples': 34, 'subsample': 0.8247865839737977, 'colsample_bytree': 0.5169056470393405, 'reg_alpha': 3.7688929761392402, 'reg_lambda': 6.556601144085903, 'min_split_gain': 0.014986947934425}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:56,515] Trial 868 finished with value: 0.2548021963468596 and parameters: {'learning_rate': 0.07126896517345892, 'num_leaves': 886, 'max_depth': 56, 'n_estimators': 901, 'min_child_samples': 36, 'subsample': 0.9219663711891464, 'colsample_bytree': 0.5168776957077438, 'reg_alpha': 4.426484637210041, 'reg_lambda': 5.7548099643898665, 'min_split_gain': 0.016084340746981382}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:57,772] Trial 887 finished with value: 0.26105698820601614 and parameters: {'learning_rate': 0.05796730087101177, 'num_leaves': 895, 'max_depth': 56, 'n_estimators': 856, 'min_child_samples': 32, 'subsample': 0.8056226810684617, 'colsample_bytree': 0.5434907969179229, 'reg_alpha': 6.585999194973892, 'reg_lambda': 2.757253477168933, 'min_split_gain': 0.8496086553347537}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:35:59,490] Trial 867 finished with value: 0.2555429123908012 and parameters: {'learning_rate': 0.05874491464599395, 'num_leaves': 885, 'max_depth': 56, 'n_estimators': 900, 'min_child_samples': 36, 'subsample': 0.8215345796138928, 'colsample_bytree': 0.5184295924241078, 'reg_alpha': 4.572660434023208, 'reg_lambda': 6.95106276010487, 'min_split_gain': 0.016980699310887182}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:06,486] Trial 764 finished with value: 0.2560459043021123 and parameters: {'learning_rate': 0.05421381442757358, 'num_leaves': 979, 'max_depth': 54, 'n_estimators': 908, 'min_child_samples': 41, 'subsample': 0.7962319509021487, 'colsample_bytree': 0.5571855638783825, 'reg_alpha': 2.9689085267706293e-05, 'reg_lambda': 0.0020960650694075798, 'min_split_gain': 0.0008961711968838768}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:08,853] Trial 889 finished with value: 0.2578676556995825 and parameters: {'learning_rate': 0.05777129855729016, 'num_leaves': 894, 'max_depth': 57, 'n_estimators': 372, 'min_child_samples': 37, 'subsample': 0.5356468171763377, 'colsample_bytree': 0.5493453113092829, 'reg_alpha': 6.212255785520148, 'reg_lambda': 9.435495828619627e-08, 'min_split_gain': 0.05374880977875773}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:09,735] Trial 891 finished with value: 0.25794266191755294 and parameters: {'learning_rate': 0.0576153144736058, 'num_leaves': 904, 'max_depth': 57, 'n_estimators': 852, 'min_child_samples': 32, 'subsample': 0.7943903817051036, 'colsample_bytree': 0.5490711006855911, 'reg_alpha': 6.596625077928442, 'reg_lambda': 2.4391554668950683, 'min_split_gain': 0.05453162795363343}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:11,097] Trial 893 finished with value: 0.26563309878359953 and parameters: {'learning_rate': 0.023410342521861836, 'num_leaves': 904, 'max_depth': 57, 'n_estimators': 865, 'min_child_samples': 32, 'subsample': 0.7947652526076883, 'colsample_bytree': 0.5528610334791484, 'reg_alpha': 6.681674010553248, 'reg_lambda': 8.301893235241552e-08, 'min_split_gain': 0.5745971983576933}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:15,722] Trial 869 finished with value: 0.25410401748294853 and parameters: {'learning_rate': 0.057219364597985975, 'num_leaves': 898, 'max_depth': 55, 'n_estimators': 541, 'min_child_samples': 37, 'subsample': 0.8241964899969245, 'colsample_bytree': 0.5180193853562677, 'reg_alpha': 4.372986075262439, 'reg_lambda': 9.996040654685975, 'min_split_gain': 0.01783154662792371}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:16,636] Trial 788 finished with value: 0.2540906634194348 and parameters: {'learning_rate': 0.04027551916864004, 'num_leaves': 804, 'max_depth': 61, 'n_estimators': 941, 'min_child_samples': 40, 'subsample': 0.6342579248605084, 'colsample_bytree': 0.5506193092549067, 'reg_alpha': 5.622378768482435, 'reg_lambda': 1.6722550720883829, 'min_split_gain': 0.00015632157521794108}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:17,154] Trial 875 finished with value: 0.25653010316481795 and parameters: {'learning_rate': 0.05754251030875734, 'num_leaves': 903, 'max_depth': 57, 'n_estimators': 892, 'min_child_samples': 21, 'subsample': 0.723588645907427, 'colsample_bytree': 0.5156495115204285, 'reg_alpha': 3.528809202385806, 'reg_lambda': 7.989184887879314e-08, 'min_split_gain': 0.014561044924203157}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:20,234] Trial 873 finished with value: 0.2566113203428639 and parameters: {'learning_rate': 0.058715138197174825, 'num_leaves': 909, 'max_depth': 56, 'n_estimators': 537, 'min_child_samples': 34, 'subsample': 0.540153056651122, 'colsample_bytree': 0.5173831216862179, 'reg_alpha': 4.088375532153915, 'reg_lambda': 6.718188503360158, 'min_split_gain': 0.018001403699703614}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:20,528] Trial 871 finished with value: 0.2583079770958621 and parameters: {'learning_rate': 0.07042968386868667, 'num_leaves': 891, 'max_depth': 55, 'n_estimators': 860, 'min_child_samples': 21, 'subsample': 0.8239837445924225, 'colsample_bytree': 0.5174679487066761, 'reg_alpha': 4.254726206757022, 'reg_lambda': 6.691406768459467, 'min_split_gain': 0.00025694207899656127}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:21,387] Trial 892 finished with value: 0.2597139317077853 and parameters: {'learning_rate': 0.05915360474897474, 'num_leaves': 902, 'max_depth': 59, 'n_estimators': 858, 'min_child_samples': 32, 'subsample': 0.7149087587596422, 'colsample_bytree': 0.5443243581765815, 'reg_alpha': 6.363382948476852, 'reg_lambda': 2.320730549077597, 'min_split_gain': 0.05345307816535257}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:23,449] Trial 896 finished with value: 0.26130350785032497 and parameters: {'learning_rate': 0.05737946901584186, 'num_leaves': 902, 'max_depth': 57, 'n_estimators': 857, 'min_child_samples': 37, 'subsample': 0.9435585592383641, 'colsample_bytree': 0.5491330839344124, 'reg_alpha': 5.334978650379543e-05, 'reg_lambda': 7.213102893045728e-08, 'min_split_gain': 0.5718537209941343}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:28,997] Trial 904 finished with value: 0.2613758768096256 and parameters: {'learning_rate': 0.05336495937998777, 'num_leaves': 1005, 'max_depth': 49, 'n_estimators': 864, 'min_child_samples': 41, 'subsample': 0.8399836259580191, 'colsample_bytree': 0.5491750038962728, 'reg_alpha': 0.0002040696469139874, 'reg_lambda': 2.7108301098747956, 'min_split_gain': 0.7318473740029612}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:30,296] Trial 886 finished with value: 0.25496275262525225 and parameters: {'learning_rate': 0.058187451567771664, 'num_leaves': 897, 'max_depth': 57, 'n_estimators': 856, 'min_child_samples': 33, 'subsample': 0.621385730705458, 'colsample_bytree': 0.5523061706639552, 'reg_alpha': 5.975938786249573, 'reg_lambda': 7.360297179509543e-08, 'min_split_gain': 0.00220863024285}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:35,725] Trial 870 finished with value: 0.2577640526280464 and parameters: {'learning_rate': 0.07068882783753194, 'num_leaves': 893, 'max_depth': 55, 'n_estimators': 899, 'min_child_samples': 21, 'subsample': 0.8253377531803111, 'colsample_bytree': 0.516498041078852, 'reg_alpha': 5.134680868333757e-05, 'reg_lambda': 7.610343991593154, 'min_split_gain': 0.01594783952680832}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:36,030] Trial 884 finished with value: 0.2563872383608878 and parameters: {'learning_rate': 0.05754749492965666, 'num_leaves': 887, 'max_depth': 57, 'n_estimators': 534, 'min_child_samples': 20, 'subsample': 0.5411575665423151, 'colsample_bytree': 0.5485058889157287, 'reg_alpha': 5.823502441197103, 'reg_lambda': 1.0112123280001313e-06, 'min_split_gain': 0.001574879931030766}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:37,037] Trial 877 finished with value: 0.2547828201304822 and parameters: {'learning_rate': 0.057023692521262874, 'num_leaves': 891, 'max_depth': 57, 'n_estimators': 858, 'min_child_samples': 21, 'subsample': 0.5361925634329633, 'colsample_bytree': 0.549630263096948, 'reg_alpha': 4.805276373583688, 'reg_lambda': 6.786525386742132, 'min_split_gain': 0.0021283789570379057}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:39,801] Trial 880 finished with value: 0.2512626702944788 and parameters: {'learning_rate': 0.060387136377347855, 'num_leaves': 891, 'max_depth': 57, 'n_estimators': 858, 'min_child_samples': 21, 'subsample': 0.9218406960269193, 'colsample_bytree': 0.5489929212650287, 'reg_alpha': 5.190806910265158, 'reg_lambda': 8.706605074547784e-08, 'min_split_gain': 0.0018065403827701762}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:40,790] Trial 901 finished with value: 0.2574121798682699 and parameters: {'learning_rate': 0.06054319204494808, 'num_leaves': 1006, 'max_depth': 57, 'n_estimators': 844, 'min_child_samples': 20, 'subsample': 0.9453562793826239, 'colsample_bytree': 0.5477870004358053, 'reg_alpha': 2.8292998758062957, 'reg_lambda': 2.9677915706775027, 'min_split_gain': 0.0512685176996437}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:41,539] Trial 903 finished with value: 0.25665552593225593 and parameters: {'learning_rate': 0.10886570751981123, 'num_leaves': 735, 'max_depth': 49, 'n_estimators': 847, 'min_child_samples': 40, 'subsample': 0.9498738759705807, 'colsample_bytree': 0.5455659899144799, 'reg_alpha': 0.01505520940200775, 'reg_lambda': 2.380481762648587, 'min_split_gain': 0.054435753895239865}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:42,148] Trial 872 finished with value: 0.2548480206738576 and parameters: {'learning_rate': 0.05807585531191257, 'num_leaves': 898, 'max_depth': 57, 'n_estimators': 377, 'min_child_samples': 20, 'subsample': 0.8001990603603207, 'colsample_bytree': 0.5178254887999034, 'reg_alpha': 4.113484952510208, 'reg_lambda': 9.90811677751398, 'min_split_gain': 0.0005859903787405023}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:43,792] Trial 911 finished with value: 0.26359870460488477 and parameters: {'learning_rate': 0.10371484890073993, 'num_leaves': 951, 'max_depth': 59, 'n_estimators': 839, 'min_child_samples': 40, 'subsample': 0.808482335070703, 'colsample_bytree': 0.5335028209775954, 'reg_alpha': 2.520986025902814, 'reg_lambda': 9.66398750825595, 'min_split_gain': 0.27152706100402824}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:44,368] Trial 885 finished with value: 0.25476261811815876 and parameters: {'learning_rate': 0.05520375518385723, 'num_leaves': 898, 'max_depth': 57, 'n_estimators': 862, 'min_child_samples': 33, 'subsample': 0.7149846859051744, 'colsample_bytree': 0.5469065991440332, 'reg_alpha': 6.030546715068843, 'reg_lambda': 8.091477281322248e-08, 'min_split_gain': 0.0022137070830636984}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:52,243] Trial 915 finished with value: 0.25724216078096096 and parameters: {'learning_rate': 0.10723610445922083, 'num_leaves': 946, 'max_depth': 59, 'n_estimators': 839, 'min_child_samples': 41, 'subsample': 0.944323179094268, 'colsample_bytree': 0.5373026923761356, 'reg_alpha': 7.322512035827188, 'reg_lambda': 1.6059006490349289e-07, 'min_split_gain': 0.04308261756422083}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:54,762] Trial 898 finished with value: 0.25654042559435447 and parameters: {'learning_rate': 0.05044142520103951, 'num_leaves': 1004, 'max_depth': 59, 'n_estimators': 860, 'min_child_samples': 40, 'subsample': 0.6200157603763163, 'colsample_bytree': 0.5474672384401477, 'reg_alpha': 6.133590391725556, 'reg_lambda': 2.7974121578473343, 'min_split_gain': 0.001054787137097041}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:36:59,101] Trial 912 finished with value: 0.25688327891819646 and parameters: {'learning_rate': 0.10440009731979345, 'num_leaves': 935, 'max_depth': 58, 'n_estimators': 836, 'min_child_samples': 41, 'subsample': 0.9268221803675787, 'colsample_bytree': 0.5649300201356509, 'reg_alpha': 2.7282874410570437, 'reg_lambda': 0.05982705287859078, 'min_split_gain': 0.05343412648426623}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:00,427] Trial 908 finished with value: 0.2582977352207481 and parameters: {'learning_rate': 0.04921506606996779, 'num_leaves': 617, 'max_depth': 59, 'n_estimators': 424, 'min_child_samples': 40, 'subsample': 0.8101111317343299, 'colsample_bytree': 0.5336645523450122, 'reg_alpha': 2.4612000271785823, 'reg_lambda': 0.001034660619725971, 'min_split_gain': 0.04920957789102335}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:03,446] Trial 883 finished with value: 0.25606731530635374 and parameters: {'learning_rate': 0.05531683195236777, 'num_leaves': 899, 'max_depth': 57, 'n_estimators': 857, 'min_child_samples': 20, 'subsample': 0.920391617920558, 'colsample_bytree': 0.5512532459649085, 'reg_alpha': 4.964357759933413, 'reg_lambda': 8.618543332638285e-08, 'min_split_gain': 0.0009603908722586671}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:04,132] Trial 899 finished with value: 0.2575236981492454 and parameters: {'learning_rate': 0.10809011893090191, 'num_leaves': 1002, 'max_depth': 59, 'n_estimators': 848, 'min_child_samples': 51, 'subsample': 0.5451799244501007, 'colsample_bytree': 0.5492498805515433, 'reg_alpha': 2.880033459296321, 'reg_lambda': 2.664535455737384, 'min_split_gain': 6.508897086898811e-06}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:04,839] Trial 909 finished with value: 0.2566845456907923 and parameters: {'learning_rate': 0.051405027827386554, 'num_leaves': 754, 'max_depth': 59, 'n_estimators': 840, 'min_child_samples': 51, 'subsample': 0.8090738107507696, 'colsample_bytree': 0.5336097914047752, 'reg_alpha': 2.7831982982571266, 'reg_lambda': 0.9532599067266964, 'min_split_gain': 0.047531714545651804}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:06,570] Trial 916 finished with value: 0.255918055012482 and parameters: {'learning_rate': 0.04976650320067947, 'num_leaves': 944, 'max_depth': 59, 'n_estimators': 826, 'min_child_samples': 50, 'subsample': 0.9237053025083237, 'colsample_bytree': 0.5084923568317093, 'reg_alpha': 7.450606904404235, 'reg_lambda': 1.0940301747022629e-07, 'min_split_gain': 0.040990106507584056}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:07,161] Trial 919 finished with value: 0.26348344318457606 and parameters: {'learning_rate': 0.049134250848398446, 'num_leaves': 925, 'max_depth': 59, 'n_estimators': 822, 'min_child_samples': 51, 'subsample': 0.9143651066515287, 'colsample_bytree': 0.5091591765472895, 'reg_alpha': 0.09867217005129134, 'reg_lambda': 1.1944641775185418e-07, 'min_split_gain': 0.7727208186850384}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:07,579] Trial 878 finished with value: 0.25518456992614025 and parameters: {'learning_rate': 0.05824869193314474, 'num_leaves': 901, 'max_depth': 57, 'n_estimators': 321, 'min_child_samples': 21, 'subsample': 0.7206491785155404, 'colsample_bytree': 0.5472174626278385, 'reg_alpha': 3.5220543638793695, 'reg_lambda': 6.7550676553280695, 'min_split_gain': 0.0021629245974962765}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:10,383] Trial 897 finished with value: 0.25252090057180143 and parameters: {'learning_rate': 0.05877468512012674, 'num_leaves': 906, 'max_depth': 49, 'n_estimators': 862, 'min_child_samples': 40, 'subsample': 0.9467004073212454, 'colsample_bytree': 0.5492143469279874, 'reg_alpha': 6.706276773634164, 'reg_lambda': 9.82056663137562, 'min_split_gain': 0.0008726232155524152}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:11,287] Trial 905 finished with value: 0.25694242006091983 and parameters: {'learning_rate': 0.05163132033513919, 'num_leaves': 1003, 'max_depth': 59, 'n_estimators': 836, 'min_child_samples': 40, 'subsample': 0.6201184507701232, 'colsample_bytree': 0.5481050312647499, 'reg_alpha': 0.00021502605952943552, 'reg_lambda': 3.0720598850160683, 'min_split_gain': 0.04944804367514594}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:13,595] Trial 906 finished with value: 0.26313327854553425 and parameters: {'learning_rate': 0.051603859936417563, 'num_leaves': 1003, 'max_depth': 59, 'n_estimators': 841, 'min_child_samples': 41, 'subsample': 0.5974196919818803, 'colsample_bytree': 0.5349732384533294, 'reg_alpha': 0.0001661029048375681, 'reg_lambda': 3.270911370323372, 'min_split_gain': 0.04648153474274218}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:15,421] Trial 920 finished with value: 0.2568427248336392 and parameters: {'learning_rate': 0.06703535105582023, 'num_leaves': 923, 'max_depth': 59, 'n_estimators': 417, 'min_child_samples': 30, 'subsample': 0.910302496103852, 'colsample_bytree': 0.5351680734133947, 'reg_alpha': 7.62523012812023, 'reg_lambda': 1.1084694483421287e-07, 'min_split_gain': 0.041814339533166836}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:15,654] Trial 902 finished with value: 0.2557764052133977 and parameters: {'learning_rate': 0.051916739571038685, 'num_leaves': 748, 'max_depth': 59, 'n_estimators': 844, 'min_child_samples': 41, 'subsample': 0.5437696014250445, 'colsample_bytree': 0.5491832822243843, 'reg_alpha': 2.843975740102909, 'reg_lambda': 2.840279860234101, 'min_split_gain': 0.0001855283950336806}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:18,547] Trial 910 finished with value: 0.2590386548844674 and parameters: {'learning_rate': 0.05016388972948714, 'num_leaves': 746, 'max_depth': 58, 'n_estimators': 998, 'min_child_samples': 40, 'subsample': 0.8388048999427188, 'colsample_bytree': 0.5345870055501787, 'reg_alpha': 2.3137938160885625, 'reg_lambda': 0.0002368095881719137, 'min_split_gain': 0.05159085901312683}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:19,505] Trial 923 finished with value: 0.2571507698951354 and parameters: {'learning_rate': 0.0650411002244976, 'num_leaves': 927, 'max_depth': 59, 'n_estimators': 884, 'min_child_samples': 36, 'subsample': 0.9235538736034274, 'colsample_bytree': 0.5074103365201633, 'reg_alpha': 9.866648985438017, 'reg_lambda': 1.2185668594410242e-07, 'min_split_gain': 0.04155625613833035}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:20,282] Trial 922 finished with value: 0.2582549277269363 and parameters: {'learning_rate': 0.06666173019151801, 'num_leaves': 923, 'max_depth': 58, 'n_estimators': 881, 'min_child_samples': 30, 'subsample': 0.9212216169116605, 'colsample_bytree': 0.5076330886772026, 'reg_alpha': 7.915988272979271, 'reg_lambda': 6.497407815012853e-08, 'min_split_gain': 0.037862612368961525}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:20,761] Trial 882 finished with value: 0.2539388449441813 and parameters: {'learning_rate': 0.05701528235255051, 'num_leaves': 892, 'max_depth': 57, 'n_estimators': 857, 'min_child_samples': 20, 'subsample': 0.5398749963440556, 'colsample_bytree': 0.5521960794726188, 'reg_alpha': 4.229734481815683, 'reg_lambda': 1.0207795283377962e-06, 'min_split_gain': 0.00032976368649513933}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:22,562] Trial 874 finished with value: 0.2527441652096169 and parameters: {'learning_rate': 0.05837599869899827, 'num_leaves': 895, 'max_depth': 56, 'n_estimators': 858, 'min_child_samples': 21, 'subsample': 0.7162870798656494, 'colsample_bytree': 0.5170644217231086, 'reg_alpha': 4.203142313799506, 'reg_lambda': 8.115241792573292e-08, 'min_split_gain': 0.000385152661648248}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:23,733] Trial 930 finished with value: 0.2573421803121783 and parameters: {'learning_rate': 0.06710070175282497, 'num_leaves': 926, 'max_depth': 58, 'n_estimators': 874, 'min_child_samples': 29, 'subsample': 0.9487973764172285, 'colsample_bytree': 0.5667149500350634, 'reg_alpha': 7.705088808217737, 'reg_lambda': 0.00027752463703547544, 'min_split_gain': 0.14319882573845788}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:24,113] Trial 927 finished with value: 0.2562113567906778 and parameters: {'learning_rate': 0.06794551375996008, 'num_leaves': 967, 'max_depth': 58, 'n_estimators': 881, 'min_child_samples': 37, 'subsample': 0.909006539642868, 'colsample_bytree': 0.5644742811216715, 'reg_alpha': 7.062401589131185, 'reg_lambda': 5.588250806630866e-08, 'min_split_gain': 0.037354571105421376}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:24,953] Trial 926 finished with value: 0.2580360234588861 and parameters: {'learning_rate': 0.06472197072418813, 'num_leaves': 966, 'max_depth': 56, 'n_estimators': 999, 'min_child_samples': 37, 'subsample': 0.9256905505955685, 'colsample_bytree': 0.5328054356673053, 'reg_alpha': 6.7712675823441435, 'reg_lambda': 3.6454835284579485, 'min_split_gain': 0.035615261651015064}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:26,521] Trial 928 finished with value: 0.2579079417614009 and parameters: {'learning_rate': 0.06723839330689897, 'num_leaves': 1024, 'max_depth': 47, 'n_estimators': 883, 'min_child_samples': 36, 'subsample': 0.9317540701609444, 'colsample_bytree': 0.5719915192867895, 'reg_alpha': 9.993673710190663, 'reg_lambda': 2.4859153504327942e-06, 'min_split_gain': 0.03445796838159716}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:28,181] Trial 929 finished with value: 0.2588040541331583 and parameters: {'learning_rate': 0.06590307866281568, 'num_leaves': 928, 'max_depth': 58, 'n_estimators': 818, 'min_child_samples': 38, 'subsample': 0.9310910546426636, 'colsample_bytree': 0.5710487629586405, 'reg_alpha': 9.368086639340264, 'reg_lambda': 5.601059168120643, 'min_split_gain': 0.03419128934861603}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:32,704] Trial 931 finished with value: 0.25840231718515794 and parameters: {'learning_rate': 0.06640498209466451, 'num_leaves': 960, 'max_depth': 58, 'n_estimators': 879, 'min_child_samples': 29, 'subsample': 0.9310210557464133, 'colsample_bytree': 0.5619144696022137, 'reg_alpha': 9.991296374043898, 'reg_lambda': 0.00033543031390965636, 'min_split_gain': 0.03467412006578993}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:34,070] Trial 913 finished with value: 0.2547056499998255 and parameters: {'learning_rate': 0.11294269913540099, 'num_leaves': 951, 'max_depth': 59, 'n_estimators': 831, 'min_child_samples': 40, 'subsample': 0.9393701400359868, 'colsample_bytree': 0.5345476395767184, 'reg_alpha': 2.429698039535938, 'reg_lambda': 9.439540022571266e-08, 'min_split_gain': 0.00028297566276588165}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:34,384] Trial 932 finished with value: 0.25788755279405645 and parameters: {'learning_rate': 0.06406058384110297, 'num_leaves': 1024, 'max_depth': 48, 'n_estimators': 881, 'min_child_samples': 37, 'subsample': 0.9408562744518385, 'colsample_bytree': 0.5694831910465973, 'reg_alpha': 7.969611056420069, 'reg_lambda': 4.719658000135353, 'min_split_gain': 0.032935814217995865}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:36,105] Trial 895 finished with value: 0.2520308968034978 and parameters: {'learning_rate': 0.05612066644203194, 'num_leaves': 914, 'max_depth': 57, 'n_estimators': 863, 'min_child_samples': 40, 'subsample': 0.5453661427123014, 'colsample_bytree': 0.5429544964217402, 'reg_alpha': 6.514498978027664, 'reg_lambda': 0.0002658020249257671, 'min_split_gain': 8.348331360124054e-06}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:37,403] Trial 933 finished with value: 0.25772027897624517 and parameters: {'learning_rate': 0.06547811765293188, 'num_leaves': 1024, 'max_depth': 48, 'n_estimators': 878, 'min_child_samples': 37, 'subsample': 0.9369184554615709, 'colsample_bytree': 0.5741609300078004, 'reg_alpha': 7.904862673129673, 'reg_lambda': 5.121276518826962, 'min_split_gain': 0.03450551907710811}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:38,901] Trial 934 finished with value: 0.2599431129556851 and parameters: {'learning_rate': 0.06681428958390015, 'num_leaves': 961, 'max_depth': 46, 'n_estimators': 874, 'min_child_samples': 29, 'subsample': 0.9508720967065813, 'colsample_bytree': 0.5729567672775037, 'reg_alpha': 9.610286665388399, 'reg_lambda': 5.851329816733171, 'min_split_gain': 0.030002915602020768}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:39,701] Trial 935 finished with value: 0.261400048050673 and parameters: {'learning_rate': 0.06727090993657935, 'num_leaves': 963, 'max_depth': 47, 'n_estimators': 557, 'min_child_samples': 38, 'subsample': 0.9349227445748043, 'colsample_bytree': 0.5640280589799315, 'reg_alpha': 9.368774613549542, 'reg_lambda': 1.1909070785960405, 'min_split_gain': 0.029294345630121506}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:42,514] Trial 938 finished with value: 0.25890375525225445 and parameters: {'learning_rate': 0.06985470236809796, 'num_leaves': 918, 'max_depth': 48, 'n_estimators': 817, 'min_child_samples': 27, 'subsample': 0.9315321446734828, 'colsample_bytree': 0.527752849774907, 'reg_alpha': 9.68429861310442, 'reg_lambda': 2.43050853968159e-07, 'min_split_gain': 0.03300107426498023}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:42,806] Trial 936 finished with value: 0.2618673080448385 and parameters: {'learning_rate': 0.06328033094033647, 'num_leaves': 962, 'max_depth': 47, 'n_estimators': 512, 'min_child_samples': 37, 'subsample': 0.9449506076264991, 'colsample_bytree': 0.5626068389519658, 'reg_alpha': 7.114972814702106, 'reg_lambda': 9.858447711134257, 'min_split_gain': 0.03058658965216663}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:43,781] Trial 939 finished with value: 0.2587195395882744 and parameters: {'learning_rate': 0.06661637075891538, 'num_leaves': 935, 'max_depth': 47, 'n_estimators': 817, 'min_child_samples': 26, 'subsample': 0.9411698728895707, 'colsample_bytree': 0.5000315906449715, 'reg_alpha': 9.62350583568547, 'reg_lambda': 5.946357404394182, 'min_split_gain': 0.0342612627795483}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:44,183] Trial 937 finished with value: 0.25772126899475645 and parameters: {'learning_rate': 0.06661676430021252, 'num_leaves': 917, 'max_depth': 56, 'n_estimators': 876, 'min_child_samples': 27, 'subsample': 0.9369240350603976, 'colsample_bytree': 0.566785805509882, 'reg_alpha': 7.73700468216382, 'reg_lambda': 3.1419326372154085e-07, 'min_split_gain': 0.032443355763935366}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:45,274] Trial 940 finished with value: 0.25859414304750866 and parameters: {'learning_rate': 0.06682131670096696, 'num_leaves': 914, 'max_depth': 47, 'n_estimators': 819, 'min_child_samples': 19, 'subsample': 0.9527242956772772, 'colsample_bytree': 0.5297885861684819, 'reg_alpha': 9.860770025538061, 'reg_lambda': 2.215405187489674e-07, 'min_split_gain': 0.031468921966544154}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:47,902] Trial 914 finished with value: 0.2552636420048918 and parameters: {'learning_rate': 0.04917159664947664, 'num_leaves': 940, 'max_depth': 59, 'n_estimators': 830, 'min_child_samples': 40, 'subsample': 0.9232497625137527, 'colsample_bytree': 0.5646513665852775, 'reg_alpha': 7.294618941160464, 'reg_lambda': 6.088915884508447e-08, 'min_split_gain': 0.0006479730605609287}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:48,567] Trial 941 finished with value: 0.25622644747434076 and parameters: {'learning_rate': 0.06435163650740566, 'num_leaves': 915, 'max_depth': 56, 'n_estimators': 825, 'min_child_samples': 19, 'subsample': 0.9620090655841091, 'colsample_bytree': 0.5289468011302659, 'reg_alpha': 9.805529803798745, 'reg_lambda': 1.6156193646302445e-07, 'min_split_gain': 0.03057964414876553}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:49,046] Trial 949 finished with value: 0.26320398537595735 and parameters: {'learning_rate': 0.05138464804884926, 'num_leaves': 914, 'max_depth': 56, 'n_estimators': 815, 'min_child_samples': 19, 'subsample': 0.7430478845372258, 'colsample_bytree': 0.733887443767333, 'reg_alpha': 3.9626143757450527, 'reg_lambda': 4.880677865904767e-05, 'min_split_gain': 0.9009482201679191}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:51,375] Trial 890 finished with value: 0.2555174383566804 and parameters: {'learning_rate': 0.05714086592831461, 'num_leaves': 908, 'max_depth': 57, 'n_estimators': 409, 'min_child_samples': 36, 'subsample': 0.9195314614234038, 'colsample_bytree': 0.5473198629553541, 'reg_alpha': 6.80829356769042, 'reg_lambda': 8.141707391332395e-08, 'min_split_gain': 4.915811013297248e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:53,470] Trial 924 finished with value: 0.2582139938446446 and parameters: {'learning_rate': 0.06358606092483836, 'num_leaves': 938, 'max_depth': 59, 'n_estimators': 881, 'min_child_samples': 36, 'subsample': 0.9351760856916496, 'colsample_bytree': 0.5059205023555128, 'reg_alpha': 0.10551981384402609, 'reg_lambda': 5.5368894317188584e-08, 'min_split_gain': 0.03558843193302317}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:53,926] Trial 921 finished with value: 0.25847725661687254 and parameters: {'learning_rate': 0.0665175568909047, 'num_leaves': 1024, 'max_depth': 59, 'n_estimators': 818, 'min_child_samples': 29, 'subsample': 0.9198704976796493, 'colsample_bytree': 0.5050877438352331, 'reg_alpha': 1.3839399856277672e-05, 'reg_lambda': 6.222362305395925e-08, 'min_split_gain': 0.040059540406529134}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:57,595] Trial 918 finished with value: 0.2575089385083468 and parameters: {'learning_rate': 0.04861287539146603, 'num_leaves': 936, 'max_depth': 60, 'n_estimators': 828, 'min_child_samples': 37, 'subsample': 0.9155299811757985, 'colsample_bytree': 0.5080427524310384, 'reg_alpha': 4.216344410000105e-06, 'reg_lambda': 1.0982091154223154e-07, 'min_split_gain': 0.04182236480778953}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:37:58,320] Trial 944 finished with value: 0.2584446563060575 and parameters: {'learning_rate': 0.06659605386871577, 'num_leaves': 918, 'max_depth': 56, 'n_estimators': 872, 'min_child_samples': 19, 'subsample': 0.9327913380335268, 'colsample_bytree': 0.5284815367730924, 'reg_alpha': 5.537865447847277, 'reg_lambda': 2.3083140514488673e-07, 'min_split_gain': 0.032775549103258514}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:00,427] Trial 943 finished with value: 0.2556133296210777 and parameters: {'learning_rate': 0.06422883679142181, 'num_leaves': 914, 'max_depth': 56, 'n_estimators': 817, 'min_child_samples': 19, 'subsample': 0.7378549359398181, 'colsample_bytree': 0.500064339245486, 'reg_alpha': 5.202686650661048, 'reg_lambda': 4.099394877313703e-07, 'min_split_gain': 0.029917853616691248}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:02,712] Trial 957 finished with value: 0.2589764545582458 and parameters: {'learning_rate': 0.047808651235394566, 'num_leaves': 345, 'max_depth': 55, 'n_estimators': 848, 'min_child_samples': 15, 'subsample': 0.9717721069426049, 'colsample_bytree': 0.5383936809382224, 'reg_alpha': 3.6244040825107966, 'reg_lambda': 1.4163895725790918, 'min_split_gain': 0.35127272093929374}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:11,970] Trial 917 finished with value: 0.25466592236161556 and parameters: {'learning_rate': 0.04854953637415722, 'num_leaves': 943, 'max_depth': 59, 'n_estimators': 822, 'min_child_samples': 29, 'subsample': 0.9539233081304362, 'colsample_bytree': 0.5071059018298602, 'reg_alpha': 7.68415094016751, 'reg_lambda': 1.0376414273053092e-07, 'min_split_gain': 9.667275370642953e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:13,902] Trial 963 finished with value: 0.25909003473832637 and parameters: {'learning_rate': 0.04687023308513231, 'num_leaves': 933, 'max_depth': 54, 'n_estimators': 843, 'min_child_samples': 25, 'subsample': 0.9737385322207665, 'colsample_bytree': 0.5399643150159233, 'reg_alpha': 4.157690241841473, 'reg_lambda': 1.4860687037891858, 'min_split_gain': 0.3151736428130748}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:18,269] Trial 964 finished with value: 0.2571523680969077 and parameters: {'learning_rate': 0.08476220629158793, 'num_leaves': 879, 'max_depth': 54, 'n_estimators': 446, 'min_child_samples': 25, 'subsample': 0.5185037605577812, 'colsample_bytree': 0.5422732499673786, 'reg_alpha': 3.3541406442950987, 'reg_lambda': 2.0588638611264316, 'min_split_gain': 0.06753245597834252}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:19,530] Trial 900 finished with value: 0.2593343429925243 and parameters: {'learning_rate': 0.05098436485857124, 'num_leaves': 1002, 'max_depth': 59, 'n_estimators': 846, 'min_child_samples': 40, 'subsample': 0.8392614672243858, 'colsample_bytree': 0.5474446107024583, 'reg_alpha': 0.016986365116855546, 'reg_lambda': 9.771903521901898, 'min_split_gain': 0.0020196310201549775}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:20,035] Trial 946 finished with value: 0.2567348129237501 and parameters: {'learning_rate': 0.04761681795262453, 'num_leaves': 913, 'max_depth': 56, 'n_estimators': 800, 'min_child_samples': 18, 'subsample': 0.9367298731863546, 'colsample_bytree': 0.5391539133107504, 'reg_alpha': 3.539663963793206, 'reg_lambda': 0.000765729479295205, 'min_split_gain': 0.02831184425394102}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:23,733] Trial 959 finished with value: 0.2560040188621255 and parameters: {'learning_rate': 0.04681074418670915, 'num_leaves': 881, 'max_depth': 54, 'n_estimators': 846, 'min_child_samples': 18, 'subsample': 0.9614205951127603, 'colsample_bytree': 0.537724001231936, 'reg_alpha': 3.569356963772133, 'reg_lambda': 1.5491920287152366e-07, 'min_split_gain': 0.06199934263366953}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:24,748] Trial 942 finished with value: 0.25397520401557455 and parameters: {'learning_rate': 0.06611216120681153, 'num_leaves': 921, 'max_depth': 56, 'n_estimators': 819, 'min_child_samples': 20, 'subsample': 0.9597691035222629, 'colsample_bytree': 0.5283632179191488, 'reg_alpha': 9.953405717321003, 'reg_lambda': 1.3066625412525164, 'min_split_gain': 0.00010883461277013018}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:25,942] Trial 965 finished with value: 0.2601914744236218 and parameters: {'learning_rate': 0.04705871766151958, 'num_leaves': 870, 'max_depth': 54, 'n_estimators': 447, 'min_child_samples': 24, 'subsample': 0.5143065488945013, 'colsample_bytree': 0.5394923481117146, 'reg_alpha': 3.5744989625979344, 'reg_lambda': 3.4232183973485633, 'min_split_gain': 0.31742255735255326}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:26,209] Trial 907 finished with value: 0.25318015509237324 and parameters: {'learning_rate': 0.04998702617298246, 'num_leaves': 945, 'max_depth': 59, 'n_estimators': 842, 'min_child_samples': 41, 'subsample': 0.6168831209233134, 'colsample_bytree': 0.5332055294572328, 'reg_alpha': 2.534371480398288, 'reg_lambda': 3.0434747285827397, 'min_split_gain': 0.0015449231427669208}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:29,002] Trial 962 finished with value: 0.2559508031746218 and parameters: {'learning_rate': 0.04666858848616217, 'num_leaves': 874, 'max_depth': 54, 'n_estimators': 796, 'min_child_samples': 24, 'subsample': 0.9770482499774171, 'colsample_bytree': 0.5422815046216529, 'reg_alpha': 3.4388312590399566, 'reg_lambda': 0.8948634937067509, 'min_split_gain': 0.05847606238803367}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:30,469] Trial 954 finished with value: 0.255648856394604 and parameters: {'learning_rate': 0.05109255893548818, 'num_leaves': 932, 'max_depth': 56, 'n_estimators': 840, 'min_child_samples': 19, 'subsample': 0.944246120022329, 'colsample_bytree': 0.5418315427163289, 'reg_alpha': 3.880986962974663, 'reg_lambda': 0.007829991610188792, 'min_split_gain': 0.02088757348975477}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:41,981] Trial 888 finished with value: 0.2532749569651878 and parameters: {'learning_rate': 0.02248365190858203, 'num_leaves': 903, 'max_depth': 57, 'n_estimators': 856, 'min_child_samples': 36, 'subsample': 0.7989768030649147, 'colsample_bytree': 0.5461182396147376, 'reg_alpha': 6.418059126802901, 'reg_lambda': 9.242602663253951e-08, 'min_split_gain': 2.6061834577174913e-06}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:45,149] Trial 945 finished with value: 0.2564004894635223 and parameters: {'learning_rate': 0.08467052511421366, 'num_leaves': 913, 'max_depth': 55, 'n_estimators': 804, 'min_child_samples': 18, 'subsample': 0.9735344722797087, 'colsample_bytree': 0.5274585021100152, 'reg_alpha': 5.016968242219615, 'reg_lambda': 4.2431645708143665e-07, 'min_split_gain': 0.0007680412029506907}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:45,826] Trial 951 finished with value: 0.25796491987395115 and parameters: {'learning_rate': 0.048554309740454764, 'num_leaves': 916, 'max_depth': 56, 'n_estimators': 826, 'min_child_samples': 25, 'subsample': 0.9638792039665751, 'colsample_bytree': 0.5372055120912816, 'reg_alpha': 3.817749818356731, 'reg_lambda': 0.010242929157776982, 'min_split_gain': 3.628573405964973e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:46,620] Trial 966 finished with value: 0.25503520802252777 and parameters: {'learning_rate': 0.07797259141327917, 'num_leaves': 881, 'max_depth': 50, 'n_estimators': 852, 'min_child_samples': 56, 'subsample': 0.5334118452628746, 'colsample_bytree': 0.5402431781631881, 'reg_alpha': 2.9377507201984225, 'reg_lambda': 9.955889273671351, 'min_split_gain': 0.01613759942052361}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:46,801] Trial 968 finished with value: 0.25828472977729616 and parameters: {'learning_rate': 0.08084909820052592, 'num_leaves': 886, 'max_depth': 50, 'n_estimators': 800, 'min_child_samples': 45, 'subsample': 0.9681044469386814, 'colsample_bytree': 0.5374857275740408, 'reg_alpha': 3.264678654029143, 'reg_lambda': 3.560214486668991, 'min_split_gain': 0.018013591648726658}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:49,430] Trial 973 finished with value: 0.25669173654593047 and parameters: {'learning_rate': 0.082234164868264, 'num_leaves': 884, 'max_depth': 50, 'n_estimators': 864, 'min_child_samples': 22, 'subsample': 0.7668715367079475, 'colsample_bytree': 0.5260048828673685, 'reg_alpha': 5.154216618235018, 'reg_lambda': 0.018747311458776048, 'min_split_gain': 0.019908041142692297}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:50,068] Trial 970 finished with value: 0.2549206191617236 and parameters: {'learning_rate': 0.29638687946615144, 'num_leaves': 880, 'max_depth': 57, 'n_estimators': 864, 'min_child_samples': 45, 'subsample': 0.6978551327694847, 'colsample_bytree': 0.5277075258195122, 'reg_alpha': 2.443315116860427, 'reg_lambda': 8.0814758587808e-05, 'min_split_gain': 0.00017721505877550182}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:51,862] Trial 976 finished with value: 0.260681817180713 and parameters: {'learning_rate': 0.08211347562716732, 'num_leaves': 872, 'max_depth': 57, 'n_estimators': 866, 'min_child_samples': 45, 'subsample': 0.6870260619863088, 'colsample_bytree': 0.9849454371238913, 'reg_alpha': 5.137617184365924, 'reg_lambda': 0.00284768756446977, 'min_split_gain': 0.20167163841957972}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:55,295] Trial 894 finished with value: 0.2621322743380372 and parameters: {'learning_rate': 0.005381463042741133, 'num_leaves': 913, 'max_depth': 49, 'n_estimators': 852, 'min_child_samples': 40, 'subsample': 0.6217770483104739, 'colsample_bytree': 0.5397643904065923, 'reg_alpha': 6.575137327588559, 'reg_lambda': 9.018348936309667e-08, 'min_split_gain': 0.0007014030237589404}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:38:57,275] Trial 953 finished with value: 0.2551538497187669 and parameters: {'learning_rate': 0.04987345960744505, 'num_leaves': 917, 'max_depth': 54, 'n_estimators': 804, 'min_child_samples': 18, 'subsample': 0.6988010122184664, 'colsample_bytree': 0.5363758755109841, 'reg_alpha': 3.680766259747916, 'reg_lambda': 6.079294287251225e-08, 'min_split_gain': 0.001670067494818034}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:02,267] Trial 974 finished with value: 0.25394137111485854 and parameters: {'learning_rate': 0.07864737463064017, 'num_leaves': 913, 'max_depth': 57, 'n_estimators': 866, 'min_child_samples': 57, 'subsample': 0.756037468687399, 'colsample_bytree': 0.5270570946942642, 'reg_alpha': 1.9975106177137103, 'reg_lambda': 5.0276188824399775, 'min_split_gain': 0.01930906894886192}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:02,559] Trial 948 finished with value: 0.257457603540084 and parameters: {'learning_rate': 0.048161515364619226, 'num_leaves': 916, 'max_depth': 56, 'n_estimators': 798, 'min_child_samples': 19, 'subsample': 0.9699124123829979, 'colsample_bytree': 0.5413276702782905, 'reg_alpha': 4.975262639066119, 'reg_lambda': 0.007337098152114662, 'min_split_gain': 0.0010635541327316009}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:06,279] Trial 955 finished with value: 0.2552373286953329 and parameters: {'learning_rate': 0.0480923746239988, 'num_leaves': 913, 'max_depth': 56, 'n_estimators': 792, 'min_child_samples': 18, 'subsample': 0.9667986670930808, 'colsample_bytree': 0.5404293224996812, 'reg_alpha': 4.091436806047935, 'reg_lambda': 3.8562223059807575e-07, 'min_split_gain': 4.2069001757336945e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:12,898] Trial 981 finished with value: 0.25921494450134636 and parameters: {'learning_rate': 0.05726133783138574, 'num_leaves': 949, 'max_depth': 61, 'n_estimators': 863, 'min_child_samples': 62, 'subsample': 0.7266152042477906, 'colsample_bytree': 0.5266163569901614, 'reg_alpha': 2.1024740155242543, 'reg_lambda': 5.830459180316411, 'min_split_gain': 0.06192608655788358}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:13,585] Trial 969 finished with value: 0.2547879170092395 and parameters: {'learning_rate': 0.057122553549482785, 'num_leaves': 913, 'max_depth': 57, 'n_estimators': 863, 'min_child_samples': 23, 'subsample': 0.7019422452915475, 'colsample_bytree': 0.5278450683660371, 'reg_alpha': 2.1674774272547723, 'reg_lambda': 4.281089937333371, 'min_split_gain': 0.017190517201225306}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:14,770] Trial 972 finished with value: 0.25713245079290625 and parameters: {'learning_rate': 0.05855530786429954, 'num_leaves': 937, 'max_depth': 57, 'n_estimators': 864, 'min_child_samples': 22, 'subsample': 0.6942353684557551, 'colsample_bytree': 0.5274663759981019, 'reg_alpha': 2.235007778434077, 'reg_lambda': 0.7730735054140627, 'min_split_gain': 0.020769869215269632}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:16,600] Trial 984 finished with value: 0.25896037952678413 and parameters: {'learning_rate': 0.05865810422895887, 'num_leaves': 937, 'max_depth': 61, 'n_estimators': 886, 'min_child_samples': 22, 'subsample': 0.9015088622533945, 'colsample_bytree': 0.5129395892443233, 'reg_alpha': 4.985048429527738, 'reg_lambda': 1.8788981442792535e-05, 'min_split_gain': 0.05731825343332601}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:18,570] Trial 978 finished with value: 0.25590768633540717 and parameters: {'learning_rate': 0.07750239262065492, 'num_leaves': 879, 'max_depth': 50, 'n_estimators': 865, 'min_child_samples': 45, 'subsample': 0.956038077045365, 'colsample_bytree': 0.5264789346764408, 'reg_alpha': 1.8234716391485286, 'reg_lambda': 0.019889210142474786, 'min_split_gain': 0.022238709057194817}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:21,169] Trial 985 finished with value: 0.2576974377447089 and parameters: {'learning_rate': 0.058026612517227134, 'num_leaves': 731, 'max_depth': 62, 'n_estimators': 890, 'min_child_samples': 57, 'subsample': 0.7554588759186596, 'colsample_bytree': 0.5582187471543985, 'reg_alpha': 2.2781203298644437, 'reg_lambda': 6.47584538899197, 'min_split_gain': 0.05995639517165705}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:21,724] Trial 990 finished with value: 0.26288296790119253 and parameters: {'learning_rate': 0.05930344515159125, 'num_leaves': 979, 'max_depth': 58, 'n_estimators': 887, 'min_child_samples': 42, 'subsample': 0.6457325005413561, 'colsample_bytree': 0.6756390478240456, 'reg_alpha': 5.432510952264574, 'reg_lambda': 6.726185054307323, 'min_split_gain': 0.683564937262675}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:22,216] Trial 977 finished with value: 0.256401251083945 and parameters: {'learning_rate': 0.057722646254834635, 'num_leaves': 884, 'max_depth': 25, 'n_estimators': 864, 'min_child_samples': 56, 'subsample': 0.5302037932684567, 'colsample_bytree': 0.5261505355116803, 'reg_alpha': 2.3206893939693223, 'reg_lambda': 0.0023034752576442043, 'min_split_gain': 0.022819132490093152}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:22,519] Trial 983 finished with value: 0.25759944060827616 and parameters: {'learning_rate': 0.05809456911215214, 'num_leaves': 593, 'max_depth': 61, 'n_estimators': 891, 'min_child_samples': 22, 'subsample': 0.7762356556403472, 'colsample_bytree': 0.8798018059009833, 'reg_alpha': 1.9194512132542123, 'reg_lambda': 5.31348190935211, 'min_split_gain': 0.06081556377022911}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:27,016] Trial 987 finished with value: 0.25614162323723244 and parameters: {'learning_rate': 0.058528676038221004, 'num_leaves': 956, 'max_depth': 60, 'n_estimators': 892, 'min_child_samples': 62, 'subsample': 0.7805597227616812, 'colsample_bytree': 0.5562555962266086, 'reg_alpha': 2.362917484310733, 'reg_lambda': 5.700071335090002, 'min_split_gain': 0.05643181245954464}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:30,049] Trial 881 finished with value: 0.25676477465332603 and parameters: {'learning_rate': 0.057028032266739456, 'num_leaves': 890, 'max_depth': 57, 'n_estimators': 860, 'min_child_samples': 21, 'subsample': 0.6184150700428657, 'colsample_bytree': 0.5119616349964468, 'reg_alpha': 4.245342582351047e-05, 'reg_lambda': 6.686520623329406, 'min_split_gain': 0.0012423785455035364}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:34,626] Trial 986 finished with value: 0.25875427177740873 and parameters: {'learning_rate': 0.06036236069238301, 'num_leaves': 951, 'max_depth': 62, 'n_estimators': 865, 'min_child_samples': 63, 'subsample': 0.8503664519052966, 'colsample_bytree': 0.5096542260562978, 'reg_alpha': 1.9845176775141653, 'reg_lambda': 6.50553925777948, 'min_split_gain': 0.021156212599626197}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:35,390] Trial 988 finished with value: 0.25614476019885124 and parameters: {'learning_rate': 0.05851370787915055, 'num_leaves': 950, 'max_depth': 61, 'n_estimators': 876, 'min_child_samples': 22, 'subsample': 0.5283558052202856, 'colsample_bytree': 0.5123756668650814, 'reg_alpha': 1.952339229421474, 'reg_lambda': 5.977062653152916, 'min_split_gain': 0.059026508665714635}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:37,206] Trial 991 finished with value: 0.2563310495363724 and parameters: {'learning_rate': 0.06056038442542831, 'num_leaves': 595, 'max_depth': 61, 'n_estimators': 890, 'min_child_samples': 14, 'subsample': 0.9022212599629565, 'colsample_bytree': 0.5585168598179536, 'reg_alpha': 4.9656569206697, 'reg_lambda': 6.117328870500217, 'min_split_gain': 0.05938146072564899}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:43,480] Trial 995 finished with value: 0.2589355564756913 and parameters: {'learning_rate': 0.07230880806556274, 'num_leaves': 591, 'max_depth': 60, 'n_estimators': 838, 'min_child_samples': 60, 'subsample': 0.8516806400320046, 'colsample_bytree': 0.5571009889280972, 'reg_alpha': 5.459101076747811, 'reg_lambda': 5.623095336974725e-06, 'min_split_gain': 0.01926057244495079}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:44,033] Trial 993 finished with value: 0.25697966300362796 and parameters: {'learning_rate': 0.05660277659815474, 'num_leaves': 980, 'max_depth': 62, 'n_estimators': 887, 'min_child_samples': 61, 'subsample': 0.6424539685941126, 'colsample_bytree': 0.5568997447179916, 'reg_alpha': 5.9400802201953935, 'reg_lambda': 8.49289703615717e-06, 'min_split_gain': 0.021966012996947153}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:44,361] Trial 967 finished with value: 0.2499537940389705 and parameters: {'learning_rate': 0.0778624078311394, 'num_leaves': 878, 'max_depth': 57, 'n_estimators': 861, 'min_child_samples': 22, 'subsample': 0.7327459567972987, 'colsample_bytree': 0.5270720821346941, 'reg_alpha': 3.313199576803739, 'reg_lambda': 0.7724226921834203, 'min_split_gain': 0.0010098246257560799}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:45,755] Trial 975 finished with value: 0.25967427049612335 and parameters: {'learning_rate': 0.0773546306884393, 'num_leaves': 916, 'max_depth': 62, 'n_estimators': 861, 'min_child_samples': 22, 'subsample': 0.5290170417632133, 'colsample_bytree': 0.5274271595463931, 'reg_alpha': 2.2067669593597157e-06, 'reg_lambda': 0.0098939649652799, 'min_split_gain': 0.01883282040764212}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:46,797] Trial 994 finished with value: 0.2567644031994125 and parameters: {'learning_rate': 0.05759818657839729, 'num_leaves': 972, 'max_depth': 60, 'n_estimators': 883, 'min_child_samples': 61, 'subsample': 0.8517024176936628, 'colsample_bytree': 0.512042205715085, 'reg_alpha': 5.658401935997534, 'reg_lambda': 6.586720306047416, 'min_split_gain': 0.021835675532137142}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:47,137] Trial 980 finished with value: 0.2568154885564858 and parameters: {'learning_rate': 0.05828723974694685, 'num_leaves': 730, 'max_depth': 61, 'n_estimators': 869, 'min_child_samples': 22, 'subsample': 0.547492185271683, 'colsample_bytree': 0.5242250292250612, 'reg_alpha': 2.053505954449794, 'reg_lambda': 5.831180809808706, 'min_split_gain': 0.018241041410581173}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:47,413] Trial 998 finished with value: 0.2566194694690946 and parameters: {'learning_rate': 0.07273628289109726, 'num_leaves': 369, 'max_depth': 60, 'n_estimators': 836, 'min_child_samples': 38, 'subsample': 0.9476240216804287, 'colsample_bytree': 0.5127441771018532, 'reg_alpha': 5.415994298588113, 'reg_lambda': 2.0224612971824207, 'min_split_gain': 0.022643351063123932}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:47,613] Trial 960 finished with value: 0.2545865464117918 and parameters: {'learning_rate': 0.04956509964136225, 'num_leaves': 880, 'max_depth': 54, 'n_estimators': 847, 'min_child_samples': 18, 'subsample': 0.962847283154205, 'colsample_bytree': 0.541282602199141, 'reg_alpha': 3.7923387751535063, 'reg_lambda': 1.4910347092976019e-07, 'min_split_gain': 0.00033194598361750363}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:49,337] Trial 999 finished with value: 0.2528581351418772 and parameters: {'learning_rate': 0.07385841679944476, 'num_leaves': 967, 'max_depth': 60, 'n_estimators': 883, 'min_child_samples': 39, 'subsample': 0.8483834910425151, 'colsample_bytree': 0.5136449151511671, 'reg_alpha': 5.324472997954456, 'reg_lambda': 2.2736025426088067, 'min_split_gain': 0.020903357250151994}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:49,709] Trial 958 finished with value: 0.25605184984401363 and parameters: {'learning_rate': 0.048936026999865274, 'num_leaves': 877, 'max_depth': 54, 'n_estimators': 798, 'min_child_samples': 15, 'subsample': 0.6936278543724465, 'colsample_bytree': 0.5360827740210449, 'reg_alpha': 3.5060288474548202, 'reg_lambda': 0.0257199701477763, 'min_split_gain': 0.00031420299793432586}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:50,715] Trial 979 finished with value: 0.2561254424570749 and parameters: {'learning_rate': 0.05851183773656045, 'num_leaves': 883, 'max_depth': 61, 'n_estimators': 866, 'min_child_samples': 45, 'subsample': 0.9568250359423958, 'colsample_bytree': 0.5255209219751225, 'reg_alpha': 2.0826013369932705, 'reg_lambda': 6.176708148666923, 'min_split_gain': 0.0003241316074121196}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:51,886] Trial 996 finished with value: 0.2597524293200901 and parameters: {'learning_rate': 0.07369104840975198, 'num_leaves': 978, 'max_depth': 60, 'n_estimators': 892, 'min_child_samples': 61, 'subsample': 0.9513146303628939, 'colsample_bytree': 0.5559385284800532, 'reg_alpha': 1.1960866838735134, 'reg_lambda': 7.115366744045568, 'min_split_gain': 0.019925996397930842}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:52,447] Trial 992 finished with value: 0.25535823270264935 and parameters: {'learning_rate': 0.057312032076991995, 'num_leaves': 971, 'max_depth': 62, 'n_estimators': 892, 'min_child_samples': 61, 'subsample': 0.7328965340437481, 'colsample_bytree': 0.5599776061488865, 'reg_alpha': 1.847999261586309, 'reg_lambda': 6.293988145558155, 'min_split_gain': 0.021422950601293362}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:52,634] Trial 989 finished with value: 0.2576331591240798 and parameters: {'learning_rate': 0.060234786444895126, 'num_leaves': 947, 'max_depth': 62, 'n_estimators': 886, 'min_child_samples': 23, 'subsample': 0.773435134585267, 'colsample_bytree': 0.5130819542525031, 'reg_alpha': 2.5482700589839877e-07, 'reg_lambda': 6.619804152766699, 'min_split_gain': 0.060475574269070353}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:52,943] Trial 971 finished with value: 0.2541376416266148 and parameters: {'learning_rate': 0.08021304532900972, 'num_leaves': 877, 'max_depth': 50, 'n_estimators': 864, 'min_child_samples': 45, 'subsample': 0.6843654178096423, 'colsample_bytree': 0.52924845052724, 'reg_alpha': 2.1703740409370167, 'reg_lambda': 3.696046377967299, 'min_split_gain': 0.0005032643558798037}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:53,517] Trial 997 finished with value: 0.2567583468170898 and parameters: {'learning_rate': 0.07310269190594648, 'num_leaves': 959, 'max_depth': 60, 'n_estimators': 837, 'min_child_samples': 61, 'subsample': 0.8539233202342962, 'colsample_bytree': 0.5581013335971219, 'reg_alpha': 1.1230379697726036, 'reg_lambda': 9.93158460989368, 'min_split_gain': 0.02133880994714485}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:53,816] Trial 950 finished with value: 0.25518846018221775 and parameters: {'learning_rate': 0.047820481670203786, 'num_leaves': 914, 'max_depth': 56, 'n_estimators': 815, 'min_child_samples': 19, 'subsample': 0.9696390871706635, 'colsample_bytree': 0.5414954260771774, 'reg_alpha': 3.3762654003923225, 'reg_lambda': 0.01953390216827013, 'min_split_gain': 0.001130816248984038}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:54,113] Trial 947 finished with value: 0.2551614165602836 and parameters: {'learning_rate': 0.0480620414216255, 'num_leaves': 937, 'max_depth': 56, 'n_estimators': 811, 'min_child_samples': 19, 'subsample': 0.9704753846730808, 'colsample_bytree': 0.5414028646926041, 'reg_alpha': 3.841588455229635, 'reg_lambda': 1.9861091320960599e-07, 'min_split_gain': 0.00026778125360195555}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:54,636] Trial 952 finished with value: 0.2551726224230566 and parameters: {'learning_rate': 0.0473625269366767, 'num_leaves': 914, 'max_depth': 56, 'n_estimators': 828, 'min_child_samples': 25, 'subsample': 0.5197837830414725, 'colsample_bytree': 0.529123671861094, 'reg_alpha': 3.7184233770006276, 'reg_lambda': 0.011953778670595012, 'min_split_gain': 0.00023541016118242247}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:56,266] Trial 956 finished with value: 0.25361670912208556 and parameters: {'learning_rate': 0.044895477784841265, 'num_leaves': 914, 'max_depth': 57, 'n_estimators': 845, 'min_child_samples': 19, 'subsample': 0.9677186088914039, 'colsample_bytree': 0.5395848926682614, 'reg_alpha': 3.401941840893713, 'reg_lambda': 1.20933120743662e-07, 'min_split_gain': 0.00016679541562621136}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:57,328] Trial 925 finished with value: 0.261202414012821 and parameters: {'learning_rate': 0.005461271525139945, 'num_leaves': 922, 'max_depth': 59, 'n_estimators': 877, 'min_child_samples': 38, 'subsample': 0.9337403955902304, 'colsample_bytree': 0.5082689629095998, 'reg_alpha': 6.704288053701102, 'reg_lambda': 1.2618354943529123e-07, 'min_split_gain': 0.00023094126938460352}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:58,120] Trial 876 finished with value: 0.25755445886752154 and parameters: {'learning_rate': 0.05793401993622359, 'num_leaves': 890, 'max_depth': 57, 'n_estimators': 316, 'min_child_samples': 22, 'subsample': 0.8279418504226498, 'colsample_bytree': 0.5499549526223415, 'reg_alpha': 4.007079132241706e-05, 'reg_lambda': 8.111546465765742e-08, 'min_split_gain': 0.0006033509499038006}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:39:58,290] Trial 982 finished with value: 0.25504483857117805 and parameters: {'learning_rate': 0.057860005640845306, 'num_leaves': 942, 'max_depth': 61, 'n_estimators': 889, 'min_child_samples': 63, 'subsample': 0.6455346356002011, 'colsample_bytree': 0.6775752799675374, 'reg_alpha': 2.105484235129305, 'reg_lambda': 6.703718626500144, 'min_split_gain': 8.408455216584246e-05}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:40:00,810] Trial 961 finished with value: 0.2541292072109605 and parameters: {'learning_rate': 0.046273220872374565, 'num_leaves': 876, 'max_depth': 54, 'n_estimators': 848, 'min_child_samples': 14, 'subsample': 0.9752514141829213, 'colsample_bytree': 0.5400636802513328, 'reg_alpha': 4.610831339356262, 'reg_lambda': 0.0026230557613044145, 'min_split_gain': 0.00014271029190598104}. Best is trial 249 with value: 0.2497732745396593.\n",
      "[I 2025-07-14 21:40:01,234] Trial 848 finished with value: 0.25799972011591876 and parameters: {'learning_rate': 0.005354178344860432, 'num_leaves': 844, 'max_depth': 53, 'n_estimators': 941, 'min_child_samples': 16, 'subsample': 0.6007264748078075, 'colsample_bytree': 0.5204899376478147, 'reg_alpha': 2.373633135102497, 'reg_lambda': 0.00041843870413161834, 'min_split_gain': 0.016146854127318894}. Best is trial 249 with value: 0.2497732745396593.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 RMSE: [0.24977327 0.24990692 0.24995379 0.25126267 0.25173028 0.25192593\n",
      " 0.25194562 0.2520309  0.25203113 0.2520312  0.2521292  0.25224717\n",
      " 0.25237975 0.25241549 0.25242355 0.25243109 0.25248236 0.2525209\n",
      " 0.25259965 0.2527355  0.25274417 0.2528003  0.25285814 0.25291141\n",
      " 0.25297366 0.25303877 0.25306023 0.25312867 0.25317722 0.25318016\n",
      " 0.25326933 0.25327496 0.25328069 0.25332354 0.25334156 0.25336186\n",
      " 0.25338393 0.2534046  0.2534897  0.25351643 0.25354331 0.2535439\n",
      " 0.25354766 0.25361671 0.25362168 0.25363164 0.25363974 0.25364144\n",
      " 0.2536421  0.25371085]\n",
      "Top 50 MAE: [0.16771066 0.16725687 0.16692373 0.1693834  0.16932839 0.16785608\n",
      " 0.1691616  0.1694729  0.1693808  0.16961925 0.17041808 0.17036967\n",
      " 0.17043486 0.1702671  0.16996913 0.16940992 0.16845827 0.17037084\n",
      " 0.17013398 0.17099144 0.17006655 0.16948808 0.17113069 0.17064542\n",
      " 0.1698298  0.17183973 0.16992524 0.17036683 0.17079193 0.17027244\n",
      " 0.17081545 0.17145304 0.17066613 0.17095934 0.17126263 0.17135534\n",
      " 0.17049999 0.17001915 0.17036206 0.17101773 0.17142136 0.17190193\n",
      " 0.17183038 0.17110522 0.16926611 0.17032771 0.1704357  0.17100746\n",
      " 0.17093803 0.17126386]\n",
      "[{'colsample_bytree': 0.5333434673612824, 'learning_rate': 0.06231644339727185, 'max_depth': 58, 'min_child_samples': 58, 'min_split_gain': 0.002062115270766521, 'n_estimators': 844, 'num_leaves': 973, 'reg_alpha': 3.7371206169171867, 'reg_lambda': 7.172199174507102, 'subsample': 0.7971670807732923, 'random_state': 249, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5228280861398271, 'learning_rate': 0.09415473889227101, 'max_depth': 59, 'min_child_samples': 39, 'min_split_gain': 0.0002401704454612645, 'n_estimators': 867, 'num_leaves': 901, 'reg_alpha': 5.897861800674065, 'reg_lambda': 7.021842519723051, 'subsample': 0.8212381838938069, 'random_state': 588, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5270720821346941, 'learning_rate': 0.0778624078311394, 'max_depth': 57, 'min_child_samples': 22, 'min_split_gain': 0.0010098246257560799, 'n_estimators': 861, 'num_leaves': 878, 'reg_alpha': 3.313199576803739, 'reg_lambda': 0.7724226921834203, 'subsample': 0.7327459567972987, 'random_state': 967, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5489929212650287, 'learning_rate': 0.060387136377347855, 'max_depth': 57, 'min_child_samples': 21, 'min_split_gain': 0.0018065403827701762, 'n_estimators': 858, 'num_leaves': 891, 'reg_alpha': 5.190806910265158, 'reg_lambda': 8.706605074547784e-08, 'subsample': 0.9218406960269193, 'random_state': 880, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5244855751042465, 'learning_rate': 0.0536866881073233, 'max_depth': 57, 'min_child_samples': 39, 'min_split_gain': 0.002391951110936324, 'n_estimators': 864, 'num_leaves': 900, 'reg_alpha': 6.744423458134775, 'reg_lambda': 5.983870832252938e-08, 'subsample': 0.8413796611852522, 'random_state': 595, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5279342059410572, 'learning_rate': 0.06664545236488376, 'max_depth': 13, 'min_child_samples': 44, 'min_split_gain': 0.0011621208896599555, 'n_estimators': 565, 'num_leaves': 1023, 'reg_alpha': 0.09846810404695876, 'reg_lambda': 5.0183028248891753e-08, 'subsample': 0.6431620370080372, 'random_state': 396, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.52073707473068, 'learning_rate': 0.05024988063426686, 'max_depth': 55, 'min_child_samples': 61, 'min_split_gain': 4.544312058760646e-05, 'n_estimators': 855, 'num_leaves': 925, 'reg_alpha': 6.8661976852896975, 'reg_lambda': 5.205975807553854, 'subsample': 0.8789663711022501, 'random_state': 503, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5429544964217402, 'learning_rate': 0.05612066644203194, 'max_depth': 57, 'min_child_samples': 40, 'min_split_gain': 8.348331360124054e-06, 'n_estimators': 863, 'num_leaves': 914, 'reg_alpha': 6.514498978027664, 'reg_lambda': 0.0002658020249257671, 'subsample': 0.5453661427123014, 'random_state': 895, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5100048586740744, 'learning_rate': 0.051832888582275125, 'max_depth': 57, 'min_child_samples': 32, 'min_split_gain': 0.0010483765362046236, 'n_estimators': 871, 'num_leaves': 915, 'reg_alpha': 5.380537942380326, 'reg_lambda': 6.5627636208748825, 'subsample': 0.5554532515110827, 'random_state': 602, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5363730331732026, 'learning_rate': 0.0730711750787527, 'max_depth': 56, 'min_child_samples': 21, 'min_split_gain': 0.0011454809788292032, 'n_estimators': 834, 'num_leaves': 952, 'reg_alpha': 3.2860641461944105, 'reg_lambda': 6.840765700209149, 'subsample': 0.5758946629908313, 'random_state': 680, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5331911568260371, 'learning_rate': 0.053610650571659695, 'max_depth': 54, 'min_child_samples': 41, 'min_split_gain': 0.019385420828904897, 'n_estimators': 912, 'num_leaves': 835, 'reg_alpha': 3.169452186302612, 'reg_lambda': 2.638298207081307, 'subsample': 0.6184519354038941, 'random_state': 767, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5263586130926626, 'learning_rate': 0.06608454999500171, 'max_depth': 63, 'min_child_samples': 27, 'min_split_gain': 0.005058946434738559, 'n_estimators': 780, 'num_leaves': 722, 'reg_alpha': 1.347517240038013, 'reg_lambda': 1.9650876158139027e-07, 'subsample': 0.6592398555480282, 'random_state': 136, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5661543389583997, 'learning_rate': 0.08307740933382936, 'max_depth': 61, 'min_child_samples': 20, 'min_split_gain': 1.2595124108996311e-05, 'n_estimators': 427, 'num_leaves': 774, 'reg_alpha': 3.926117738831247e-05, 'reg_lambda': 2.2488452007703166, 'subsample': 0.6173982523593531, 'random_state': 174, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5376297972854727, 'learning_rate': 0.05423393026710959, 'max_depth': 14, 'min_child_samples': 40, 'min_split_gain': 0.00017198225572109122, 'n_estimators': 908, 'num_leaves': 680, 'reg_alpha': 3.211659919094692, 'reg_lambda': 2.61025137615618, 'subsample': 0.8021089589068968, 'random_state': 774, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5265249993861638, 'learning_rate': 0.07231400295849735, 'max_depth': 54, 'min_child_samples': 21, 'min_split_gain': 0.00010375851941188527, 'n_estimators': 921, 'num_leaves': 880, 'reg_alpha': 3.0519525713915003, 'reg_lambda': 9.862570457464951, 'subsample': 0.5418605095929512, 'random_state': 758, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.553424879475853, 'learning_rate': 0.0530796710691807, 'max_depth': 58, 'min_child_samples': 60, 'min_split_gain': 0.0014546701178401727, 'n_estimators': 859, 'num_leaves': 897, 'reg_alpha': 6.188965950597846, 'reg_lambda': 6.521984699562199, 'subsample': 0.8444778511435669, 'random_state': 599, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5171367042944774, 'learning_rate': 0.06629188163033072, 'max_depth': 53, 'min_child_samples': 38, 'min_split_gain': 0.0005351836303724878, 'n_estimators': 902, 'num_leaves': 898, 'reg_alpha': 0.7033360600649953, 'reg_lambda': 9.961275823403197, 'subsample': 0.9395097036303188, 'random_state': 576, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5492143469279874, 'learning_rate': 0.05877468512012674, 'max_depth': 49, 'min_child_samples': 40, 'min_split_gain': 0.0008726232155524152, 'n_estimators': 862, 'num_leaves': 906, 'reg_alpha': 6.706276773634164, 'reg_lambda': 9.82056663137562, 'subsample': 0.9467004073212454, 'random_state': 897, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5197223453981258, 'learning_rate': 0.0320766972063381, 'max_depth': 43, 'min_child_samples': 22, 'min_split_gain': 0.0009978275278068693, 'n_estimators': 891, 'num_leaves': 252, 'reg_alpha': 0.00017584654512779244, 'reg_lambda': 9.4739503826325, 'subsample': 0.9376659754907131, 'random_state': 568, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5288676624045511, 'learning_rate': 0.04529656326760325, 'max_depth': 60, 'min_child_samples': 60, 'min_split_gain': 0.0011123802855812737, 'n_estimators': 903, 'num_leaves': 921, 'reg_alpha': 6.228148217856839, 'reg_lambda': 5.172915599780887, 'subsample': 0.8750056601466312, 'random_state': 507, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5170644217231086, 'learning_rate': 0.05837599869899827, 'max_depth': 56, 'min_child_samples': 21, 'min_split_gain': 0.000385152661648248, 'n_estimators': 858, 'num_leaves': 895, 'reg_alpha': 4.203142313799506, 'reg_lambda': 8.115241792573292e-08, 'subsample': 0.7162870798656494, 'random_state': 874, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5316057379758604, 'learning_rate': 0.06294747252266736, 'max_depth': 55, 'min_child_samples': 57, 'min_split_gain': 0.003114868241095352, 'n_estimators': 848, 'num_leaves': 751, 'reg_alpha': 3.909527534975478, 'reg_lambda': 2.9109140815516083e-08, 'subsample': 0.7996547835597259, 'random_state': 254, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5136449151511671, 'learning_rate': 0.07385841679944476, 'max_depth': 60, 'min_child_samples': 39, 'min_split_gain': 0.020903357250151994, 'n_estimators': 883, 'num_leaves': 967, 'reg_alpha': 5.324472997954456, 'reg_lambda': 2.2736025426088067, 'subsample': 0.8483834910425151, 'random_state': 999, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5469554096755713, 'learning_rate': 0.05705818952220585, 'max_depth': 55, 'min_child_samples': 41, 'min_split_gain': 0.001354998068104385, 'n_estimators': 560, 'num_leaves': 1006, 'reg_alpha': 4.071507720230382, 'reg_lambda': 1.0282478539297453e-08, 'subsample': 0.8237618841159182, 'random_state': 263, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5127799479410909, 'learning_rate': 0.0527079567981938, 'max_depth': 57, 'min_child_samples': 35, 'min_split_gain': 0.0006870290245753682, 'n_estimators': 861, 'num_leaves': 911, 'reg_alpha': 5.713135546008621, 'reg_lambda': 6.9226372765761495, 'subsample': 0.6158019477381236, 'random_state': 601, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5339307278612591, 'learning_rate': 0.06250326217665085, 'max_depth': 16, 'min_child_samples': 38, 'min_split_gain': 0.0009012914030467334, 'n_estimators': 838, 'num_leaves': 1003, 'reg_alpha': 5.645226369052753, 'reg_lambda': 2.484907119994454e-08, 'subsample': 0.8165113226526663, 'random_state': 268, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5263420847108201, 'learning_rate': 0.06971566919923479, 'max_depth': 60, 'min_child_samples': 41, 'min_split_gain': 0.017551215701030676, 'n_estimators': 917, 'num_leaves': 990, 'reg_alpha': 3.028265832597941, 'reg_lambda': 9.796501906635623, 'subsample': 0.7993865050344471, 'random_state': 759, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5253437623905174, 'learning_rate': 0.11624073358794774, 'max_depth': 56, 'min_child_samples': 34, 'min_split_gain': 0.0007370784825092033, 'n_estimators': 907, 'num_leaves': 826, 'reg_alpha': 6.070400921210787, 'reg_lambda': 9.562918991669171, 'subsample': 0.5655919731583523, 'random_state': 613, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5489579014360508, 'learning_rate': 0.0626641903464264, 'max_depth': 14, 'min_child_samples': 41, 'min_split_gain': 0.00015595773162556045, 'n_estimators': 560, 'num_leaves': 999, 'reg_alpha': 0.0027576677000292114, 'reg_lambda': 2.2652980994914373e-08, 'subsample': 0.8259030175077385, 'random_state': 276, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5332055294572328, 'learning_rate': 0.04998702617298246, 'max_depth': 59, 'min_child_samples': 41, 'min_split_gain': 0.0015449231427669208, 'n_estimators': 842, 'num_leaves': 945, 'reg_alpha': 2.534371480398288, 'reg_lambda': 3.0434747285827397, 'subsample': 0.6168831209233134, 'random_state': 907, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5137815829921706, 'learning_rate': 0.05256095267899987, 'max_depth': 54, 'min_child_samples': 39, 'min_split_gain': 0.0002714187085013415, 'n_estimators': 869, 'num_leaves': 876, 'reg_alpha': 0.9278939724816249, 'reg_lambda': 6.568998549968091e-08, 'subsample': 0.8034048781493957, 'random_state': 591, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5461182396147376, 'learning_rate': 0.02248365190858203, 'max_depth': 57, 'min_child_samples': 36, 'min_split_gain': 2.6061834577174913e-06, 'n_estimators': 856, 'num_leaves': 903, 'reg_alpha': 6.418059126802901, 'reg_lambda': 9.242602663253951e-08, 'subsample': 0.7989768030649147, 'random_state': 888, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5335797020973163, 'learning_rate': 0.053611754990371865, 'max_depth': 53, 'min_child_samples': 22, 'min_split_gain': 0.0003097151868348969, 'n_estimators': 918, 'num_leaves': 844, 'reg_alpha': 3.3694109994877466, 'reg_lambda': 9.851018150594369, 'subsample': 0.6168484022365105, 'random_state': 761, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5734274437889891, 'learning_rate': 0.04146776008874663, 'max_depth': 13, 'min_child_samples': 59, 'min_split_gain': 0.0024960224279475273, 'n_estimators': 693, 'num_leaves': 911, 'reg_alpha': 5.6430435653710145, 'reg_lambda': 2.22060897184505e-08, 'subsample': 0.6492311467459078, 'random_state': 377, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5190312316093904, 'learning_rate': 0.07227095334093186, 'max_depth': 55, 'min_child_samples': 21, 'min_split_gain': 0.016851299017158358, 'n_estimators': 858, 'num_leaves': 893, 'reg_alpha': 4.215311373248343, 'reg_lambda': 6.335716545696073, 'subsample': 0.8204594741901514, 'random_state': 866, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5274895673020713, 'learning_rate': 0.05747186247763484, 'max_depth': 16, 'min_child_samples': 59, 'min_split_gain': 0.00018311472541075247, 'n_estimators': 874, 'num_leaves': 987, 'reg_alpha': 9.388961921189143, 'reg_lambda': 1.0055346054976132e-08, 'subsample': 0.8636918845819727, 'random_state': 481, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5243321100850568, 'learning_rate': 0.050751953832934, 'max_depth': 42, 'min_child_samples': 22, 'min_split_gain': 0.0004742717300235638, 'n_estimators': 860, 'num_leaves': 919, 'reg_alpha': 6.318712692486671, 'reg_lambda': 6.16351360697893, 'subsample': 0.8842348488140355, 'random_state': 504, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5171023196306238, 'learning_rate': 0.0651217726542331, 'max_depth': 54, 'min_child_samples': 37, 'min_split_gain': 0.0013309480321743146, 'n_estimators': 893, 'num_leaves': 148, 'reg_alpha': 2.2173454314246306, 'reg_lambda': 9.401273383442003, 'subsample': 0.5890446207595045, 'random_state': 575, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5169056470393405, 'learning_rate': 0.07236682920446781, 'max_depth': 56, 'min_child_samples': 34, 'min_split_gain': 0.014986947934425, 'n_estimators': 899, 'num_leaves': 897, 'reg_alpha': 3.7688929761392402, 'reg_lambda': 6.556601144085903, 'subsample': 0.8247865839737977, 'random_state': 865, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5090457160353795, 'learning_rate': 0.056021162776750164, 'max_depth': 58, 'min_child_samples': 25, 'min_split_gain': 8.171038390110798e-05, 'n_estimators': 857, 'num_leaves': 929, 'reg_alpha': 4.721628673068319, 'reg_lambda': 9.971101043925685, 'subsample': 0.5518706500556674, 'random_state': 651, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5662130806326541, 'learning_rate': 0.07607350535939446, 'max_depth': 47, 'min_child_samples': 35, 'min_split_gain': 0.0006845067851606668, 'n_estimators': 875, 'num_leaves': 899, 'reg_alpha': 4.856657815276274, 'reg_lambda': 2.6230232531911523e-08, 'subsample': 0.8643211209068412, 'random_state': 464, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.55780632074205, 'learning_rate': 0.03856522502093635, 'max_depth': 63, 'min_child_samples': 39, 'min_split_gain': 0.00024764772628177575, 'n_estimators': 945, 'num_leaves': 819, 'reg_alpha': 4.973060653833003, 'reg_lambda': 1.7882703713508816, 'subsample': 0.5092849515126023, 'random_state': 789, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5133435092942498, 'learning_rate': 0.04730570093310139, 'max_depth': 58, 'min_child_samples': 19, 'min_split_gain': 0.0004719682308533021, 'n_estimators': 897, 'num_leaves': 906, 'reg_alpha': 8.005729679980057e-05, 'reg_lambda': 9.629022574840093, 'subsample': 0.560751907344925, 'random_state': 667, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5395848926682614, 'learning_rate': 0.044895477784841265, 'max_depth': 57, 'min_child_samples': 19, 'min_split_gain': 0.00016679541562621136, 'n_estimators': 845, 'num_leaves': 914, 'reg_alpha': 3.401941840893713, 'reg_lambda': 1.20933120743662e-07, 'subsample': 0.9677186088914039, 'random_state': 956, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5382514372015093, 'learning_rate': 0.06485765146168458, 'max_depth': 17, 'min_child_samples': 45, 'min_split_gain': 0.00012999848271384013, 'n_estimators': 529, 'num_leaves': 949, 'reg_alpha': 0.0011925585369433828, 'reg_lambda': 2.7941705870565744e-08, 'subsample': 0.8241762628011553, 'random_state': 380, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5134479789943638, 'learning_rate': 0.052868747521113424, 'max_depth': 16, 'min_child_samples': 39, 'min_split_gain': 0.0013914607022652622, 'n_estimators': 917, 'num_leaves': 255, 'reg_alpha': 1.2952057659379401, 'reg_lambda': 5.741846404719004, 'subsample': 0.8260818285626025, 'random_state': 492, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5229945121367208, 'learning_rate': 0.06505464016439833, 'max_depth': 55, 'min_child_samples': 38, 'min_split_gain': 0.00020180054827743607, 'n_estimators': 870, 'num_leaves': 873, 'reg_alpha': 0.4640205452483775, 'reg_lambda': 4.204173089834077, 'subsample': 0.6519018633891417, 'random_state': 497, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.6571585170261566, 'learning_rate': 0.0491994589897865, 'max_depth': 40, 'min_child_samples': 62, 'min_split_gain': 0.0015392986861338489, 'n_estimators': 919, 'num_leaves': 920, 'reg_alpha': 6.096876205550838, 'reg_lambda': 5.378757906139428, 'subsample': 0.8833365529300939, 'random_state': 506, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5187301986780853, 'learning_rate': 0.0506142549500622, 'max_depth': 52, 'min_child_samples': 26, 'min_split_gain': 0.0005452897700963357, 'n_estimators': 862, 'num_leaves': 892, 'reg_alpha': 4.515348971897004, 'reg_lambda': 9.724891739174708e-08, 'subsample': 0.5776560640457273, 'random_state': 695, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.524357444597027, 'learning_rate': 0.06464752625994903, 'max_depth': 42, 'min_child_samples': 38, 'min_split_gain': 0.0017409454022606031, 'n_estimators': 856, 'num_leaves': 858, 'reg_alpha': 1.3975085450630336, 'reg_lambda': 6.624803112950276, 'subsample': 0.824677358947841, 'random_state': 500, 'n_jobs': -1, 'verbosity': -1}]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --- SETUP ---\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "feature_cols = [col for col in df_train.columns if col != target_col]\n",
    "\n",
    "X_tr = df_train[feature_cols]\n",
    "y_tr = df_train[target_col]\n",
    "X_val = df_val[feature_cols]\n",
    "y_val = df_val[target_col]\n",
    "\n",
    "# --- OPTUNA OBJECTIVE ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0005, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 1024),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 64),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"random_state\": trial.number,   # Distinto para cada trial\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbosity\": -1,  # MÃ­nimo verbose en LightGBM\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[early_stopping(stopping_rounds=30, verbose=False)],\n",
    "    )\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    trial.set_user_attr(\"mae\", mean_absolute_error(y_val, preds))\n",
    "    gc.collect()\n",
    "    return rmse\n",
    "\n",
    "# --- OPTIMIZE ---\n",
    "N_MODELS = 50\n",
    "N_TRIALS = 1000  \n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=101))\n",
    "study.optimize(objective_lgbm, n_trials=N_TRIALS, n_jobs=28, show_progress_bar=False)\n",
    "\n",
    "# --- EXTRACT BEST 50 PARAMS ---\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df[\"mae\"] = [t.user_attrs.get(\"mae\", np.nan) for t in study.trials]\n",
    "\n",
    "top_lgbm_trials = trials_df.sort_values(\"value\").head(N_MODELS)\n",
    "final_configs = []\n",
    "for i, row in top_lgbm_trials.iterrows():\n",
    "    params = row.filter(like='params_').to_dict()\n",
    "    params = {k.replace('params_', ''): v for k, v in params.items()}\n",
    "    for p in [\"num_leaves\", \"max_depth\", \"n_estimators\", \"min_child_samples\"]:\n",
    "        params[p] = int(params[p])\n",
    "    params[\"random_state\"] = int(row[\"number\"])\n",
    "    params[\"n_jobs\"] = -1\n",
    "    params[\"verbosity\"] = -1\n",
    "    final_configs.append(params)\n",
    "\n",
    "# --- GUARDADO ---\n",
    "top_lgbm_trials.to_csv(\"optuna_lgbm_trials.csv\", index=False)\n",
    "import json\n",
    "with open(\"lgbm_ensemble_configs.json\", \"w\") as f:\n",
    "    json.dump(final_configs, f, indent=2)\n",
    "\n",
    "# --- PRINT RESUMEN SIMPLE ---\n",
    "print(f\"Top 50 RMSE: {top_lgbm_trials['value'].head(50).values}\")\n",
    "print(f\"Top 50 MAE: {top_lgbm_trials['mae'].head(50).values}\")\n",
    "print(final_configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fa797b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'colsample_bytree': 0.5333434673612824, 'learning_rate': 0.06231644339727185, 'max_depth': 58, 'min_child_samples': 58, 'min_split_gain': 0.002062115270766521, 'n_estimators': 844, 'num_leaves': 973, 'reg_alpha': 3.7371206169171867, 'reg_lambda': 7.172199174507102, 'subsample': 0.7971670807732923, 'random_state': 249, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5228280861398271, 'learning_rate': 0.09415473889227101, 'max_depth': 59, 'min_child_samples': 39, 'min_split_gain': 0.0002401704454612645, 'n_estimators': 867, 'num_leaves': 901, 'reg_alpha': 5.897861800674065, 'reg_lambda': 7.021842519723051, 'subsample': 0.8212381838938069, 'random_state': 588, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5270720821346941, 'learning_rate': 0.0778624078311394, 'max_depth': 57, 'min_child_samples': 22, 'min_split_gain': 0.0010098246257560799, 'n_estimators': 861, 'num_leaves': 878, 'reg_alpha': 3.313199576803739, 'reg_lambda': 0.7724226921834203, 'subsample': 0.7327459567972987, 'random_state': 967, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5489929212650287, 'learning_rate': 0.060387136377347855, 'max_depth': 57, 'min_child_samples': 21, 'min_split_gain': 0.0018065403827701762, 'n_estimators': 858, 'num_leaves': 891, 'reg_alpha': 5.190806910265158, 'reg_lambda': 8.706605074547784e-08, 'subsample': 0.9218406960269193, 'random_state': 880, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5244855751042465, 'learning_rate': 0.0536866881073233, 'max_depth': 57, 'min_child_samples': 39, 'min_split_gain': 0.002391951110936324, 'n_estimators': 864, 'num_leaves': 900, 'reg_alpha': 6.744423458134775, 'reg_lambda': 5.983870832252938e-08, 'subsample': 0.8413796611852522, 'random_state': 595, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5279342059410572, 'learning_rate': 0.06664545236488376, 'max_depth': 13, 'min_child_samples': 44, 'min_split_gain': 0.0011621208896599555, 'n_estimators': 565, 'num_leaves': 1023, 'reg_alpha': 0.09846810404695876, 'reg_lambda': 5.0183028248891753e-08, 'subsample': 0.6431620370080372, 'random_state': 396, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.52073707473068, 'learning_rate': 0.05024988063426686, 'max_depth': 55, 'min_child_samples': 61, 'min_split_gain': 4.544312058760646e-05, 'n_estimators': 855, 'num_leaves': 925, 'reg_alpha': 6.8661976852896975, 'reg_lambda': 5.205975807553854, 'subsample': 0.8789663711022501, 'random_state': 503, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5429544964217402, 'learning_rate': 0.05612066644203194, 'max_depth': 57, 'min_child_samples': 40, 'min_split_gain': 8.348331360124054e-06, 'n_estimators': 863, 'num_leaves': 914, 'reg_alpha': 6.514498978027664, 'reg_lambda': 0.0002658020249257671, 'subsample': 0.5453661427123014, 'random_state': 895, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5100048586740744, 'learning_rate': 0.051832888582275125, 'max_depth': 57, 'min_child_samples': 32, 'min_split_gain': 0.0010483765362046236, 'n_estimators': 871, 'num_leaves': 915, 'reg_alpha': 5.380537942380326, 'reg_lambda': 6.5627636208748825, 'subsample': 0.5554532515110827, 'random_state': 602, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5363730331732026, 'learning_rate': 0.0730711750787527, 'max_depth': 56, 'min_child_samples': 21, 'min_split_gain': 0.0011454809788292032, 'n_estimators': 834, 'num_leaves': 952, 'reg_alpha': 3.2860641461944105, 'reg_lambda': 6.840765700209149, 'subsample': 0.5758946629908313, 'random_state': 680, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5331911568260371, 'learning_rate': 0.053610650571659695, 'max_depth': 54, 'min_child_samples': 41, 'min_split_gain': 0.019385420828904897, 'n_estimators': 912, 'num_leaves': 835, 'reg_alpha': 3.169452186302612, 'reg_lambda': 2.638298207081307, 'subsample': 0.6184519354038941, 'random_state': 767, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5263586130926626, 'learning_rate': 0.06608454999500171, 'max_depth': 63, 'min_child_samples': 27, 'min_split_gain': 0.005058946434738559, 'n_estimators': 780, 'num_leaves': 722, 'reg_alpha': 1.347517240038013, 'reg_lambda': 1.9650876158139027e-07, 'subsample': 0.6592398555480282, 'random_state': 136, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5661543389583997, 'learning_rate': 0.08307740933382936, 'max_depth': 61, 'min_child_samples': 20, 'min_split_gain': 1.2595124108996311e-05, 'n_estimators': 427, 'num_leaves': 774, 'reg_alpha': 3.926117738831247e-05, 'reg_lambda': 2.2488452007703166, 'subsample': 0.6173982523593531, 'random_state': 174, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5376297972854727, 'learning_rate': 0.05423393026710959, 'max_depth': 14, 'min_child_samples': 40, 'min_split_gain': 0.00017198225572109122, 'n_estimators': 908, 'num_leaves': 680, 'reg_alpha': 3.211659919094692, 'reg_lambda': 2.61025137615618, 'subsample': 0.8021089589068968, 'random_state': 774, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5265249993861638, 'learning_rate': 0.07231400295849735, 'max_depth': 54, 'min_child_samples': 21, 'min_split_gain': 0.00010375851941188527, 'n_estimators': 921, 'num_leaves': 880, 'reg_alpha': 3.0519525713915003, 'reg_lambda': 9.862570457464951, 'subsample': 0.5418605095929512, 'random_state': 758, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.553424879475853, 'learning_rate': 0.0530796710691807, 'max_depth': 58, 'min_child_samples': 60, 'min_split_gain': 0.0014546701178401727, 'n_estimators': 859, 'num_leaves': 897, 'reg_alpha': 6.188965950597846, 'reg_lambda': 6.521984699562199, 'subsample': 0.8444778511435669, 'random_state': 599, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5171367042944774, 'learning_rate': 0.06629188163033072, 'max_depth': 53, 'min_child_samples': 38, 'min_split_gain': 0.0005351836303724878, 'n_estimators': 902, 'num_leaves': 898, 'reg_alpha': 0.7033360600649953, 'reg_lambda': 9.961275823403197, 'subsample': 0.9395097036303188, 'random_state': 576, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5492143469279874, 'learning_rate': 0.05877468512012674, 'max_depth': 49, 'min_child_samples': 40, 'min_split_gain': 0.0008726232155524152, 'n_estimators': 862, 'num_leaves': 906, 'reg_alpha': 6.706276773634164, 'reg_lambda': 9.82056663137562, 'subsample': 0.9467004073212454, 'random_state': 897, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5197223453981258, 'learning_rate': 0.0320766972063381, 'max_depth': 43, 'min_child_samples': 22, 'min_split_gain': 0.0009978275278068693, 'n_estimators': 891, 'num_leaves': 252, 'reg_alpha': 0.00017584654512779244, 'reg_lambda': 9.4739503826325, 'subsample': 0.9376659754907131, 'random_state': 568, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5288676624045511, 'learning_rate': 0.04529656326760325, 'max_depth': 60, 'min_child_samples': 60, 'min_split_gain': 0.0011123802855812737, 'n_estimators': 903, 'num_leaves': 921, 'reg_alpha': 6.228148217856839, 'reg_lambda': 5.172915599780887, 'subsample': 0.8750056601466312, 'random_state': 507, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5170644217231086, 'learning_rate': 0.05837599869899827, 'max_depth': 56, 'min_child_samples': 21, 'min_split_gain': 0.000385152661648248, 'n_estimators': 858, 'num_leaves': 895, 'reg_alpha': 4.203142313799506, 'reg_lambda': 8.115241792573292e-08, 'subsample': 0.7162870798656494, 'random_state': 874, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5316057379758604, 'learning_rate': 0.06294747252266736, 'max_depth': 55, 'min_child_samples': 57, 'min_split_gain': 0.003114868241095352, 'n_estimators': 848, 'num_leaves': 751, 'reg_alpha': 3.909527534975478, 'reg_lambda': 2.9109140815516083e-08, 'subsample': 0.7996547835597259, 'random_state': 254, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5136449151511671, 'learning_rate': 0.07385841679944476, 'max_depth': 60, 'min_child_samples': 39, 'min_split_gain': 0.020903357250151994, 'n_estimators': 883, 'num_leaves': 967, 'reg_alpha': 5.324472997954456, 'reg_lambda': 2.2736025426088067, 'subsample': 0.8483834910425151, 'random_state': 999, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5469554096755713, 'learning_rate': 0.05705818952220585, 'max_depth': 55, 'min_child_samples': 41, 'min_split_gain': 0.001354998068104385, 'n_estimators': 560, 'num_leaves': 1006, 'reg_alpha': 4.071507720230382, 'reg_lambda': 1.0282478539297453e-08, 'subsample': 0.8237618841159182, 'random_state': 263, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5127799479410909, 'learning_rate': 0.0527079567981938, 'max_depth': 57, 'min_child_samples': 35, 'min_split_gain': 0.0006870290245753682, 'n_estimators': 861, 'num_leaves': 911, 'reg_alpha': 5.713135546008621, 'reg_lambda': 6.9226372765761495, 'subsample': 0.6158019477381236, 'random_state': 601, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5339307278612591, 'learning_rate': 0.06250326217665085, 'max_depth': 16, 'min_child_samples': 38, 'min_split_gain': 0.0009012914030467334, 'n_estimators': 838, 'num_leaves': 1003, 'reg_alpha': 5.645226369052753, 'reg_lambda': 2.484907119994454e-08, 'subsample': 0.8165113226526663, 'random_state': 268, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5263420847108201, 'learning_rate': 0.06971566919923479, 'max_depth': 60, 'min_child_samples': 41, 'min_split_gain': 0.017551215701030676, 'n_estimators': 917, 'num_leaves': 990, 'reg_alpha': 3.028265832597941, 'reg_lambda': 9.796501906635623, 'subsample': 0.7993865050344471, 'random_state': 759, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5253437623905174, 'learning_rate': 0.11624073358794774, 'max_depth': 56, 'min_child_samples': 34, 'min_split_gain': 0.0007370784825092033, 'n_estimators': 907, 'num_leaves': 826, 'reg_alpha': 6.070400921210787, 'reg_lambda': 9.562918991669171, 'subsample': 0.5655919731583523, 'random_state': 613, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5489579014360508, 'learning_rate': 0.0626641903464264, 'max_depth': 14, 'min_child_samples': 41, 'min_split_gain': 0.00015595773162556045, 'n_estimators': 560, 'num_leaves': 999, 'reg_alpha': 0.0027576677000292114, 'reg_lambda': 2.2652980994914373e-08, 'subsample': 0.8259030175077385, 'random_state': 276, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5332055294572328, 'learning_rate': 0.04998702617298246, 'max_depth': 59, 'min_child_samples': 41, 'min_split_gain': 0.0015449231427669208, 'n_estimators': 842, 'num_leaves': 945, 'reg_alpha': 2.534371480398288, 'reg_lambda': 3.0434747285827397, 'subsample': 0.6168831209233134, 'random_state': 907, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5137815829921706, 'learning_rate': 0.05256095267899987, 'max_depth': 54, 'min_child_samples': 39, 'min_split_gain': 0.0002714187085013415, 'n_estimators': 869, 'num_leaves': 876, 'reg_alpha': 0.9278939724816249, 'reg_lambda': 6.568998549968091e-08, 'subsample': 0.8034048781493957, 'random_state': 591, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5461182396147376, 'learning_rate': 0.02248365190858203, 'max_depth': 57, 'min_child_samples': 36, 'min_split_gain': 2.6061834577174913e-06, 'n_estimators': 856, 'num_leaves': 903, 'reg_alpha': 6.418059126802901, 'reg_lambda': 9.242602663253951e-08, 'subsample': 0.7989768030649147, 'random_state': 888, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5335797020973163, 'learning_rate': 0.053611754990371865, 'max_depth': 53, 'min_child_samples': 22, 'min_split_gain': 0.0003097151868348969, 'n_estimators': 918, 'num_leaves': 844, 'reg_alpha': 3.3694109994877466, 'reg_lambda': 9.851018150594369, 'subsample': 0.6168484022365105, 'random_state': 761, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5734274437889891, 'learning_rate': 0.04146776008874663, 'max_depth': 13, 'min_child_samples': 59, 'min_split_gain': 0.0024960224279475273, 'n_estimators': 693, 'num_leaves': 911, 'reg_alpha': 5.6430435653710145, 'reg_lambda': 2.22060897184505e-08, 'subsample': 0.6492311467459078, 'random_state': 377, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5190312316093904, 'learning_rate': 0.07227095334093186, 'max_depth': 55, 'min_child_samples': 21, 'min_split_gain': 0.016851299017158358, 'n_estimators': 858, 'num_leaves': 893, 'reg_alpha': 4.215311373248343, 'reg_lambda': 6.335716545696073, 'subsample': 0.8204594741901514, 'random_state': 866, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5274895673020713, 'learning_rate': 0.05747186247763484, 'max_depth': 16, 'min_child_samples': 59, 'min_split_gain': 0.00018311472541075247, 'n_estimators': 874, 'num_leaves': 987, 'reg_alpha': 9.388961921189143, 'reg_lambda': 1.0055346054976132e-08, 'subsample': 0.8636918845819727, 'random_state': 481, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5243321100850568, 'learning_rate': 0.050751953832934, 'max_depth': 42, 'min_child_samples': 22, 'min_split_gain': 0.0004742717300235638, 'n_estimators': 860, 'num_leaves': 919, 'reg_alpha': 6.318712692486671, 'reg_lambda': 6.16351360697893, 'subsample': 0.8842348488140355, 'random_state': 504, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5171023196306238, 'learning_rate': 0.0651217726542331, 'max_depth': 54, 'min_child_samples': 37, 'min_split_gain': 0.0013309480321743146, 'n_estimators': 893, 'num_leaves': 148, 'reg_alpha': 2.2173454314246306, 'reg_lambda': 9.401273383442003, 'subsample': 0.5890446207595045, 'random_state': 575, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5169056470393405, 'learning_rate': 0.07236682920446781, 'max_depth': 56, 'min_child_samples': 34, 'min_split_gain': 0.014986947934425, 'n_estimators': 899, 'num_leaves': 897, 'reg_alpha': 3.7688929761392402, 'reg_lambda': 6.556601144085903, 'subsample': 0.8247865839737977, 'random_state': 865, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5090457160353795, 'learning_rate': 0.056021162776750164, 'max_depth': 58, 'min_child_samples': 25, 'min_split_gain': 8.171038390110798e-05, 'n_estimators': 857, 'num_leaves': 929, 'reg_alpha': 4.721628673068319, 'reg_lambda': 9.971101043925685, 'subsample': 0.5518706500556674, 'random_state': 651, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5662130806326541, 'learning_rate': 0.07607350535939446, 'max_depth': 47, 'min_child_samples': 35, 'min_split_gain': 0.0006845067851606668, 'n_estimators': 875, 'num_leaves': 899, 'reg_alpha': 4.856657815276274, 'reg_lambda': 2.6230232531911523e-08, 'subsample': 0.8643211209068412, 'random_state': 464, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.55780632074205, 'learning_rate': 0.03856522502093635, 'max_depth': 63, 'min_child_samples': 39, 'min_split_gain': 0.00024764772628177575, 'n_estimators': 945, 'num_leaves': 819, 'reg_alpha': 4.973060653833003, 'reg_lambda': 1.7882703713508816, 'subsample': 0.5092849515126023, 'random_state': 789, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5133435092942498, 'learning_rate': 0.04730570093310139, 'max_depth': 58, 'min_child_samples': 19, 'min_split_gain': 0.0004719682308533021, 'n_estimators': 897, 'num_leaves': 906, 'reg_alpha': 8.005729679980057e-05, 'reg_lambda': 9.629022574840093, 'subsample': 0.560751907344925, 'random_state': 667, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5395848926682614, 'learning_rate': 0.044895477784841265, 'max_depth': 57, 'min_child_samples': 19, 'min_split_gain': 0.00016679541562621136, 'n_estimators': 845, 'num_leaves': 914, 'reg_alpha': 3.401941840893713, 'reg_lambda': 1.20933120743662e-07, 'subsample': 0.9677186088914039, 'random_state': 956, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5382514372015093, 'learning_rate': 0.06485765146168458, 'max_depth': 17, 'min_child_samples': 45, 'min_split_gain': 0.00012999848271384013, 'n_estimators': 529, 'num_leaves': 949, 'reg_alpha': 0.0011925585369433828, 'reg_lambda': 2.7941705870565744e-08, 'subsample': 0.8241762628011553, 'random_state': 380, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5134479789943638, 'learning_rate': 0.052868747521113424, 'max_depth': 16, 'min_child_samples': 39, 'min_split_gain': 0.0013914607022652622, 'n_estimators': 917, 'num_leaves': 255, 'reg_alpha': 1.2952057659379401, 'reg_lambda': 5.741846404719004, 'subsample': 0.8260818285626025, 'random_state': 492, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5229945121367208, 'learning_rate': 0.06505464016439833, 'max_depth': 55, 'min_child_samples': 38, 'min_split_gain': 0.00020180054827743607, 'n_estimators': 870, 'num_leaves': 873, 'reg_alpha': 0.4640205452483775, 'reg_lambda': 4.204173089834077, 'subsample': 0.6519018633891417, 'random_state': 497, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.6571585170261566, 'learning_rate': 0.0491994589897865, 'max_depth': 40, 'min_child_samples': 62, 'min_split_gain': 0.0015392986861338489, 'n_estimators': 919, 'num_leaves': 920, 'reg_alpha': 6.096876205550838, 'reg_lambda': 5.378757906139428, 'subsample': 0.8833365529300939, 'random_state': 506, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.5187301986780853, 'learning_rate': 0.0506142549500622, 'max_depth': 52, 'min_child_samples': 26, 'min_split_gain': 0.0005452897700963357, 'n_estimators': 862, 'num_leaves': 892, 'reg_alpha': 4.515348971897004, 'reg_lambda': 9.724891739174708e-08, 'subsample': 0.5776560640457273, 'random_state': 695, 'n_jobs': -1, 'verbosity': -1}, {'colsample_bytree': 0.524357444597027, 'learning_rate': 0.06464752625994903, 'max_depth': 42, 'min_child_samples': 38, 'min_split_gain': 0.0017409454022606031, 'n_estimators': 856, 'num_leaves': 858, 'reg_alpha': 1.3975085450630336, 'reg_lambda': 6.624803112950276, 'subsample': 0.824677358947841, 'random_state': 500, 'n_jobs': -1, 'verbosity': -1}]\n"
     ]
    }
   ],
   "source": [
    "print(final_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11a3b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([df_train, df_val], ignore_index=True)[feature_cols]\n",
    "y_full = pd.concat([df_train[target_col], df_val[target_col]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48913620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo 1/50...\n",
      "Entrenando modelo 5/50...\n",
      "Entrenando modelo 2/50...\n",
      "Entrenando modelo 3/50...\n",
      "Entrenando modelo 7/50...\n",
      "Entrenando modelo 11/50...\n",
      "Entrenando modelo 13/50...\n",
      "Entrenando modelo 12/50...\n",
      "Entrenando modelo 16/50...\n",
      "Entrenando modelo 9/50...\n",
      "Entrenando modelo 4/50...\n",
      "Entrenando modelo 6/50...\n",
      "Entrenando modelo 8/50...\n",
      "Entrenando modelo 15/50...\n",
      "Entrenando modelo 10/50...\n",
      "Entrenando modelo 14/50...\n",
      "Entrenando modelo 17/50...\n",
      "Entrenando modelo 18/50...\n",
      "Entrenando modelo 19/50...\n",
      "Entrenando modelo 20/50...\n",
      "Entrenando modelo 21/50...\n",
      "Entrenando modelo 22/50...\n",
      "Entrenando modelo 23/50...\n",
      "Entrenando modelo 24/50...\n",
      "Entrenando modelo 25/50...\n",
      "Entrenando modelo 26/50...\n",
      "Entrenando modelo 27/50...\n",
      "Entrenando modelo 28/50...\n",
      "Entrenando modelo 29/50...\n",
      "Entrenando modelo 30/50...\n",
      "Entrenando modelo 31/50...\n",
      "Entrenando modelo 32/50...\n",
      "Entrenando modelo 33/50...\n",
      "Entrenando modelo 34/50...\n",
      "Entrenando modelo 35/50...\n",
      "Entrenando modelo 36/50...\n",
      "Entrenando modelo 37/50...\n",
      "Entrenando modelo 38/50...\n",
      "Entrenando modelo 39/50...\n",
      "Entrenando modelo 40/50...\n",
      "Entrenando modelo 41/50...\n",
      "Entrenando modelo 42/50...\n",
      "Entrenando modelo 43/50...\n",
      "Entrenando modelo 44/50...\n",
      "Entrenando modelo 45/50...\n",
      "Entrenando modelo 46/50...\n",
      "Entrenando modelo 47/50...\n",
      "Entrenando modelo 48/50...\n",
      "Entrenando modelo 49/50...\n",
      "Entrenando modelo 50/50...\n",
      "['Modelo 1 terminado', 'Modelo 2 terminado', 'Modelo 3 terminado', 'Modelo 4 terminado', 'Modelo 5 terminado', 'Modelo 6 terminado', 'Modelo 7 terminado', 'Modelo 8 terminado', 'Modelo 9 terminado', 'Modelo 10 terminado', 'Modelo 11 terminado', 'Modelo 12 terminado', 'Modelo 13 terminado', 'Modelo 14 terminado', 'Modelo 15 terminado', 'Modelo 16 terminado', 'Modelo 17 terminado', 'Modelo 18 terminado', 'Modelo 19 terminado', 'Modelo 20 terminado', 'Modelo 21 terminado', 'Modelo 22 terminado', 'Modelo 23 terminado', 'Modelo 24 terminado', 'Modelo 25 terminado', 'Modelo 26 terminado', 'Modelo 27 terminado', 'Modelo 28 terminado', 'Modelo 29 terminado', 'Modelo 30 terminado', 'Modelo 31 terminado', 'Modelo 32 terminado', 'Modelo 33 terminado', 'Modelo 34 terminado', 'Modelo 35 terminado', 'Modelo 36 terminado', 'Modelo 37 terminado', 'Modelo 38 terminado', 'Modelo 39 terminado', 'Modelo 40 terminado', 'Modelo 41 terminado', 'Modelo 42 terminado', 'Modelo 43 terminado', 'Modelo 44 terminado', 'Modelo 45 terminado', 'Modelo 46 terminado', 'Modelo 47 terminado', 'Modelo 48 terminado', 'Modelo 49 terminado', 'Modelo 50 terminado']\n",
      "Â¡Entrenamiento y guardado de los 50 modelos finalizado!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def train_and_save_model(i, params, X_full, y_full):\n",
    "    print(f\"Entrenando modelo {i+1}/50...\")\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_full, y_full)\n",
    "    joblib.dump(model, f'lgbm_model_{i+1:02d}.pkl')\n",
    "    return f\"Modelo {i+1} terminado\"\n",
    "\n",
    "results = joblib.Parallel(n_jobs=20)(\n",
    "    joblib.delayed(train_and_save_model)(i, params, X_full, y_full)\n",
    "    for i, params in enumerate(final_configs[:50])\n",
    ")\n",
    "\n",
    "print(results)\n",
    "print(\"Â¡Entrenamiento y guardado de los 50 modelos finalizado!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "239c4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Cargar los 50 modelos LightGBM entrenados\n",
    "lgbm_models = []\n",
    "for i in range(1, 51):\n",
    "    model = joblib.load(f'lgbm_model_{i:02d}.pkl')\n",
    "    lgbm_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54e148e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciendo LightGBM 1/50...\n",
      "Prediciendo LightGBM 2/50...\n",
      "Prediciendo LightGBM 3/50...\n",
      "Prediciendo LightGBM 4/50...\n",
      "Prediciendo LightGBM 5/50...\n",
      "Prediciendo LightGBM 6/50...\n",
      "Prediciendo LightGBM 7/50...\n",
      "Prediciendo LightGBM 8/50...\n",
      "Prediciendo LightGBM 9/50...\n",
      "Prediciendo LightGBM 10/50...\n",
      "Prediciendo LightGBM 11/50...\n",
      "Prediciendo LightGBM 12/50...\n",
      "Prediciendo LightGBM 13/50...\n",
      "Prediciendo LightGBM 14/50...\n",
      "Prediciendo LightGBM 15/50...\n",
      "Prediciendo LightGBM 16/50...\n",
      "Prediciendo LightGBM 17/50...\n",
      "Prediciendo LightGBM 18/50...\n",
      "Prediciendo LightGBM 19/50...\n",
      "Prediciendo LightGBM 20/50...\n",
      "Prediciendo LightGBM 21/50...\n",
      "Prediciendo LightGBM 22/50...\n",
      "Prediciendo LightGBM 23/50...\n",
      "Prediciendo LightGBM 24/50...\n",
      "Prediciendo LightGBM 25/50...\n",
      "Prediciendo LightGBM 26/50...\n",
      "Prediciendo LightGBM 27/50...\n",
      "Prediciendo LightGBM 28/50...\n",
      "Prediciendo LightGBM 29/50...\n",
      "Prediciendo LightGBM 30/50...\n",
      "Prediciendo LightGBM 31/50...\n",
      "Prediciendo LightGBM 32/50...\n",
      "Prediciendo LightGBM 33/50...\n",
      "Prediciendo LightGBM 34/50...\n",
      "Prediciendo LightGBM 35/50...\n",
      "Prediciendo LightGBM 36/50...\n",
      "Prediciendo LightGBM 37/50...\n",
      "Prediciendo LightGBM 38/50...\n",
      "Prediciendo LightGBM 39/50...\n",
      "Prediciendo LightGBM 40/50...\n",
      "Prediciendo LightGBM 41/50...\n",
      "Prediciendo LightGBM 42/50...\n",
      "Prediciendo LightGBM 43/50...\n",
      "Prediciendo LightGBM 44/50...\n",
      "Prediciendo LightGBM 45/50...\n",
      "Prediciendo LightGBM 46/50...\n",
      "Prediciendo LightGBM 47/50...\n",
      "Prediciendo LightGBM 48/50...\n",
      "Prediciendo LightGBM 49/50...\n",
      "Prediciendo LightGBM 50/50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_656/4155869407.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[f'lgbm_pred_LOG1P_Z_{i+1}'] = lgbm_preds[:, i]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- PredicciÃ³n LightGBM (50 modelos) ---\n",
    "lgbm_preds = []\n",
    "\n",
    "feature_cols = [col for col in df_pred.columns if col != target_col]\n",
    "\n",
    "# Asegurarse de que las columnas categÃ³ricas tengan el mismo dtype que en el entrenamiento\n",
    "for col in cat_cols:\n",
    "    if col in df_pred.columns and col in df_train.columns:\n",
    "        df_pred[col] = df_pred[col].astype(df_train[col].dtype)\n",
    "\n",
    "X_pred_lgbm = df_pred[feature_cols]  \n",
    "\n",
    "for i, model in enumerate(lgbm_models):\n",
    "    print(f\"Prediciendo LightGBM {i+1}/50...\")\n",
    "    preds = model.predict(X_pred_lgbm)\n",
    "    lgbm_preds.append(preds)\n",
    "\n",
    "lgbm_preds = np.stack(lgbm_preds).T  # shape (N, 50)\n",
    "\n",
    "# --- Agregar predicciones LGBM al DataFrame existente con resultados de MLP ---\n",
    "for i in range(50):\n",
    "    df_preds_final[f'lgbm_pred_LOG1P_Z_{i+1}'] = lgbm_preds[:, i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f88f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERIODO', 'MES_SIN', 'MES_COS', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'PRODUCT_ID', 'MES_PROBLEMATICO', 'IS_FEBRERO', 'ESTOY_PREDICIENDO_FEBRERO', 'CAIDA_ABRUPTA', 'CLASE_LOG1P_Z', 'PRODUCT_RANK_BIN', 'ORDINAL_Z', 'ANIO_Z', 'MES_Z', 'TRIMESTRE_Z', 'PRODUCT_ID_Z', 'TN_Z', 'PROM_ULT_3_FEBREROS_Z', 'DIF_TN_VS_FEBREROS_ULT_3_Z', 'TN_MAX_HISTORICO_Z', 'TN_DIST_A_MAX_HIST_Z', 'TN_RATIO_VS_MAX_HIST_Z', 'TN_MEAN_03_Z', 'PENDIENTE_TENDENCIA_3_Z', 'TN_EWMA_03_Z', 'TN_MEDIAN_03_Z', 'TN_MIN_03_Z', 'TN_MAX_03_Z', 'TN_STD_03_Z', 'TN_SKEW_03_Z', 'TN_KURT_03_Z', 'TN_GROWTH_03_Z', 'TN_IQR_03_Z', 'TN_SUM_03_Z', 'TN_COUNT_POS_03_Z', 'TN_PCT_ZERO_03_Z', 'TN_LAST_03_Z', 'TN_LAST_DIFF_03_Z', 'TN_COEF_VAR_3_Z', 'TN_MAXMIN_RATIO_3_Z', 'TN_RANGO_3_Z', 'TN_RANGO_REL_3_Z', 'TN_LAST_VS_MEDIAN_3_Z', 'TN_CHANGE_PREV_WINDOW_3_Z', 'TN_ZEROS_END_3_Z', 'TN_LAST_PCT_SUM_3_Z', 'TN_PCT90_3_Z', 'TN_PCT10_3_Z', 'TN_PCT_WIDTH_3_Z', 'TN_MINUS_MEAN_03_Z', 'TN_MINUS_MEDIAN_03_Z', 'TN_MINUS_EWMA_03_Z', 'TN_OVER_MEAN_03_Z', 'TN_OVER_MEDIAN_03_Z', 'TN_OVER_EWMA_03_Z', 'TN_MEAN_06_Z', 'PENDIENTE_TENDENCIA_6_Z', 'TN_EWMA_06_Z', 'TN_MEDIAN_06_Z', 'TN_MIN_06_Z', 'TN_MAX_06_Z', 'TN_STD_06_Z', 'TN_SKEW_06_Z', 'TN_KURT_06_Z', 'TN_GROWTH_06_Z', 'TN_IQR_06_Z', 'TN_SUM_06_Z', 'TN_COUNT_POS_06_Z', 'TN_PCT_ZERO_06_Z', 'TN_LAST_06_Z', 'TN_LAST_DIFF_06_Z', 'TN_COEF_VAR_6_Z', 'TN_MAXMIN_RATIO_6_Z', 'TN_RANGO_6_Z', 'TN_RANGO_REL_6_Z', 'TN_LAST_VS_MEDIAN_6_Z', 'TN_CHANGE_PREV_WINDOW_6_Z', 'TN_ZEROS_END_6_Z', 'TN_LAST_PCT_SUM_6_Z', 'TN_PCT90_6_Z', 'TN_PCT10_6_Z', 'TN_PCT_WIDTH_6_Z', 'TN_MINUS_MEAN_06_Z', 'TN_MINUS_MEDIAN_06_Z', 'TN_MINUS_EWMA_06_Z', 'TN_OVER_MEAN_06_Z', 'TN_OVER_MEDIAN_06_Z', 'TN_OVER_EWMA_06_Z', 'TN_MEAN_09_Z', 'PENDIENTE_TENDENCIA_9_Z', 'TN_EWMA_09_Z', 'TN_MEDIAN_09_Z', 'TN_MIN_09_Z', 'TN_MAX_09_Z', 'TN_STD_09_Z', 'TN_SKEW_09_Z', 'TN_KURT_09_Z', 'TN_GROWTH_09_Z', 'TN_IQR_09_Z', 'TN_SUM_09_Z', 'TN_COUNT_POS_09_Z', 'TN_PCT_ZERO_09_Z', 'TN_LAST_09_Z', 'TN_LAST_DIFF_09_Z', 'TN_COEF_VAR_9_Z', 'TN_MAXMIN_RATIO_9_Z', 'TN_RANGO_9_Z', 'TN_RANGO_REL_9_Z', 'TN_LAST_VS_MEDIAN_9_Z', 'TN_CHANGE_PREV_WINDOW_9_Z', 'TN_ZEROS_END_9_Z', 'TN_LAST_PCT_SUM_9_Z', 'TN_PCT90_9_Z', 'TN_PCT10_9_Z', 'TN_PCT_WIDTH_9_Z', 'TN_MINUS_MEAN_09_Z', 'TN_MINUS_MEDIAN_09_Z', 'TN_MINUS_EWMA_09_Z', 'TN_OVER_MEAN_09_Z', 'TN_OVER_MEDIAN_09_Z', 'TN_OVER_EWMA_09_Z', 'TN_MEAN_12_Z', 'PENDIENTE_TENDENCIA_12_Z', 'TN_EWMA_12_Z', 'TN_MEDIAN_12_Z', 'TN_MIN_12_Z', 'TN_MAX_12_Z', 'TN_STD_12_Z', 'TN_SKEW_12_Z', 'TN_KURT_12_Z', 'TN_GROWTH_12_Z', 'TN_IQR_12_Z', 'TN_SUM_12_Z', 'TN_COUNT_POS_12_Z', 'TN_PCT_ZERO_12_Z', 'TN_LAST_12_Z', 'TN_LAST_DIFF_12_Z', 'TN_COEF_VAR_12_Z', 'TN_MAXMIN_RATIO_12_Z', 'TN_RANGO_12_Z', 'TN_RANGO_REL_12_Z', 'TN_LAST_VS_MEDIAN_12_Z', 'TN_CHANGE_PREV_WINDOW_12_Z', 'TN_ZEROS_END_12_Z', 'TN_LAST_PCT_SUM_12_Z', 'TN_PCT90_12_Z', 'TN_PCT10_12_Z', 'TN_PCT_WIDTH_12_Z', 'TN_MINUS_MEAN_12_Z', 'TN_MINUS_MEDIAN_12_Z', 'TN_MINUS_EWMA_12_Z', 'TN_OVER_MEAN_12_Z', 'TN_OVER_MEDIAN_12_Z', 'TN_OVER_EWMA_12_Z', 'TN_MEAN_18_Z', 'PENDIENTE_TENDENCIA_18_Z', 'TN_EWMA_18_Z', 'TN_MEDIAN_18_Z', 'TN_MIN_18_Z', 'TN_MAX_18_Z', 'TN_STD_18_Z', 'TN_SKEW_18_Z', 'TN_KURT_18_Z', 'TN_GROWTH_18_Z', 'TN_IQR_18_Z', 'TN_SUM_18_Z', 'TN_COUNT_POS_18_Z', 'TN_PCT_ZERO_18_Z', 'TN_LAST_18_Z', 'TN_LAST_DIFF_18_Z', 'TN_COEF_VAR_18_Z', 'TN_MAXMIN_RATIO_18_Z', 'TN_RANGO_18_Z', 'TN_RANGO_REL_18_Z', 'TN_LAST_VS_MEDIAN_18_Z', 'TN_CHANGE_PREV_WINDOW_18_Z', 'TN_ZEROS_END_18_Z', 'TN_LAST_PCT_SUM_18_Z', 'TN_PCT90_18_Z', 'TN_PCT10_18_Z', 'TN_PCT_WIDTH_18_Z', 'TN_MINUS_MEAN_18_Z', 'TN_MINUS_MEDIAN_18_Z', 'TN_MINUS_EWMA_18_Z', 'TN_OVER_MEAN_18_Z', 'TN_OVER_MEDIAN_18_Z', 'TN_OVER_EWMA_18_Z', 'TN_LAG_03_Z', 'TN_DELTA_03_Z', 'TN_LAG_06_Z', 'TN_DELTA_06_Z', 'TN_LAG_09_Z', 'TN_DELTA_09_Z', 'TN_LAG_12_Z', 'TN_DELTA_12_Z', 'TN_LAG_18_Z', 'TN_DELTA_18_Z', 'TN_COUNT_ZERO_03_Z', 'TN_SIGN_CHANGES_03_Z', 'TN_COEF_VAR_03_Z', 'TN_OUTLIER_PROP_03_Z', 'TN_PCT_POS_03_Z', 'TN_COUNT_ZERO_06_Z', 'TN_SIGN_CHANGES_06_Z', 'TN_COEF_VAR_06_Z', 'TN_OUTLIER_PROP_06_Z', 'TN_PCT_POS_06_Z', 'TN_COUNT_ZERO_09_Z', 'TN_SIGN_CHANGES_09_Z', 'TN_COEF_VAR_09_Z', 'TN_OUTLIER_PROP_09_Z', 'TN_PCT_POS_09_Z', 'TN_COUNT_ZERO_12_Z', 'TN_SIGN_CHANGES_12_Z', 'TN_OUTLIER_PROP_12_Z', 'TN_PCT_POS_12_Z', 'TN_COUNT_ZERO_18_Z', 'TN_SIGN_CHANGES_18_Z', 'TN_OUTLIER_PROP_18_Z', 'TN_PCT_POS_18_Z']\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d1c3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PRODUCT_ID  mlp_pred_LOG1P_Z_1  mlp_pred_LOG1P_Z_2  mlp_pred_LOG1P_Z_3  mlp_pred_LOG1P_Z_4  mlp_pred_LOG1P_Z_5  mlp_pred_LOG1P_Z_6  mlp_pred_LOG1P_Z_7  mlp_pred_LOG1P_Z_8  mlp_pred_LOG1P_Z_9  mlp_pred_LOG1P_Z_10  mlp_pred_LOG1P_Z_11  mlp_pred_LOG1P_Z_12  mlp_pred_LOG1P_Z_13  mlp_pred_LOG1P_Z_14  mlp_pred_LOG1P_Z_15  mlp_pred_LOG1P_Z_16  mlp_pred_LOG1P_Z_17  mlp_pred_LOG1P_Z_18  mlp_pred_LOG1P_Z_19  mlp_pred_LOG1P_Z_20  mlp_pred_LOG1P_Z_21  mlp_pred_LOG1P_Z_22  mlp_pred_LOG1P_Z_23  mlp_pred_LOG1P_Z_24  mlp_pred_LOG1P_Z_25  mlp_pred_LOG1P_Z_26  mlp_pred_LOG1P_Z_27  mlp_pred_LOG1P_Z_28  mlp_pred_LOG1P_Z_29  mlp_pred_LOG1P_Z_30  mlp_pred_LOG1P_Z_31  mlp_pred_LOG1P_Z_32  mlp_pred_LOG1P_Z_33  mlp_pred_LOG1P_Z_34  mlp_pred_LOG1P_Z_35  mlp_pred_LOG1P_Z_36  mlp_pred_LOG1P_Z_37  mlp_pred_LOG1P_Z_38  mlp_pred_LOG1P_Z_39  mlp_pred_LOG1P_Z_40  mlp_pred_LOG1P_Z_41  mlp_pred_LOG1P_Z_42  mlp_pred_LOG1P_Z_43  mlp_pred_LOG1P_Z_44  mlp_pred_LOG1P_Z_45  mlp_pred_LOG1P_Z_46  mlp_pred_LOG1P_Z_47  mlp_pred_LOG1P_Z_48  mlp_pred_LOG1P_Z_49  mlp_pred_LOG1P_Z_50  lgbm_pred_LOG1P_Z_1  lgbm_pred_LOG1P_Z_2  lgbm_pred_LOG1P_Z_3  lgbm_pred_LOG1P_Z_4  lgbm_pred_LOG1P_Z_5  lgbm_pred_LOG1P_Z_6  lgbm_pred_LOG1P_Z_7  lgbm_pred_LOG1P_Z_8  lgbm_pred_LOG1P_Z_9  lgbm_pred_LOG1P_Z_10  lgbm_pred_LOG1P_Z_11  lgbm_pred_LOG1P_Z_12  lgbm_pred_LOG1P_Z_13  lgbm_pred_LOG1P_Z_14  lgbm_pred_LOG1P_Z_15  lgbm_pred_LOG1P_Z_16  lgbm_pred_LOG1P_Z_17  lgbm_pred_LOG1P_Z_18  lgbm_pred_LOG1P_Z_19  lgbm_pred_LOG1P_Z_20  lgbm_pred_LOG1P_Z_21  lgbm_pred_LOG1P_Z_22  lgbm_pred_LOG1P_Z_23  lgbm_pred_LOG1P_Z_24  lgbm_pred_LOG1P_Z_25  lgbm_pred_LOG1P_Z_26  lgbm_pred_LOG1P_Z_27  lgbm_pred_LOG1P_Z_28  lgbm_pred_LOG1P_Z_29  lgbm_pred_LOG1P_Z_30  lgbm_pred_LOG1P_Z_31  lgbm_pred_LOG1P_Z_32  lgbm_pred_LOG1P_Z_33  lgbm_pred_LOG1P_Z_34  lgbm_pred_LOG1P_Z_35  lgbm_pred_LOG1P_Z_36  lgbm_pred_LOG1P_Z_37  lgbm_pred_LOG1P_Z_38  lgbm_pred_LOG1P_Z_39  lgbm_pred_LOG1P_Z_40  lgbm_pred_LOG1P_Z_41  lgbm_pred_LOG1P_Z_42  lgbm_pred_LOG1P_Z_43  lgbm_pred_LOG1P_Z_44  lgbm_pred_LOG1P_Z_45  lgbm_pred_LOG1P_Z_46  lgbm_pred_LOG1P_Z_47  lgbm_pred_LOG1P_Z_48  lgbm_pred_LOG1P_Z_49  lgbm_pred_LOG1P_Z_50\n",
      "0        20001              3.3112              3.7873              2.5869              2.5227              3.0096              3.4089              4.0206              2.6809              3.2068               3.3151               3.7040               3.3659               3.6678               2.9816               3.7408               2.7755               2.5239               3.5820               3.1835               2.8451               3.0998               2.6710               3.2188               3.1715               3.5924               4.0483               3.2025               4.4497               3.4430               3.4615               3.6929               3.6429               2.4279               3.5640               4.2474               3.6136               3.3809               3.1565               3.4720               3.4411               3.4519               2.7296               3.6570               3.3814               1.7822               2.9102               2.8732               2.9969               3.3712               2.8768               2.6829               2.6757               2.6600               2.6207               2.6198               2.8190               2.6487               2.6149               2.6637                2.7242                2.6857                2.7801                2.8009                2.6997                2.6948                2.6671                2.7930                2.6367                2.8651                2.6569                2.6561                2.6961                2.6478                2.6678                2.6513                2.6519                2.7238                2.6441                2.8386                2.7297                2.7227                2.6452                2.7148                2.6095                2.6681                2.6010                2.6201                2.7551                2.6925                2.6631                2.6187                2.6750                2.8548                2.6628                2.8808                2.7391                2.7971                2.6345                2.6370                2.7710\n",
      "1        20002              2.2490              3.3892              2.3997              2.3587              2.5948              2.5559              3.0043              2.4961              2.9181               2.6303               3.0639               2.6379               2.6091               2.7172               3.3138               2.2647               2.1043               2.9216               2.5053               2.5001               2.5018               2.2901               2.8300               2.8724               2.7373               3.3927               2.6638               3.0866               2.8765               3.1230               3.0441               3.1353               2.0981               3.1266               3.3252               3.2027               2.8161               2.7683               2.9402               2.5292               2.1994               2.1356               2.8097               2.7495               2.5175               2.2424               2.5279               2.4068               2.8530               2.3010               2.7139               2.6860               2.6764               2.6626               2.6273               2.7671               2.6280               2.6316               2.6816                2.7233                2.7171                2.7174                2.7196                2.7106                2.7091                2.6625                2.7406                2.6566                2.7138                2.6717                2.6653                2.6931                2.6246                2.6863                2.6475                2.6671                2.7045                2.6790                2.7359                2.7284                2.6807                2.6588                2.6647                2.6329                2.6715                2.6075                2.6884                2.7197                2.7043                2.6940                2.6515                2.6829                2.7454                2.7112                2.7524                2.7817                2.7703                2.6460                2.6607                2.7346\n",
      "2        20003              2.6467              2.8577              2.0408              1.7278              2.3668              2.5065              2.9698              2.2507              2.6377               2.5599               2.8479               2.6176               2.8815               2.2237               2.8796               2.2111               2.0268               2.7763               2.5351               2.2265               2.5698               2.3459               2.4928               2.5934               2.6708               3.1684               2.4850               2.9959               2.6435               2.6856               2.5727               2.9198               1.6670               2.9341               2.8766               2.7090               2.6406               2.4827               2.8787               2.3799               2.4154               2.1095               2.4550               2.5215               1.7458               2.2201               2.3102               2.0761               2.6761               2.2655               2.5057               2.5178               2.4745               2.5198               2.5065               2.4912               2.4838               2.4581               2.5041                2.4379                2.5030                2.5301                2.5394                2.4984                2.5087                2.5168                2.5243                2.4943                2.5198                2.4991                2.5129                2.4847                2.4756                2.5075                2.4665                2.5003                2.5331                2.5037                2.5927                2.4557                2.5081                2.4910                2.4817                2.5003                2.5378                2.4923                2.5044                2.4341                2.5033                2.4973                2.4863                2.4905                2.4912                2.5232                2.5837                2.5040                2.4669                2.4638                2.4955                2.5591\n",
      "3        20004              1.9102              2.3475              1.8013              1.5883              2.0129              1.9554              2.3767              1.8346              2.2277               2.1459               2.2963               2.0975               2.1333               1.8113               2.4533               1.7360               1.6777               2.1303               2.0747               1.8525               1.8873               1.8864               2.1127               2.2048               2.1907               2.5706               2.1650               2.2286               2.1691               2.1991               2.0246               2.2595               1.4281               2.2892               2.3817               2.3922               2.1298               2.0574               2.3108               1.7418               1.7792               1.7344               2.0580               2.0804               1.7571               1.7558               1.9113               1.6107               2.1400               1.7893               2.3265               2.3553               2.2663               2.2578               2.2969               2.2104               2.3058               2.3022               2.2970                2.2822                2.2636                2.2995                2.3029                2.3037                2.2855                2.3099                2.2791                2.3012                2.2989                2.3125                2.3204                2.3119                2.2869                2.2921                2.2902                2.3277                2.3218                2.3083                2.2193                2.2786                2.2595                2.2997                2.2880                2.2938                2.2832                2.3416                2.3064                2.2814                2.2888                2.2639                2.2865                2.2888                2.3057                2.3153                2.1785                2.3060                2.3018                2.2919                2.2658                2.2472\n",
      "4        20005              2.1855              2.5290              1.9806              1.5892              2.2210              2.1373              2.6113              2.0223              2.4076               2.3451               2.5701               2.2610               2.2290               2.0452               2.6126               1.8060               1.8089               2.3051               2.2174               2.0816               1.9471               2.1748               2.2551               2.2799               2.4594               2.8473               2.3367               2.3576               2.4741               2.3329               2.1447               2.4685               1.6109               2.4788               2.6383               2.6845               2.3454               2.2463               2.4736               1.7873               1.9336               1.9226               2.2584               2.2451               1.7290               1.9306               2.0797               1.7774               2.2060               1.9399               2.3371               2.3818               2.3646               2.3172               2.3544               2.3395               2.3668               2.3354               2.3354                2.3470                2.3174                2.3694                2.3116                2.3712                2.3514                2.3411                2.3939                2.3242                2.3987                2.3371                2.3528                2.3464                2.2808                2.3635                2.3074                2.3542                2.3259                2.3995                2.3265                2.3243                2.3550                2.3304                2.2990                2.3566                2.3408                2.3650                2.3186                2.3687                2.3168                2.3180                2.3041                2.3581                2.3632                2.3121                2.3214                2.3597                2.3584                2.3458                2.3661                2.3254\n",
      "5        20006              1.9153              1.9511              1.6991              1.6725              1.9919              2.0401              2.2732              1.9005              2.1041               2.1744               2.1614               2.0071               2.3058               1.7529               2.1982               1.7937               1.6725               2.0522               1.9941               1.7936               2.0520               1.8750               1.9449               2.0466               2.0803               2.2672               2.0379               2.1165               2.1098               2.0564               1.9922               2.0778               1.8567               2.2487               2.2944               2.3210               2.1811               1.8951               2.0776               1.8162               1.7540               1.7118               2.0516               1.9561               1.4786               1.8395               1.8645               1.7166               2.0100               1.8197               2.1479               2.1333               2.1881               2.1410               2.1309               2.1307               2.1534               2.1475               2.1578                2.1562                2.1305                2.1249                2.1247                2.1364                2.1472                2.1693                2.0941                2.1654                2.0808                2.1459                2.1577                2.1451                2.1506                2.1853                2.1325                2.1595                2.1696                2.1431                2.1472                2.1091                2.1305                2.1473                2.1487                2.1795                2.1396                2.1637                2.1650                2.1550                2.1398                2.1459                2.1563                2.1545                2.0844                2.1686                2.1801                2.0798                2.1420                2.1353                2.1685                2.1206\n",
      "6        20007              1.9493              2.0240              1.7891              1.6065              1.9354              1.9977              2.1919              1.9698              2.0636               2.2050               2.2172               2.0631               2.2551               1.7709               2.2317               1.8785               1.6964               2.1200               1.9382               1.8669               1.9916               1.9404               1.9778               2.1473               2.1013               2.2951               2.0280               2.0887               2.1066               2.0830               1.9474               2.0461               2.0010               2.1658               2.2618               2.2415               2.1642               1.8592               2.0090               1.8234               1.8443               1.7065               2.0375               1.9491               1.6040               1.7713               1.9014               1.7219               1.9895               1.8459               2.1591               2.1364               2.1753               2.1100               2.0878               2.0699               2.1183               2.1228               2.1038                2.0436                2.1232                2.1018                2.1246                2.0902                2.1062                2.1471                2.1272                2.1005                2.0948                2.1193                2.1123                2.1226                2.1125                2.1267                2.1090                2.1367                2.1335                2.1147                2.0474                2.0928                2.1365                2.1043                2.1194                2.1441                2.1037                2.1039                2.1152                2.1176                2.0932                2.1207                2.0837                2.1208                2.1061                2.1115                2.1372                2.1421                2.1753                2.0943                2.0901                2.1352\n",
      "7        20008              1.6947              1.9234              2.0256              1.7723              2.0370              1.9738              1.8829              2.0646              2.1607               2.0814               2.3185               1.9507               2.0427               1.9145               2.3783               2.2274               1.5891               2.1923               1.7979               1.8180               1.8180               1.9531               2.0045               2.0455               1.9479               2.3547               2.1369               2.0082               2.1095               2.3548               1.8569               1.9576               1.8009               2.0661               2.0574               2.2455               1.9677               1.8875               2.0238               1.5586               1.5173               1.6943               2.0361               2.0150               1.3893               1.4789               1.8477               1.7360               1.8887               1.7993               1.9620               1.9518               2.0005               1.9451               1.9309               2.0246               1.9347               1.9419               1.9218                1.8843                1.9742                2.0045                1.9775                1.9768                1.9609                1.9601                1.9645                1.9058                2.0103                1.9363                1.9677                2.0190                1.9503                1.9407                1.9511                1.9945                1.9161                1.8737                2.0439                2.0084                2.0380                1.9206                1.9298                1.9654                1.9459                1.9172                1.8908                1.9768                1.9008                1.9122                1.9495                1.9443                1.9462                1.9276                2.1199                2.0452                2.0059                1.9594                1.9159                2.0193\n",
      "8        20009              1.9323              2.2695              1.9928              1.9326              1.9569              2.0499              2.2078              1.9387              2.1156               2.1215               2.2518               2.0762               2.0766               1.8285               2.2682               1.8418               1.6713               2.2093               2.0717               1.8461               1.7283               1.9973               2.0383               1.9701               2.1000               2.4321               2.1318               2.0404               2.1615               2.1341               1.9210               2.2278               1.7829               2.2278               2.1664               2.3202               2.0739               2.0372               2.1407               1.5645               1.7198               1.6286               2.0434               1.9786               1.6805               1.7145               1.8457               1.7965               2.1732               1.8204               2.3450               2.3175               2.2850               2.3039               2.2648               2.3126               2.2895               2.2847               2.2884                2.2961                2.2615                2.2782                2.3362                2.2950                2.2836                2.3149                2.3267                2.2730                2.3046                2.2914                2.3231                2.2985                2.2869                2.2580                2.2773                2.2688                2.3002                2.3190                2.2953                2.2693                2.2447                2.2812                2.2592                2.2789                2.2532                2.2919                2.2799                2.3004                2.2534                2.2706                2.2903                2.2937                2.2532                2.2981                2.2469                2.3324                2.2783                2.2618                2.2726                2.2671\n",
      "9        20010              1.8823              2.0034              1.8678              1.7314              2.0294              1.9527              2.1418              2.0234              2.1677               2.2477               2.2836               2.0552               2.1269               1.8553               2.2904               1.8377               1.6999               2.1740               1.9680               1.8402               1.7885               1.9993               1.9690               1.9907               2.0832               2.3240               2.1089               2.0020               2.1776               2.1991               1.9944               2.2081               1.9064               2.2731               2.2779               2.3247               1.9812               1.9272               2.0721               1.7243               1.7162               1.6894               2.0667               1.9518               1.5384               1.7030               1.8860               1.8489               2.0158               1.9018               2.0929               2.0715               2.1154               2.0177               2.0406               2.0674               2.0610               2.0947               2.0596                2.0655                2.0899                2.0421                2.0526                2.0513                2.0690                2.0626                2.0268                2.0854                2.0497                2.0603                2.0442                2.0791                2.0926                2.1086                2.0424                2.0688                2.0502                2.0662                1.9916                2.0398                2.0602                2.0557                2.0772                2.1238                2.0491                2.0744                2.0409                2.0429                2.0555                2.0388                2.0684                2.0584                2.0250                2.0370                2.0451                2.0894                2.0762                2.0979                2.0806                2.0616\n",
      "10       20011              2.0703              2.0797              1.7321              1.7734              1.8800              1.9358              2.1534              1.9252              1.9465               2.0652               2.1510               2.0082               2.0346               1.7131               2.1018               1.8172               1.7257               1.9720               1.9940               1.8311               2.0956               1.9749               1.9096               1.9127               2.0304               2.2165               1.9608               2.1674               1.9903               1.9385               2.0591               2.0584               1.7785               2.1485               2.1217               2.1668               2.1149               1.9307               2.0470               1.7997               1.8563               1.6859               1.9229               1.8651               1.6023               1.8501               1.7813               1.6956               1.9109               1.8450               2.0363               2.0437               2.0318               1.9919               2.0143               2.0819               2.0173               2.0767               2.0250                2.0027                1.9966                1.9945                1.9660                2.0153                2.0289                2.0431                1.9893                2.0253                1.9718                2.0411                2.0070                2.0421                2.0612                2.0571                2.0147                2.0538                2.0160                1.9988                1.9789                1.9746                1.9907                2.0198                2.0045                2.0596                2.0465                2.0199                2.0275                2.0049                1.9754                2.0048                2.0340                2.0207                1.9458                1.9900                2.0654                1.9826                1.9954                2.0280                2.0020                2.0373\n",
      "11       20012              1.5277              1.7472              1.8444              1.4361              1.8191              1.8141              1.6422              1.8695              1.9423               1.9176               2.0384               1.7032               1.8118               1.6679               2.0316               1.7867               1.4832               1.9183               1.6621               1.6679               1.6363               1.6761               1.7822               1.8383               1.7694               2.0384               1.8963               1.7143               1.9159               1.9546               1.5445               1.7801               1.7245               1.8665               1.8622               1.9398               1.7854               1.6808               1.7817               1.4637               1.4368               1.5851               1.8647               1.7716               1.3710               1.4280               1.7442               1.6205               1.7332               1.5296               1.8486               1.8381               1.8702               1.8249               1.7968               2.0158               1.8185               1.8667               1.8407                1.8725                1.8852                1.9201                2.0528                1.8580                1.8962                1.8483                1.9730                1.8125                1.9665                1.8347                1.8642                1.8762                1.8321                1.8350                1.8278                1.8561                1.8166                1.8261                1.9883                1.8550                1.9825                1.8194                1.8545                1.8367                1.8599                1.8002                1.8363                1.8336                1.8055                1.8381                1.8179                1.8446                1.9592                1.8807                1.9917                1.8891                2.0121                1.8265                1.8508                1.8989\n",
      "12       20013              1.7103              1.9059              1.7598              1.5783              1.8515              1.7719              2.0372              1.8973              2.0672               2.1218               2.1193               1.8751               2.0480               1.6923               2.1396               1.7517               1.6285               1.9063               1.8601               1.7652               1.6227               1.8794               1.8691               1.9579               1.9228               2.1411               1.9665               1.9149               2.0467               1.8889               1.8875               1.9857               1.7280               1.9950               2.1012               2.2350               2.0321               1.8251               2.0050               1.6197               1.6883               1.6151               1.9251               1.8448               1.5004               1.6544               1.8240               1.5819               1.8390               1.6974               2.0526               2.0933               2.0876               2.0806               2.0531               2.0425               2.0463               2.0448               2.0483                2.0779                2.0728                2.0605                2.0477                2.0438                2.0546                2.0829                2.0568                2.0327                2.0188                2.0486                2.1022                2.0808                2.1253                2.0890                2.0222                2.0993                2.0536                2.0742                2.0255                2.0835                2.0333                2.0504                2.0268                2.0762                2.1071                2.0054                2.0538                2.0490                2.0379                2.0406                2.1215                2.0881                2.0307                2.0467                2.0236                2.0785                2.0835                2.0478                2.0586                2.1008\n",
      "13       20014              1.6453              1.9246              2.0289              1.6506              1.8879              1.8742              1.8883              1.9949              2.1241               2.0968               2.2564               1.8898               1.9530               1.7893               2.2150               1.9024               1.6440               2.0817               1.8101               1.7953               1.7957               1.8083               1.8406               2.0098               1.9842               2.2835               2.0494               1.8303               2.1613               2.1060               1.8383               1.9510               1.7574               1.9379               2.0964               2.2271               1.8849               1.8544               1.9723               1.5241               1.6220               1.6355               2.0243               1.9287               1.5429               1.5759               1.8507               1.7268               1.9062               1.7688               1.9825               1.9546               1.9852               1.9306               1.9163               2.0159               1.9341               1.9449               1.9662                1.9297                1.9407                2.0151                1.9795                1.9819                1.9684                1.9650                1.9652                1.9077                1.9653                1.9651                1.9210                1.9546                1.9591                1.9501                1.9491                1.9734                1.9489                1.9627                1.9975                1.9911                2.0227                1.9278                1.9561                1.9909                1.9763                1.9020                1.9527                1.8874                1.8796                1.9219                1.9575                1.9503                1.9374                1.9315                1.9863                1.9715                2.0005                1.9319                1.9716                2.0126\n",
      "14       20015              1.8331              1.8409              1.7928              1.5713              1.8965              1.8934              2.0129              1.8309              2.0211               2.0492               2.0995               1.9046               2.0452               1.6844               2.1033               1.7876               1.6968               2.0167               1.8149               1.8011               1.7863               1.8528               1.9020               1.9371               1.9859               2.1297               1.9709               1.9418               1.9890               1.9469               1.8286               1.9626               1.7380               2.0652               2.0981               2.1484               1.9517               1.7925               1.9711               1.7090               1.6866               1.6204               2.0254               1.8540               1.6392               1.6848               1.8088               1.7015               1.9544               1.8445               1.9639               1.9636               1.9221               1.9334               1.9109               1.9036               1.9344               1.9395               1.9570                1.8750                1.9329                1.9817                1.9903                1.9562                1.9529                1.9447                1.9265                1.9264                1.9293                1.9299                1.9185                1.9523                1.9630                1.9307                1.9402                1.9065                1.9465                1.9567                1.9436                1.9459                1.9365                1.9213                1.9140                1.9354                1.9075                1.9534                1.9289                1.9444                1.9490                1.8994                1.9172                1.9245                1.8987                1.9173                1.9388                1.9647                1.9685                1.9402                1.9289                1.9641\n",
      "15       20016              1.8822              1.7849              1.8080              1.7435              1.7917              1.7829              1.8697              1.7658              1.8830               1.9872               1.9589               1.8677               1.9291               1.6025               2.0392               1.7981               1.6556               1.8712               1.7298               1.7239               1.9039               1.7458               1.8707               1.8676               1.9256               1.9557               1.8895               1.9077               1.8059               1.8742               1.7077               1.8975               1.8001               1.9699               2.0346               1.9971               1.8252               1.7421               1.8728               1.7435               1.7303               1.6662               1.8801               1.7497               1.5763               1.6745               1.7349               1.6401               1.8056               1.7463               1.8606               1.8416               1.8069               1.7966               1.7678               1.7792               1.8195               1.7975               1.8494                1.7806                1.8091                1.8044                1.7603                1.8583                1.8138                1.8062                1.7810                1.8052                1.7681                1.8353                1.7993                1.8215                1.8095                1.7781                1.7912                1.8096                1.8319                1.7793                1.7790                1.8016                1.7860                1.7920                1.7960                1.8441                1.8235                1.7951                1.7602                1.7756                1.7528                1.8055                1.7735                1.8086                1.7549                1.7956                1.7778                1.8091                1.7972                1.8120                1.7828                1.8009\n",
      "16       20017              1.8237              1.7016              1.6305              1.7344              1.8039              1.8279              1.8944              1.7164              1.8812               1.9978               1.9434               1.8226               1.9946               1.5859               1.9933               1.8749               1.6115               1.7782               1.6786               1.6931               1.7275               1.7611               1.7762               1.8275               1.8513               1.8520               1.9053               1.9093               1.7705               1.8678               1.6793               1.8550               1.7725               1.9631               2.0292               2.0475               1.8691               1.6983               1.7747               1.7366               1.7075               1.6872               1.8619               1.7162               1.5772               1.6187               1.7170               1.5982               1.7726               1.7085               1.8348               1.8283               1.8951               1.8462               1.7908               1.9471               1.8235               1.8351               1.8189                1.8565                1.8506                1.8738                1.8975                1.8628                1.8607                1.8397                1.8814                1.8336                1.8621                1.8247                1.8931                1.8560                1.8260                1.8410                1.8164                1.8385                1.8205                1.7790                1.9159                1.8629                1.8992                1.8219                1.8249                1.8535                1.8033                1.7900                1.8046                1.8330                1.8068                1.8521                1.8810                1.8679                1.9108                1.8535                1.9707                1.8790                1.9067                1.7980                1.8341                1.8637\n",
      "17       20018              1.5025              1.6090              1.6450              1.4145              1.7053              1.6251              1.6310              1.6525              1.8666               1.8856               1.8197               1.6477               1.7624               1.5289               1.8902               1.7224               1.4667               1.7823               1.6184               1.5953               1.5105               1.6628               1.6437               1.7205               1.6833               1.8023               1.8538               1.7364               1.7148               1.8784               1.5119               1.6862               1.5219               1.8157               1.7962               1.8258               1.6499               1.5495               1.6920               1.4937               1.3797               1.5893               1.7095               1.6447               1.4258               1.4056               1.6560               1.4403               1.5657               1.4719               1.8007               1.7948               1.8110               1.8131               1.7399               1.8756               1.7718               1.7940               1.7887                1.8178                1.8394                1.8362                1.8500                1.8437                1.8649                1.7899                1.8547                1.7780                1.8677                1.7929                1.8708                1.8151                1.7933                1.8092                1.7710                1.7974                1.7829                1.7849                1.8159                1.8083                1.8460                1.7729                1.7946                1.8213                1.7644                1.7571                1.7771                1.8370                1.7367                1.7808                1.8259                1.8021                1.9332                1.8355                1.8818                1.8065                1.8316                1.7837                1.8170                1.8256\n",
      "18       20019              2.0252              1.9542              1.7518              1.7576              1.7952              1.8041              2.0467              1.8055              1.9906               2.1012               2.0449               1.8908               2.0819               1.6258               2.0343               1.7429               1.6942               1.9407               1.8596               1.7294               1.9545               1.7872               1.7656               2.0231               1.9127               2.1273               1.8972               1.9971               1.9393               1.9206               1.8240               1.9234               1.6454               2.1021               1.9526               2.0387               2.0504               1.8017               1.9399               1.7540               1.6856               1.6337               1.8121               1.7881               1.6795               1.7434               1.7867               1.5487               1.8999               1.7610               1.8160               1.8570               1.8591               1.8632               1.8615               1.7862               1.8830               1.8800               1.8592                1.8619                1.8736                1.8825                1.8940                1.8415                1.8003                1.8623                1.7951                1.8544                1.8928                1.8547                1.8846                1.8458                1.8550                1.8852                1.8576                1.8438                1.8472                1.8950                1.8636                1.8258                1.8533                1.8508                1.8190                1.8990                1.8584                1.8586                1.8491                1.8214                1.8284                1.7899                1.8338                1.8553                1.8605                1.8387                1.8525                1.8873                1.9049                1.8375                1.8459                1.9246\n",
      "19       20020              1.8416              1.8045              1.7029              1.7109              1.8122              1.7171              1.8784              1.7364              1.8507               1.8058               1.9042               1.8253               1.8123               1.6096               1.9353               1.7879               1.6438               1.8349               1.6738               1.7386               1.7619               1.7667               1.6960               1.7979               1.8761               1.9061               1.8522               1.8930               1.7355               1.8592               1.7234               1.8352               1.6754               1.9943               1.9494               1.9373               1.7626               1.7417               1.8391               1.7519               1.7428               1.6451               1.8125               1.6994               1.5747               1.6731               1.7132               1.6690               1.7716               1.7303               1.8388               1.8607               1.8497               1.8456               1.7823               2.0139               1.8655               1.8809               1.8429                1.8654                1.8727                1.9212                1.9141                1.8844                1.8348                1.8719                1.8919                1.8458                1.8874                1.8831                1.8609                1.9148                1.8678                1.8933                1.8529                1.8807                1.8452                1.8794                1.9203                1.8822                1.9138                1.8461                1.8499                1.9108                1.8397                1.8496                1.8755                1.8902                1.8060                1.8459                1.8732                1.8635                1.8185                1.8738                1.9963                1.9238                1.9103                1.8593                1.8520                1.8891\n",
      "20       20021              1.7203              1.7119              1.7110              1.6531              1.7383              1.7000              1.7785              1.6891              1.8461               1.8799               1.8736               1.7828               1.8491               1.5506               1.9640               1.7839               1.5961               1.8374               1.6135               1.6658               1.7172               1.7111               1.7237               1.7953               1.8085               1.8432               1.8548               1.8344               1.7006               1.8576               1.6461               1.8652               1.6914               1.9244               1.8788               1.9431               1.6984               1.6367               1.7727               1.6856               1.6482               1.6378               1.8066               1.6659               1.5321               1.5711               1.6743               1.6315               1.7267               1.6566               1.8244               1.7719               1.8211               1.8164               1.7631               1.8487               1.7815               1.8109               1.7841                1.8543                1.8319                1.8360                1.7962                1.8482                1.8065                1.8011                1.8640                1.8000                1.8221                1.7902                1.8625                1.8435                1.7978                1.8118                1.8063                1.7834                1.7991                1.8000                1.8272                1.8568                1.8454                1.7800                1.7925                1.8075                1.7805                1.7842                1.7843                1.7966                1.7509                1.7855                1.8158                1.8267                1.8116                1.7988                1.8374                1.8354                1.8442                1.7943                1.8012                1.8517\n",
      "21       20022              1.7127              1.6847              1.5064              1.5296              1.7730              1.6501              1.9167              1.7086              1.8483               1.8533               1.9492               1.7263               1.6666               1.5990               1.8280               1.6752               1.5735               1.7703               1.6440               1.6382               1.5431               1.7545               1.5761               1.7201               1.7822               1.9023               1.8244               1.8250               1.7646               1.8028               1.7273               1.7235               1.6899               2.0076               1.9219               1.9895               1.7353               1.7048               1.8528               1.6230               1.5540               1.6039               1.8708               1.6588               1.4945               1.5473               1.6824               1.5813               1.7633               1.5852               1.7237               1.7803               1.7346               1.7122               1.7126               1.7419               1.7209               1.7229               1.6871                1.7440                1.7576                1.8034                1.8554                1.7661                1.7378                1.7384                1.7989                1.7350                1.7758                1.7101                1.7299                1.7215                1.7363                1.7622                1.7072                1.7317                1.7481                1.7570                1.6955                1.7752                1.8313                1.7174                1.6970                1.7458                1.7564                1.6988                1.7029                1.7739                1.7194                1.6797                1.7130                1.7214                1.7830                1.7229                1.7778                1.7242                1.8274                1.7177                1.7340                1.8165\n",
      "22       20023              1.5361              1.6659              1.6688              1.5286              1.6195              1.5855              1.6693              1.7039              1.8183               1.8096               1.8218               1.6954               1.7224               1.5512               1.7907               1.7139               1.5660               1.7270               1.6299               1.6056               1.4504               1.5846               1.5829               1.7444               1.7454               1.8537               1.8292               1.7311               1.7558               1.7072               1.6580               1.6859               1.5007               1.7809               1.7699               1.8932               1.6004               1.6326               1.7268               1.5287               1.4632               1.5708               1.6771               1.6368               1.5400               1.5080               1.6604               1.4950               1.6944               1.5088               1.6074               1.6615               1.6733               1.6763               1.6311               1.6872               1.6132               1.6804               1.6359                1.6607                1.6914                1.7142                1.7368                1.6738                1.6604                1.6934                1.6700                1.6757                1.6806                1.6541                1.6754                1.6691                1.6575                1.6586                1.6501                1.6662                1.6527                1.6910                1.6995                1.6304                1.6895                1.6550                1.7183                1.6740                1.6941                1.6354                1.6531                1.6703                1.6618                1.6444                1.6613                1.6680                1.6447                1.6493                1.7062                1.6170                1.7084                1.6385                1.6339                1.6957\n",
      "23       20024              1.6894              1.5429              1.5603              1.4937              1.6607              1.6039              1.7253              1.5758              1.5459               1.6799               1.7479               1.7426               1.6382               1.5159               1.7102               1.5119               1.5313               1.6567               1.6013               1.6314               1.6225               1.6534               1.5401               1.5315               1.7677               1.7181               1.7184               1.8219               1.5684               1.6363               1.7666               1.7028               1.5813               1.8100               1.7670               1.7195               1.7090               1.5546               1.6412               1.6181               1.6998               1.5658               1.6169               1.5994               1.4019               1.5980               1.5722               1.4808               1.6102               1.5522               1.6419               1.7046               1.6728               1.6751               1.6336               1.6935               1.6690               1.6453               1.6427                1.6692                1.6668                1.6824                1.7126                1.6812                1.6659                1.6737                1.6583                1.6552                1.6312                1.6577                1.6698                1.6514                1.7070                1.6378                1.6502                1.6715                1.6845                1.6635                1.6427                1.6678                1.6701                1.6576                1.6666                1.6691                1.6470                1.6437                1.6586                1.6688                1.6659                1.6305                1.6474                1.6590                1.5918                1.6547                1.6173                1.6621                1.6300                1.6378                1.6727                1.6969\n",
      "24       20025              1.7046              1.6093              1.6365              1.5795              1.7021              1.6828              1.6850              1.5939              1.6558               1.7595               1.7739               1.7498               1.6959               1.5244               1.8664               1.6649               1.5529               1.7267               1.5868               1.6618               1.7416               1.6846               1.7164               1.6468               1.8039               1.7404               1.7952               1.7462               1.5697               1.7981               1.5532               1.8298               1.6374               1.8686               1.7523               1.8317               1.6317               1.5826               1.6885               1.6163               1.6760               1.6024               1.6894               1.6039               1.5025               1.5828               1.5810               1.6410               1.6784               1.6989               1.6338               1.7004               1.6602               1.6395               1.6362               1.6803               1.6420               1.7009               1.6274                1.6863                1.6785                1.6927                1.7289                1.7144                1.6700                1.6718                1.6316                1.6849                1.6631                1.6618                1.6371                1.6712                1.6438                1.6349                1.6568                1.6722                1.6602                1.6278                1.7069                1.6923                1.6997                1.6768                1.6421                1.6693                1.6508                1.6561                1.6574                1.6848                1.6226                1.6610                1.6539                1.6625                1.6913                1.6596                1.7143                1.6772                1.7605                1.6830                1.6541                1.7347\n",
      "25       20026              1.7669              1.6606              1.6349              1.6891              1.6767              1.6520              1.7317              1.6714              1.7840               1.8559               1.8268               1.7322               1.8070               1.5394               1.9094               1.6536               1.6185               1.7945               1.6054               1.6789               1.7021               1.6795               1.7021               1.7755               1.7925               1.8035               1.8169               1.7835               1.6658               1.8008               1.6646               1.7255               1.6232               1.8089               1.8497               1.8515               1.6902               1.6224               1.7452               1.6841               1.5735               1.5799               1.7066               1.6494               1.6093               1.6132               1.6529               1.5462               1.7423               1.6300               1.6692               1.6749               1.6406               1.6559               1.6718               1.6582               1.6689               1.7097               1.7086                1.6433                1.6885                1.6774                1.6700                1.7069                1.6888                1.6723                1.5916                1.6893                1.7106                1.6924                1.7001                1.6247                1.6249                1.6498                1.7063                1.6634                1.6610                1.6085                1.7086                1.6816                1.6875                1.6624                1.6505                1.6600                1.6580                1.6525                1.7159                1.6867                1.6417                1.6527                1.6482                1.6762                1.6502                1.6582                1.6971                1.6757                1.7084                1.6988                1.6605                1.7190\n",
      "26       20027              1.7140              1.6046              1.6850              1.6471              1.6851              1.6278              1.7401              1.6426              1.8350               1.8267               1.7884               1.7005               1.8201               1.5393               1.9082               1.7462               1.5632               1.7574               1.6118               1.6427               1.5969               1.5908               1.6262               1.7600               1.7913               1.7711               1.8051               1.8088               1.6657               1.8195               1.5881               1.7839               1.6569               1.8034               1.8135               1.8728               1.6537               1.6163               1.7002               1.6079               1.5726               1.6676               1.7505               1.6357               1.5355               1.5180               1.6879               1.5983               1.6273               1.6445               1.7164               1.7091               1.7028               1.7403               1.7098               1.6683               1.7093               1.7081               1.6974                1.7020                1.7605                1.6975                1.6629                1.7615                1.7241                1.7277                1.6939                1.7136                1.6841                1.7261                1.7445                1.7704                1.7185                1.7428                1.7012                1.7065                1.7504                1.6831                1.6844                1.7361                1.7289                1.6928                1.6984                1.7254                1.7089                1.6792                1.6841                1.7341                1.6875                1.7012                1.6884                1.7173                1.6549                1.7276                1.7524                1.7319                1.6698                1.7124                1.7171                1.7129\n",
      "27       20028              1.5120              1.5194              1.5574              1.4345              1.6177              1.4920              1.6784              1.6217              1.7834               1.7441               1.7822               1.6031               1.5915               1.4882               1.7367               1.6983               1.4685               1.6151               1.5570               1.5387               1.3894               1.6702               1.6381               1.6872               1.6522               1.7498               1.7584               1.7481               1.6156               1.7410               1.5418               1.7553               1.5119               1.7434               1.6906               1.7730               1.5604               1.5493               1.6080               1.4246               1.3986               1.5498               1.6805               1.5330               1.4007               1.3728               1.5856               1.3649               1.5739               1.4245               1.5648               1.5896               1.5613               1.6211               1.5383               1.5525               1.6083               1.5804               1.5445                1.6132                1.5976                1.5942                1.5677                1.5645                1.5836                1.5998                1.5604                1.5686                1.5777                1.5725                1.5757                1.5986                1.5941                1.5427                1.6062                1.5892                1.5916                1.5908                1.5191                1.6037                1.5796                1.5908                1.6249                1.5993                1.5827                1.6104                1.6013                1.5943                1.5206                1.5788                1.5566                1.6102                1.6660                1.5695                1.4736                1.5357                1.5918                1.5910                1.5805                1.5991\n",
      "28       20029              1.5626              1.4642              1.4181              1.5265              1.6027              1.6712              1.5551              1.4854              1.5308               1.5873               1.5746               1.5825               1.6397               1.4318               1.7756               1.5050               1.4009               1.5711               1.4840               1.5274               1.5122               1.5230               1.6062               1.5563               1.6317               1.5450               1.7030               1.4925               1.4260               1.7265               1.3992               1.6192               1.5071               1.6780               1.5983               1.6565               1.4823               1.4468               1.5403               1.4916               1.4882               1.5028               1.5446               1.4767               1.2853               1.4627               1.4856               1.5185               1.5180               1.6158               1.5305               1.5466               1.5571               1.5621               1.5354               1.5202               1.5727               1.5736               1.5760                1.6074                1.5989                1.5561                1.5900                1.6084                1.5454                1.5648                1.5901                1.5994                1.5462                1.5695                1.5745                1.5306                1.6161                1.5652                1.6070                1.5908                1.5627                1.5733                1.4514                1.5585                1.4883                1.5612                1.6114                1.5777                1.5486                1.5467                1.5750                1.5622                1.5317                1.5799                1.5543                1.5646                1.5527                1.5542                1.5229                1.5950                1.6355                1.5604                1.5821                1.5469\n",
      "29       20030              1.2196              1.3202              1.2006              1.2120              1.3733              1.3369              1.2773              1.2910              1.2475               1.3930               1.3517               1.3583               1.3716               1.3054               1.2985               1.2700               1.2086               1.3760               1.2016               1.2718               1.1728               1.2437               1.2035               1.2877               1.3914               1.3561               1.3791               1.2734               1.2214               1.3236               1.1621               1.4056               1.2416               1.4051               1.3081               1.3397               1.1671               1.2095               1.2529               1.2929               1.3725               1.3383               1.3445               1.2443               1.0964               1.2465               1.2518               1.2446               1.2124               1.3629               1.3618               1.4804               1.3414               1.3998               1.4118               1.3959               1.4221               1.4086               1.3420                1.4236                1.4313                1.3414                1.3889                1.4197                1.4585                1.4265                1.4459                1.4058                1.3648                1.3876                1.4323                1.4030                1.4320                1.4175                1.4160                1.4273                1.4050                1.4144                1.3067                1.4270                1.3909                1.4082                1.4088                1.4082                1.3718                1.4311                1.3907                1.4156                1.4507                1.3885                1.4290                1.4373                1.3615                1.3889                1.3905                1.4020                1.4122                1.4154                1.4229                1.3610\n",
      "30       20031              1.5143              1.4341              1.4606              1.4563              1.5939              1.4600              1.6084              1.5413              1.6249               1.6510               1.7127               1.5750               1.4825               1.4847               1.6316               1.5805               1.4845               1.6040               1.5031               1.5219               1.4075               1.5006               1.4508               1.5729               1.6534               1.6630               1.7261               1.6603               1.5218               1.6041               1.5163               1.5693               1.5353               1.7952               1.6653               1.6764               1.5326               1.4924               1.6018               1.4834               1.3455               1.5408               1.6452               1.5181               1.3985               1.4398               1.5657               1.4259               1.5695               1.3899               1.5557               1.5976               1.5377               1.6039               1.5713               1.5731               1.5939               1.5843               1.5870                1.6193                1.5983                1.6261                1.6438                1.6254                1.5905                1.5877                1.6859                1.6292                1.6099                1.5882                1.6180                1.5487                1.6408                1.5939                1.6179                1.6180                1.5944                1.5949                1.5975                1.6333                1.6534                1.5924                1.6039                1.5992                1.6120                1.5769                1.6249                1.5913                1.5631                1.5872                1.5798                1.6314                1.6539                1.6078                1.6304                1.6347                1.6913                1.5706                1.6025                1.6684\n",
      "31       20032              1.9440              1.7643              1.9715              1.9105              1.9374              1.9720              2.1564              2.0988              2.2577               2.1429               2.3045               2.1427               1.8636               2.0420               2.5104               2.0213               1.7200               2.3121               2.0119               1.9329               2.0732               1.9046               2.0127               1.9966               2.1337               2.3098               2.0032               2.3531               2.2278               2.3530               2.1342               2.0078               1.7872               1.8492               2.0368               2.1255               1.9710               1.9667               2.1263               1.4491               1.9428               1.7148               2.1095               1.9306               1.7863               1.8201               1.9530               1.7561               2.1973               1.8065               2.0746               2.1095               2.0756               2.0927               2.1367               1.9349               2.0835               2.0499               2.1369                2.0357                2.0356                1.9804                2.0386                2.0206                2.0690                2.1182                1.9866                2.0836                1.9185                2.0747                2.1045                2.0582                2.0436                2.0689                2.0816                2.0611                2.0274                2.1654                1.9412                2.0501                1.9576                2.0998                2.0242                2.0751                2.1086                2.1039                2.1345                2.0177                2.0835                2.0736                2.1038                2.1151                1.9122                2.0540                1.9356                1.9950                1.9156                2.0746                2.0278                2.0095\n",
      "32       20033              1.5036              1.4691              1.4813              1.4249              1.5267              1.5412              1.5028              1.4624              1.7117               1.5268               1.6119               1.5936               1.6089               1.4429               1.7015               1.5202               1.4576               1.6193               1.4510               1.5313               1.4353               1.4345               1.4933               1.6716               1.6434               1.6050               1.6924               1.5733               1.5465               1.6438               1.4162               1.5298               1.3786               1.5502               1.5838               1.7072               1.4869               1.4080               1.5188               1.4585               1.4134               1.5684               1.5184               1.5249               1.3062               1.3625               1.5649               1.3503               1.5187               1.4368               1.5274               1.5612               1.5331               1.5618               1.4475               1.5529               1.5313               1.5270               1.4961                1.4988                1.5393                1.5630                1.5723                1.5551                1.5508                1.4895                1.5648                1.5016                1.5410                1.5094                1.5247                1.5166                1.4991                1.5243                1.5275                1.5659                1.5623                1.5311                1.4869                1.5714                1.5687                1.5124                1.5673                1.5468                1.5552                1.5081                1.5098                1.4767                1.4949                1.5176                1.5219                1.5459                1.6305                1.5143                1.5288                1.5362                1.5188                1.4969                1.5565                1.5264\n",
      "33       20035              1.4877              1.4851              1.4998              1.4758              1.5718              1.5927              1.5043              1.4650              1.4653               1.5742               1.5638               1.5914               1.5048               1.4146               1.7043               1.4648               1.4028               1.5692               1.4103               1.5170               1.5002               1.5167               1.5833               1.4811               1.6503               1.5344               1.6268               1.5270               1.3811               1.6903               1.3657               1.6816               1.4716               1.6037               1.5150               1.5833               1.4039               1.4305               1.5243               1.4862               1.4903               1.4721               1.4553               1.4699               1.4178               1.4581               1.4443               1.5900               1.4925               1.6269               1.5950               1.6485               1.6102               1.6238               1.6079               1.6326               1.6206               1.6508               1.5832                1.6295                1.6353                1.5637                1.6732                1.6711                1.6327                1.6309                1.6194                1.6591                1.6035                1.5942                1.6378                1.6101                1.6403                1.5782                1.6388                1.6565                1.5802                1.6407                1.5627                1.6455                1.6008                1.6055                1.6587                1.6123                1.6341                1.6041                1.6089                1.6315                1.5740                1.6142                1.6294                1.6454                1.5975                1.5937                1.6823                1.6598                1.6446                1.6211                1.6244                1.6290\n",
      "34       20037              1.2581              1.3558              1.2278              1.2573              1.3970              1.3919              1.3082              1.3637              1.4640               1.4038               1.4352               1.3896               1.5241               1.3563               1.4402               1.3789               1.3091               1.3558               1.3008               1.3526               1.1516               1.3082               1.3257               1.3735               1.4330               1.4221               1.5586               1.3013               1.3959               1.4214               1.2279               1.4900               1.2434               1.5043               1.4086               1.4826               1.3028               1.3387               1.3178               1.2911               1.2668               1.4201               1.4420               1.3652               1.0063               1.2356               1.4090               1.1746               1.3013               1.3109               1.1341               1.2007               1.2753               1.2517               1.2371               1.2702               1.2078               1.1597               1.1520                1.1813                1.2050                1.2782                1.3915                1.1341                1.2009                1.2266                1.1887                1.1935                1.1201                1.1402                1.2295                1.2064                1.2649                1.2050                1.2518                1.1985                1.2043                1.2470                1.2982                1.1722                1.2899                1.1781                1.1427                1.2015                1.2445                1.1787                1.2266                1.1692                1.2136                1.1718                1.1805                1.1940                1.2096                1.1855                1.2973                1.2111                1.2025                1.2201                1.2381                1.2231\n",
      "35       20038              1.4935              1.4642              1.4699              1.5176              1.5413              1.5268              1.5681              1.4838              1.4842               1.5590               1.5785               1.5602               1.5137               1.4234               1.6120               1.4972               1.3892               1.5472               1.4159               1.5223               1.4405               1.5197               1.4749               1.4997               1.6341               1.5063               1.5886               1.4688               1.4057               1.6039               1.3861               1.6849               1.4896               1.5803               1.5658               1.5621               1.4443               1.4209               1.4981               1.4893               1.4813               1.4426               1.4688               1.4404               1.4218               1.4430               1.4563               1.5135               1.4926               1.5050               1.4093               1.4150               1.4532               1.3904               1.4436               1.5120               1.3997               1.4286               1.3953                1.4535                1.4477                1.4215                1.5171                1.4612                1.4475                1.4263                1.4421                1.4777                1.4616                1.3992                1.4492                1.4156                1.3903                1.4373                1.4091                1.4163                1.4586                1.3803                1.4849                1.4267                1.5106                1.4401                1.4630                1.4218                1.4195                1.3803                1.4651                1.4563                1.3871                1.4632                1.4296                1.4425                1.4420                1.4640                1.5701                1.4544                1.5005                1.3913                1.4190                1.4643\n",
      "36       20039              1.3215              1.3131              1.2871              1.3965              1.4008              1.4801              1.3385              1.2836              1.3052               1.3291               1.4307               1.4054               1.3996               1.2755               1.5384               1.3479               1.2780               1.4189               1.2556               1.3455               1.3197               1.3453               1.3746               1.3273               1.4729               1.3562               1.4746               1.3331               1.2851               1.5492               1.2678               1.4136               1.3257               1.3290               1.3778               1.4739               1.3856               1.2879               1.4079               1.3671               1.3171               1.3597               1.3520               1.3262               1.3025               1.3185               1.3046               1.3707               1.3369               1.4931               1.4027               1.4014               1.3949               1.3664               1.3609               1.4621               1.4153               1.4146               1.3375                1.4315                1.3691                1.4130                1.4709                1.4283                1.4383                1.3711                1.4068                1.3969                1.4019                1.3866                1.4074                1.3744                1.3317                1.3982                1.3976                1.4037                1.4080                1.3830                1.4421                1.3993                1.4563                1.3872                1.3587                1.3989                1.3308                1.3839                1.3825                1.4435                1.3575                1.4165                1.4232                1.4031                1.4252                1.4136                1.4140                1.4197                1.4570                1.3849                1.3734                1.4205\n",
      "37       20041              1.3568              1.3378              1.3499              1.4234              1.4166              1.4813              1.3669              1.3507              1.3056               1.4252               1.4373               1.4632               1.4123               1.3447               1.5565               1.4173               1.2743               1.4647               1.3032               1.4083               1.3238               1.3675               1.4307               1.3641               1.5311               1.4058               1.5003               1.3453               1.2717               1.5726               1.2355               1.4818               1.3235               1.4344               1.4033               1.4917               1.3421               1.3057               1.3918               1.3432               1.3525               1.4110               1.4300               1.3811               1.2390               1.3302               1.3240               1.4380               1.3513               1.4847               1.3178               1.3429               1.3762               1.3609               1.3211               1.3787               1.3397               1.3429               1.3178                1.3950                1.3453                1.3527                1.5097                1.4073                1.3565                1.3420                1.3443                1.3732                1.3233                1.3623                1.3749                1.3278                1.3755                1.3810                1.3869                1.3599                1.4135                1.3595                1.3080                1.3429                1.4208                1.3485                1.3468                1.3639                1.3656                1.3566                1.3530                1.3377                1.3445                1.3879                1.3315                1.3768                1.4209                1.3799                1.3709                1.3180                1.4324                1.3284                1.3669                1.3173\n",
      "38       20042              1.5571              1.5113              1.5014              1.5314              1.5483              1.5810              1.5181              1.4930              1.6916               1.6141               1.6250               1.5635               1.5481               1.4755               1.6485               1.6118               1.4835               1.5752               1.4692               1.5639               1.5226               1.4882               1.5230               1.6386               1.6205               1.5911               1.6694               1.6091               1.5176               1.6054               1.4500               1.5993               1.4957               1.6433               1.5956               1.6900               1.4879               1.4790               1.5238               1.5110               1.5043               1.5153               1.5363               1.4963               1.4508               1.4439               1.5359               1.4913               1.5376               1.5325               1.4813               1.4730               1.5219               1.4986               1.4704               1.4789               1.5588               1.5240               1.4922                1.4850                1.4982                1.5070                1.5332                1.5367                1.5066                1.4967                1.5261                1.4833                1.4750                1.4815                1.5021                1.5118                1.4861                1.5408                1.5068                1.4950                1.5079                1.4721                1.4327                1.5239                1.5053                1.5087                1.5033                1.5199                1.5184                1.4503                1.4868                1.5264                1.4467                1.4787                1.5000                1.5208                1.4973                1.4750                1.3963                1.4977                1.5236                1.4679                1.5160                1.5371\n",
      "39       20043              1.3292              1.3610              1.3406              1.3998              1.4638              1.4154              1.3735              1.3478              1.4198               1.4582               1.4714               1.4661               1.4418               1.3999               1.5842               1.4003               1.2965               1.4941               1.3143               1.4370               1.3240               1.3581               1.3517               1.3998               1.5388               1.4512               1.5205               1.3866               1.3199               1.5518               1.2568               1.4719               1.3818               1.4362               1.4339               1.4469               1.2818               1.3036               1.4397               1.3733               1.3663               1.4313               1.4161               1.4107               1.2822               1.2858               1.3701               1.3964               1.3733               1.3477               1.3790               1.3311               1.4396               1.4039               1.3615               1.4167               1.4317               1.4321               1.3312                1.4172                1.3703                1.4189                1.4550                1.4601                1.4309                1.3587                1.4461                1.3606                1.4473                1.4019                1.4348                1.4057                1.3713                1.4085                1.4283                1.3891                1.4001                1.3512                1.3894                1.4259                1.4105                1.3658                1.4732                1.4333                1.4500                1.3736                1.3883                1.3959                1.3610                1.3980                1.4231                1.3975                1.3802                1.4151                1.3676                1.3746                1.3604                1.3786                1.3821                1.4175\n",
      "40       20044              1.2479              1.3906              1.5278              1.4355              1.4266              1.4698              1.4804              1.4455              1.6653               1.5459               1.5945               1.4705               1.6328               1.3761               1.4130               1.4928               1.3821               1.5414               1.4649               1.3969               1.3077               1.4352               1.3744               1.4892               1.5081               1.5967               1.6366               1.5003               1.5169               1.5137               1.4361               1.4654               1.3547               1.3778               1.4798               1.5699               1.4171               1.4050               1.4549               1.3184               1.2968               1.5078               1.4923               1.4423               1.2579               1.3026               1.5302               1.2700               1.4881               1.3541               1.3314               1.4162               1.5028               1.3671               1.3823               1.4605               1.3948               1.3692               1.3246                1.3114                1.3473                1.4321                1.4865                1.3984                1.3699                1.3979                1.3962                1.3719                1.4111                1.3423                1.3752                1.3805                1.3676                1.4488                1.4089                1.4276                1.3758                1.3542                1.4515                1.3695                1.5027                1.3775                1.3416                1.4248                1.3876                1.3619                1.3415                1.3527                1.3549                1.3781                1.3401                1.4449                1.5168                1.3286                1.4678                1.4029                1.4980                1.3701                1.4007                1.4496\n",
      "41       20045              1.4210              1.4495              1.4220              1.4789              1.5031              1.5542              1.4794              1.4313              1.4034               1.5090               1.5490               1.4943               1.4907               1.3589               1.6011               1.4764               1.3575               1.5539               1.4059               1.5094               1.4994               1.5050               1.5981               1.4696               1.5726               1.4745               1.5562               1.4242               1.3483               1.5988               1.3098               1.5900               1.5190               1.4956               1.5263               1.4930               1.4585               1.3943               1.4825               1.4539               1.4102               1.4352               1.4429               1.4395               1.4187               1.4438               1.4006               1.5159               1.4600               1.5589               1.4222               1.4264               1.4566               1.4286               1.4079               1.4557               1.4292               1.4485               1.4164                1.4648                1.4323                1.4053                1.5032                1.4159                1.4187                1.4075                1.4279                1.4467                1.4266                1.4481                1.3890                1.3933                1.4580                1.4210                1.4409                1.4048                1.4266                1.3658                1.5011                1.4360                1.4420                1.4137                1.4287                1.4003                1.4269                1.4046                1.4535                1.3859                1.4013                1.4279                1.3733                1.4379                1.4072                1.4224                1.4547                1.4535                1.5000                1.4091                1.4300                1.4704\n",
      "42       20046              1.4751              1.3860              1.4402              1.4862              1.4827              1.5528              1.4590              1.4003              1.4474               1.4624               1.5306               1.4452               1.4787               1.3587               1.6304               1.4195               1.3762               1.4809               1.3713               1.4717               1.4433               1.4145               1.4795               1.4712               1.5351               1.4455               1.5212               1.4592               1.3763               1.5787               1.3298               1.5005               1.4357               1.4947               1.4900               1.6200               1.4493               1.4088               1.5034               1.4303               1.3978               1.4344               1.4128               1.3985               1.3606               1.3982               1.4171               1.4022               1.4454               1.5232               1.4358               1.4479               1.4929               1.4725               1.4643               1.4580               1.4513               1.4426               1.4516                1.4894                1.4558                1.4836                1.5212                1.4603                1.4809                1.4692                1.4184                1.4972                1.4637                1.4787                1.5252                1.4846                1.4226                1.4333                1.4770                1.4292                1.4671                1.4480                1.4337                1.4333                1.4352                1.4592                1.4242                1.4510                1.4709                1.4761                1.5047                1.3795                1.4495                1.4361                1.4841                1.4588                1.4989                1.4663                1.4829                1.4465                1.4965                1.4776                1.4773                1.4484\n",
      "43       20047              1.3352              1.3847              1.4433              1.3746              1.4621              1.4561              1.5266              1.4301              1.6169               1.5653               1.5833               1.4603               1.5959               1.4073               1.4072               1.5080               1.4153               1.5207               1.4701               1.3822               1.2500               1.4210               1.3816               1.5278               1.5244               1.5413               1.6320               1.4813               1.4845               1.4828               1.4105               1.4854               1.4095               1.5101               1.5080               1.5110               1.4101               1.4274               1.4057               1.3611               1.2225               1.4989               1.5128               1.4272               1.2609               1.3272               1.5142               1.2521               1.4713               1.3705               1.3794               1.4461               1.5109               1.4120               1.3989               1.4454               1.3894               1.4124               1.3380                1.3302                1.3534                1.5032                1.7019                1.4214                1.4496                1.4276                1.4510                1.3964                1.4337                1.3778                1.3962                1.4499                1.3509                1.4386                1.4539                1.3981                1.3792                1.4125                1.4204                1.3893                1.5308                1.4168                1.3763                1.4201                1.4323                1.4011                1.4019                1.4203                1.3685                1.4023                1.4053                1.4681                1.5685                1.4170                1.5164                1.3917                1.4307                1.3745                1.4326                1.4728\n",
      "44       20049              1.5102              1.4674              1.5721              1.4088              1.5428              1.5442              1.5044              1.5174              1.7107               1.6837               1.6804               1.5961               1.7219               1.4496               1.6834               1.5769               1.4600               1.5551               1.4808               1.5440               1.5336               1.4100               1.5807               1.6822               1.6798               1.6578               1.6774               1.5877               1.5702               1.6509               1.4008               1.5092               1.4668               1.6180               1.5712               1.7540               1.5493               1.4555               1.5261               1.4897               1.5225               1.5849               1.6531               1.5397               1.4728               1.4378               1.5864               1.3791               1.5216               1.4575               1.4403               1.4326               1.5159               1.4629               1.4129               1.4107               1.4494               1.4525               1.3925                1.4596                1.4165                1.4739                1.4212                1.4811                1.4583                1.4422                1.4548                1.4089                1.4368                1.4416                1.4349                1.4557                1.4631                1.4464                1.4592                1.4126                1.4674                1.4168                1.3782                1.4304                1.5055                1.4234                1.4129                1.4595                1.4783                1.4281                1.4458                1.4257                1.4079                1.4193                1.4090                1.4617                1.4607                1.4791                1.3777                1.4208                1.4358                1.4052                1.4361                1.4849\n",
      "45       20050              1.3412              1.3450              1.3466              1.4060              1.4581              1.4587              1.4056              1.3532              1.3663               1.4336               1.4472               1.4663               1.4119               1.3634               1.5738               1.3411               1.2968               1.4270               1.3316               1.4007               1.3625               1.3883               1.4363               1.3805               1.5292               1.4001               1.4927               1.3506               1.3128               1.5459               1.2410               1.4922               1.3894               1.4670               1.4179               1.4303               1.2975               1.3170               1.4460               1.3815               1.3212               1.4048               1.3786               1.3688               1.2485               1.3231               1.3540               1.4823               1.3870               1.4381               1.3407               1.3627               1.3409               1.3623               1.3537               1.3472               1.3865               1.3801               1.3257                1.3820                1.3479                1.3749                1.2851                1.3792                1.3190                1.3605                1.3567                1.3513                1.4075                1.3990                1.3668                1.4002                1.3082                1.3753                1.3773                1.3623                1.3505                1.3784                1.2887                1.3302                1.3189                1.3640                1.3934                1.4048                1.3985                1.3450                1.3692                1.3693                1.3065                1.3494                1.3558                1.3683                1.3471                1.3247                1.3571                1.3789                1.2790                1.3769                1.3279                1.3352\n",
      "46       20051              1.3269              1.2882              1.3325              1.4317              1.3722              1.4813              1.3154              1.3352              1.3320               1.3978               1.3757               1.4133               1.4359               1.2809               1.5193               1.2773               1.2474               1.3939               1.2708               1.3815               1.3414               1.2883               1.3469               1.3272               1.4688               1.3590               1.4712               1.3259               1.2571               1.4990               1.2349               1.4250               1.2924               1.3522               1.3455               1.4407               1.3406               1.2509               1.3446               1.3671               1.3260               1.3664               1.3338               1.3052               1.2711               1.3139               1.3344               1.3348               1.3592               1.4324               1.2291               1.2340               1.2583               1.2766               1.2551               1.2136               1.2191               1.2528               1.2503                1.2570                1.2580                1.2650                1.2238                1.2308                1.3126                1.2756                1.2648                1.2829                1.2685                1.2838                1.2333                1.2422                1.2924                1.2321                1.2878                1.2466                1.2434                1.2855                1.1904                1.3021                1.2572                1.2526                1.2531                1.2695                1.3038                1.2481                1.2818                1.2941                1.2736                1.2398                1.2475                1.2401                1.2646                1.2537                1.2534                1.2143                1.2838                1.2255                1.2572                1.3077\n",
      "47       20052              1.1630              1.2334              1.1626              1.3090              1.3138              1.3416              1.1894              1.2389              1.2604               1.2525               1.3195               1.2629               1.2602               1.1984               1.3190               1.2934               1.2025               1.2971               1.1873               1.2373               1.2734               1.2382               1.2155               1.2713               1.3252               1.2800               1.3561               1.2217               1.2331               1.3064               1.1375               1.3327               1.1848               1.2125               1.2206               1.3331               1.1848               1.1827               1.2695               1.2478               1.2981               1.2958               1.2440               1.2541               1.1797               1.1996               1.2303               1.2355               1.2305               1.2824               1.1967               1.1806               1.2213               1.2198               1.1991               1.1699               1.1899               1.2367               1.1929                1.2777                1.2036                1.2859                1.3857                1.2452                1.2783                1.2242                1.2595                1.2421                1.2541                1.2384                1.2484                1.2540                1.2383                1.2517                1.2830                1.2343                1.2642                1.1758                1.2522                1.2658                1.2627                1.2290                1.2695                1.2119                1.2980                1.2142                1.2105                1.2759                1.2144                1.1833                1.2601                1.2192                1.2341                1.2067                1.2045                1.2608                1.2788                1.2196                1.2007                1.2535\n",
      "48       20053              1.4352              1.4388              1.4762              1.4507              1.4495              1.5598              1.4150              1.4169              1.4252               1.4761               1.4922               1.4321               1.5327               1.3510               1.6132               1.4656               1.3277               1.4546               1.3672               1.4466               1.4095               1.4204               1.4621               1.4846               1.4989               1.4174               1.5248               1.3830               1.3111               1.5630               1.3376               1.5420               1.4665               1.4362               1.4531               1.5468               1.4281               1.3473               1.4394               1.4302               1.3367               1.3964               1.3994               1.3398               1.3445               1.3748               1.3826               1.4536               1.4276               1.5243               1.4332               1.3924               1.3817               1.3959               1.4099               1.4776               1.4044               1.3800               1.4057                1.4436                1.4169                1.3909                1.4279                1.4696                1.4411                1.4460                1.4019                1.4824                1.4337                1.4298                1.3886                1.4176                1.3476                1.3962                1.4552                1.4285                1.3958                1.4229                1.4172                1.4126                1.3722                1.4155                1.4512                1.4101                1.4096                1.3816                1.3948                1.4254                1.3753                1.4273                1.4157                1.4261                1.3869                1.4348                1.4637                1.4130                1.4107                1.4277                1.4019                1.4305\n",
      "49       20054              1.3633              1.3913              1.3783              1.4285              1.4415              1.5033              1.4346              1.3898              1.4507               1.3816               1.4997               1.4207               1.4407               1.3514               1.5531               1.3508               1.3669               1.4483               1.3568               1.3680               1.4629               1.3947               1.4172               1.4268               1.4877               1.4121               1.4913               1.3972               1.3522               1.5194               1.3167               1.5222               1.3660               1.3797               1.4499               1.5526               1.4028               1.3467               1.4612               1.4109               1.3054               1.4050               1.3776               1.3761               1.3229               1.3617               1.3797               1.4286               1.4381               1.5254               1.3435               1.3289               1.3654               1.3852               1.3456               1.3484               1.3525               1.3841               1.3784                1.4331                1.3501                1.4051                1.3591                1.3937                1.4014                1.3674                1.3210                1.3458                1.3845                1.3755                1.4051                1.3435                1.3438                1.3781                1.3619                1.3845                1.3479                1.4133                1.2691                1.3873                1.3402                1.3592                1.3525                1.3782                1.3991                1.3468                1.3618                1.3910                1.3328                1.3352                1.3517                1.4042                1.4083                1.3809                1.3599                1.4083                1.3384                1.3332                1.3989                1.3188\n"
     ]
    }
   ],
   "source": [
    "# Imprimir Solo las columnas PRODUCT_ID, mlp_pred_LOG1P_Z_1 a mlp_pred_LOG1P_Z_10  mostrando todas las columnas dando mas ancho \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(df_preds_final.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f07627ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar desde disco\n",
    "scaler_y = joblib.load('scaler_y_CLASE_LOG1P.joblib')\n",
    "#y_pred_log1p = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ee5129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n",
      "/tmp/ipykernel_656/3128550729.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)\n"
     ]
    }
   ],
   "source": [
    "def inverse_log_transform_signed(x):\n",
    "    return np.sign(x) * np.expm1(np.abs(x))\n",
    "\n",
    "def inv_transform_with_scaler(arr, scaler_y):\n",
    "    # arr debe tener forma (N,)\n",
    "    arr = arr.reshape(-1, 1)  # scaler espera 2D\n",
    "    # Paso 1: inversa del escalado\n",
    "    log1p_vals = scaler_y.inverse_transform(arr).flatten()\n",
    "    # Paso 2: inversa de log1p con signo\n",
    "    orig_vals = inverse_log_transform_signed(log1p_vals)\n",
    "    return orig_vals\n",
    "\n",
    "# Crear columnas nuevas con sufijo \"_ORIG\"\n",
    "for col in df_preds_final.columns:\n",
    "    if col not in ['CUSTOMER_ID', 'PRODUCT_ID']:\n",
    "        arr = df_preds_final[col].values\n",
    "        df_preds_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform_with_scaler(arr, scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc3fe460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar de df_preds_final las columnas con \"_LOG1P_Z_\"\n",
    "cols_to_remove = [col for col in df_preds_final.columns if '_LOG1P_Z_' in col]\n",
    "df_preds_final.drop(columns=cols_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4e8530a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>mlp_pred_ORIG_1</th>\n",
       "      <th>mlp_pred_ORIG_2</th>\n",
       "      <th>mlp_pred_ORIG_3</th>\n",
       "      <th>mlp_pred_ORIG_4</th>\n",
       "      <th>mlp_pred_ORIG_5</th>\n",
       "      <th>mlp_pred_ORIG_6</th>\n",
       "      <th>mlp_pred_ORIG_7</th>\n",
       "      <th>mlp_pred_ORIG_8</th>\n",
       "      <th>mlp_pred_ORIG_9</th>\n",
       "      <th>mlp_pred_ORIG_10</th>\n",
       "      <th>mlp_pred_ORIG_11</th>\n",
       "      <th>mlp_pred_ORIG_12</th>\n",
       "      <th>mlp_pred_ORIG_13</th>\n",
       "      <th>mlp_pred_ORIG_14</th>\n",
       "      <th>mlp_pred_ORIG_15</th>\n",
       "      <th>mlp_pred_ORIG_16</th>\n",
       "      <th>mlp_pred_ORIG_17</th>\n",
       "      <th>mlp_pred_ORIG_18</th>\n",
       "      <th>mlp_pred_ORIG_19</th>\n",
       "      <th>mlp_pred_ORIG_20</th>\n",
       "      <th>mlp_pred_ORIG_21</th>\n",
       "      <th>mlp_pred_ORIG_22</th>\n",
       "      <th>mlp_pred_ORIG_23</th>\n",
       "      <th>mlp_pred_ORIG_24</th>\n",
       "      <th>mlp_pred_ORIG_25</th>\n",
       "      <th>mlp_pred_ORIG_26</th>\n",
       "      <th>mlp_pred_ORIG_27</th>\n",
       "      <th>mlp_pred_ORIG_28</th>\n",
       "      <th>mlp_pred_ORIG_29</th>\n",
       "      <th>mlp_pred_ORIG_30</th>\n",
       "      <th>mlp_pred_ORIG_31</th>\n",
       "      <th>mlp_pred_ORIG_32</th>\n",
       "      <th>mlp_pred_ORIG_33</th>\n",
       "      <th>mlp_pred_ORIG_34</th>\n",
       "      <th>mlp_pred_ORIG_35</th>\n",
       "      <th>mlp_pred_ORIG_36</th>\n",
       "      <th>mlp_pred_ORIG_37</th>\n",
       "      <th>mlp_pred_ORIG_38</th>\n",
       "      <th>mlp_pred_ORIG_39</th>\n",
       "      <th>mlp_pred_ORIG_40</th>\n",
       "      <th>mlp_pred_ORIG_41</th>\n",
       "      <th>mlp_pred_ORIG_42</th>\n",
       "      <th>mlp_pred_ORIG_43</th>\n",
       "      <th>mlp_pred_ORIG_44</th>\n",
       "      <th>mlp_pred_ORIG_45</th>\n",
       "      <th>mlp_pred_ORIG_46</th>\n",
       "      <th>mlp_pred_ORIG_47</th>\n",
       "      <th>mlp_pred_ORIG_48</th>\n",
       "      <th>mlp_pred_ORIG_49</th>\n",
       "      <th>mlp_pred_ORIG_50</th>\n",
       "      <th>lgbm_pred_ORIG_1</th>\n",
       "      <th>lgbm_pred_ORIG_2</th>\n",
       "      <th>lgbm_pred_ORIG_3</th>\n",
       "      <th>lgbm_pred_ORIG_4</th>\n",
       "      <th>lgbm_pred_ORIG_5</th>\n",
       "      <th>lgbm_pred_ORIG_6</th>\n",
       "      <th>lgbm_pred_ORIG_7</th>\n",
       "      <th>lgbm_pred_ORIG_8</th>\n",
       "      <th>lgbm_pred_ORIG_9</th>\n",
       "      <th>lgbm_pred_ORIG_10</th>\n",
       "      <th>lgbm_pred_ORIG_11</th>\n",
       "      <th>lgbm_pred_ORIG_12</th>\n",
       "      <th>lgbm_pred_ORIG_13</th>\n",
       "      <th>lgbm_pred_ORIG_14</th>\n",
       "      <th>lgbm_pred_ORIG_15</th>\n",
       "      <th>lgbm_pred_ORIG_16</th>\n",
       "      <th>lgbm_pred_ORIG_17</th>\n",
       "      <th>lgbm_pred_ORIG_18</th>\n",
       "      <th>lgbm_pred_ORIG_19</th>\n",
       "      <th>lgbm_pred_ORIG_20</th>\n",
       "      <th>lgbm_pred_ORIG_21</th>\n",
       "      <th>lgbm_pred_ORIG_22</th>\n",
       "      <th>lgbm_pred_ORIG_23</th>\n",
       "      <th>lgbm_pred_ORIG_24</th>\n",
       "      <th>lgbm_pred_ORIG_25</th>\n",
       "      <th>lgbm_pred_ORIG_26</th>\n",
       "      <th>lgbm_pred_ORIG_27</th>\n",
       "      <th>lgbm_pred_ORIG_28</th>\n",
       "      <th>lgbm_pred_ORIG_29</th>\n",
       "      <th>lgbm_pred_ORIG_30</th>\n",
       "      <th>lgbm_pred_ORIG_31</th>\n",
       "      <th>lgbm_pred_ORIG_32</th>\n",
       "      <th>lgbm_pred_ORIG_33</th>\n",
       "      <th>lgbm_pred_ORIG_34</th>\n",
       "      <th>lgbm_pred_ORIG_35</th>\n",
       "      <th>lgbm_pred_ORIG_36</th>\n",
       "      <th>lgbm_pred_ORIG_37</th>\n",
       "      <th>lgbm_pred_ORIG_38</th>\n",
       "      <th>lgbm_pred_ORIG_39</th>\n",
       "      <th>lgbm_pred_ORIG_40</th>\n",
       "      <th>lgbm_pred_ORIG_41</th>\n",
       "      <th>lgbm_pred_ORIG_42</th>\n",
       "      <th>lgbm_pred_ORIG_43</th>\n",
       "      <th>lgbm_pred_ORIG_44</th>\n",
       "      <th>lgbm_pred_ORIG_45</th>\n",
       "      <th>lgbm_pred_ORIG_46</th>\n",
       "      <th>lgbm_pred_ORIG_47</th>\n",
       "      <th>lgbm_pred_ORIG_48</th>\n",
       "      <th>lgbm_pred_ORIG_49</th>\n",
       "      <th>lgbm_pred_ORIG_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>2499.2090</td>\n",
       "      <td>5263.2959</td>\n",
       "      <td>804.5912</td>\n",
       "      <td>727.6192</td>\n",
       "      <td>1559.3298</td>\n",
       "      <td>2911.9277</td>\n",
       "      <td>7580.3813</td>\n",
       "      <td>932.1174</td>\n",
       "      <td>2122.7937</td>\n",
       "      <td>2514.7844</td>\n",
       "      <td>4620.2842</td>\n",
       "      <td>2722.4951</td>\n",
       "      <td>4365.8901</td>\n",
       "      <td>1492.4357</td>\n",
       "      <td>4894.0991</td>\n",
       "      <td>1080.9789</td>\n",
       "      <td>728.9855</td>\n",
       "      <td>3817.6021</td>\n",
       "      <td>2046.8887</td>\n",
       "      <td>1205.3357</td>\n",
       "      <td>1795.4785</td>\n",
       "      <td>917.8847</td>\n",
       "      <td>2162.9702</td>\n",
       "      <td>2008.6067</td>\n",
       "      <td>3880.3560</td>\n",
       "      <td>7916.3921</td>\n",
       "      <td>2108.4348</td>\n",
       "      <td>14830.9072</td>\n",
       "      <td>3071.4585</td>\n",
       "      <td>3162.0374</td>\n",
       "      <td>4540.4404</td>\n",
       "      <td>4199.3940</td>\n",
       "      <td>627.3073</td>\n",
       "      <td>3711.7502</td>\n",
       "      <td>10808.7129</td>\n",
       "      <td>4010.7842</td>\n",
       "      <td>2787.4878</td>\n",
       "      <td>1962.2367</td>\n",
       "      <td>3214.1221</td>\n",
       "      <td>3062.6428</td>\n",
       "      <td>3114.7310</td>\n",
       "      <td>1005.9449</td>\n",
       "      <td>4292.7256</td>\n",
       "      <td>2789.5337</td>\n",
       "      <td>227.9098</td>\n",
       "      <td>1334.6522</td>\n",
       "      <td>1259.5654</td>\n",
       "      <td>1528.6150</td>\n",
       "      <td>2745.5435</td>\n",
       "      <td>1266.5941</td>\n",
       "      <td>935.1566</td>\n",
       "      <td>924.6894</td>\n",
       "      <td>902.1176</td>\n",
       "      <td>848.3029</td>\n",
       "      <td>847.1355</td>\n",
       "      <td>1157.0333</td>\n",
       "      <td>886.3787</td>\n",
       "      <td>840.6570</td>\n",
       "      <td>907.4833</td>\n",
       "      <td>997.5659</td>\n",
       "      <td>939.1530</td>\n",
       "      <td>1088.7173</td>\n",
       "      <td>1124.7459</td>\n",
       "      <td>960.0096</td>\n",
       "      <td>952.6442</td>\n",
       "      <td>912.2195</td>\n",
       "      <td>1110.8863</td>\n",
       "      <td>869.8642</td>\n",
       "      <td>1243.7613</td>\n",
       "      <td>897.8728</td>\n",
       "      <td>896.6645</td>\n",
       "      <td>954.6576</td>\n",
       "      <td>885.1398</td>\n",
       "      <td>913.2840</td>\n",
       "      <td>889.9761</td>\n",
       "      <td>890.7729</td>\n",
       "      <td>996.9416</td>\n",
       "      <td>879.9595</td>\n",
       "      <td>1193.1274</td>\n",
       "      <td>1006.1139</td>\n",
       "      <td>995.2010</td>\n",
       "      <td>881.5662</td>\n",
       "      <td>982.9409</td>\n",
       "      <td>833.5339</td>\n",
       "      <td>913.6344</td>\n",
       "      <td>822.5711</td>\n",
       "      <td>847.4863</td>\n",
       "      <td>1046.9595</td>\n",
       "      <td>949.3221</td>\n",
       "      <td>906.5048</td>\n",
       "      <td>845.7527</td>\n",
       "      <td>923.6794</td>\n",
       "      <td>1223.6973</td>\n",
       "      <td>906.0816</td>\n",
       "      <td>1274.6605</td>\n",
       "      <td>1021.0181</td>\n",
       "      <td>1118.0922</td>\n",
       "      <td>866.9102</td>\n",
       "      <td>870.2219</td>\n",
       "      <td>1073.2910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>473.9576</td>\n",
       "      <td>2823.5747</td>\n",
       "      <td>600.2195</td>\n",
       "      <td>562.8087</td>\n",
       "      <td>814.6390</td>\n",
       "      <td>766.4780</td>\n",
       "      <td>1546.3629</td>\n",
       "      <td>697.9310</td>\n",
       "      <td>1351.1823</td>\n",
       "      <td>861.1847</td>\n",
       "      <td>1697.4403</td>\n",
       "      <td>871.4789</td>\n",
       "      <td>833.1289</td>\n",
       "      <td>986.6110</td>\n",
       "      <td>2509.4326</td>\n",
       "      <td>485.7554</td>\n",
       "      <td>377.7762</td>\n",
       "      <td>1358.7043</td>\n",
       "      <td>708.0589</td>\n",
       "      <td>702.3293</td>\n",
       "      <td>704.1918</td>\n",
       "      <td>505.4628</td>\n",
       "      <td>1177.1813</td>\n",
       "      <td>1257.8733</td>\n",
       "      <td>1018.1789</td>\n",
       "      <td>2839.2078</td>\n",
       "      <td>907.6165</td>\n",
       "      <td>1758.9507</td>\n",
       "      <td>1266.1028</td>\n",
       "      <td>1861.8225</td>\n",
       "      <td>1645.7792</td>\n",
       "      <td>1898.1858</td>\n",
       "      <td>374.1505</td>\n",
       "      <td>1872.4506</td>\n",
       "      <td>2554.7678</td>\n",
       "      <td>2109.0835</td>\n",
       "      <td>1151.9299</td>\n",
       "      <td>1068.7759</td>\n",
       "      <td>1398.7549</td>\n",
       "      <td>735.1407</td>\n",
       "      <td>438.5002</td>\n",
       "      <td>396.7874</td>\n",
       "      <td>1140.4252</td>\n",
       "      <td>1037.7700</td>\n",
       "      <td>721.7929</td>\n",
       "      <td>469.1016</td>\n",
       "      <td>733.5745</td>\n",
       "      <td>606.8829</td>\n",
       "      <td>1220.3689</td>\n",
       "      <td>514.1923</td>\n",
       "      <td>981.6201</td>\n",
       "      <td>939.5924</td>\n",
       "      <td>925.6344</td>\n",
       "      <td>905.8875</td>\n",
       "      <td>857.1993</td>\n",
       "      <td>1066.8393</td>\n",
       "      <td>858.0403</td>\n",
       "      <td>862.9338</td>\n",
       "      <td>933.1881</td>\n",
       "      <td>996.1841</td>\n",
       "      <td>986.4728</td>\n",
       "      <td>987.0411</td>\n",
       "      <td>990.3274</td>\n",
       "      <td>976.5534</td>\n",
       "      <td>974.2095</td>\n",
       "      <td>905.7628</td>\n",
       "      <td>1023.4710</td>\n",
       "      <td>897.3699</td>\n",
       "      <td>981.4370</td>\n",
       "      <td>918.8154</td>\n",
       "      <td>909.6917</td>\n",
       "      <td>950.0992</td>\n",
       "      <td>853.5017</td>\n",
       "      <td>940.0597</td>\n",
       "      <td>884.6668</td>\n",
       "      <td>912.2222</td>\n",
       "      <td>967.2535</td>\n",
       "      <td>929.4169</td>\n",
       "      <td>1016.0062</td>\n",
       "      <td>1004.1714</td>\n",
       "      <td>931.8859</td>\n",
       "      <td>900.4557</td>\n",
       "      <td>908.7761</td>\n",
       "      <td>864.7181</td>\n",
       "      <td>918.5266</td>\n",
       "      <td>830.9508</td>\n",
       "      <td>943.2141</td>\n",
       "      <td>990.5508</td>\n",
       "      <td>966.8951</td>\n",
       "      <td>951.4487</td>\n",
       "      <td>890.2561</td>\n",
       "      <td>935.1005</td>\n",
       "      <td>1031.1569</td>\n",
       "      <td>977.4998</td>\n",
       "      <td>1042.5057</td>\n",
       "      <td>1091.4635</td>\n",
       "      <td>1072.1501</td>\n",
       "      <td>882.6033</td>\n",
       "      <td>903.1474</td>\n",
       "      <td>1013.8543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>883.6436</td>\n",
       "      <td>1229.2775</td>\n",
       "      <td>341.9880</td>\n",
       "      <td>209.2350</td>\n",
       "      <td>570.0003</td>\n",
       "      <td>709.4800</td>\n",
       "      <td>1465.1033</td>\n",
       "      <td>475.2508</td>\n",
       "      <td>871.2306</td>\n",
       "      <td>771.3541</td>\n",
       "      <td>1210.7092</td>\n",
       "      <td>844.2320</td>\n",
       "      <td>1276.0896</td>\n",
       "      <td>455.5036</td>\n",
       "      <td>1272.2114</td>\n",
       "      <td>446.6269</td>\n",
       "      <td>334.5750</td>\n",
       "      <td>1082.2994</td>\n",
       "      <td>741.9630</td>\n",
       "      <td>457.5691</td>\n",
       "      <td>783.3557</td>\n",
       "      <td>551.6803</td>\n",
       "      <td>694.3302</td>\n",
       "      <td>812.8923</td>\n",
       "      <td>917.5224</td>\n",
       "      <td>1999.0488</td>\n",
       "      <td>685.9417</td>\n",
       "      <td>1526.1064</td>\n",
       "      <td>879.1579</td>\n",
       "      <td>939.0835</td>\n",
       "      <td>786.8970</td>\n",
       "      <td>1354.7971</td>\n",
       "      <td>190.1784</td>\n",
       "      <td>1385.3892</td>\n",
       "      <td>1266.3289</td>\n",
       "      <td>974.0811</td>\n",
       "      <td>875.1936</td>\n",
       "      <td>683.4855</td>\n",
       "      <td>1270.4812</td>\n",
       "      <td>581.8795</td>\n",
       "      <td>615.1618</td>\n",
       "      <td>380.8811</td>\n",
       "      <td>654.5162</td>\n",
       "      <td>726.2853</td>\n",
       "      <td>215.2493</td>\n",
       "      <td>453.0035</td>\n",
       "      <td>521.6562</td>\n",
       "      <td>361.4240</td>\n",
       "      <td>925.2378</td>\n",
       "      <td>486.3553</td>\n",
       "      <td>708.5550</td>\n",
       "      <td>722.1337</td>\n",
       "      <td>674.7821</td>\n",
       "      <td>724.4121</td>\n",
       "      <td>709.4367</td>\n",
       "      <td>692.6146</td>\n",
       "      <td>684.6285</td>\n",
       "      <td>657.6063</td>\n",
       "      <td>706.7446</td>\n",
       "      <td>637.2066</td>\n",
       "      <td>705.5091</td>\n",
       "      <td>736.1898</td>\n",
       "      <td>746.9853</td>\n",
       "      <td>700.4674</td>\n",
       "      <td>711.8722</td>\n",
       "      <td>720.9611</td>\n",
       "      <td>729.5378</td>\n",
       "      <td>696.0280</td>\n",
       "      <td>724.3710</td>\n",
       "      <td>701.2526</td>\n",
       "      <td>716.6207</td>\n",
       "      <td>685.6679</td>\n",
       "      <td>675.9561</td>\n",
       "      <td>710.5982</td>\n",
       "      <td>666.4150</td>\n",
       "      <td>702.5856</td>\n",
       "      <td>739.6281</td>\n",
       "      <td>706.2956</td>\n",
       "      <td>811.9448</td>\n",
       "      <td>655.1595</td>\n",
       "      <td>711.2714</td>\n",
       "      <td>692.4726</td>\n",
       "      <td>682.4361</td>\n",
       "      <td>702.6108</td>\n",
       "      <td>745.0326</td>\n",
       "      <td>693.8825</td>\n",
       "      <td>707.0578</td>\n",
       "      <td>633.3803</td>\n",
       "      <td>705.9305</td>\n",
       "      <td>699.2798</td>\n",
       "      <td>687.3723</td>\n",
       "      <td>691.9151</td>\n",
       "      <td>692.6686</td>\n",
       "      <td>728.2671</td>\n",
       "      <td>800.5936</td>\n",
       "      <td>706.6698</td>\n",
       "      <td>666.8135</td>\n",
       "      <td>663.5753</td>\n",
       "      <td>697.2887</td>\n",
       "      <td>770.3446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>278.6172</td>\n",
       "      <td>553.0844</td>\n",
       "      <td>234.8324</td>\n",
       "      <td>168.0436</td>\n",
       "      <td>327.3300</td>\n",
       "      <td>299.1143</td>\n",
       "      <td>578.9184</td>\n",
       "      <td>247.4313</td>\n",
       "      <td>458.3942</td>\n",
       "      <td>403.2302</td>\n",
       "      <td>510.4264</td>\n",
       "      <td>373.7491</td>\n",
       "      <td>395.3320</td>\n",
       "      <td>238.5614</td>\n",
       "      <td>652.7551</td>\n",
       "      <td>211.9489</td>\n",
       "      <td>193.3931</td>\n",
       "      <td>393.5305</td>\n",
       "      <td>360.6262</td>\n",
       "      <td>254.4985</td>\n",
       "      <td>268.7941</td>\n",
       "      <td>268.4264</td>\n",
       "      <td>382.7681</td>\n",
       "      <td>442.2457</td>\n",
       "      <td>432.5879</td>\n",
       "      <td>784.3494</td>\n",
       "      <td>415.4915</td>\n",
       "      <td>459.0695</td>\n",
       "      <td>418.1459</td>\n",
       "      <td>438.2862</td>\n",
       "      <td>333.4021</td>\n",
       "      <td>481.8074</td>\n",
       "      <td>130.5738</td>\n",
       "      <td>504.7629</td>\n",
       "      <td>583.4701</td>\n",
       "      <td>593.1573</td>\n",
       "      <td>393.1740</td>\n",
       "      <td>351.0249</td>\n",
       "      <td>522.1809</td>\n",
       "      <td>213.8898</td>\n",
       "      <td>226.8187</td>\n",
       "      <td>211.4141</td>\n",
       "      <td>351.3539</td>\n",
       "      <td>363.8842</td>\n",
       "      <td>219.0842</td>\n",
       "      <td>218.6337</td>\n",
       "      <td>279.0984</td>\n",
       "      <td>174.0665</td>\n",
       "      <td>399.5264</td>\n",
       "      <td>230.4446</td>\n",
       "      <td>535.1559</td>\n",
       "      <td>559.8694</td>\n",
       "      <td>487.0127</td>\n",
       "      <td>480.5612</td>\n",
       "      <td>510.9151</td>\n",
       "      <td>446.1754</td>\n",
       "      <td>518.0812</td>\n",
       "      <td>515.1366</td>\n",
       "      <td>510.9783</td>\n",
       "      <td>499.3051</td>\n",
       "      <td>484.9185</td>\n",
       "      <td>512.9588</td>\n",
       "      <td>515.7613</td>\n",
       "      <td>516.3664</td>\n",
       "      <td>501.8349</td>\n",
       "      <td>521.4044</td>\n",
       "      <td>496.8330</td>\n",
       "      <td>514.3620</td>\n",
       "      <td>512.5281</td>\n",
       "      <td>523.5468</td>\n",
       "      <td>530.0845</td>\n",
       "      <td>523.0726</td>\n",
       "      <td>502.9984</td>\n",
       "      <td>507.1017</td>\n",
       "      <td>505.5286</td>\n",
       "      <td>536.1644</td>\n",
       "      <td>531.2275</td>\n",
       "      <td>520.1293</td>\n",
       "      <td>452.3890</td>\n",
       "      <td>496.4564</td>\n",
       "      <td>481.8401</td>\n",
       "      <td>513.1341</td>\n",
       "      <td>503.8631</td>\n",
       "      <td>508.4100</td>\n",
       "      <td>500.0813</td>\n",
       "      <td>547.9576</td>\n",
       "      <td>518.5657</td>\n",
       "      <td>498.6645</td>\n",
       "      <td>504.4555</td>\n",
       "      <td>485.1947</td>\n",
       "      <td>502.6226</td>\n",
       "      <td>504.4424</td>\n",
       "      <td>518.0172</td>\n",
       "      <td>525.8581</td>\n",
       "      <td>424.3959</td>\n",
       "      <td>518.2056</td>\n",
       "      <td>514.8020</td>\n",
       "      <td>506.9468</td>\n",
       "      <td>486.6326</td>\n",
       "      <td>472.6493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>429.0883</td>\n",
       "      <td>734.8737</td>\n",
       "      <td>311.1428</td>\n",
       "      <td>168.2841</td>\n",
       "      <td>453.6467</td>\n",
       "      <td>397.8544</td>\n",
       "      <td>835.8878</td>\n",
       "      <td>332.2200</td>\n",
       "      <td>607.6149</td>\n",
       "      <td>551.0172</td>\n",
       "      <td>783.7721</td>\n",
       "      <td>482.9267</td>\n",
       "      <td>459.3329</td>\n",
       "      <td>344.3236</td>\n",
       "      <td>837.6432</td>\n",
       "      <td>236.5896</td>\n",
       "      <td>237.6687</td>\n",
       "      <td>517.5449</td>\n",
       "      <td>451.0930</td>\n",
       "      <td>364.5672</td>\n",
       "      <td>295.2185</td>\n",
       "      <td>421.9180</td>\n",
       "      <td>478.5123</td>\n",
       "      <td>497.4446</td>\n",
       "      <td>658.9768</td>\n",
       "      <td>1209.5479</td>\n",
       "      <td>543.8024</td>\n",
       "      <td>561.8757</td>\n",
       "      <td>674.3246</td>\n",
       "      <td>540.5536</td>\n",
       "      <td>402.4775</td>\n",
       "      <td>668.4355</td>\n",
       "      <td>174.1039</td>\n",
       "      <td>679.2991</td>\n",
       "      <td>872.0078</td>\n",
       "      <td>937.3920</td>\n",
       "      <td>551.2717</td>\n",
       "      <td>471.9512</td>\n",
       "      <td>673.8182</td>\n",
       "      <td>229.7278</td>\n",
       "      <td>289.0652</td>\n",
       "      <td>284.1077</td>\n",
       "      <td>480.9610</td>\n",
       "      <td>471.0372</td>\n",
       "      <td>209.6224</td>\n",
       "      <td>287.6818</td>\n",
       "      <td>363.4773</td>\n",
       "      <td>226.2050</td>\n",
       "      <td>443.1060</td>\n",
       "      <td>291.9259</td>\n",
       "      <td>544.1441</td>\n",
       "      <td>583.5600</td>\n",
       "      <td>568.0413</td>\n",
       "      <td>527.3896</td>\n",
       "      <td>559.0232</td>\n",
       "      <td>546.1975</td>\n",
       "      <td>569.9907</td>\n",
       "      <td>542.7089</td>\n",
       "      <td>542.6554</td>\n",
       "      <td>552.5827</td>\n",
       "      <td>527.6042</td>\n",
       "      <td>572.3730</td>\n",
       "      <td>522.8171</td>\n",
       "      <td>573.9738</td>\n",
       "      <td>556.4121</td>\n",
       "      <td>547.5127</td>\n",
       "      <td>594.7303</td>\n",
       "      <td>533.1944</td>\n",
       "      <td>599.2221</td>\n",
       "      <td>544.0785</td>\n",
       "      <td>557.6838</td>\n",
       "      <td>552.0713</td>\n",
       "      <td>498.2050</td>\n",
       "      <td>567.1273</td>\n",
       "      <td>519.3687</td>\n",
       "      <td>558.8593</td>\n",
       "      <td>534.6386</td>\n",
       "      <td>599.9881</td>\n",
       "      <td>535.1184</td>\n",
       "      <td>533.3071</td>\n",
       "      <td>559.5628</td>\n",
       "      <td>538.4627</td>\n",
       "      <td>512.5910</td>\n",
       "      <td>561.0331</td>\n",
       "      <td>547.2715</td>\n",
       "      <td>568.3845</td>\n",
       "      <td>528.5830</td>\n",
       "      <td>571.7185</td>\n",
       "      <td>527.1043</td>\n",
       "      <td>528.0528</td>\n",
       "      <td>516.6702</td>\n",
       "      <td>562.3251</td>\n",
       "      <td>566.8520</td>\n",
       "      <td>523.2167</td>\n",
       "      <td>530.9225</td>\n",
       "      <td>563.7212</td>\n",
       "      <td>562.6040</td>\n",
       "      <td>551.5517</td>\n",
       "      <td>569.3718</td>\n",
       "      <td>534.2661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20006</td>\n",
       "      <td>280.8558</td>\n",
       "      <td>297.0925</td>\n",
       "      <td>200.0112</td>\n",
       "      <td>191.8327</td>\n",
       "      <td>316.7312</td>\n",
       "      <td>341.5848</td>\n",
       "      <td>492.2798</td>\n",
       "      <td>274.4301</td>\n",
       "      <td>377.6528</td>\n",
       "      <td>421.6972</td>\n",
       "      <td>413.1416</td>\n",
       "      <td>324.3731</td>\n",
       "      <td>518.1161</td>\n",
       "      <td>217.6581</td>\n",
       "      <td>437.6769</td>\n",
       "      <td>232.0661</td>\n",
       "      <td>191.8179</td>\n",
       "      <td>348.1488</td>\n",
       "      <td>317.8459</td>\n",
       "      <td>232.0272</td>\n",
       "      <td>348.0276</td>\n",
       "      <td>263.6566</td>\n",
       "      <td>294.2041</td>\n",
       "      <td>345.0845</td>\n",
       "      <td>363.8317</td>\n",
       "      <td>487.6766</td>\n",
       "      <td>340.4414</td>\n",
       "      <td>385.0577</td>\n",
       "      <td>381.0684</td>\n",
       "      <td>350.4612</td>\n",
       "      <td>316.8833</td>\n",
       "      <td>362.4133</td>\n",
       "      <td>256.2034</td>\n",
       "      <td>473.7647</td>\n",
       "      <td>508.9326</td>\n",
       "      <td>530.5219</td>\n",
       "      <td>426.1308</td>\n",
       "      <td>272.1187</td>\n",
       "      <td>362.3022</td>\n",
       "      <td>240.4158</td>\n",
       "      <td>218.0258</td>\n",
       "      <td>204.0284</td>\n",
       "      <td>347.8195</td>\n",
       "      <td>299.4139</td>\n",
       "      <td>141.3815</td>\n",
       "      <td>249.3513</td>\n",
       "      <td>259.3560</td>\n",
       "      <td>205.5951</td>\n",
       "      <td>325.8334</td>\n",
       "      <td>241.7221</td>\n",
       "      <td>404.5213</td>\n",
       "      <td>395.3825</td>\n",
       "      <td>430.8314</td>\n",
       "      <td>400.1686</td>\n",
       "      <td>393.8954</td>\n",
       "      <td>393.7507</td>\n",
       "      <td>408.0349</td>\n",
       "      <td>404.2596</td>\n",
       "      <td>410.8109</td>\n",
       "      <td>409.8276</td>\n",
       "      <td>393.6516</td>\n",
       "      <td>390.2073</td>\n",
       "      <td>390.0506</td>\n",
       "      <td>397.2758</td>\n",
       "      <td>404.0817</td>\n",
       "      <td>418.2886</td>\n",
       "      <td>371.7756</td>\n",
       "      <td>415.7784</td>\n",
       "      <td>364.1279</td>\n",
       "      <td>403.2674</td>\n",
       "      <td>410.7887</td>\n",
       "      <td>402.7183</td>\n",
       "      <td>406.2387</td>\n",
       "      <td>428.8983</td>\n",
       "      <td>394.8739</td>\n",
       "      <td>411.9006</td>\n",
       "      <td>418.5347</td>\n",
       "      <td>401.4928</td>\n",
       "      <td>404.0425</td>\n",
       "      <td>380.6641</td>\n",
       "      <td>393.6092</td>\n",
       "      <td>404.1576</td>\n",
       "      <td>405.0446</td>\n",
       "      <td>425.0734</td>\n",
       "      <td>399.2897</td>\n",
       "      <td>414.6534</td>\n",
       "      <td>415.5160</td>\n",
       "      <td>409.0630</td>\n",
       "      <td>399.4240</td>\n",
       "      <td>403.2469</td>\n",
       "      <td>409.8919</td>\n",
       "      <td>408.7155</td>\n",
       "      <td>366.1578</td>\n",
       "      <td>417.8358</td>\n",
       "      <td>425.4475</td>\n",
       "      <td>363.5404</td>\n",
       "      <td>400.7977</td>\n",
       "      <td>396.5690</td>\n",
       "      <td>417.7821</td>\n",
       "      <td>387.5821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20007</td>\n",
       "      <td>296.2708</td>\n",
       "      <td>333.1086</td>\n",
       "      <td>230.3921</td>\n",
       "      <td>172.9154</td>\n",
       "      <td>289.8467</td>\n",
       "      <td>319.6021</td>\n",
       "      <td>433.3800</td>\n",
       "      <td>305.9345</td>\n",
       "      <td>354.4533</td>\n",
       "      <td>442.3618</td>\n",
       "      <td>450.8955</td>\n",
       "      <td>354.1555</td>\n",
       "      <td>478.4842</td>\n",
       "      <td>223.8989</td>\n",
       "      <td>461.2814</td>\n",
       "      <td>265.0870</td>\n",
       "      <td>199.1672</td>\n",
       "      <td>387.2059</td>\n",
       "      <td>291.1370</td>\n",
       "      <td>260.3091</td>\n",
       "      <td>316.5783</td>\n",
       "      <td>292.1387</td>\n",
       "      <td>309.7810</td>\n",
       "      <td>404.1246</td>\n",
       "      <td>375.9898</td>\n",
       "      <td>509.4577</td>\n",
       "      <td>335.1607</td>\n",
       "      <td>368.6393</td>\n",
       "      <td>379.1519</td>\n",
       "      <td>365.4007</td>\n",
       "      <td>295.3626</td>\n",
       "      <td>344.8303</td>\n",
       "      <td>321.2924</td>\n",
       "      <td>416.0146</td>\n",
       "      <td>483.5619</td>\n",
       "      <td>468.4059</td>\n",
       "      <td>414.9564</td>\n",
       "      <td>257.1808</td>\n",
       "      <td>325.3233</td>\n",
       "      <td>243.1497</td>\n",
       "      <td>251.2310</td>\n",
       "      <td>202.3355</td>\n",
       "      <td>340.2116</td>\n",
       "      <td>296.1609</td>\n",
       "      <td>172.2334</td>\n",
       "      <td>224.0494</td>\n",
       "      <td>274.8239</td>\n",
       "      <td>207.3195</td>\n",
       "      <td>315.5432</td>\n",
       "      <td>251.8892</td>\n",
       "      <td>411.6529</td>\n",
       "      <td>397.2526</td>\n",
       "      <td>422.2391</td>\n",
       "      <td>381.1503</td>\n",
       "      <td>368.1242</td>\n",
       "      <td>357.9685</td>\n",
       "      <td>386.1497</td>\n",
       "      <td>388.8716</td>\n",
       "      <td>377.5113</td>\n",
       "      <td>343.4933</td>\n",
       "      <td>389.1583</td>\n",
       "      <td>376.3100</td>\n",
       "      <td>390.0131</td>\n",
       "      <td>369.5434</td>\n",
       "      <td>378.9110</td>\n",
       "      <td>404.0249</td>\n",
       "      <td>391.5741</td>\n",
       "      <td>375.5589</td>\n",
       "      <td>372.1809</td>\n",
       "      <td>386.7686</td>\n",
       "      <td>382.5604</td>\n",
       "      <td>388.7550</td>\n",
       "      <td>382.6798</td>\n",
       "      <td>391.2735</td>\n",
       "      <td>380.5871</td>\n",
       "      <td>397.4668</td>\n",
       "      <td>395.5004</td>\n",
       "      <td>384.0225</td>\n",
       "      <td>345.5175</td>\n",
       "      <td>371.0485</td>\n",
       "      <td>397.3732</td>\n",
       "      <td>377.7882</td>\n",
       "      <td>386.8561</td>\n",
       "      <td>402.1145</td>\n",
       "      <td>377.3998</td>\n",
       "      <td>377.5702</td>\n",
       "      <td>384.2676</td>\n",
       "      <td>385.7685</td>\n",
       "      <td>371.2544</td>\n",
       "      <td>387.6094</td>\n",
       "      <td>365.7843</td>\n",
       "      <td>387.6839</td>\n",
       "      <td>378.8600</td>\n",
       "      <td>382.0768</td>\n",
       "      <td>397.7851</td>\n",
       "      <td>400.8314</td>\n",
       "      <td>422.2712</td>\n",
       "      <td>371.9059</td>\n",
       "      <td>369.4626</td>\n",
       "      <td>396.5289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20008</td>\n",
       "      <td>198.6281</td>\n",
       "      <td>284.4379</td>\n",
       "      <td>333.9463</td>\n",
       "      <td>224.3885</td>\n",
       "      <td>339.9262</td>\n",
       "      <td>307.8564</td>\n",
       "      <td>266.9429</td>\n",
       "      <td>354.9914</td>\n",
       "      <td>412.6861</td>\n",
       "      <td>364.4836</td>\n",
       "      <td>528.4730</td>\n",
       "      <td>296.9058</td>\n",
       "      <td>343.0237</td>\n",
       "      <td>280.4902</td>\n",
       "      <td>580.3459</td>\n",
       "      <td>458.2091</td>\n",
       "      <td>168.2576</td>\n",
       "      <td>433.6935</td>\n",
       "      <td>233.6079</td>\n",
       "      <td>241.0914</td>\n",
       "      <td>241.0656</td>\n",
       "      <td>298.0276</td>\n",
       "      <td>323.0456</td>\n",
       "      <td>344.4931</td>\n",
       "      <td>295.6278</td>\n",
       "      <td>559.3028</td>\n",
       "      <td>397.6200</td>\n",
       "      <td>324.9277</td>\n",
       "      <td>380.9037</td>\n",
       "      <td>559.4137</td>\n",
       "      <td>256.2604</td>\n",
       "      <td>300.1310</td>\n",
       "      <td>234.6953</td>\n",
       "      <td>355.8054</td>\n",
       "      <td>350.9953</td>\n",
       "      <td>471.3720</td>\n",
       "      <td>304.9469</td>\n",
       "      <td>268.8729</td>\n",
       "      <td>332.9715</td>\n",
       "      <td>160.3580</td>\n",
       "      <td>150.2656</td>\n",
       "      <td>198.5053</td>\n",
       "      <td>339.4487</td>\n",
       "      <td>328.4052</td>\n",
       "      <td>122.8347</td>\n",
       "      <td>141.4607</td>\n",
       "      <td>252.6100</td>\n",
       "      <td>211.9576</td>\n",
       "      <td>269.3908</td>\n",
       "      <td>234.1145</td>\n",
       "      <td>302.2056</td>\n",
       "      <td>297.4131</td>\n",
       "      <td>321.0250</td>\n",
       "      <td>294.3056</td>\n",
       "      <td>287.8162</td>\n",
       "      <td>333.3961</td>\n",
       "      <td>289.5512</td>\n",
       "      <td>292.8300</td>\n",
       "      <td>283.7474</td>\n",
       "      <td>267.5091</td>\n",
       "      <td>308.0737</td>\n",
       "      <td>323.0754</td>\n",
       "      <td>309.6527</td>\n",
       "      <td>309.2946</td>\n",
       "      <td>301.6813</td>\n",
       "      <td>301.3387</td>\n",
       "      <td>303.3863</td>\n",
       "      <td>276.7282</td>\n",
       "      <td>325.9806</td>\n",
       "      <td>290.2612</td>\n",
       "      <td>304.9485</td>\n",
       "      <td>330.4738</td>\n",
       "      <td>296.7137</td>\n",
       "      <td>292.2735</td>\n",
       "      <td>297.0934</td>\n",
       "      <td>318.0235</td>\n",
       "      <td>281.2386</td>\n",
       "      <td>263.1197</td>\n",
       "      <td>343.6680</td>\n",
       "      <td>325.0155</td>\n",
       "      <td>340.4911</td>\n",
       "      <td>283.2066</td>\n",
       "      <td>287.3470</td>\n",
       "      <td>303.8380</td>\n",
       "      <td>294.6661</td>\n",
       "      <td>281.6929</td>\n",
       "      <td>270.2542</td>\n",
       "      <td>309.3288</td>\n",
       "      <td>274.5401</td>\n",
       "      <td>279.5149</td>\n",
       "      <td>296.3370</td>\n",
       "      <td>293.9492</td>\n",
       "      <td>294.8128</td>\n",
       "      <td>286.3248</td>\n",
       "      <td>387.1135</td>\n",
       "      <td>344.3502</td>\n",
       "      <td>323.7404</td>\n",
       "      <td>300.9923</td>\n",
       "      <td>281.1303</td>\n",
       "      <td>330.6605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20009</td>\n",
       "      <td>288.4441</td>\n",
       "      <td>489.4061</td>\n",
       "      <td>317.1825</td>\n",
       "      <td>288.5723</td>\n",
       "      <td>299.8247</td>\n",
       "      <td>346.9036</td>\n",
       "      <td>444.3189</td>\n",
       "      <td>291.3856</td>\n",
       "      <td>384.5548</td>\n",
       "      <td>388.0910</td>\n",
       "      <td>476.0784</td>\n",
       "      <td>361.4950</td>\n",
       "      <td>361.7348</td>\n",
       "      <td>245.0797</td>\n",
       "      <td>488.4361</td>\n",
       "      <td>250.2549</td>\n",
       "      <td>191.4568</td>\n",
       "      <td>445.3661</td>\n",
       "      <td>358.9844</td>\n",
       "      <td>251.9758</td>\n",
       "      <td>209.3933</td>\n",
       "      <td>319.4068</td>\n",
       "      <td>340.6290</td>\n",
       "      <td>306.0844</td>\n",
       "      <td>375.2443</td>\n",
       "      <td>631.3837</td>\n",
       "      <td>394.4087</td>\n",
       "      <td>341.7485</td>\n",
       "      <td>413.2034</td>\n",
       "      <td>395.8785</td>\n",
       "      <td>283.3976</td>\n",
       "      <td>458.4435</td>\n",
       "      <td>228.1709</td>\n",
       "      <td>458.4774</td>\n",
       "      <td>416.3999</td>\n",
       "      <td>529.8915</td>\n",
       "      <td>360.2247</td>\n",
       "      <td>340.0416</td>\n",
       "      <td>399.9753</td>\n",
       "      <td>161.8585</td>\n",
       "      <td>206.6255</td>\n",
       "      <td>179.0312</td>\n",
       "      <td>343.3600</td>\n",
       "      <td>310.1695</td>\n",
       "      <td>194.2325</td>\n",
       "      <td>204.9117</td>\n",
       "      <td>251.8172</td>\n",
       "      <td>233.0932</td>\n",
       "      <td>420.8718</td>\n",
       "      <td>242.0098</td>\n",
       "      <td>550.8647</td>\n",
       "      <td>527.6336</td>\n",
       "      <td>501.4489</td>\n",
       "      <td>516.5569</td>\n",
       "      <td>485.8251</td>\n",
       "      <td>523.6324</td>\n",
       "      <td>505.0479</td>\n",
       "      <td>501.2233</td>\n",
       "      <td>504.1180</td>\n",
       "      <td>510.2945</td>\n",
       "      <td>483.3552</td>\n",
       "      <td>496.1605</td>\n",
       "      <td>543.3705</td>\n",
       "      <td>509.3443</td>\n",
       "      <td>500.3376</td>\n",
       "      <td>525.5023</td>\n",
       "      <td>535.2814</td>\n",
       "      <td>492.1381</td>\n",
       "      <td>517.1385</td>\n",
       "      <td>506.5189</td>\n",
       "      <td>532.3069</td>\n",
       "      <td>512.1559</td>\n",
       "      <td>502.9536</td>\n",
       "      <td>480.6616</td>\n",
       "      <td>495.4314</td>\n",
       "      <td>488.9331</td>\n",
       "      <td>513.5672</td>\n",
       "      <td>528.8721</td>\n",
       "      <td>509.6337</td>\n",
       "      <td>489.2893</td>\n",
       "      <td>470.7552</td>\n",
       "      <td>498.4859</td>\n",
       "      <td>481.5913</td>\n",
       "      <td>496.6831</td>\n",
       "      <td>477.0782</td>\n",
       "      <td>506.9008</td>\n",
       "      <td>497.4663</td>\n",
       "      <td>513.7261</td>\n",
       "      <td>477.2397</td>\n",
       "      <td>490.2736</td>\n",
       "      <td>505.6559</td>\n",
       "      <td>508.3209</td>\n",
       "      <td>477.1259</td>\n",
       "      <td>511.8977</td>\n",
       "      <td>472.4033</td>\n",
       "      <td>540.0916</td>\n",
       "      <td>496.2674</td>\n",
       "      <td>483.5904</td>\n",
       "      <td>491.7839</td>\n",
       "      <td>487.5864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20010</td>\n",
       "      <td>266.6828</td>\n",
       "      <td>322.5042</td>\n",
       "      <td>260.6862</td>\n",
       "      <td>210.4291</td>\n",
       "      <td>335.9005</td>\n",
       "      <td>297.8471</td>\n",
       "      <td>400.6422</td>\n",
       "      <td>332.7712</td>\n",
       "      <td>417.2506</td>\n",
       "      <td>472.9805</td>\n",
       "      <td>500.3463</td>\n",
       "      <td>349.7953</td>\n",
       "      <td>391.3864</td>\n",
       "      <td>255.6364</td>\n",
       "      <td>505.7065</td>\n",
       "      <td>248.6692</td>\n",
       "      <td>200.2703</td>\n",
       "      <td>421.4165</td>\n",
       "      <td>305.0933</td>\n",
       "      <td>249.6359</td>\n",
       "      <td>230.1592</td>\n",
       "      <td>320.4061</td>\n",
       "      <td>305.5734</td>\n",
       "      <td>316.1431</td>\n",
       "      <td>365.4896</td>\n",
       "      <td>533.0317</td>\n",
       "      <td>380.5413</td>\n",
       "      <td>321.7933</td>\n",
       "      <td>423.7956</td>\n",
       "      <td>438.3373</td>\n",
       "      <td>317.9659</td>\n",
       "      <td>444.5315</td>\n",
       "      <td>276.9650</td>\n",
       "      <td>492.2325</td>\n",
       "      <td>495.9268</td>\n",
       "      <td>533.6289</td>\n",
       "      <td>311.4711</td>\n",
       "      <td>286.1570</td>\n",
       "      <td>359.1872</td>\n",
       "      <td>208.0858</td>\n",
       "      <td>205.4515</td>\n",
       "      <td>196.9852</td>\n",
       "      <td>356.1472</td>\n",
       "      <td>297.4243</td>\n",
       "      <td>155.3479</td>\n",
       "      <td>201.2424</td>\n",
       "      <td>268.2568</td>\n",
       "      <td>253.0703</td>\n",
       "      <td>328.8226</td>\n",
       "      <td>274.9599</td>\n",
       "      <td>371.1073</td>\n",
       "      <td>358.8705</td>\n",
       "      <td>384.4238</td>\n",
       "      <td>329.7878</td>\n",
       "      <td>341.8712</td>\n",
       "      <td>356.5273</td>\n",
       "      <td>352.9622</td>\n",
       "      <td>372.1420</td>\n",
       "      <td>352.2015</td>\n",
       "      <td>355.5036</td>\n",
       "      <td>369.3624</td>\n",
       "      <td>342.6946</td>\n",
       "      <td>348.3828</td>\n",
       "      <td>347.6320</td>\n",
       "      <td>357.4441</td>\n",
       "      <td>353.8671</td>\n",
       "      <td>334.5460</td>\n",
       "      <td>366.7285</td>\n",
       "      <td>346.7992</td>\n",
       "      <td>352.5910</td>\n",
       "      <td>343.8249</td>\n",
       "      <td>363.1518</td>\n",
       "      <td>370.8933</td>\n",
       "      <td>380.3673</td>\n",
       "      <td>342.8132</td>\n",
       "      <td>357.3465</td>\n",
       "      <td>347.0594</td>\n",
       "      <td>355.8490</td>\n",
       "      <td>316.6055</td>\n",
       "      <td>341.4541</td>\n",
       "      <td>352.5249</td>\n",
       "      <td>350.0840</td>\n",
       "      <td>362.0490</td>\n",
       "      <td>389.4851</td>\n",
       "      <td>346.4531</td>\n",
       "      <td>360.4720</td>\n",
       "      <td>342.0493</td>\n",
       "      <td>343.1289</td>\n",
       "      <td>349.9807</td>\n",
       "      <td>340.8923</td>\n",
       "      <td>357.0990</td>\n",
       "      <td>351.5350</td>\n",
       "      <td>333.6103</td>\n",
       "      <td>339.9525</td>\n",
       "      <td>344.2737</td>\n",
       "      <td>369.0546</td>\n",
       "      <td>361.5238</td>\n",
       "      <td>373.9888</td>\n",
       "      <td>364.0245</td>\n",
       "      <td>353.2926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20011</td>\n",
       "      <td>358.1515</td>\n",
       "      <td>363.5171</td>\n",
       "      <td>210.6704</td>\n",
       "      <td>224.7597</td>\n",
       "      <td>265.7324</td>\n",
       "      <td>290.0558</td>\n",
       "      <td>408.0057</td>\n",
       "      <td>285.2424</td>\n",
       "      <td>294.9691</td>\n",
       "      <td>355.2982</td>\n",
       "      <td>406.4937</td>\n",
       "      <td>324.9219</td>\n",
       "      <td>338.6895</td>\n",
       "      <td>204.4557</td>\n",
       "      <td>376.3290</td>\n",
       "      <td>240.7869</td>\n",
       "      <td>208.5515</td>\n",
       "      <td>307.0099</td>\n",
       "      <td>317.7848</td>\n",
       "      <td>246.0946</td>\n",
       "      <td>372.6419</td>\n",
       "      <td>308.3842</td>\n",
       "      <td>278.3726</td>\n",
       "      <td>279.7374</td>\n",
       "      <td>336.4323</td>\n",
       "      <td>450.4112</td>\n",
       "      <td>301.6436</td>\n",
       "      <td>417.0648</td>\n",
       "      <td>315.9605</td>\n",
       "      <td>291.2880</td>\n",
       "      <td>351.9256</td>\n",
       "      <td>351.5687</td>\n",
       "      <td>226.5748</td>\n",
       "      <td>404.8754</td>\n",
       "      <td>388.2604</td>\n",
       "      <td>416.7029</td>\n",
       "      <td>384.0909</td>\n",
       "      <td>287.7541</td>\n",
       "      <td>345.2965</td>\n",
       "      <td>234.2582</td>\n",
       "      <td>256.0314</td>\n",
       "      <td>195.8936</td>\n",
       "      <td>284.2144</td>\n",
       "      <td>259.5845</td>\n",
       "      <td>171.7628</td>\n",
       "      <td>253.5425</td>\n",
       "      <td>227.5767</td>\n",
       "      <td>198.8994</td>\n",
       "      <td>278.9482</td>\n",
       "      <td>251.5134</td>\n",
       "      <td>339.5526</td>\n",
       "      <td>343.5585</td>\n",
       "      <td>337.1693</td>\n",
       "      <td>316.7349</td>\n",
       "      <td>328.0732</td>\n",
       "      <td>364.7697</td>\n",
       "      <td>329.6051</td>\n",
       "      <td>361.7843</td>\n",
       "      <td>333.5858</td>\n",
       "      <td>322.1346</td>\n",
       "      <td>319.0740</td>\n",
       "      <td>318.0418</td>\n",
       "      <td>304.1087</td>\n",
       "      <td>328.5819</td>\n",
       "      <td>335.6796</td>\n",
       "      <td>343.1943</td>\n",
       "      <td>315.4289</td>\n",
       "      <td>333.7452</td>\n",
       "      <td>306.8825</td>\n",
       "      <td>342.1581</td>\n",
       "      <td>324.3047</td>\n",
       "      <td>342.6534</td>\n",
       "      <td>353.0988</td>\n",
       "      <td>350.8351</td>\n",
       "      <td>328.2565</td>\n",
       "      <td>348.9957</td>\n",
       "      <td>328.9253</td>\n",
       "      <td>320.1536</td>\n",
       "      <td>310.3553</td>\n",
       "      <td>308.2447</td>\n",
       "      <td>316.1441</td>\n",
       "      <td>330.8929</td>\n",
       "      <td>323.0334</td>\n",
       "      <td>352.2038</td>\n",
       "      <td>345.0770</td>\n",
       "      <td>330.9756</td>\n",
       "      <td>334.9295</td>\n",
       "      <td>323.2631</td>\n",
       "      <td>308.6554</td>\n",
       "      <td>323.1946</td>\n",
       "      <td>338.3553</td>\n",
       "      <td>331.3668</td>\n",
       "      <td>294.6330</td>\n",
       "      <td>315.7709</td>\n",
       "      <td>355.4569</td>\n",
       "      <td>312.1224</td>\n",
       "      <td>318.4558</td>\n",
       "      <td>335.1929</td>\n",
       "      <td>321.7832</td>\n",
       "      <td>340.1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20012</td>\n",
       "      <td>152.7624</td>\n",
       "      <td>215.7090</td>\n",
       "      <td>251.2873</td>\n",
       "      <td>132.2263</td>\n",
       "      <td>241.5075</td>\n",
       "      <td>239.6093</td>\n",
       "      <td>182.9111</td>\n",
       "      <td>261.3687</td>\n",
       "      <td>292.9967</td>\n",
       "      <td>281.8690</td>\n",
       "      <td>340.6937</td>\n",
       "      <td>201.2960</td>\n",
       "      <td>238.7414</td>\n",
       "      <td>190.4474</td>\n",
       "      <td>337.0681</td>\n",
       "      <td>229.5300</td>\n",
       "      <td>142.4247</td>\n",
       "      <td>282.1682</td>\n",
       "      <td>188.7193</td>\n",
       "      <td>190.4253</td>\n",
       "      <td>181.2191</td>\n",
       "      <td>192.9039</td>\n",
       "      <td>227.8955</td>\n",
       "      <td>248.9048</td>\n",
       "      <td>223.3717</td>\n",
       "      <td>340.7058</td>\n",
       "      <td>272.6329</td>\n",
       "      <td>204.8439</td>\n",
       "      <td>281.1164</td>\n",
       "      <td>298.7244</td>\n",
       "      <td>156.8392</td>\n",
       "      <td>227.1407</td>\n",
       "      <td>208.1576</td>\n",
       "      <td>260.1514</td>\n",
       "      <td>258.3961</td>\n",
       "      <td>291.8790</td>\n",
       "      <td>229.0626</td>\n",
       "      <td>194.3356</td>\n",
       "      <td>227.7295</td>\n",
       "      <td>138.1050</td>\n",
       "      <td>132.3875</td>\n",
       "      <td>167.1885</td>\n",
       "      <td>259.4132</td>\n",
       "      <td>224.1344</td>\n",
       "      <td>119.3456</td>\n",
       "      <td>130.5496</td>\n",
       "      <td>214.6899</td>\n",
       "      <td>176.7520</td>\n",
       "      <td>211.0146</td>\n",
       "      <td>153.2183</td>\n",
       "      <td>252.9333</td>\n",
       "      <td>248.7924</td>\n",
       "      <td>261.6723</td>\n",
       "      <td>243.7042</td>\n",
       "      <td>233.2065</td>\n",
       "      <td>328.8353</td>\n",
       "      <td>241.2763</td>\n",
       "      <td>260.2333</td>\n",
       "      <td>249.8184</td>\n",
       "      <td>262.6249</td>\n",
       "      <td>267.8960</td>\n",
       "      <td>282.9733</td>\n",
       "      <td>348.4836</td>\n",
       "      <td>256.6907</td>\n",
       "      <td>272.5592</td>\n",
       "      <td>252.8324</td>\n",
       "      <td>307.5003</td>\n",
       "      <td>238.9906</td>\n",
       "      <td>304.3496</td>\n",
       "      <td>247.4864</td>\n",
       "      <td>259.2143</td>\n",
       "      <td>264.1375</td>\n",
       "      <td>246.4765</td>\n",
       "      <td>247.6196</td>\n",
       "      <td>244.8284</td>\n",
       "      <td>255.9343</td>\n",
       "      <td>240.5712</td>\n",
       "      <td>244.1653</td>\n",
       "      <td>314.9363</td>\n",
       "      <td>255.4976</td>\n",
       "      <td>312.0808</td>\n",
       "      <td>241.5967</td>\n",
       "      <td>255.3192</td>\n",
       "      <td>248.2541</td>\n",
       "      <td>257.4813</td>\n",
       "      <td>234.4254</td>\n",
       "      <td>248.0974</td>\n",
       "      <td>247.0509</td>\n",
       "      <td>236.3967</td>\n",
       "      <td>248.8217</td>\n",
       "      <td>241.0563</td>\n",
       "      <td>251.3604</td>\n",
       "      <td>300.8934</td>\n",
       "      <td>266.0387</td>\n",
       "      <td>316.6324</td>\n",
       "      <td>269.5446</td>\n",
       "      <td>326.9195</td>\n",
       "      <td>244.3334</td>\n",
       "      <td>253.8105</td>\n",
       "      <td>273.7076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20013</td>\n",
       "      <td>203.5723</td>\n",
       "      <td>276.7628</td>\n",
       "      <td>220.0168</td>\n",
       "      <td>165.4129</td>\n",
       "      <td>254.1183</td>\n",
       "      <td>224.2620</td>\n",
       "      <td>340.0742</td>\n",
       "      <td>273.0446</td>\n",
       "      <td>356.4384</td>\n",
       "      <td>388.2974</td>\n",
       "      <td>386.7734</td>\n",
       "      <td>263.7076</td>\n",
       "      <td>345.8524</td>\n",
       "      <td>197.8754</td>\n",
       "      <td>399.2851</td>\n",
       "      <td>217.2249</td>\n",
       "      <td>178.9970</td>\n",
       "      <td>276.9226</td>\n",
       "      <td>257.5556</td>\n",
       "      <td>221.9117</td>\n",
       "      <td>177.3840</td>\n",
       "      <td>265.4717</td>\n",
       "      <td>261.2240</td>\n",
       "      <td>300.2694</td>\n",
       "      <td>284.2005</td>\n",
       "      <td>400.2375</td>\n",
       "      <td>304.3654</td>\n",
       "      <td>280.6865</td>\n",
       "      <td>345.1459</td>\n",
       "      <td>269.4763</td>\n",
       "      <td>268.8552</td>\n",
       "      <td>313.6452</td>\n",
       "      <td>209.2873</td>\n",
       "      <td>318.2873</td>\n",
       "      <td>375.9600</td>\n",
       "      <td>463.6531</td>\n",
       "      <td>337.3481</td>\n",
       "      <td>243.7714</td>\n",
       "      <td>323.3137</td>\n",
       "      <td>176.5419</td>\n",
       "      <td>196.6377</td>\n",
       "      <td>175.2677</td>\n",
       "      <td>285.2388</td>\n",
       "      <td>251.4367</td>\n",
       "      <td>146.3316</td>\n",
       "      <td>186.4424</td>\n",
       "      <td>243.3664</td>\n",
       "      <td>166.3571</td>\n",
       "      <td>249.1559</td>\n",
       "      <td>199.4850</td>\n",
       "      <td>348.3843</td>\n",
       "      <td>371.3341</td>\n",
       "      <td>368.0367</td>\n",
       "      <td>363.9905</td>\n",
       "      <td>348.6221</td>\n",
       "      <td>342.9157</td>\n",
       "      <td>344.9277</td>\n",
       "      <td>344.1065</td>\n",
       "      <td>346.0304</td>\n",
       "      <td>362.4932</td>\n",
       "      <td>359.5920</td>\n",
       "      <td>352.6896</td>\n",
       "      <td>345.6869</td>\n",
       "      <td>343.5948</td>\n",
       "      <td>349.4741</td>\n",
       "      <td>365.3018</td>\n",
       "      <td>350.6750</td>\n",
       "      <td>337.6674</td>\n",
       "      <td>330.4016</td>\n",
       "      <td>346.1878</td>\n",
       "      <td>376.5712</td>\n",
       "      <td>364.1119</td>\n",
       "      <td>390.4145</td>\n",
       "      <td>368.8434</td>\n",
       "      <td>332.1354</td>\n",
       "      <td>374.8305</td>\n",
       "      <td>348.8912</td>\n",
       "      <td>360.3807</td>\n",
       "      <td>333.8558</td>\n",
       "      <td>365.6669</td>\n",
       "      <td>337.9571</td>\n",
       "      <td>347.1644</td>\n",
       "      <td>334.5584</td>\n",
       "      <td>361.4828</td>\n",
       "      <td>379.4414</td>\n",
       "      <td>323.5271</td>\n",
       "      <td>349.0451</td>\n",
       "      <td>346.3999</td>\n",
       "      <td>340.4211</td>\n",
       "      <td>341.8714</td>\n",
       "      <td>388.1345</td>\n",
       "      <td>368.3155</td>\n",
       "      <td>336.5977</td>\n",
       "      <td>345.1350</td>\n",
       "      <td>332.8562</td>\n",
       "      <td>362.8109</td>\n",
       "      <td>365.6833</td>\n",
       "      <td>345.7487</td>\n",
       "      <td>351.6586</td>\n",
       "      <td>375.7341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20014</td>\n",
       "      <td>183.7936</td>\n",
       "      <td>285.0066</td>\n",
       "      <td>335.6780</td>\n",
       "      <td>185.3158</td>\n",
       "      <td>269.0562</td>\n",
       "      <td>263.3122</td>\n",
       "      <td>269.2075</td>\n",
       "      <td>318.2182</td>\n",
       "      <td>389.6828</td>\n",
       "      <td>373.3569</td>\n",
       "      <td>479.5167</td>\n",
       "      <td>269.8575</td>\n",
       "      <td>297.9926</td>\n",
       "      <td>230.4515</td>\n",
       "      <td>449.3620</td>\n",
       "      <td>275.2258</td>\n",
       "      <td>183.4015</td>\n",
       "      <td>364.6313</td>\n",
       "      <td>238.0948</td>\n",
       "      <td>232.6436</td>\n",
       "      <td>232.7774</td>\n",
       "      <td>237.4342</td>\n",
       "      <td>249.8011</td>\n",
       "      <td>325.7656</td>\n",
       "      <td>312.9087</td>\n",
       "      <td>500.2722</td>\n",
       "      <td>346.6465</td>\n",
       "      <td>245.7995</td>\n",
       "      <td>413.0675</td>\n",
       "      <td>378.8074</td>\n",
       "      <td>248.9028</td>\n",
       "      <td>297.0576</td>\n",
       "      <td>219.1790</td>\n",
       "      <td>291.0026</td>\n",
       "      <td>373.1500</td>\n",
       "      <td>457.9652</td>\n",
       "      <td>267.7935</td>\n",
       "      <td>255.2622</td>\n",
       "      <td>307.1646</td>\n",
       "      <td>151.8990</td>\n",
       "      <td>177.1693</td>\n",
       "      <td>180.9748</td>\n",
       "      <td>333.2203</td>\n",
       "      <td>286.8439</td>\n",
       "      <td>156.4577</td>\n",
       "      <td>164.7794</td>\n",
       "      <td>253.8020</td>\n",
       "      <td>208.8929</td>\n",
       "      <td>276.8939</td>\n",
       "      <td>223.1575</td>\n",
       "      <td>312.0953</td>\n",
       "      <td>298.7461</td>\n",
       "      <td>313.4228</td>\n",
       "      <td>287.7018</td>\n",
       "      <td>281.3126</td>\n",
       "      <td>328.8973</td>\n",
       "      <td>289.2942</td>\n",
       "      <td>294.1994</td>\n",
       "      <td>304.2267</td>\n",
       "      <td>287.2626</td>\n",
       "      <td>292.2864</td>\n",
       "      <td>328.4848</td>\n",
       "      <td>310.6166</td>\n",
       "      <td>311.8122</td>\n",
       "      <td>305.2852</td>\n",
       "      <td>303.6402</td>\n",
       "      <td>303.7448</td>\n",
       "      <td>277.5135</td>\n",
       "      <td>303.8037</td>\n",
       "      <td>303.6988</td>\n",
       "      <td>283.4085</td>\n",
       "      <td>298.7447</td>\n",
       "      <td>300.8626</td>\n",
       "      <td>296.6203</td>\n",
       "      <td>296.1587</td>\n",
       "      <td>307.6573</td>\n",
       "      <td>296.0689</td>\n",
       "      <td>302.5408</td>\n",
       "      <td>319.5431</td>\n",
       "      <td>316.3471</td>\n",
       "      <td>332.4023</td>\n",
       "      <td>286.4057</td>\n",
       "      <td>299.4545</td>\n",
       "      <td>316.2545</td>\n",
       "      <td>309.0601</td>\n",
       "      <td>275.0566</td>\n",
       "      <td>297.8221</td>\n",
       "      <td>268.8218</td>\n",
       "      <td>265.5662</td>\n",
       "      <td>283.7745</td>\n",
       "      <td>300.0795</td>\n",
       "      <td>296.7173</td>\n",
       "      <td>290.7662</td>\n",
       "      <td>288.1087</td>\n",
       "      <td>313.9839</td>\n",
       "      <td>306.7648</td>\n",
       "      <td>321.0337</td>\n",
       "      <td>288.2582</td>\n",
       "      <td>306.8109</td>\n",
       "      <td>327.2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20015</td>\n",
       "      <td>246.8527</td>\n",
       "      <td>249.8953</td>\n",
       "      <td>231.7329</td>\n",
       "      <td>163.6028</td>\n",
       "      <td>272.7079</td>\n",
       "      <td>271.3721</td>\n",
       "      <td>327.3424</td>\n",
       "      <td>246.0096</td>\n",
       "      <td>331.5491</td>\n",
       "      <td>346.5272</td>\n",
       "      <td>374.9778</td>\n",
       "      <td>276.1705</td>\n",
       "      <td>344.3305</td>\n",
       "      <td>195.4508</td>\n",
       "      <td>377.2167</td>\n",
       "      <td>229.8274</td>\n",
       "      <td>199.2773</td>\n",
       "      <td>329.3021</td>\n",
       "      <td>239.9137</td>\n",
       "      <td>234.7773</td>\n",
       "      <td>229.3601</td>\n",
       "      <td>254.6186</td>\n",
       "      <td>275.0423</td>\n",
       "      <td>290.6320</td>\n",
       "      <td>313.7548</td>\n",
       "      <td>393.1445</td>\n",
       "      <td>306.4591</td>\n",
       "      <td>292.8059</td>\n",
       "      <td>315.2889</td>\n",
       "      <td>295.1258</td>\n",
       "      <td>245.1120</td>\n",
       "      <td>302.4919</td>\n",
       "      <td>212.6149</td>\n",
       "      <td>355.3387</td>\n",
       "      <td>374.1417</td>\n",
       "      <td>404.7961</td>\n",
       "      <td>297.3698</td>\n",
       "      <td>231.6110</td>\n",
       "      <td>306.5431</td>\n",
       "      <td>203.1491</td>\n",
       "      <td>196.1242</td>\n",
       "      <td>176.7333</td>\n",
       "      <td>333.8192</td>\n",
       "      <td>255.0990</td>\n",
       "      <td>182.0360</td>\n",
       "      <td>195.5681</td>\n",
       "      <td>237.6357</td>\n",
       "      <td>200.7593</td>\n",
       "      <td>298.6372</td>\n",
       "      <td>251.3197</td>\n",
       "      <td>303.1101</td>\n",
       "      <td>302.9817</td>\n",
       "      <td>283.8931</td>\n",
       "      <td>288.9779</td>\n",
       "      <td>278.9329</td>\n",
       "      <td>275.7628</td>\n",
       "      <td>289.4160</td>\n",
       "      <td>291.7311</td>\n",
       "      <td>299.8438</td>\n",
       "      <td>263.6394</td>\n",
       "      <td>288.7189</td>\n",
       "      <td>311.7105</td>\n",
       "      <td>315.9326</td>\n",
       "      <td>299.4993</td>\n",
       "      <td>297.9475</td>\n",
       "      <td>294.1146</td>\n",
       "      <td>285.8324</td>\n",
       "      <td>285.8009</td>\n",
       "      <td>287.1232</td>\n",
       "      <td>287.3833</td>\n",
       "      <td>282.2822</td>\n",
       "      <td>297.6469</td>\n",
       "      <td>302.6973</td>\n",
       "      <td>287.7439</td>\n",
       "      <td>292.0352</td>\n",
       "      <td>277.0015</td>\n",
       "      <td>294.9486</td>\n",
       "      <td>299.7024</td>\n",
       "      <td>293.6153</td>\n",
       "      <td>294.6836</td>\n",
       "      <td>290.3659</td>\n",
       "      <td>283.5177</td>\n",
       "      <td>280.2875</td>\n",
       "      <td>289.8427</td>\n",
       "      <td>277.4487</td>\n",
       "      <td>298.1681</td>\n",
       "      <td>286.9127</td>\n",
       "      <td>293.9831</td>\n",
       "      <td>296.1117</td>\n",
       "      <td>273.9433</td>\n",
       "      <td>281.6906</td>\n",
       "      <td>284.9567</td>\n",
       "      <td>273.6435</td>\n",
       "      <td>281.7536</td>\n",
       "      <td>291.4132</td>\n",
       "      <td>303.4870</td>\n",
       "      <td>305.2968</td>\n",
       "      <td>292.0608</td>\n",
       "      <td>286.9406</td>\n",
       "      <td>303.2368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20016</td>\n",
       "      <td>266.6426</td>\n",
       "      <td>228.8782</td>\n",
       "      <td>237.3260</td>\n",
       "      <td>214.4464</td>\n",
       "      <td>231.3451</td>\n",
       "      <td>228.1676</td>\n",
       "      <td>261.4848</td>\n",
       "      <td>222.1040</td>\n",
       "      <td>266.9767</td>\n",
       "      <td>314.4025</td>\n",
       "      <td>300.7541</td>\n",
       "      <td>260.6566</td>\n",
       "      <td>287.0256</td>\n",
       "      <td>171.8345</td>\n",
       "      <td>341.1310</td>\n",
       "      <td>233.6638</td>\n",
       "      <td>186.7841</td>\n",
       "      <td>262.0691</td>\n",
       "      <td>209.8882</td>\n",
       "      <td>207.9468</td>\n",
       "      <td>275.8710</td>\n",
       "      <td>215.2436</td>\n",
       "      <td>261.8897</td>\n",
       "      <td>260.6083</td>\n",
       "      <td>285.4200</td>\n",
       "      <td>299.2360</td>\n",
       "      <td>269.7359</td>\n",
       "      <td>277.5331</td>\n",
       "      <td>236.5473</td>\n",
       "      <td>263.3135</td>\n",
       "      <td>202.7196</td>\n",
       "      <td>273.1141</td>\n",
       "      <td>234.4125</td>\n",
       "      <td>305.9764</td>\n",
       "      <td>338.6490</td>\n",
       "      <td>319.3119</td>\n",
       "      <td>243.8154</td>\n",
       "      <td>214.0019</td>\n",
       "      <td>262.7527</td>\n",
       "      <td>214.4642</td>\n",
       "      <td>210.0442</td>\n",
       "      <td>189.9209</td>\n",
       "      <td>265.7623</td>\n",
       "      <td>216.5580</td>\n",
       "      <td>164.8940</td>\n",
       "      <td>192.4227</td>\n",
       "      <td>211.5972</td>\n",
       "      <td>182.2968</td>\n",
       "      <td>236.4318</td>\n",
       "      <td>215.4122</td>\n",
       "      <td>257.7431</td>\n",
       "      <td>250.1832</td>\n",
       "      <td>236.9269</td>\n",
       "      <td>233.1023</td>\n",
       "      <td>222.8216</td>\n",
       "      <td>226.8299</td>\n",
       "      <td>241.6387</td>\n",
       "      <td>233.4437</td>\n",
       "      <td>253.2596</td>\n",
       "      <td>227.3268</td>\n",
       "      <td>237.7536</td>\n",
       "      <td>235.9877</td>\n",
       "      <td>220.1904</td>\n",
       "      <td>256.8346</td>\n",
       "      <td>239.4916</td>\n",
       "      <td>236.6657</td>\n",
       "      <td>227.4562</td>\n",
       "      <td>236.2756</td>\n",
       "      <td>222.8971</td>\n",
       "      <td>247.7284</td>\n",
       "      <td>234.1190</td>\n",
       "      <td>242.3980</td>\n",
       "      <td>237.8890</td>\n",
       "      <td>226.4333</td>\n",
       "      <td>231.1324</td>\n",
       "      <td>237.9142</td>\n",
       "      <td>246.3950</td>\n",
       "      <td>226.8603</td>\n",
       "      <td>226.7440</td>\n",
       "      <td>234.9632</td>\n",
       "      <td>229.2556</td>\n",
       "      <td>231.4324</td>\n",
       "      <td>232.8846</td>\n",
       "      <td>251.1509</td>\n",
       "      <td>243.1565</td>\n",
       "      <td>232.5724</td>\n",
       "      <td>220.1743</td>\n",
       "      <td>225.5598</td>\n",
       "      <td>217.6012</td>\n",
       "      <td>236.4127</td>\n",
       "      <td>224.7922</td>\n",
       "      <td>237.5574</td>\n",
       "      <td>218.3215</td>\n",
       "      <td>232.7664</td>\n",
       "      <td>226.3217</td>\n",
       "      <td>237.7535</td>\n",
       "      <td>233.3358</td>\n",
       "      <td>238.8180</td>\n",
       "      <td>228.1093</td>\n",
       "      <td>234.6848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20017</td>\n",
       "      <td>243.2464</td>\n",
       "      <td>200.8084</td>\n",
       "      <td>179.5604</td>\n",
       "      <td>211.4150</td>\n",
       "      <td>235.8199</td>\n",
       "      <td>244.8614</td>\n",
       "      <td>271.7946</td>\n",
       "      <td>205.5155</td>\n",
       "      <td>266.2471</td>\n",
       "      <td>319.6903</td>\n",
       "      <td>293.5272</td>\n",
       "      <td>242.8428</td>\n",
       "      <td>318.0764</td>\n",
       "      <td>167.4097</td>\n",
       "      <td>317.4213</td>\n",
       "      <td>263.5899</td>\n",
       "      <td>174.2771</td>\n",
       "      <td>226.4587</td>\n",
       "      <td>193.6739</td>\n",
       "      <td>198.1180</td>\n",
       "      <td>209.1451</td>\n",
       "      <td>220.4877</td>\n",
       "      <td>225.7672</td>\n",
       "      <td>244.7135</td>\n",
       "      <td>254.0255</td>\n",
       "      <td>254.2892</td>\n",
       "      <td>276.5022</td>\n",
       "      <td>278.2473</td>\n",
       "      <td>223.7553</td>\n",
       "      <td>260.6682</td>\n",
       "      <td>193.8855</td>\n",
       "      <td>255.4921</td>\n",
       "      <td>224.4456</td>\n",
       "      <td>302.7612</td>\n",
       "      <td>335.8323</td>\n",
       "      <td>345.5919</td>\n",
       "      <td>261.2160</td>\n",
       "      <td>199.7535</td>\n",
       "      <td>225.2262</td>\n",
       "      <td>212.1377</td>\n",
       "      <td>202.6634</td>\n",
       "      <td>196.3146</td>\n",
       "      <td>258.2674</td>\n",
       "      <td>205.4587</td>\n",
       "      <td>165.1194</td>\n",
       "      <td>176.2527</td>\n",
       "      <td>205.7049</td>\n",
       "      <td>170.6700</td>\n",
       "      <td>224.4894</td>\n",
       "      <td>202.9740</td>\n",
       "      <td>247.5417</td>\n",
       "      <td>244.9961</td>\n",
       "      <td>272.0844</td>\n",
       "      <td>251.9825</td>\n",
       "      <td>231.0031</td>\n",
       "      <td>295.2462</td>\n",
       "      <td>243.1884</td>\n",
       "      <td>247.6595</td>\n",
       "      <td>241.4108</td>\n",
       "      <td>256.0950</td>\n",
       "      <td>253.7249</td>\n",
       "      <td>263.1696</td>\n",
       "      <td>273.1190</td>\n",
       "      <td>258.6289</td>\n",
       "      <td>257.8021</td>\n",
       "      <td>249.4350</td>\n",
       "      <td>266.3261</td>\n",
       "      <td>247.0410</td>\n",
       "      <td>258.3509</td>\n",
       "      <td>243.6188</td>\n",
       "      <td>271.2289</td>\n",
       "      <td>255.8925</td>\n",
       "      <td>244.1262</td>\n",
       "      <td>249.9372</td>\n",
       "      <td>240.4596</td>\n",
       "      <td>248.9844</td>\n",
       "      <td>242.0144</td>\n",
       "      <td>226.7629</td>\n",
       "      <td>281.1349</td>\n",
       "      <td>258.6830</td>\n",
       "      <td>273.8358</td>\n",
       "      <td>242.5474</td>\n",
       "      <td>243.7231</td>\n",
       "      <td>254.8829</td>\n",
       "      <td>235.5841</td>\n",
       "      <td>230.7175</td>\n",
       "      <td>236.0453</td>\n",
       "      <td>246.8413</td>\n",
       "      <td>236.8873</td>\n",
       "      <td>254.3229</td>\n",
       "      <td>266.1317</td>\n",
       "      <td>260.7343</td>\n",
       "      <td>278.9049</td>\n",
       "      <td>254.9065</td>\n",
       "      <td>306.3779</td>\n",
       "      <td>265.2905</td>\n",
       "      <td>277.0872</td>\n",
       "      <td>233.6228</td>\n",
       "      <td>247.2711</td>\n",
       "      <td>259.0181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20018</td>\n",
       "      <td>146.8092</td>\n",
       "      <td>173.5994</td>\n",
       "      <td>183.7167</td>\n",
       "      <td>127.8062</td>\n",
       "      <td>201.9778</td>\n",
       "      <td>178.0499</td>\n",
       "      <td>179.7174</td>\n",
       "      <td>185.8828</td>\n",
       "      <td>260.1848</td>\n",
       "      <td>268.0592</td>\n",
       "      <td>241.7156</td>\n",
       "      <td>184.4933</td>\n",
       "      <td>220.9180</td>\n",
       "      <td>153.0389</td>\n",
       "      <td>270.0186</td>\n",
       "      <td>207.4811</td>\n",
       "      <td>138.7725</td>\n",
       "      <td>227.9469</td>\n",
       "      <td>176.1756</td>\n",
       "      <td>169.8983</td>\n",
       "      <td>148.6796</td>\n",
       "      <td>188.9097</td>\n",
       "      <td>183.3200</td>\n",
       "      <td>206.8537</td>\n",
       "      <td>195.0901</td>\n",
       "      <td>235.2041</td>\n",
       "      <td>255.0103</td>\n",
       "      <td>212.0715</td>\n",
       "      <td>205.0110</td>\n",
       "      <td>265.0485</td>\n",
       "      <td>149.0026</td>\n",
       "      <td>195.9894</td>\n",
       "      <td>151.3649</td>\n",
       "      <td>240.1951</td>\n",
       "      <td>232.9833</td>\n",
       "      <td>244.0610</td>\n",
       "      <td>185.1324</td>\n",
       "      <td>158.0910</td>\n",
       "      <td>197.8043</td>\n",
       "      <td>144.7892</td>\n",
       "      <td>120.9814</td>\n",
       "      <td>168.2998</td>\n",
       "      <td>203.3011</td>\n",
       "      <td>183.6302</td>\n",
       "      <td>130.0967</td>\n",
       "      <td>126.0358</td>\n",
       "      <td>186.8956</td>\n",
       "      <td>133.1093</td>\n",
       "      <td>162.1756</td>\n",
       "      <td>139.8946</td>\n",
       "      <td>234.6080</td>\n",
       "      <td>232.4463</td>\n",
       "      <td>238.4623</td>\n",
       "      <td>239.2159</td>\n",
       "      <td>213.2384</td>\n",
       "      <td>263.9108</td>\n",
       "      <td>224.2107</td>\n",
       "      <td>232.1559</td>\n",
       "      <td>230.2469</td>\n",
       "      <td>240.9964</td>\n",
       "      <td>249.3047</td>\n",
       "      <td>248.0594</td>\n",
       "      <td>253.5145</td>\n",
       "      <td>251.0162</td>\n",
       "      <td>259.5062</td>\n",
       "      <td>230.6581</td>\n",
       "      <td>255.3788</td>\n",
       "      <td>226.3872</td>\n",
       "      <td>260.6573</td>\n",
       "      <td>231.7550</td>\n",
       "      <td>261.9050</td>\n",
       "      <td>239.9854</td>\n",
       "      <td>231.8995</td>\n",
       "      <td>237.7901</td>\n",
       "      <td>223.9133</td>\n",
       "      <td>233.4132</td>\n",
       "      <td>228.1611</td>\n",
       "      <td>228.8844</td>\n",
       "      <td>240.3058</td>\n",
       "      <td>237.4203</td>\n",
       "      <td>251.9219</td>\n",
       "      <td>224.5933</td>\n",
       "      <td>232.3907</td>\n",
       "      <td>242.3174</td>\n",
       "      <td>221.6363</td>\n",
       "      <td>219.0845</td>\n",
       "      <td>226.0891</td>\n",
       "      <td>248.3749</td>\n",
       "      <td>212.1899</td>\n",
       "      <td>227.3979</td>\n",
       "      <td>244.1002</td>\n",
       "      <td>235.1446</td>\n",
       "      <td>288.8717</td>\n",
       "      <td>247.8035</td>\n",
       "      <td>266.4924</td>\n",
       "      <td>236.7673</td>\n",
       "      <td>246.2668</td>\n",
       "      <td>228.4297</td>\n",
       "      <td>240.7052</td>\n",
       "      <td>243.9670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20019</td>\n",
       "      <td>333.7240</td>\n",
       "      <td>298.5388</td>\n",
       "      <td>217.2804</td>\n",
       "      <td>219.2756</td>\n",
       "      <td>232.6180</td>\n",
       "      <td>235.8926</td>\n",
       "      <td>345.1527</td>\n",
       "      <td>236.3890</td>\n",
       "      <td>316.0638</td>\n",
       "      <td>375.9659</td>\n",
       "      <td>344.1932</td>\n",
       "      <td>270.2515</td>\n",
       "      <td>364.7344</td>\n",
       "      <td>178.2398</td>\n",
       "      <td>338.5114</td>\n",
       "      <td>214.2638</td>\n",
       "      <td>198.4787</td>\n",
       "      <td>292.2658</td>\n",
       "      <td>257.3390</td>\n",
       "      <td>209.7615</td>\n",
       "      <td>298.6723</td>\n",
       "      <td>229.6966</td>\n",
       "      <td>222.0267</td>\n",
       "      <td>332.6259</td>\n",
       "      <td>279.7207</td>\n",
       "      <td>391.6745</td>\n",
       "      <td>273.0007</td>\n",
       "      <td>319.3423</td>\n",
       "      <td>291.6534</td>\n",
       "      <td>283.1917</td>\n",
       "      <td>243.3550</td>\n",
       "      <td>284.4539</td>\n",
       "      <td>183.8125</td>\n",
       "      <td>376.4846</td>\n",
       "      <td>297.8148</td>\n",
       "      <td>340.8358</td>\n",
       "      <td>347.1460</td>\n",
       "      <td>235.0005</td>\n",
       "      <td>291.9161</td>\n",
       "      <td>218.0198</td>\n",
       "      <td>195.8167</td>\n",
       "      <td>180.4761</td>\n",
       "      <td>238.8743</td>\n",
       "      <td>230.0132</td>\n",
       "      <td>193.9374</td>\n",
       "      <td>214.4140</td>\n",
       "      <td>229.5230</td>\n",
       "      <td>157.8925</td>\n",
       "      <td>274.1786</td>\n",
       "      <td>220.4323</td>\n",
       "      <td>240.3388</td>\n",
       "      <td>256.2993</td>\n",
       "      <td>257.1513</td>\n",
       "      <td>258.8263</td>\n",
       "      <td>258.1109</td>\n",
       "      <td>229.3509</td>\n",
       "      <td>266.9618</td>\n",
       "      <td>265.7363</td>\n",
       "      <td>257.1720</td>\n",
       "      <td>258.3042</td>\n",
       "      <td>263.0757</td>\n",
       "      <td>266.7926</td>\n",
       "      <td>271.6118</td>\n",
       "      <td>250.1527</td>\n",
       "      <td>234.4701</td>\n",
       "      <td>258.4594</td>\n",
       "      <td>232.5545</td>\n",
       "      <td>255.2572</td>\n",
       "      <td>271.1297</td>\n",
       "      <td>255.3679</td>\n",
       "      <td>267.6658</td>\n",
       "      <td>251.8240</td>\n",
       "      <td>255.5083</td>\n",
       "      <td>267.8983</td>\n",
       "      <td>256.5400</td>\n",
       "      <td>251.0364</td>\n",
       "      <td>252.4101</td>\n",
       "      <td>272.0752</td>\n",
       "      <td>258.9598</td>\n",
       "      <td>244.0611</td>\n",
       "      <td>254.8120</td>\n",
       "      <td>253.8101</td>\n",
       "      <td>241.4689</td>\n",
       "      <td>273.7900</td>\n",
       "      <td>256.8817</td>\n",
       "      <td>256.9428</td>\n",
       "      <td>253.1288</td>\n",
       "      <td>242.3638</td>\n",
       "      <td>245.0456</td>\n",
       "      <td>230.6809</td>\n",
       "      <td>247.1403</td>\n",
       "      <td>255.6287</td>\n",
       "      <td>257.6972</td>\n",
       "      <td>249.0658</td>\n",
       "      <td>254.5104</td>\n",
       "      <td>268.7801</td>\n",
       "      <td>276.3046</td>\n",
       "      <td>248.5835</td>\n",
       "      <td>251.8889</td>\n",
       "      <td>284.9751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20020</td>\n",
       "      <td>250.1889</td>\n",
       "      <td>236.0363</td>\n",
       "      <td>201.2217</td>\n",
       "      <td>203.7375</td>\n",
       "      <td>238.8985</td>\n",
       "      <td>205.7505</td>\n",
       "      <td>265.0449</td>\n",
       "      <td>212.0956</td>\n",
       "      <td>253.8010</td>\n",
       "      <td>236.5114</td>\n",
       "      <td>276.0221</td>\n",
       "      <td>243.8452</td>\n",
       "      <td>238.9370</td>\n",
       "      <td>173.7466</td>\n",
       "      <td>289.8298</td>\n",
       "      <td>229.9689</td>\n",
       "      <td>183.3561</td>\n",
       "      <td>247.5479</td>\n",
       "      <td>192.2135</td>\n",
       "      <td>212.8287</td>\n",
       "      <td>220.7552</td>\n",
       "      <td>222.4062</td>\n",
       "      <td>199.0481</td>\n",
       "      <td>233.6088</td>\n",
       "      <td>264.1161</td>\n",
       "      <td>276.8528</td>\n",
       "      <td>254.3776</td>\n",
       "      <td>271.2151</td>\n",
       "      <td>211.7924</td>\n",
       "      <td>257.2044</td>\n",
       "      <td>207.8034</td>\n",
       "      <td>247.6765</td>\n",
       "      <td>192.7023</td>\n",
       "      <td>317.9188</td>\n",
       "      <td>296.2874</td>\n",
       "      <td>290.7239</td>\n",
       "      <td>221.0001</td>\n",
       "      <td>213.8420</td>\n",
       "      <td>249.1900</td>\n",
       "      <td>217.3087</td>\n",
       "      <td>214.2203</td>\n",
       "      <td>183.7230</td>\n",
       "      <td>239.0018</td>\n",
       "      <td>200.0947</td>\n",
       "      <td>164.4724</td>\n",
       "      <td>191.9957</td>\n",
       "      <td>204.4753</td>\n",
       "      <td>190.7695</td>\n",
       "      <td>224.1274</td>\n",
       "      <td>210.0432</td>\n",
       "      <td>249.0808</td>\n",
       "      <td>257.7846</td>\n",
       "      <td>253.3750</td>\n",
       "      <td>251.7753</td>\n",
       "      <td>227.9519</td>\n",
       "      <td>327.8557</td>\n",
       "      <td>259.7601</td>\n",
       "      <td>266.1178</td>\n",
       "      <td>250.6958</td>\n",
       "      <td>259.7081</td>\n",
       "      <td>262.7161</td>\n",
       "      <td>283.4750</td>\n",
       "      <td>280.3413</td>\n",
       "      <td>267.5752</td>\n",
       "      <td>247.5188</td>\n",
       "      <td>262.3790</td>\n",
       "      <td>270.7496</td>\n",
       "      <td>251.8425</td>\n",
       "      <td>268.8173</td>\n",
       "      <td>267.0324</td>\n",
       "      <td>257.8770</td>\n",
       "      <td>280.6467</td>\n",
       "      <td>260.7018</td>\n",
       "      <td>271.3517</td>\n",
       "      <td>254.6701</td>\n",
       "      <td>266.0238</td>\n",
       "      <td>251.6134</td>\n",
       "      <td>265.4634</td>\n",
       "      <td>283.0759</td>\n",
       "      <td>266.6378</td>\n",
       "      <td>280.2208</td>\n",
       "      <td>251.9611</td>\n",
       "      <td>253.4478</td>\n",
       "      <td>278.8942</td>\n",
       "      <td>249.4346</td>\n",
       "      <td>253.3386</td>\n",
       "      <td>263.8619</td>\n",
       "      <td>270.0034</td>\n",
       "      <td>236.5778</td>\n",
       "      <td>251.8727</td>\n",
       "      <td>262.9104</td>\n",
       "      <td>258.9364</td>\n",
       "      <td>241.2694</td>\n",
       "      <td>263.1436</td>\n",
       "      <td>318.9176</td>\n",
       "      <td>284.6452</td>\n",
       "      <td>278.6710</td>\n",
       "      <td>257.2354</td>\n",
       "      <td>254.3079</td>\n",
       "      <td>269.5311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20021</td>\n",
       "      <td>206.7702</td>\n",
       "      <td>204.0871</td>\n",
       "      <td>203.7863</td>\n",
       "      <td>186.0510</td>\n",
       "      <td>212.7186</td>\n",
       "      <td>200.2862</td>\n",
       "      <td>226.5852</td>\n",
       "      <td>196.8774</td>\n",
       "      <td>251.9747</td>\n",
       "      <td>265.6888</td>\n",
       "      <td>263.0918</td>\n",
       "      <td>228.1276</td>\n",
       "      <td>253.1646</td>\n",
       "      <td>158.3613</td>\n",
       "      <td>303.1847</td>\n",
       "      <td>228.5077</td>\n",
       "      <td>170.1047</td>\n",
       "      <td>248.5453</td>\n",
       "      <td>174.8180</td>\n",
       "      <td>189.7979</td>\n",
       "      <td>205.7954</td>\n",
       "      <td>203.8327</td>\n",
       "      <td>207.8907</td>\n",
       "      <td>232.6257</td>\n",
       "      <td>237.5254</td>\n",
       "      <td>250.8132</td>\n",
       "      <td>255.4241</td>\n",
       "      <td>247.3859</td>\n",
       "      <td>200.4808</td>\n",
       "      <td>256.5458</td>\n",
       "      <td>184.0125</td>\n",
       "      <td>259.6241</td>\n",
       "      <td>197.6077</td>\n",
       "      <td>284.8824</td>\n",
       "      <td>265.2211</td>\n",
       "      <td>293.3830</td>\n",
       "      <td>199.7913</td>\n",
       "      <td>181.3211</td>\n",
       "      <td>224.5322</td>\n",
       "      <td>195.8175</td>\n",
       "      <td>184.6268</td>\n",
       "      <td>181.6390</td>\n",
       "      <td>236.8113</td>\n",
       "      <td>189.8426</td>\n",
       "      <td>153.8181</td>\n",
       "      <td>163.5551</td>\n",
       "      <td>192.3511</td>\n",
       "      <td>179.8380</td>\n",
       "      <td>208.8595</td>\n",
       "      <td>187.0916</td>\n",
       "      <td>243.5035</td>\n",
       "      <td>224.2617</td>\n",
       "      <td>242.2694</td>\n",
       "      <td>240.4688</td>\n",
       "      <td>221.1545</td>\n",
       "      <td>252.9762</td>\n",
       "      <td>227.6605</td>\n",
       "      <td>238.4102</td>\n",
       "      <td>228.5842</td>\n",
       "      <td>255.2341</td>\n",
       "      <td>246.3959</td>\n",
       "      <td>248.0023</td>\n",
       "      <td>232.9677</td>\n",
       "      <td>252.7705</td>\n",
       "      <td>236.7849</td>\n",
       "      <td>234.7719</td>\n",
       "      <td>259.1571</td>\n",
       "      <td>234.3625</td>\n",
       "      <td>242.6581</td>\n",
       "      <td>230.7865</td>\n",
       "      <td>258.5150</td>\n",
       "      <td>250.9109</td>\n",
       "      <td>233.5472</td>\n",
       "      <td>238.7637</td>\n",
       "      <td>236.6973</td>\n",
       "      <td>228.3477</td>\n",
       "      <td>234.0382</td>\n",
       "      <td>234.3605</td>\n",
       "      <td>244.6056</td>\n",
       "      <td>256.2115</td>\n",
       "      <td>251.6724</td>\n",
       "      <td>227.1327</td>\n",
       "      <td>231.6205</td>\n",
       "      <td>237.1501</td>\n",
       "      <td>227.2787</td>\n",
       "      <td>228.6161</td>\n",
       "      <td>228.6585</td>\n",
       "      <td>233.1159</td>\n",
       "      <td>216.9694</td>\n",
       "      <td>229.0769</td>\n",
       "      <td>240.2588</td>\n",
       "      <td>244.3884</td>\n",
       "      <td>238.6540</td>\n",
       "      <td>233.9305</td>\n",
       "      <td>248.5294</td>\n",
       "      <td>247.7609</td>\n",
       "      <td>251.1889</td>\n",
       "      <td>232.2856</td>\n",
       "      <td>234.8131</td>\n",
       "      <td>254.1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20022</td>\n",
       "      <td>204.3186</td>\n",
       "      <td>195.5432</td>\n",
       "      <td>147.7194</td>\n",
       "      <td>153.2001</td>\n",
       "      <td>224.6390</td>\n",
       "      <td>185.1861</td>\n",
       "      <td>281.4965</td>\n",
       "      <td>203.0135</td>\n",
       "      <td>252.8136</td>\n",
       "      <td>254.8137</td>\n",
       "      <td>296.2015</td>\n",
       "      <td>208.7445</td>\n",
       "      <td>190.0431</td>\n",
       "      <td>170.8985</td>\n",
       "      <td>244.9019</td>\n",
       "      <td>192.6418</td>\n",
       "      <td>164.1596</td>\n",
       "      <td>223.6768</td>\n",
       "      <td>183.4022</td>\n",
       "      <td>181.7544</td>\n",
       "      <td>156.4992</td>\n",
       "      <td>218.1961</td>\n",
       "      <td>164.8275</td>\n",
       "      <td>206.7107</td>\n",
       "      <td>227.9048</td>\n",
       "      <td>275.1958</td>\n",
       "      <td>243.4991</td>\n",
       "      <td>243.7484</td>\n",
       "      <td>221.6953</td>\n",
       "      <td>235.4086</td>\n",
       "      <td>209.0647</td>\n",
       "      <td>207.8151</td>\n",
       "      <td>197.1470</td>\n",
       "      <td>324.6418</td>\n",
       "      <td>283.7852</td>\n",
       "      <td>315.5475</td>\n",
       "      <td>211.7318</td>\n",
       "      <td>201.8108</td>\n",
       "      <td>254.6193</td>\n",
       "      <td>177.4630</td>\n",
       "      <td>159.1949</td>\n",
       "      <td>172.1994</td>\n",
       "      <td>261.9374</td>\n",
       "      <td>187.7452</td>\n",
       "      <td>144.9734</td>\n",
       "      <td>157.5455</td>\n",
       "      <td>194.8225</td>\n",
       "      <td>166.1870</td>\n",
       "      <td>221.2229</td>\n",
       "      <td>167.2166</td>\n",
       "      <td>207.8845</td>\n",
       "      <td>227.2167</td>\n",
       "      <td>211.4826</td>\n",
       "      <td>204.1718</td>\n",
       "      <td>204.2903</td>\n",
       "      <td>213.9242</td>\n",
       "      <td>206.9639</td>\n",
       "      <td>207.6323</td>\n",
       "      <td>196.2679</td>\n",
       "      <td>214.6273</td>\n",
       "      <td>219.2804</td>\n",
       "      <td>235.6299</td>\n",
       "      <td>255.6617</td>\n",
       "      <td>222.2275</td>\n",
       "      <td>212.5497</td>\n",
       "      <td>212.7485</td>\n",
       "      <td>233.9722</td>\n",
       "      <td>211.6037</td>\n",
       "      <td>225.6409</td>\n",
       "      <td>203.5099</td>\n",
       "      <td>209.9220</td>\n",
       "      <td>207.1628</td>\n",
       "      <td>212.0513</td>\n",
       "      <td>220.8605</td>\n",
       "      <td>202.5719</td>\n",
       "      <td>210.5072</td>\n",
       "      <td>216.0306</td>\n",
       "      <td>219.0675</td>\n",
       "      <td>198.8880</td>\n",
       "      <td>225.4129</td>\n",
       "      <td>246.1837</td>\n",
       "      <td>205.8427</td>\n",
       "      <td>199.3638</td>\n",
       "      <td>215.2501</td>\n",
       "      <td>218.8605</td>\n",
       "      <td>199.9041</td>\n",
       "      <td>201.2129</td>\n",
       "      <td>224.9609</td>\n",
       "      <td>206.4833</td>\n",
       "      <td>194.0175</td>\n",
       "      <td>204.4173</td>\n",
       "      <td>207.1302</td>\n",
       "      <td>228.1789</td>\n",
       "      <td>207.6221</td>\n",
       "      <td>226.3187</td>\n",
       "      <td>208.0615</td>\n",
       "      <td>244.6846</td>\n",
       "      <td>205.9263</td>\n",
       "      <td>211.2751</td>\n",
       "      <td>240.5112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20023</td>\n",
       "      <td>154.7739</td>\n",
       "      <td>189.8394</td>\n",
       "      <td>190.7063</td>\n",
       "      <td>152.9794</td>\n",
       "      <td>176.4942</td>\n",
       "      <td>167.2994</td>\n",
       "      <td>190.8615</td>\n",
       "      <td>201.5220</td>\n",
       "      <td>241.2120</td>\n",
       "      <td>237.9286</td>\n",
       "      <td>242.5268</td>\n",
       "      <td>198.8588</td>\n",
       "      <td>207.4656</td>\n",
       "      <td>158.5059</td>\n",
       "      <td>230.9552</td>\n",
       "      <td>204.7216</td>\n",
       "      <td>162.2477</td>\n",
       "      <td>208.9885</td>\n",
       "      <td>179.4072</td>\n",
       "      <td>172.6681</td>\n",
       "      <td>135.2368</td>\n",
       "      <td>167.0702</td>\n",
       "      <td>166.6089</td>\n",
       "      <td>214.7499</td>\n",
       "      <td>215.0936</td>\n",
       "      <td>254.9729</td>\n",
       "      <td>245.3691</td>\n",
       "      <td>210.3238</td>\n",
       "      <td>218.6446</td>\n",
       "      <td>202.5740</td>\n",
       "      <td>187.5085</td>\n",
       "      <td>195.9113</td>\n",
       "      <td>146.3852</td>\n",
       "      <td>227.4288</td>\n",
       "      <td>223.5308</td>\n",
       "      <td>271.3043</td>\n",
       "      <td>171.2497</td>\n",
       "      <td>180.1453</td>\n",
       "      <td>208.9044</td>\n",
       "      <td>152.9818</td>\n",
       "      <td>137.9991</td>\n",
       "      <td>163.4818</td>\n",
       "      <td>193.2145</td>\n",
       "      <td>181.3427</td>\n",
       "      <td>155.7369</td>\n",
       "      <td>148.0953</td>\n",
       "      <td>188.1926</td>\n",
       "      <td>145.0883</td>\n",
       "      <td>198.5445</td>\n",
       "      <td>148.2832</td>\n",
       "      <td>173.1491</td>\n",
       "      <td>188.5196</td>\n",
       "      <td>192.0669</td>\n",
       "      <td>192.9574</td>\n",
       "      <td>179.7390</td>\n",
       "      <td>196.3033</td>\n",
       "      <td>174.7519</td>\n",
       "      <td>194.2208</td>\n",
       "      <td>181.0947</td>\n",
       "      <td>188.2932</td>\n",
       "      <td>197.5973</td>\n",
       "      <td>204.8207</td>\n",
       "      <td>212.2163</td>\n",
       "      <td>192.2057</td>\n",
       "      <td>188.2158</td>\n",
       "      <td>198.2343</td>\n",
       "      <td>191.0649</td>\n",
       "      <td>192.7913</td>\n",
       "      <td>194.2718</td>\n",
       "      <td>186.3374</td>\n",
       "      <td>192.7077</td>\n",
       "      <td>190.8062</td>\n",
       "      <td>187.3523</td>\n",
       "      <td>187.6778</td>\n",
       "      <td>185.1813</td>\n",
       "      <td>189.9220</td>\n",
       "      <td>185.9377</td>\n",
       "      <td>197.4658</td>\n",
       "      <td>200.1503</td>\n",
       "      <td>179.5375</td>\n",
       "      <td>197.0140</td>\n",
       "      <td>186.6268</td>\n",
       "      <td>206.1380</td>\n",
       "      <td>192.2795</td>\n",
       "      <td>198.4527</td>\n",
       "      <td>180.9501</td>\n",
       "      <td>186.0529</td>\n",
       "      <td>191.1680</td>\n",
       "      <td>188.6187</td>\n",
       "      <td>183.5219</td>\n",
       "      <td>188.4692</td>\n",
       "      <td>190.4764</td>\n",
       "      <td>183.6228</td>\n",
       "      <td>184.9470</td>\n",
       "      <td>202.2642</td>\n",
       "      <td>175.7830</td>\n",
       "      <td>202.9636</td>\n",
       "      <td>181.8300</td>\n",
       "      <td>180.5297</td>\n",
       "      <td>198.9536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20024</td>\n",
       "      <td>196.9871</td>\n",
       "      <td>156.4449</td>\n",
       "      <td>160.7994</td>\n",
       "      <td>144.7862</td>\n",
       "      <td>188.3029</td>\n",
       "      <td>172.1950</td>\n",
       "      <td>208.4146</td>\n",
       "      <td>164.7698</td>\n",
       "      <td>157.1953</td>\n",
       "      <td>194.0513</td>\n",
       "      <td>215.9570</td>\n",
       "      <td>214.1749</td>\n",
       "      <td>181.7366</td>\n",
       "      <td>149.9376</td>\n",
       "      <td>203.5142</td>\n",
       "      <td>148.9879</td>\n",
       "      <td>153.6296</td>\n",
       "      <td>187.1028</td>\n",
       "      <td>171.5111</td>\n",
       "      <td>179.8085</td>\n",
       "      <td>177.3112</td>\n",
       "      <td>186.1347</td>\n",
       "      <td>155.7732</td>\n",
       "      <td>153.6751</td>\n",
       "      <td>222.7785</td>\n",
       "      <td>206.0609</td>\n",
       "      <td>206.1670</td>\n",
       "      <td>242.5574</td>\n",
       "      <td>162.8626</td>\n",
       "      <td>181.1994</td>\n",
       "      <td>222.3933</td>\n",
       "      <td>201.1889</td>\n",
       "      <td>166.1943</td>\n",
       "      <td>238.0730</td>\n",
       "      <td>222.5297</td>\n",
       "      <td>206.5183</td>\n",
       "      <td>203.1564</td>\n",
       "      <td>159.3543</td>\n",
       "      <td>182.6046</td>\n",
       "      <td>176.1098</td>\n",
       "      <td>200.2392</td>\n",
       "      <td>162.1997</td>\n",
       "      <td>175.7723</td>\n",
       "      <td>171.0054</td>\n",
       "      <td>125.2981</td>\n",
       "      <td>170.6263</td>\n",
       "      <td>163.8259</td>\n",
       "      <td>141.8834</td>\n",
       "      <td>173.9342</td>\n",
       "      <td>158.7463</td>\n",
       "      <td>182.8201</td>\n",
       "      <td>201.7316</td>\n",
       "      <td>191.8966</td>\n",
       "      <td>192.5946</td>\n",
       "      <td>180.4501</td>\n",
       "      <td>198.2704</td>\n",
       "      <td>190.7530</td>\n",
       "      <td>183.7925</td>\n",
       "      <td>183.0519</td>\n",
       "      <td>190.8219</td>\n",
       "      <td>190.1079</td>\n",
       "      <td>194.8258</td>\n",
       "      <td>204.3025</td>\n",
       "      <td>194.4530</td>\n",
       "      <td>189.8451</td>\n",
       "      <td>192.1940</td>\n",
       "      <td>187.5720</td>\n",
       "      <td>186.6807</td>\n",
       "      <td>179.7751</td>\n",
       "      <td>187.4232</td>\n",
       "      <td>191.0067</td>\n",
       "      <td>185.5486</td>\n",
       "      <td>202.4988</td>\n",
       "      <td>181.6377</td>\n",
       "      <td>185.2015</td>\n",
       "      <td>191.5141</td>\n",
       "      <td>195.4827</td>\n",
       "      <td>189.1234</td>\n",
       "      <td>183.0305</td>\n",
       "      <td>190.4109</td>\n",
       "      <td>191.1020</td>\n",
       "      <td>187.3794</td>\n",
       "      <td>190.0439</td>\n",
       "      <td>190.8034</td>\n",
       "      <td>184.2814</td>\n",
       "      <td>183.3200</td>\n",
       "      <td>187.6816</td>\n",
       "      <td>190.7123</td>\n",
       "      <td>189.8471</td>\n",
       "      <td>179.5776</td>\n",
       "      <td>184.4081</td>\n",
       "      <td>187.8047</td>\n",
       "      <td>168.9537</td>\n",
       "      <td>186.5142</td>\n",
       "      <td>175.8858</td>\n",
       "      <td>188.7178</td>\n",
       "      <td>179.4281</td>\n",
       "      <td>181.6267</td>\n",
       "      <td>191.8945</td>\n",
       "      <td>199.3252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20025</td>\n",
       "      <td>201.7556</td>\n",
       "      <td>173.6738</td>\n",
       "      <td>181.2639</td>\n",
       "      <td>165.7191</td>\n",
       "      <td>200.9400</td>\n",
       "      <td>194.9510</td>\n",
       "      <td>195.6301</td>\n",
       "      <td>169.5098</td>\n",
       "      <td>186.8392</td>\n",
       "      <td>219.9137</td>\n",
       "      <td>224.9564</td>\n",
       "      <td>216.5952</td>\n",
       "      <td>198.9951</td>\n",
       "      <td>151.9683</td>\n",
       "      <td>260.1266</td>\n",
       "      <td>189.5292</td>\n",
       "      <td>158.9270</td>\n",
       "      <td>208.8615</td>\n",
       "      <td>167.6308</td>\n",
       "      <td>188.6310</td>\n",
       "      <td>213.8370</td>\n",
       "      <td>195.5070</td>\n",
       "      <td>205.5137</td>\n",
       "      <td>184.2327</td>\n",
       "      <td>235.7895</td>\n",
       "      <td>213.4122</td>\n",
       "      <td>232.6076</td>\n",
       "      <td>215.3630</td>\n",
       "      <td>163.1975</td>\n",
       "      <td>233.6811</td>\n",
       "      <td>159.0175</td>\n",
       "      <td>245.6071</td>\n",
       "      <td>181.5168</td>\n",
       "      <td>261.0289</td>\n",
       "      <td>217.4312</td>\n",
       "      <td>246.3075</td>\n",
       "      <td>179.9082</td>\n",
       "      <td>166.5412</td>\n",
       "      <td>196.7020</td>\n",
       "      <td>175.6000</td>\n",
       "      <td>192.8822</td>\n",
       "      <td>171.7903</td>\n",
       "      <td>196.9897</td>\n",
       "      <td>172.2155</td>\n",
       "      <td>146.8083</td>\n",
       "      <td>166.5977</td>\n",
       "      <td>166.1239</td>\n",
       "      <td>182.5636</td>\n",
       "      <td>193.6123</td>\n",
       "      <td>199.9544</td>\n",
       "      <td>180.4972</td>\n",
       "      <td>200.4107</td>\n",
       "      <td>188.1480</td>\n",
       "      <td>182.1181</td>\n",
       "      <td>181.1941</td>\n",
       "      <td>194.1809</td>\n",
       "      <td>182.8368</td>\n",
       "      <td>200.5765</td>\n",
       "      <td>178.6984</td>\n",
       "      <td>196.0340</td>\n",
       "      <td>193.6315</td>\n",
       "      <td>197.9958</td>\n",
       "      <td>209.5999</td>\n",
       "      <td>204.8867</td>\n",
       "      <td>191.0695</td>\n",
       "      <td>191.5946</td>\n",
       "      <td>179.8772</td>\n",
       "      <td>195.6026</td>\n",
       "      <td>189.0002</td>\n",
       "      <td>188.6341</td>\n",
       "      <td>181.4438</td>\n",
       "      <td>191.4342</td>\n",
       "      <td>183.3453</td>\n",
       "      <td>180.8001</td>\n",
       "      <td>187.1319</td>\n",
       "      <td>191.7446</td>\n",
       "      <td>188.1347</td>\n",
       "      <td>178.7979</td>\n",
       "      <td>202.4864</td>\n",
       "      <td>197.8702</td>\n",
       "      <td>200.1934</td>\n",
       "      <td>193.1235</td>\n",
       "      <td>182.8550</td>\n",
       "      <td>190.8551</td>\n",
       "      <td>185.3785</td>\n",
       "      <td>186.9336</td>\n",
       "      <td>187.3188</td>\n",
       "      <td>195.5642</td>\n",
       "      <td>177.3376</td>\n",
       "      <td>188.3717</td>\n",
       "      <td>186.2788</td>\n",
       "      <td>188.8161</td>\n",
       "      <td>197.5693</td>\n",
       "      <td>187.9844</td>\n",
       "      <td>204.8362</td>\n",
       "      <td>193.2576</td>\n",
       "      <td>220.2527</td>\n",
       "      <td>194.9981</td>\n",
       "      <td>186.3425</td>\n",
       "      <td>211.5309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20026</td>\n",
       "      <td>222.4757</td>\n",
       "      <td>188.2728</td>\n",
       "      <td>180.7988</td>\n",
       "      <td>196.8964</td>\n",
       "      <td>193.0990</td>\n",
       "      <td>185.7327</td>\n",
       "      <td>210.5284</td>\n",
       "      <td>191.4829</td>\n",
       "      <td>228.5465</td>\n",
       "      <td>255.8821</td>\n",
       "      <td>244.4364</td>\n",
       "      <td>210.6821</td>\n",
       "      <td>236.9547</td>\n",
       "      <td>155.6008</td>\n",
       "      <td>278.2791</td>\n",
       "      <td>186.2167</td>\n",
       "      <td>176.2196</td>\n",
       "      <td>232.3537</td>\n",
       "      <td>172.6164</td>\n",
       "      <td>193.7608</td>\n",
       "      <td>200.9583</td>\n",
       "      <td>193.9479</td>\n",
       "      <td>200.9692</td>\n",
       "      <td>225.5047</td>\n",
       "      <td>231.6045</td>\n",
       "      <td>235.6431</td>\n",
       "      <td>240.6661</td>\n",
       "      <td>228.3780</td>\n",
       "      <td>189.8014</td>\n",
       "      <td>234.6398</td>\n",
       "      <td>189.4548</td>\n",
       "      <td>208.4728</td>\n",
       "      <td>177.5191</td>\n",
       "      <td>237.6749</td>\n",
       "      <td>253.3883</td>\n",
       "      <td>254.0898</td>\n",
       "      <td>197.2449</td>\n",
       "      <td>177.2830</td>\n",
       "      <td>215.0225</td>\n",
       "      <td>195.3503</td>\n",
       "      <td>164.1656</td>\n",
       "      <td>165.8256</td>\n",
       "      <td>202.3752</td>\n",
       "      <td>184.9746</td>\n",
       "      <td>173.6719</td>\n",
       "      <td>174.7483</td>\n",
       "      <td>186.0022</td>\n",
       "      <td>157.2683</td>\n",
       "      <td>214.0684</td>\n",
       "      <td>179.4345</td>\n",
       "      <td>190.8153</td>\n",
       "      <td>192.5578</td>\n",
       "      <td>182.4458</td>\n",
       "      <td>186.8770</td>\n",
       "      <td>191.5965</td>\n",
       "      <td>187.5576</td>\n",
       "      <td>190.7332</td>\n",
       "      <td>203.3693</td>\n",
       "      <td>203.0208</td>\n",
       "      <td>183.2128</td>\n",
       "      <td>196.7162</td>\n",
       "      <td>193.3122</td>\n",
       "      <td>191.0769</td>\n",
       "      <td>202.4683</td>\n",
       "      <td>196.8036</td>\n",
       "      <td>191.7657</td>\n",
       "      <td>168.9156</td>\n",
       "      <td>196.9417</td>\n",
       "      <td>203.6723</td>\n",
       "      <td>197.9112</td>\n",
       "      <td>200.3313</td>\n",
       "      <td>177.9448</td>\n",
       "      <td>177.9914</td>\n",
       "      <td>185.0922</td>\n",
       "      <td>202.2777</td>\n",
       "      <td>189.1094</td>\n",
       "      <td>188.3963</td>\n",
       "      <td>173.4599</td>\n",
       "      <td>203.0107</td>\n",
       "      <td>194.5953</td>\n",
       "      <td>196.4094</td>\n",
       "      <td>188.8118</td>\n",
       "      <td>185.2894</td>\n",
       "      <td>188.0799</td>\n",
       "      <td>187.4962</td>\n",
       "      <td>185.8885</td>\n",
       "      <td>205.3672</td>\n",
       "      <td>196.1646</td>\n",
       "      <td>182.7661</td>\n",
       "      <td>185.9377</td>\n",
       "      <td>184.6391</td>\n",
       "      <td>192.9258</td>\n",
       "      <td>185.1989</td>\n",
       "      <td>187.5488</td>\n",
       "      <td>199.3900</td>\n",
       "      <td>192.7880</td>\n",
       "      <td>202.9658</td>\n",
       "      <td>199.9068</td>\n",
       "      <td>188.2445</td>\n",
       "      <td>206.3556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20027</td>\n",
       "      <td>204.7608</td>\n",
       "      <td>172.4065</td>\n",
       "      <td>195.6333</td>\n",
       "      <td>184.3229</td>\n",
       "      <td>195.6511</td>\n",
       "      <td>178.7893</td>\n",
       "      <td>213.3148</td>\n",
       "      <td>183.0247</td>\n",
       "      <td>247.6133</td>\n",
       "      <td>244.3981</td>\n",
       "      <td>230.1248</td>\n",
       "      <td>200.4477</td>\n",
       "      <td>241.8817</td>\n",
       "      <td>155.5597</td>\n",
       "      <td>277.7380</td>\n",
       "      <td>215.3830</td>\n",
       "      <td>161.5149</td>\n",
       "      <td>219.1905</td>\n",
       "      <td>174.3703</td>\n",
       "      <td>183.0352</td>\n",
       "      <td>170.3108</td>\n",
       "      <td>168.6970</td>\n",
       "      <td>178.3614</td>\n",
       "      <td>220.0963</td>\n",
       "      <td>231.1775</td>\n",
       "      <td>223.9741</td>\n",
       "      <td>236.2650</td>\n",
       "      <td>237.6187</td>\n",
       "      <td>189.7766</td>\n",
       "      <td>241.6551</td>\n",
       "      <td>167.9886</td>\n",
       "      <td>228.5193</td>\n",
       "      <td>187.1740</td>\n",
       "      <td>235.6314</td>\n",
       "      <td>239.3994</td>\n",
       "      <td>262.7574</td>\n",
       "      <td>186.2386</td>\n",
       "      <td>175.5962</td>\n",
       "      <td>200.3568</td>\n",
       "      <td>173.2937</td>\n",
       "      <td>163.9376</td>\n",
       "      <td>190.3548</td>\n",
       "      <td>216.8224</td>\n",
       "      <td>181.0292</td>\n",
       "      <td>154.6323</td>\n",
       "      <td>150.4325</td>\n",
       "      <td>196.5087</td>\n",
       "      <td>170.7045</td>\n",
       "      <td>178.6544</td>\n",
       "      <td>183.5594</td>\n",
       "      <td>205.5056</td>\n",
       "      <td>203.1725</td>\n",
       "      <td>201.1727</td>\n",
       "      <td>213.3744</td>\n",
       "      <td>203.4035</td>\n",
       "      <td>190.5596</td>\n",
       "      <td>203.2466</td>\n",
       "      <td>202.8692</td>\n",
       "      <td>199.4680</td>\n",
       "      <td>200.9211</td>\n",
       "      <td>220.2669</td>\n",
       "      <td>199.5130</td>\n",
       "      <td>188.9622</td>\n",
       "      <td>220.6252</td>\n",
       "      <td>208.0071</td>\n",
       "      <td>209.1899</td>\n",
       "      <td>198.3902</td>\n",
       "      <td>204.6120</td>\n",
       "      <td>195.3398</td>\n",
       "      <td>208.6758</td>\n",
       "      <td>214.7945</td>\n",
       "      <td>223.7071</td>\n",
       "      <td>206.1927</td>\n",
       "      <td>214.2165</td>\n",
       "      <td>200.6769</td>\n",
       "      <td>202.3346</td>\n",
       "      <td>216.7990</td>\n",
       "      <td>195.0555</td>\n",
       "      <td>195.4341</td>\n",
       "      <td>211.9662</td>\n",
       "      <td>209.6077</td>\n",
       "      <td>198.0541</td>\n",
       "      <td>199.7941</td>\n",
       "      <td>208.4376</td>\n",
       "      <td>203.0989</td>\n",
       "      <td>193.8628</td>\n",
       "      <td>195.3484</td>\n",
       "      <td>211.3170</td>\n",
       "      <td>196.3968</td>\n",
       "      <td>200.6615</td>\n",
       "      <td>196.6780</td>\n",
       "      <td>205.8097</td>\n",
       "      <td>186.6006</td>\n",
       "      <td>209.1679</td>\n",
       "      <td>217.4850</td>\n",
       "      <td>210.5829</td>\n",
       "      <td>190.9985</td>\n",
       "      <td>204.2205</td>\n",
       "      <td>205.7524</td>\n",
       "      <td>204.4006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20028</td>\n",
       "      <td>149.0241</td>\n",
       "      <td>150.7628</td>\n",
       "      <td>160.0666</td>\n",
       "      <td>131.8998</td>\n",
       "      <td>175.9729</td>\n",
       "      <td>144.3996</td>\n",
       "      <td>193.6075</td>\n",
       "      <td>177.0878</td>\n",
       "      <td>228.3310</td>\n",
       "      <td>214.6624</td>\n",
       "      <td>227.9122</td>\n",
       "      <td>172.0022</td>\n",
       "      <td>168.8715</td>\n",
       "      <td>143.5322</td>\n",
       "      <td>212.1931</td>\n",
       "      <td>199.7673</td>\n",
       "      <td>139.1575</td>\n",
       "      <td>175.2804</td>\n",
       "      <td>159.9554</td>\n",
       "      <td>155.4294</td>\n",
       "      <td>122.8522</td>\n",
       "      <td>191.1388</td>\n",
       "      <td>181.7122</td>\n",
       "      <td>196.2915</td>\n",
       "      <td>185.7934</td>\n",
       "      <td>216.6078</td>\n",
       "      <td>219.5240</td>\n",
       "      <td>216.0235</td>\n",
       "      <td>175.4108</td>\n",
       "      <td>213.6347</td>\n",
       "      <td>156.1745</td>\n",
       "      <td>218.4753</td>\n",
       "      <td>148.9964</td>\n",
       "      <td>214.4248</td>\n",
       "      <td>197.3648</td>\n",
       "      <td>224.6489</td>\n",
       "      <td>160.8052</td>\n",
       "      <td>158.0359</td>\n",
       "      <td>173.3082</td>\n",
       "      <td>129.8645</td>\n",
       "      <td>124.6533</td>\n",
       "      <td>158.1649</td>\n",
       "      <td>194.2608</td>\n",
       "      <td>154.0205</td>\n",
       "      <td>125.0561</td>\n",
       "      <td>119.6689</td>\n",
       "      <td>167.3328</td>\n",
       "      <td>118.1868</td>\n",
       "      <td>164.2632</td>\n",
       "      <td>129.8396</td>\n",
       "      <td>161.9298</td>\n",
       "      <td>168.3757</td>\n",
       "      <td>161.0553</td>\n",
       "      <td>176.9267</td>\n",
       "      <td>155.3122</td>\n",
       "      <td>158.8228</td>\n",
       "      <td>173.3927</td>\n",
       "      <td>165.9532</td>\n",
       "      <td>156.8536</td>\n",
       "      <td>174.7564</td>\n",
       "      <td>170.5007</td>\n",
       "      <td>169.5987</td>\n",
       "      <td>162.6755</td>\n",
       "      <td>161.8589</td>\n",
       "      <td>166.7904</td>\n",
       "      <td>171.1117</td>\n",
       "      <td>160.8149</td>\n",
       "      <td>162.9085</td>\n",
       "      <td>165.2477</td>\n",
       "      <td>163.9082</td>\n",
       "      <td>164.7386</td>\n",
       "      <td>170.7662</td>\n",
       "      <td>169.5804</td>\n",
       "      <td>156.4031</td>\n",
       "      <td>172.8289</td>\n",
       "      <td>168.2795</td>\n",
       "      <td>168.9028</td>\n",
       "      <td>168.6903</td>\n",
       "      <td>150.7001</td>\n",
       "      <td>172.1648</td>\n",
       "      <td>165.7515</td>\n",
       "      <td>168.6835</td>\n",
       "      <td>177.9942</td>\n",
       "      <td>170.9547</td>\n",
       "      <td>166.5719</td>\n",
       "      <td>173.9749</td>\n",
       "      <td>171.5150</td>\n",
       "      <td>169.6386</td>\n",
       "      <td>151.0595</td>\n",
       "      <td>165.5343</td>\n",
       "      <td>159.8534</td>\n",
       "      <td>173.9284</td>\n",
       "      <td>189.8815</td>\n",
       "      <td>163.1335</td>\n",
       "      <td>140.2785</td>\n",
       "      <td>154.6837</td>\n",
       "      <td>168.9486</td>\n",
       "      <td>168.7375</td>\n",
       "      <td>165.9787</td>\n",
       "      <td>170.9215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20029</td>\n",
       "      <td>161.3855</td>\n",
       "      <td>138.2247</td>\n",
       "      <td>128.5308</td>\n",
       "      <td>152.4532</td>\n",
       "      <td>171.8926</td>\n",
       "      <td>191.4201</td>\n",
       "      <td>159.4756</td>\n",
       "      <td>142.9141</td>\n",
       "      <td>153.4922</td>\n",
       "      <td>167.7637</td>\n",
       "      <td>164.4575</td>\n",
       "      <td>166.4948</td>\n",
       "      <td>182.1825</td>\n",
       "      <td>131.3469</td>\n",
       "      <td>225.5374</td>\n",
       "      <td>147.3836</td>\n",
       "      <td>125.1048</td>\n",
       "      <td>163.5588</td>\n",
       "      <td>142.6067</td>\n",
       "      <td>152.6889</td>\n",
       "      <td>149.0735</td>\n",
       "      <td>151.6318</td>\n",
       "      <td>172.8207</td>\n",
       "      <td>159.7892</td>\n",
       "      <td>179.9001</td>\n",
       "      <td>156.9738</td>\n",
       "      <td>201.2384</td>\n",
       "      <td>144.5189</td>\n",
       "      <td>130.1391</td>\n",
       "      <td>208.8112</td>\n",
       "      <td>124.7660</td>\n",
       "      <td>176.4053</td>\n",
       "      <td>147.8783</td>\n",
       "      <td>193.4897</td>\n",
       "      <td>170.6870</td>\n",
       "      <td>187.0580</td>\n",
       "      <td>142.2234</td>\n",
       "      <td>134.4911</td>\n",
       "      <td>155.8109</td>\n",
       "      <td>144.3076</td>\n",
       "      <td>143.5409</td>\n",
       "      <td>146.8772</td>\n",
       "      <td>156.8738</td>\n",
       "      <td>140.9689</td>\n",
       "      <td>104.2430</td>\n",
       "      <td>137.8999</td>\n",
       "      <td>142.9662</td>\n",
       "      <td>150.5483</td>\n",
       "      <td>150.4303</td>\n",
       "      <td>175.4655</td>\n",
       "      <td>153.4266</td>\n",
       "      <td>157.3747</td>\n",
       "      <td>159.9828</td>\n",
       "      <td>161.2484</td>\n",
       "      <td>154.6130</td>\n",
       "      <td>150.9601</td>\n",
       "      <td>163.9516</td>\n",
       "      <td>164.1916</td>\n",
       "      <td>164.8137</td>\n",
       "      <td>173.1691</td>\n",
       "      <td>170.8680</td>\n",
       "      <td>159.7416</td>\n",
       "      <td>168.4870</td>\n",
       "      <td>173.4338</td>\n",
       "      <td>157.0724</td>\n",
       "      <td>161.9402</td>\n",
       "      <td>168.4999</td>\n",
       "      <td>170.9995</td>\n",
       "      <td>157.2673</td>\n",
       "      <td>163.1237</td>\n",
       "      <td>164.4243</td>\n",
       "      <td>153.4433</td>\n",
       "      <td>175.5446</td>\n",
       "      <td>162.0389</td>\n",
       "      <td>173.0614</td>\n",
       "      <td>168.6940</td>\n",
       "      <td>161.3936</td>\n",
       "      <td>164.1024</td>\n",
       "      <td>135.4664</td>\n",
       "      <td>160.3269</td>\n",
       "      <td>143.5543</td>\n",
       "      <td>161.0234</td>\n",
       "      <td>174.2537</td>\n",
       "      <td>165.2522</td>\n",
       "      <td>157.8482</td>\n",
       "      <td>157.3934</td>\n",
       "      <td>164.5414</td>\n",
       "      <td>161.2702</td>\n",
       "      <td>153.7216</td>\n",
       "      <td>165.8208</td>\n",
       "      <td>159.2868</td>\n",
       "      <td>161.8892</td>\n",
       "      <td>158.8796</td>\n",
       "      <td>159.2497</td>\n",
       "      <td>151.5982</td>\n",
       "      <td>169.8101</td>\n",
       "      <td>180.9886</td>\n",
       "      <td>160.8167</td>\n",
       "      <td>166.4131</td>\n",
       "      <td>157.4406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20030</td>\n",
       "      <td>93.9724</td>\n",
       "      <td>110.1499</td>\n",
       "      <td>91.1880</td>\n",
       "      <td>92.8479</td>\n",
       "      <td>119.7675</td>\n",
       "      <td>113.0949</td>\n",
       "      <td>102.9308</td>\n",
       "      <td>105.1892</td>\n",
       "      <td>98.2036</td>\n",
       "      <td>123.5401</td>\n",
       "      <td>115.7651</td>\n",
       "      <td>116.9693</td>\n",
       "      <td>119.4522</td>\n",
       "      <td>107.6014</td>\n",
       "      <td>106.4437</td>\n",
       "      <td>101.7516</td>\n",
       "      <td>92.3532</td>\n",
       "      <td>120.2847</td>\n",
       "      <td>91.3352</td>\n",
       "      <td>102.0478</td>\n",
       "      <td>87.2728</td>\n",
       "      <td>97.6167</td>\n",
       "      <td>91.6074</td>\n",
       "      <td>104.6411</td>\n",
       "      <td>123.2338</td>\n",
       "      <td>116.5608</td>\n",
       "      <td>120.8807</td>\n",
       "      <td>102.3065</td>\n",
       "      <td>94.2406</td>\n",
       "      <td>110.7323</td>\n",
       "      <td>85.7984</td>\n",
       "      <td>126.0250</td>\n",
       "      <td>97.2901</td>\n",
       "      <td>125.9381</td>\n",
       "      <td>108.0583</td>\n",
       "      <td>113.5843</td>\n",
       "      <td>86.4895</td>\n",
       "      <td>92.4885</td>\n",
       "      <td>99.0431</td>\n",
       "      <td>105.5050</td>\n",
       "      <td>119.6268</td>\n",
       "      <td>113.3322</td>\n",
       "      <td>114.4529</td>\n",
       "      <td>97.7112</td>\n",
       "      <td>77.3268</td>\n",
       "      <td>98.0522</td>\n",
       "      <td>98.8708</td>\n",
       "      <td>97.7567</td>\n",
       "      <td>92.8996</td>\n",
       "      <td>117.8156</td>\n",
       "      <td>117.6203</td>\n",
       "      <td>141.7923</td>\n",
       "      <td>113.8913</td>\n",
       "      <td>124.8845</td>\n",
       "      <td>127.2723</td>\n",
       "      <td>124.1096</td>\n",
       "      <td>129.3386</td>\n",
       "      <td>126.6222</td>\n",
       "      <td>113.9999</td>\n",
       "      <td>129.6498</td>\n",
       "      <td>131.2299</td>\n",
       "      <td>113.8999</td>\n",
       "      <td>122.7629</td>\n",
       "      <td>128.8505</td>\n",
       "      <td>136.9892</td>\n",
       "      <td>130.2413</td>\n",
       "      <td>134.2951</td>\n",
       "      <td>126.0730</td>\n",
       "      <td>118.1757</td>\n",
       "      <td>122.5079</td>\n",
       "      <td>131.4391</td>\n",
       "      <td>125.5049</td>\n",
       "      <td>131.3727</td>\n",
       "      <td>128.4164</td>\n",
       "      <td>128.1179</td>\n",
       "      <td>130.4106</td>\n",
       "      <td>125.9034</td>\n",
       "      <td>127.7948</td>\n",
       "      <td>107.8309</td>\n",
       "      <td>130.3470</td>\n",
       "      <td>123.1481</td>\n",
       "      <td>126.5420</td>\n",
       "      <td>126.6739</td>\n",
       "      <td>126.5410</td>\n",
       "      <td>119.4832</td>\n",
       "      <td>131.1901</td>\n",
       "      <td>123.1050</td>\n",
       "      <td>128.0390</td>\n",
       "      <td>135.3104</td>\n",
       "      <td>122.6758</td>\n",
       "      <td>130.7518</td>\n",
       "      <td>132.4906</td>\n",
       "      <td>117.5598</td>\n",
       "      <td>122.7631</td>\n",
       "      <td>123.0540</td>\n",
       "      <td>125.3133</td>\n",
       "      <td>127.3516</td>\n",
       "      <td>127.9951</td>\n",
       "      <td>129.5071</td>\n",
       "      <td>117.4723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20031</td>\n",
       "      <td>149.5538</td>\n",
       "      <td>131.8266</td>\n",
       "      <td>137.4409</td>\n",
       "      <td>136.5184</td>\n",
       "      <td>169.5321</td>\n",
       "      <td>137.3079</td>\n",
       "      <td>173.4217</td>\n",
       "      <td>156.0653</td>\n",
       "      <td>177.9780</td>\n",
       "      <td>185.4482</td>\n",
       "      <td>204.3158</td>\n",
       "      <td>164.5594</td>\n",
       "      <td>142.2590</td>\n",
       "      <td>142.7616</td>\n",
       "      <td>179.8644</td>\n",
       "      <td>165.9944</td>\n",
       "      <td>142.7045</td>\n",
       "      <td>172.2354</td>\n",
       "      <td>146.9605</td>\n",
       "      <td>151.3652</td>\n",
       "      <td>126.4088</td>\n",
       "      <td>146.3604</td>\n",
       "      <td>135.3381</td>\n",
       "      <td>164.0225</td>\n",
       "      <td>186.1351</td>\n",
       "      <td>188.9631</td>\n",
       "      <td>208.6886</td>\n",
       "      <td>188.1834</td>\n",
       "      <td>151.3405</td>\n",
       "      <td>172.2497</td>\n",
       "      <td>150.0432</td>\n",
       "      <td>163.0929</td>\n",
       "      <td>154.5921</td>\n",
       "      <td>232.5879</td>\n",
       "      <td>189.6538</td>\n",
       "      <td>192.9857</td>\n",
       "      <td>153.9412</td>\n",
       "      <td>144.4969</td>\n",
       "      <td>171.6511</td>\n",
       "      <td>142.4617</td>\n",
       "      <td>114.6349</td>\n",
       "      <td>155.9208</td>\n",
       "      <td>183.7528</td>\n",
       "      <td>150.4608</td>\n",
       "      <td>124.6302</td>\n",
       "      <td>133.0132</td>\n",
       "      <td>162.1536</td>\n",
       "      <td>130.1334</td>\n",
       "      <td>163.1354</td>\n",
       "      <td>122.9517</td>\n",
       "      <td>159.6235</td>\n",
       "      <td>170.5202</td>\n",
       "      <td>155.1799</td>\n",
       "      <td>172.1941</td>\n",
       "      <td>163.6023</td>\n",
       "      <td>164.0670</td>\n",
       "      <td>169.5248</td>\n",
       "      <td>166.9896</td>\n",
       "      <td>167.6808</td>\n",
       "      <td>176.4302</td>\n",
       "      <td>170.6967</td>\n",
       "      <td>178.3387</td>\n",
       "      <td>183.3684</td>\n",
       "      <td>178.1217</td>\n",
       "      <td>168.6252</td>\n",
       "      <td>167.8783</td>\n",
       "      <td>195.9046</td>\n",
       "      <td>179.1960</td>\n",
       "      <td>173.8303</td>\n",
       "      <td>167.9997</td>\n",
       "      <td>176.0619</td>\n",
       "      <td>157.8743</td>\n",
       "      <td>182.5080</td>\n",
       "      <td>169.5152</td>\n",
       "      <td>176.0296</td>\n",
       "      <td>176.0653</td>\n",
       "      <td>169.6586</td>\n",
       "      <td>169.7925</td>\n",
       "      <td>170.4700</td>\n",
       "      <td>180.3523</td>\n",
       "      <td>186.1322</td>\n",
       "      <td>169.1157</td>\n",
       "      <td>172.2018</td>\n",
       "      <td>170.9430</td>\n",
       "      <td>174.4274</td>\n",
       "      <td>165.0365</td>\n",
       "      <td>177.9984</td>\n",
       "      <td>168.8361</td>\n",
       "      <td>161.5027</td>\n",
       "      <td>167.7327</td>\n",
       "      <td>165.7992</td>\n",
       "      <td>179.8213</td>\n",
       "      <td>186.3055</td>\n",
       "      <td>173.2551</td>\n",
       "      <td>179.5217</td>\n",
       "      <td>180.7552</td>\n",
       "      <td>197.5646</td>\n",
       "      <td>163.4244</td>\n",
       "      <td>171.8347</td>\n",
       "      <td>190.5813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20032</td>\n",
       "      <td>293.8077</td>\n",
       "      <td>221.5754</td>\n",
       "      <td>306.7467</td>\n",
       "      <td>278.7379</td>\n",
       "      <td>290.7786</td>\n",
       "      <td>306.9963</td>\n",
       "      <td>409.9538</td>\n",
       "      <td>374.5432</td>\n",
       "      <td>480.4341</td>\n",
       "      <td>401.3495</td>\n",
       "      <td>517.0447</td>\n",
       "      <td>401.2173</td>\n",
       "      <td>258.9606</td>\n",
       "      <td>342.6221</td>\n",
       "      <td>713.7483</td>\n",
       "      <td>331.6915</td>\n",
       "      <td>206.6797</td>\n",
       "      <td>523.1787</td>\n",
       "      <td>326.8314</td>\n",
       "      <td>288.7405</td>\n",
       "      <td>359.8306</td>\n",
       "      <td>276.1968</td>\n",
       "      <td>327.2204</td>\n",
       "      <td>319.0630</td>\n",
       "      <td>395.6288</td>\n",
       "      <td>521.3525</td>\n",
       "      <td>322.3907</td>\n",
       "      <td>557.8931</td>\n",
       "      <td>458.4570</td>\n",
       "      <td>557.8550</td>\n",
       "      <td>395.9073</td>\n",
       "      <td>324.7355</td>\n",
       "      <td>229.6913</td>\n",
       "      <td>253.1717</td>\n",
       "      <td>339.8326</td>\n",
       "      <td>390.5334</td>\n",
       "      <td>306.5388</td>\n",
       "      <td>304.4779</td>\n",
       "      <td>391.0621</td>\n",
       "      <td>134.9783</td>\n",
       "      <td>293.2355</td>\n",
       "      <td>205.0012</td>\n",
       "      <td>380.8655</td>\n",
       "      <td>287.6712</td>\n",
       "      <td>229.3710</td>\n",
       "      <td>241.8626</td>\n",
       "      <td>297.9864</td>\n",
       "      <td>218.7566</td>\n",
       "      <td>437.0433</td>\n",
       "      <td>236.7548</td>\n",
       "      <td>360.6154</td>\n",
       "      <td>380.8710</td>\n",
       "      <td>361.1404</td>\n",
       "      <td>370.9860</td>\n",
       "      <td>397.4942</td>\n",
       "      <td>289.6452</td>\n",
       "      <td>365.6733</td>\n",
       "      <td>346.8838</td>\n",
       "      <td>397.5960</td>\n",
       "      <td>339.2564</td>\n",
       "      <td>339.2108</td>\n",
       "      <td>311.0655</td>\n",
       "      <td>340.7786</td>\n",
       "      <td>331.3242</td>\n",
       "      <td>357.4355</td>\n",
       "      <td>386.1326</td>\n",
       "      <td>314.1153</td>\n",
       "      <td>365.6909</td>\n",
       "      <td>282.2910</td>\n",
       "      <td>360.6281</td>\n",
       "      <td>377.9049</td>\n",
       "      <td>351.4260</td>\n",
       "      <td>343.4717</td>\n",
       "      <td>357.3763</td>\n",
       "      <td>364.5757</td>\n",
       "      <td>353.0154</td>\n",
       "      <td>334.8573</td>\n",
       "      <td>415.7653</td>\n",
       "      <td>292.5078</td>\n",
       "      <td>346.9949</td>\n",
       "      <td>300.1604</td>\n",
       "      <td>375.1366</td>\n",
       "      <td>333.2039</td>\n",
       "      <td>360.8628</td>\n",
       "      <td>380.3383</td>\n",
       "      <td>377.5653</td>\n",
       "      <td>396.1262</td>\n",
       "      <td>329.7908</td>\n",
       "      <td>365.6472</td>\n",
       "      <td>360.0468</td>\n",
       "      <td>377.4798</td>\n",
       "      <td>384.2602</td>\n",
       "      <td>279.4871</td>\n",
       "      <td>349.1280</td>\n",
       "      <td>289.9366</td>\n",
       "      <td>318.2822</td>\n",
       "      <td>281.0026</td>\n",
       "      <td>360.5740</td>\n",
       "      <td>335.0682</td>\n",
       "      <td>325.6003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20033</td>\n",
       "      <td>147.0707</td>\n",
       "      <td>139.2879</td>\n",
       "      <td>141.9818</td>\n",
       "      <td>129.9245</td>\n",
       "      <td>152.5105</td>\n",
       "      <td>156.0215</td>\n",
       "      <td>146.8749</td>\n",
       "      <td>137.8159</td>\n",
       "      <td>204.0022</td>\n",
       "      <td>152.5423</td>\n",
       "      <td>174.3877</td>\n",
       "      <td>169.4315</td>\n",
       "      <td>173.5629</td>\n",
       "      <td>133.6483</td>\n",
       "      <td>200.7517</td>\n",
       "      <td>150.9508</td>\n",
       "      <td>136.7839</td>\n",
       "      <td>176.4219</td>\n",
       "      <td>135.3735</td>\n",
       "      <td>153.6187</td>\n",
       "      <td>132.0650</td>\n",
       "      <td>131.9070</td>\n",
       "      <td>144.7022</td>\n",
       "      <td>191.5445</td>\n",
       "      <td>183.2472</td>\n",
       "      <td>172.5116</td>\n",
       "      <td>197.9057</td>\n",
       "      <td>164.1146</td>\n",
       "      <td>157.3339</td>\n",
       "      <td>183.3693</td>\n",
       "      <td>128.1466</td>\n",
       "      <td>153.2623</td>\n",
       "      <td>120.7799</td>\n",
       "      <td>158.2563</td>\n",
       "      <td>166.8435</td>\n",
       "      <td>202.5654</td>\n",
       "      <td>143.2403</td>\n",
       "      <td>126.5126</td>\n",
       "      <td>150.6224</td>\n",
       "      <td>136.9909</td>\n",
       "      <td>127.5857</td>\n",
       "      <td>162.8592</td>\n",
       "      <td>150.5226</td>\n",
       "      <td>152.0828</td>\n",
       "      <td>107.7393</td>\n",
       "      <td>117.7514</td>\n",
       "      <td>161.9576</td>\n",
       "      <td>115.5054</td>\n",
       "      <td>150.6046</td>\n",
       "      <td>132.3869</td>\n",
       "      <td>152.6740</td>\n",
       "      <td>161.0171</td>\n",
       "      <td>154.0657</td>\n",
       "      <td>161.1739</td>\n",
       "      <td>134.6203</td>\n",
       "      <td>158.9357</td>\n",
       "      <td>153.6290</td>\n",
       "      <td>152.5943</td>\n",
       "      <td>145.3410</td>\n",
       "      <td>145.9474</td>\n",
       "      <td>155.5663</td>\n",
       "      <td>161.4639</td>\n",
       "      <td>163.8484</td>\n",
       "      <td>159.4729</td>\n",
       "      <td>158.4050</td>\n",
       "      <td>143.8312</td>\n",
       "      <td>161.9339</td>\n",
       "      <td>146.5934</td>\n",
       "      <td>155.9855</td>\n",
       "      <td>148.4028</td>\n",
       "      <td>152.0356</td>\n",
       "      <td>150.1118</td>\n",
       "      <td>146.0164</td>\n",
       "      <td>151.9261</td>\n",
       "      <td>152.6968</td>\n",
       "      <td>162.2125</td>\n",
       "      <td>161.3070</td>\n",
       "      <td>153.5735</td>\n",
       "      <td>143.2479</td>\n",
       "      <td>163.6287</td>\n",
       "      <td>162.9206</td>\n",
       "      <td>149.1253</td>\n",
       "      <td>162.5769</td>\n",
       "      <td>157.4079</td>\n",
       "      <td>159.4977</td>\n",
       "      <td>148.1175</td>\n",
       "      <td>148.5178</td>\n",
       "      <td>140.9751</td>\n",
       "      <td>145.0534</td>\n",
       "      <td>150.3364</td>\n",
       "      <td>151.3664</td>\n",
       "      <td>157.1866</td>\n",
       "      <td>179.5706</td>\n",
       "      <td>149.5516</td>\n",
       "      <td>153.0176</td>\n",
       "      <td>154.8152</td>\n",
       "      <td>150.6167</td>\n",
       "      <td>145.5199</td>\n",
       "      <td>159.8416</td>\n",
       "      <td>152.4497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20035</td>\n",
       "      <td>143.4371</td>\n",
       "      <td>142.8364</td>\n",
       "      <td>146.1861</td>\n",
       "      <td>140.7676</td>\n",
       "      <td>163.7396</td>\n",
       "      <td>169.2113</td>\n",
       "      <td>147.2383</td>\n",
       "      <td>138.3887</td>\n",
       "      <td>138.4473</td>\n",
       "      <td>164.3530</td>\n",
       "      <td>161.6685</td>\n",
       "      <td>168.8474</td>\n",
       "      <td>147.3448</td>\n",
       "      <td>127.8374</td>\n",
       "      <td>201.6550</td>\n",
       "      <td>138.3497</td>\n",
       "      <td>125.4764</td>\n",
       "      <td>163.0695</td>\n",
       "      <td>126.9716</td>\n",
       "      <td>150.1888</td>\n",
       "      <td>146.2881</td>\n",
       "      <td>150.1362</td>\n",
       "      <td>166.7038</td>\n",
       "      <td>141.9548</td>\n",
       "      <td>185.2390</td>\n",
       "      <td>154.3720</td>\n",
       "      <td>178.5173</td>\n",
       "      <td>152.5718</td>\n",
       "      <td>121.2555</td>\n",
       "      <td>197.2472</td>\n",
       "      <td>118.3496</td>\n",
       "      <td>194.5986</td>\n",
       "      <td>139.8323</td>\n",
       "      <td>172.1453</td>\n",
       "      <td>149.7245</td>\n",
       "      <td>166.7070</td>\n",
       "      <td>125.6850</td>\n",
       "      <td>131.0723</td>\n",
       "      <td>151.9287</td>\n",
       "      <td>143.0878</td>\n",
       "      <td>144.0166</td>\n",
       "      <td>139.9561</td>\n",
       "      <td>136.2970</td>\n",
       "      <td>139.4698</td>\n",
       "      <td>128.4745</td>\n",
       "      <td>136.8866</td>\n",
       "      <td>133.9539</td>\n",
       "      <td>168.4924</td>\n",
       "      <td>144.5071</td>\n",
       "      <td>178.5392</td>\n",
       "      <td>169.8240</td>\n",
       "      <td>184.7186</td>\n",
       "      <td>173.9340</td>\n",
       "      <td>177.6883</td>\n",
       "      <td>173.2986</td>\n",
       "      <td>180.1557</td>\n",
       "      <td>176.7811</td>\n",
       "      <td>185.3754</td>\n",
       "      <td>166.7029</td>\n",
       "      <td>179.2706</td>\n",
       "      <td>180.9199</td>\n",
       "      <td>161.6526</td>\n",
       "      <td>192.0314</td>\n",
       "      <td>191.3981</td>\n",
       "      <td>180.1943</td>\n",
       "      <td>179.6839</td>\n",
       "      <td>176.4440</td>\n",
       "      <td>187.8086</td>\n",
       "      <td>172.0887</td>\n",
       "      <td>169.5977</td>\n",
       "      <td>181.6283</td>\n",
       "      <td>173.9056</td>\n",
       "      <td>182.3582</td>\n",
       "      <td>165.3770</td>\n",
       "      <td>181.9295</td>\n",
       "      <td>187.0482</td>\n",
       "      <td>165.9020</td>\n",
       "      <td>182.4737</td>\n",
       "      <td>161.4001</td>\n",
       "      <td>183.8603</td>\n",
       "      <td>171.3620</td>\n",
       "      <td>172.6371</td>\n",
       "      <td>187.7009</td>\n",
       "      <td>174.4993</td>\n",
       "      <td>180.5801</td>\n",
       "      <td>172.2739</td>\n",
       "      <td>173.5679</td>\n",
       "      <td>179.8597</td>\n",
       "      <td>164.2872</td>\n",
       "      <td>175.0192</td>\n",
       "      <td>179.2498</td>\n",
       "      <td>183.8127</td>\n",
       "      <td>170.4757</td>\n",
       "      <td>169.4707</td>\n",
       "      <td>194.8038</td>\n",
       "      <td>188.0344</td>\n",
       "      <td>183.5897</td>\n",
       "      <td>176.9232</td>\n",
       "      <td>177.8582</td>\n",
       "      <td>179.1412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20037</td>\n",
       "      <td>99.8669</td>\n",
       "      <td>116.5094</td>\n",
       "      <td>95.2026</td>\n",
       "      <td>99.7441</td>\n",
       "      <td>124.3372</td>\n",
       "      <td>123.3444</td>\n",
       "      <td>108.0891</td>\n",
       "      <td>117.9680</td>\n",
       "      <td>138.1783</td>\n",
       "      <td>125.6735</td>\n",
       "      <td>132.0409</td>\n",
       "      <td>122.8899</td>\n",
       "      <td>151.8933</td>\n",
       "      <td>116.6103</td>\n",
       "      <td>133.0978</td>\n",
       "      <td>120.8281</td>\n",
       "      <td>108.2332</td>\n",
       "      <td>116.5078</td>\n",
       "      <td>106.8207</td>\n",
       "      <td>115.9274</td>\n",
       "      <td>84.3842</td>\n",
       "      <td>108.0805</td>\n",
       "      <td>111.1125</td>\n",
       "      <td>119.8159</td>\n",
       "      <td>131.5833</td>\n",
       "      <td>129.3511</td>\n",
       "      <td>160.3539</td>\n",
       "      <td>106.9095</td>\n",
       "      <td>124.1121</td>\n",
       "      <td>129.2110</td>\n",
       "      <td>95.2042</td>\n",
       "      <td>143.9558</td>\n",
       "      <td>97.5699</td>\n",
       "      <td>147.2337</td>\n",
       "      <td>126.6272</td>\n",
       "      <td>142.2823</td>\n",
       "      <td>107.1593</td>\n",
       "      <td>113.4130</td>\n",
       "      <td>109.7311</td>\n",
       "      <td>105.2100</td>\n",
       "      <td>101.2379</td>\n",
       "      <td>128.9412</td>\n",
       "      <td>133.4731</td>\n",
       "      <td>118.2429</td>\n",
       "      <td>67.0356</td>\n",
       "      <td>96.3697</td>\n",
       "      <td>126.6978</td>\n",
       "      <td>87.5188</td>\n",
       "      <td>106.9121</td>\n",
       "      <td>108.5374</td>\n",
       "      <td>82.0805</td>\n",
       "      <td>91.2024</td>\n",
       "      <td>102.6056</td>\n",
       "      <td>98.8600</td>\n",
       "      <td>96.6082</td>\n",
       "      <td>101.7833</td>\n",
       "      <td>92.2282</td>\n",
       "      <td>85.4809</td>\n",
       "      <td>84.4405</td>\n",
       "      <td>88.4554</td>\n",
       "      <td>91.8238</td>\n",
       "      <td>103.0833</td>\n",
       "      <td>123.2556</td>\n",
       "      <td>82.0875</td>\n",
       "      <td>91.2302</td>\n",
       "      <td>95.0121</td>\n",
       "      <td>89.4963</td>\n",
       "      <td>90.1772</td>\n",
       "      <td>80.2903</td>\n",
       "      <td>82.8869</td>\n",
       "      <td>95.4519</td>\n",
       "      <td>92.0316</td>\n",
       "      <td>100.9470</td>\n",
       "      <td>91.8333</td>\n",
       "      <td>98.8745</td>\n",
       "      <td>90.8852</td>\n",
       "      <td>91.7316</td>\n",
       "      <td>98.1267</td>\n",
       "      <td>106.3843</td>\n",
       "      <td>87.1880</td>\n",
       "      <td>104.9997</td>\n",
       "      <td>87.9972</td>\n",
       "      <td>83.2134</td>\n",
       "      <td>91.3137</td>\n",
       "      <td>97.7382</td>\n",
       "      <td>88.0808</td>\n",
       "      <td>95.0126</td>\n",
       "      <td>86.7734</td>\n",
       "      <td>93.0855</td>\n",
       "      <td>87.1346</td>\n",
       "      <td>88.3346</td>\n",
       "      <td>90.2364</td>\n",
       "      <td>92.4915</td>\n",
       "      <td>89.0421</td>\n",
       "      <td>106.2461</td>\n",
       "      <td>92.7213</td>\n",
       "      <td>91.4649</td>\n",
       "      <td>94.0510</td>\n",
       "      <td>96.7610</td>\n",
       "      <td>94.4856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>20038</td>\n",
       "      <td>144.7399</td>\n",
       "      <td>138.2283</td>\n",
       "      <td>139.4684</td>\n",
       "      <td>150.3530</td>\n",
       "      <td>156.0508</td>\n",
       "      <td>152.5462</td>\n",
       "      <td>162.7816</td>\n",
       "      <td>142.5520</td>\n",
       "      <td>142.6426</td>\n",
       "      <td>160.4717</td>\n",
       "      <td>165.4502</td>\n",
       "      <td>160.7731</td>\n",
       "      <td>149.4334</td>\n",
       "      <td>129.6129</td>\n",
       "      <td>174.4251</td>\n",
       "      <td>145.6009</td>\n",
       "      <td>122.8172</td>\n",
       "      <td>157.5076</td>\n",
       "      <td>128.0863</td>\n",
       "      <td>151.4524</td>\n",
       "      <td>133.1522</td>\n",
       "      <td>150.8343</td>\n",
       "      <td>140.5670</td>\n",
       "      <td>146.1563</td>\n",
       "      <td>180.5960</td>\n",
       "      <td>147.6876</td>\n",
       "      <td>168.1225</td>\n",
       "      <td>139.2202</td>\n",
       "      <td>126.0470</td>\n",
       "      <td>172.2079</td>\n",
       "      <td>122.2128</td>\n",
       "      <td>195.5916</td>\n",
       "      <td>143.8683</td>\n",
       "      <td>165.9235</td>\n",
       "      <td>162.1844</td>\n",
       "      <td>161.2547</td>\n",
       "      <td>133.9495</td>\n",
       "      <td>129.0941</td>\n",
       "      <td>145.8009</td>\n",
       "      <td>143.7886</td>\n",
       "      <td>141.9816</td>\n",
       "      <td>133.5973</td>\n",
       "      <td>139.2274</td>\n",
       "      <td>133.1335</td>\n",
       "      <td>129.2899</td>\n",
       "      <td>133.6828</td>\n",
       "      <td>136.4994</td>\n",
       "      <td>149.3817</td>\n",
       "      <td>144.5341</td>\n",
       "      <td>147.3884</td>\n",
       "      <td>126.7722</td>\n",
       "      <td>127.9022</td>\n",
       "      <td>135.8337</td>\n",
       "      <td>123.0386</td>\n",
       "      <td>133.7974</td>\n",
       "      <td>149.0318</td>\n",
       "      <td>124.8588</td>\n",
       "      <td>130.6720</td>\n",
       "      <td>123.9969</td>\n",
       "      <td>135.9127</td>\n",
       "      <td>134.6636</td>\n",
       "      <td>129.2286</td>\n",
       "      <td>150.2301</td>\n",
       "      <td>137.5690</td>\n",
       "      <td>134.6355</td>\n",
       "      <td>130.2049</td>\n",
       "      <td>133.4851</td>\n",
       "      <td>141.1923</td>\n",
       "      <td>137.6484</td>\n",
       "      <td>124.7656</td>\n",
       "      <td>134.9871</td>\n",
       "      <td>128.0318</td>\n",
       "      <td>123.0293</td>\n",
       "      <td>132.4739</td>\n",
       "      <td>126.7291</td>\n",
       "      <td>128.1731</td>\n",
       "      <td>137.0092</td>\n",
       "      <td>121.0921</td>\n",
       "      <td>142.7866</td>\n",
       "      <td>130.2934</td>\n",
       "      <td>148.6982</td>\n",
       "      <td>133.0732</td>\n",
       "      <td>137.9582</td>\n",
       "      <td>129.2926</td>\n",
       "      <td>128.8121</td>\n",
       "      <td>121.1073</td>\n",
       "      <td>138.4083</td>\n",
       "      <td>136.5179</td>\n",
       "      <td>122.4147</td>\n",
       "      <td>138.0092</td>\n",
       "      <td>130.8860</td>\n",
       "      <td>133.5628</td>\n",
       "      <td>133.4755</td>\n",
       "      <td>138.1780</td>\n",
       "      <td>163.2792</td>\n",
       "      <td>136.1106</td>\n",
       "      <td>146.3470</td>\n",
       "      <td>123.2269</td>\n",
       "      <td>128.7197</td>\n",
       "      <td>138.2303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20039</td>\n",
       "      <td>110.3789</td>\n",
       "      <td>108.9284</td>\n",
       "      <td>104.5499</td>\n",
       "      <td>124.2283</td>\n",
       "      <td>125.0757</td>\n",
       "      <td>141.7249</td>\n",
       "      <td>113.3660</td>\n",
       "      <td>103.9642</td>\n",
       "      <td>107.5785</td>\n",
       "      <td>111.6985</td>\n",
       "      <td>131.1115</td>\n",
       "      <td>125.9840</td>\n",
       "      <td>124.8481</td>\n",
       "      <td>102.6389</td>\n",
       "      <td>155.3345</td>\n",
       "      <td>115.0663</td>\n",
       "      <td>103.0565</td>\n",
       "      <td>128.6975</td>\n",
       "      <td>99.4725</td>\n",
       "      <td>114.6393</td>\n",
       "      <td>110.0601</td>\n",
       "      <td>114.5994</td>\n",
       "      <td>120.0077</td>\n",
       "      <td>111.3927</td>\n",
       "      <td>140.1224</td>\n",
       "      <td>116.5938</td>\n",
       "      <td>140.5085</td>\n",
       "      <td>112.4132</td>\n",
       "      <td>104.2110</td>\n",
       "      <td>158.0055</td>\n",
       "      <td>101.4026</td>\n",
       "      <td>127.6344</td>\n",
       "      <td>111.1005</td>\n",
       "      <td>111.6860</td>\n",
       "      <td>120.6298</td>\n",
       "      <td>140.3429</td>\n",
       "      <td>122.1249</td>\n",
       "      <td>104.6693</td>\n",
       "      <td>126.4792</td>\n",
       "      <td>118.6021</td>\n",
       "      <td>109.6189</td>\n",
       "      <td>117.2344</td>\n",
       "      <td>115.8060</td>\n",
       "      <td>111.1877</td>\n",
       "      <td>107.1197</td>\n",
       "      <td>109.8603</td>\n",
       "      <td>107.4721</td>\n",
       "      <td>119.2870</td>\n",
       "      <td>113.0822</td>\n",
       "      <td>144.6469</td>\n",
       "      <td>125.4597</td>\n",
       "      <td>125.1881</td>\n",
       "      <td>123.9174</td>\n",
       "      <td>118.4792</td>\n",
       "      <td>117.4450</td>\n",
       "      <td>137.7511</td>\n",
       "      <td>127.9736</td>\n",
       "      <td>127.8227</td>\n",
       "      <td>113.1887</td>\n",
       "      <td>131.2800</td>\n",
       "      <td>118.9790</td>\n",
       "      <td>127.5017</td>\n",
       "      <td>139.6781</td>\n",
       "      <td>130.6146</td>\n",
       "      <td>132.7026</td>\n",
       "      <td>119.3582</td>\n",
       "      <td>126.2601</td>\n",
       "      <td>124.3176</td>\n",
       "      <td>125.2898</td>\n",
       "      <td>122.3137</td>\n",
       "      <td>126.3856</td>\n",
       "      <td>119.9864</td>\n",
       "      <td>112.1625</td>\n",
       "      <td>124.5754</td>\n",
       "      <td>124.4437</td>\n",
       "      <td>125.6594</td>\n",
       "      <td>126.5154</td>\n",
       "      <td>121.6095</td>\n",
       "      <td>133.4891</td>\n",
       "      <td>124.7810</td>\n",
       "      <td>136.5167</td>\n",
       "      <td>122.4327</td>\n",
       "      <td>117.0441</td>\n",
       "      <td>124.6958</td>\n",
       "      <td>112.0019</td>\n",
       "      <td>121.7805</td>\n",
       "      <td>121.5130</td>\n",
       "      <td>133.7806</td>\n",
       "      <td>116.8150</td>\n",
       "      <td>128.2109</td>\n",
       "      <td>129.5751</td>\n",
       "      <td>125.5405</td>\n",
       "      <td>129.9859</td>\n",
       "      <td>127.6204</td>\n",
       "      <td>127.7045</td>\n",
       "      <td>128.8667</td>\n",
       "      <td>136.6615</td>\n",
       "      <td>121.9893</td>\n",
       "      <td>119.7872</td>\n",
       "      <td>129.0150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20041</td>\n",
       "      <td>116.6943</td>\n",
       "      <td>113.2528</td>\n",
       "      <td>115.4239</td>\n",
       "      <td>129.6136</td>\n",
       "      <td>128.2218</td>\n",
       "      <td>141.9884</td>\n",
       "      <td>118.5737</td>\n",
       "      <td>115.5773</td>\n",
       "      <td>107.6348</td>\n",
       "      <td>129.9765</td>\n",
       "      <td>132.4853</td>\n",
       "      <td>137.9937</td>\n",
       "      <td>127.3719</td>\n",
       "      <td>114.4955</td>\n",
       "      <td>159.8279</td>\n",
       "      <td>128.3643</td>\n",
       "      <td>102.4466</td>\n",
       "      <td>138.3189</td>\n",
       "      <td>107.2258</td>\n",
       "      <td>126.5717</td>\n",
       "      <td>110.7823</td>\n",
       "      <td>118.6859</td>\n",
       "      <td>131.1102</td>\n",
       "      <td>118.0458</td>\n",
       "      <td>153.5787</td>\n",
       "      <td>126.0750</td>\n",
       "      <td>146.3040</td>\n",
       "      <td>114.5976</td>\n",
       "      <td>102.0287</td>\n",
       "      <td>163.9224</td>\n",
       "      <td>96.3636</td>\n",
       "      <td>142.0979</td>\n",
       "      <td>110.7196</td>\n",
       "      <td>131.8866</td>\n",
       "      <td>125.5708</td>\n",
       "      <td>144.3454</td>\n",
       "      <td>114.0227</td>\n",
       "      <td>107.6500</td>\n",
       "      <td>123.3211</td>\n",
       "      <td>114.2257</td>\n",
       "      <td>115.9034</td>\n",
       "      <td>127.0972</td>\n",
       "      <td>130.9606</td>\n",
       "      <td>121.2551</td>\n",
       "      <td>96.8991</td>\n",
       "      <td>111.9013</td>\n",
       "      <td>110.8084</td>\n",
       "      <td>132.6254</td>\n",
       "      <td>115.6802</td>\n",
       "      <td>142.7584</td>\n",
       "      <td>109.7369</td>\n",
       "      <td>114.1631</td>\n",
       "      <td>120.3225</td>\n",
       "      <td>117.4432</td>\n",
       "      <td>110.2971</td>\n",
       "      <td>120.8008</td>\n",
       "      <td>113.5941</td>\n",
       "      <td>114.1584</td>\n",
       "      <td>109.7372</td>\n",
       "      <td>123.9470</td>\n",
       "      <td>114.6036</td>\n",
       "      <td>115.9419</td>\n",
       "      <td>148.4755</td>\n",
       "      <td>126.3713</td>\n",
       "      <td>116.6357</td>\n",
       "      <td>114.0014</td>\n",
       "      <td>114.4256</td>\n",
       "      <td>119.7452</td>\n",
       "      <td>110.6857</td>\n",
       "      <td>117.7073</td>\n",
       "      <td>120.0833</td>\n",
       "      <td>111.4837</td>\n",
       "      <td>120.1851</td>\n",
       "      <td>121.2393</td>\n",
       "      <td>122.3760</td>\n",
       "      <td>117.2666</td>\n",
       "      <td>127.6008</td>\n",
       "      <td>117.1966</td>\n",
       "      <td>108.0432</td>\n",
       "      <td>114.1732</td>\n",
       "      <td>129.0926</td>\n",
       "      <td>115.1785</td>\n",
       "      <td>114.8752</td>\n",
       "      <td>118.0085</td>\n",
       "      <td>118.3207</td>\n",
       "      <td>116.6613</td>\n",
       "      <td>116.0024</td>\n",
       "      <td>113.2249</td>\n",
       "      <td>114.4562</td>\n",
       "      <td>122.5673</td>\n",
       "      <td>112.1269</td>\n",
       "      <td>120.4343</td>\n",
       "      <td>129.1037</td>\n",
       "      <td>121.0256</td>\n",
       "      <td>119.3263</td>\n",
       "      <td>109.7617</td>\n",
       "      <td>131.4555</td>\n",
       "      <td>111.5880</td>\n",
       "      <td>118.5640</td>\n",
       "      <td>109.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20042</td>\n",
       "      <td>159.9865</td>\n",
       "      <td>148.8530</td>\n",
       "      <td>146.5676</td>\n",
       "      <td>153.6497</td>\n",
       "      <td>157.7733</td>\n",
       "      <td>166.1177</td>\n",
       "      <td>150.4509</td>\n",
       "      <td>144.6221</td>\n",
       "      <td>197.6780</td>\n",
       "      <td>174.9880</td>\n",
       "      <td>178.0265</td>\n",
       "      <td>161.5965</td>\n",
       "      <td>157.7426</td>\n",
       "      <td>140.7046</td>\n",
       "      <td>184.7221</td>\n",
       "      <td>174.3629</td>\n",
       "      <td>142.4898</td>\n",
       "      <td>164.5987</td>\n",
       "      <td>139.3142</td>\n",
       "      <td>161.7037</td>\n",
       "      <td>151.5219</td>\n",
       "      <td>143.5305</td>\n",
       "      <td>151.6303</td>\n",
       "      <td>181.8778</td>\n",
       "      <td>176.7753</td>\n",
       "      <td>168.7838</td>\n",
       "      <td>190.8953</td>\n",
       "      <td>173.6293</td>\n",
       "      <td>150.3520</td>\n",
       "      <td>172.6195</td>\n",
       "      <td>135.1622</td>\n",
       "      <td>170.9543</td>\n",
       "      <td>145.2398</td>\n",
       "      <td>183.2045</td>\n",
       "      <td>169.9793</td>\n",
       "      <td>197.1689</td>\n",
       "      <td>143.4695</td>\n",
       "      <td>141.4785</td>\n",
       "      <td>151.8205</td>\n",
       "      <td>148.7818</td>\n",
       "      <td>147.2323</td>\n",
       "      <td>149.8004</td>\n",
       "      <td>154.8309</td>\n",
       "      <td>145.3748</td>\n",
       "      <td>135.3343</td>\n",
       "      <td>133.8757</td>\n",
       "      <td>154.7262</td>\n",
       "      <td>144.2402</td>\n",
       "      <td>155.1571</td>\n",
       "      <td>153.9082</td>\n",
       "      <td>141.9906</td>\n",
       "      <td>140.1508</td>\n",
       "      <td>151.3614</td>\n",
       "      <td>145.9008</td>\n",
       "      <td>139.5827</td>\n",
       "      <td>141.4578</td>\n",
       "      <td>160.4123</td>\n",
       "      <td>151.8647</td>\n",
       "      <td>144.4574</td>\n",
       "      <td>142.8151</td>\n",
       "      <td>145.8115</td>\n",
       "      <td>147.8484</td>\n",
       "      <td>154.0840</td>\n",
       "      <td>154.9397</td>\n",
       "      <td>147.7575</td>\n",
       "      <td>145.4863</td>\n",
       "      <td>152.3641</td>\n",
       "      <td>142.4368</td>\n",
       "      <td>140.5887</td>\n",
       "      <td>142.0334</td>\n",
       "      <td>146.7145</td>\n",
       "      <td>148.9789</td>\n",
       "      <td>143.0665</td>\n",
       "      <td>155.9237</td>\n",
       "      <td>147.8032</td>\n",
       "      <td>145.0917</td>\n",
       "      <td>148.0746</td>\n",
       "      <td>139.9452</td>\n",
       "      <td>131.5185</td>\n",
       "      <td>151.8284</td>\n",
       "      <td>147.4619</td>\n",
       "      <td>148.2473</td>\n",
       "      <td>147.0051</td>\n",
       "      <td>150.8811</td>\n",
       "      <td>150.5424</td>\n",
       "      <td>135.2271</td>\n",
       "      <td>143.2361</td>\n",
       "      <td>152.4308</td>\n",
       "      <td>134.4564</td>\n",
       "      <td>141.4033</td>\n",
       "      <td>146.2342</td>\n",
       "      <td>151.0918</td>\n",
       "      <td>145.6117</td>\n",
       "      <td>140.5945</td>\n",
       "      <td>124.2010</td>\n",
       "      <td>145.7154</td>\n",
       "      <td>151.7642</td>\n",
       "      <td>139.0290</td>\n",
       "      <td>149.9572</td>\n",
       "      <td>155.0182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20043</td>\n",
       "      <td>111.7149</td>\n",
       "      <td>117.4667</td>\n",
       "      <td>113.7577</td>\n",
       "      <td>124.8901</td>\n",
       "      <td>138.1347</td>\n",
       "      <td>127.9820</td>\n",
       "      <td>119.8007</td>\n",
       "      <td>115.0544</td>\n",
       "      <td>128.8901</td>\n",
       "      <td>136.9164</td>\n",
       "      <td>139.7854</td>\n",
       "      <td>138.6323</td>\n",
       "      <td>133.4297</td>\n",
       "      <td>124.9072</td>\n",
       "      <td>166.9571</td>\n",
       "      <td>124.9850</td>\n",
       "      <td>106.1039</td>\n",
       "      <td>144.8774</td>\n",
       "      <td>109.1230</td>\n",
       "      <td>132.4126</td>\n",
       "      <td>110.8035</td>\n",
       "      <td>116.9261</td>\n",
       "      <td>115.7571</td>\n",
       "      <td>124.8778</td>\n",
       "      <td>155.4358</td>\n",
       "      <td>135.4146</td>\n",
       "      <td>151.0351</td>\n",
       "      <td>122.3160</td>\n",
       "      <td>110.0924</td>\n",
       "      <td>158.6599</td>\n",
       "      <td>99.6517</td>\n",
       "      <td>139.9108</td>\n",
       "      <td>121.3831</td>\n",
       "      <td>132.2557</td>\n",
       "      <td>131.7651</td>\n",
       "      <td>134.5080</td>\n",
       "      <td>103.6642</td>\n",
       "      <td>107.3088</td>\n",
       "      <td>132.9827</td>\n",
       "      <td>119.7772</td>\n",
       "      <td>118.4612</td>\n",
       "      <td>131.2399</td>\n",
       "      <td>128.1320</td>\n",
       "      <td>127.0353</td>\n",
       "      <td>103.7317</td>\n",
       "      <td>104.3203</td>\n",
       "      <td>119.1684</td>\n",
       "      <td>124.2115</td>\n",
       "      <td>119.7640</td>\n",
       "      <td>115.0307</td>\n",
       "      <td>120.8504</td>\n",
       "      <td>112.0642</td>\n",
       "      <td>132.9710</td>\n",
       "      <td>125.6856</td>\n",
       "      <td>117.5572</td>\n",
       "      <td>128.2544</td>\n",
       "      <td>131.3250</td>\n",
       "      <td>131.3921</td>\n",
       "      <td>112.0700</td>\n",
       "      <td>128.3590</td>\n",
       "      <td>119.2013</td>\n",
       "      <td>128.6890</td>\n",
       "      <td>136.2385</td>\n",
       "      <td>137.3303</td>\n",
       "      <td>131.1523</td>\n",
       "      <td>117.0530</td>\n",
       "      <td>134.3293</td>\n",
       "      <td>117.3966</td>\n",
       "      <td>134.5886</td>\n",
       "      <td>125.2923</td>\n",
       "      <td>131.9566</td>\n",
       "      <td>126.0413</td>\n",
       "      <td>119.3996</td>\n",
       "      <td>126.6011</td>\n",
       "      <td>130.6147</td>\n",
       "      <td>122.7916</td>\n",
       "      <td>124.9323</td>\n",
       "      <td>115.6650</td>\n",
       "      <td>122.8478</td>\n",
       "      <td>130.1197</td>\n",
       "      <td>127.0007</td>\n",
       "      <td>118.3631</td>\n",
       "      <td>140.1864</td>\n",
       "      <td>131.6452</td>\n",
       "      <td>135.1641</td>\n",
       "      <td>119.8297</td>\n",
       "      <td>122.6380</td>\n",
       "      <td>124.1217</td>\n",
       "      <td>117.4798</td>\n",
       "      <td>124.5238</td>\n",
       "      <td>129.5584</td>\n",
       "      <td>124.4200</td>\n",
       "      <td>121.0740</td>\n",
       "      <td>127.9312</td>\n",
       "      <td>118.6947</td>\n",
       "      <td>120.0195</td>\n",
       "      <td>117.3686</td>\n",
       "      <td>120.7703</td>\n",
       "      <td>121.4383</td>\n",
       "      <td>128.4083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20044</td>\n",
       "      <td>98.2742</td>\n",
       "      <td>123.0857</td>\n",
       "      <td>152.7743</td>\n",
       "      <td>132.1182</td>\n",
       "      <td>130.2750</td>\n",
       "      <td>139.4480</td>\n",
       "      <td>141.7809</td>\n",
       "      <td>134.2030</td>\n",
       "      <td>189.6644</td>\n",
       "      <td>157.1923</td>\n",
       "      <td>169.6821</td>\n",
       "      <td>139.6038</td>\n",
       "      <td>180.2266</td>\n",
       "      <td>120.2961</td>\n",
       "      <td>127.5143</td>\n",
       "      <td>144.5782</td>\n",
       "      <td>121.4368</td>\n",
       "      <td>156.0736</td>\n",
       "      <td>138.3712</td>\n",
       "      <td>124.3114</td>\n",
       "      <td>108.0032</td>\n",
       "      <td>132.0442</td>\n",
       "      <td>119.9837</td>\n",
       "      <td>143.7612</td>\n",
       "      <td>148.1151</td>\n",
       "      <td>170.2813</td>\n",
       "      <td>181.3060</td>\n",
       "      <td>146.3005</td>\n",
       "      <td>150.1703</td>\n",
       "      <td>149.4258</td>\n",
       "      <td>132.2264</td>\n",
       "      <td>138.4821</td>\n",
       "      <td>116.3148</td>\n",
       "      <td>120.6318</td>\n",
       "      <td>141.6637</td>\n",
       "      <td>163.2272</td>\n",
       "      <td>128.3400</td>\n",
       "      <td>125.9149</td>\n",
       "      <td>136.2038</td>\n",
       "      <td>109.8446</td>\n",
       "      <td>106.1540</td>\n",
       "      <td>148.0516</td>\n",
       "      <td>144.4605</td>\n",
       "      <td>133.5214</td>\n",
       "      <td>99.8351</td>\n",
       "      <td>107.1362</td>\n",
       "      <td>153.3463</td>\n",
       "      <td>101.7502</td>\n",
       "      <td>143.5213</td>\n",
       "      <td>116.1908</td>\n",
       "      <td>112.1055</td>\n",
       "      <td>128.1485</td>\n",
       "      <td>146.8683</td>\n",
       "      <td>118.6127</td>\n",
       "      <td>121.4749</td>\n",
       "      <td>137.4110</td>\n",
       "      <td>123.8984</td>\n",
       "      <td>119.0080</td>\n",
       "      <td>110.9129</td>\n",
       "      <td>108.6261</td>\n",
       "      <td>114.9579</td>\n",
       "      <td>131.3961</td>\n",
       "      <td>143.1553</td>\n",
       "      <td>124.6046</td>\n",
       "      <td>119.1289</td>\n",
       "      <td>124.5161</td>\n",
       "      <td>124.1823</td>\n",
       "      <td>119.5163</td>\n",
       "      <td>127.1172</td>\n",
       "      <td>114.0531</td>\n",
       "      <td>120.1379</td>\n",
       "      <td>121.1377</td>\n",
       "      <td>118.7033</td>\n",
       "      <td>134.8957</td>\n",
       "      <td>126.6791</td>\n",
       "      <td>130.4721</td>\n",
       "      <td>120.2436</td>\n",
       "      <td>116.2238</td>\n",
       "      <td>135.4753</td>\n",
       "      <td>119.0525</td>\n",
       "      <td>146.8516</td>\n",
       "      <td>120.5690</td>\n",
       "      <td>113.9354</td>\n",
       "      <td>129.8898</td>\n",
       "      <td>122.5103</td>\n",
       "      <td>117.6289</td>\n",
       "      <td>113.9200</td>\n",
       "      <td>115.9493</td>\n",
       "      <td>116.3527</td>\n",
       "      <td>120.6736</td>\n",
       "      <td>113.6537</td>\n",
       "      <td>134.0726</td>\n",
       "      <td>150.1549</td>\n",
       "      <td>111.6192</td>\n",
       "      <td>139.0059</td>\n",
       "      <td>125.4890</td>\n",
       "      <td>145.7708</td>\n",
       "      <td>119.1710</td>\n",
       "      <td>125.0577</td>\n",
       "      <td>135.0803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20045</td>\n",
       "      <td>129.1256</td>\n",
       "      <td>135.0495</td>\n",
       "      <td>129.3313</td>\n",
       "      <td>141.4520</td>\n",
       "      <td>146.9566</td>\n",
       "      <td>159.2517</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>131.2389</td>\n",
       "      <td>125.5955</td>\n",
       "      <td>148.3171</td>\n",
       "      <td>157.9689</td>\n",
       "      <td>144.9264</td>\n",
       "      <td>144.1110</td>\n",
       "      <td>117.0766</td>\n",
       "      <td>171.4623</td>\n",
       "      <td>140.8984</td>\n",
       "      <td>116.8304</td>\n",
       "      <td>159.1907</td>\n",
       "      <td>126.0840</td>\n",
       "      <td>148.4241</td>\n",
       "      <td>146.0917</td>\n",
       "      <td>147.3873</td>\n",
       "      <td>170.6536</td>\n",
       "      <td>139.3944</td>\n",
       "      <td>163.9376</td>\n",
       "      <td>140.4694</td>\n",
       "      <td>159.7523</td>\n",
       "      <td>129.7696</td>\n",
       "      <td>115.1425</td>\n",
       "      <td>170.8185</td>\n",
       "      <td>108.3640</td>\n",
       "      <td>168.4754</td>\n",
       "      <td>150.6828</td>\n",
       "      <td>145.2205</td>\n",
       "      <td>152.4082</td>\n",
       "      <td>144.6310</td>\n",
       "      <td>136.9914</td>\n",
       "      <td>123.8105</td>\n",
       "      <td>142.2598</td>\n",
       "      <td>136.0004</td>\n",
       "      <td>126.9364</td>\n",
       "      <td>132.0378</td>\n",
       "      <td>133.6658</td>\n",
       "      <td>132.9411</td>\n",
       "      <td>128.6526</td>\n",
       "      <td>133.8551</td>\n",
       "      <td>125.0320</td>\n",
       "      <td>149.9517</td>\n",
       "      <td>137.3138</td>\n",
       "      <td>160.4400</td>\n",
       "      <td>129.3702</td>\n",
       "      <td>130.2229</td>\n",
       "      <td>136.5659</td>\n",
       "      <td>130.6829</td>\n",
       "      <td>126.4945</td>\n",
       "      <td>136.3831</td>\n",
       "      <td>130.8027</td>\n",
       "      <td>134.8438</td>\n",
       "      <td>128.1850</td>\n",
       "      <td>138.3470</td>\n",
       "      <td>131.4334</td>\n",
       "      <td>125.9621</td>\n",
       "      <td>146.9784</td>\n",
       "      <td>128.0966</td>\n",
       "      <td>128.6594</td>\n",
       "      <td>126.4036</td>\n",
       "      <td>130.5360</td>\n",
       "      <td>134.4693</td>\n",
       "      <td>130.2593</td>\n",
       "      <td>134.7523</td>\n",
       "      <td>122.7645</td>\n",
       "      <td>123.6030</td>\n",
       "      <td>136.8738</td>\n",
       "      <td>129.1278</td>\n",
       "      <td>133.2368</td>\n",
       "      <td>125.8634</td>\n",
       "      <td>130.2669</td>\n",
       "      <td>118.3637</td>\n",
       "      <td>146.4860</td>\n",
       "      <td>132.2060</td>\n",
       "      <td>133.4628</td>\n",
       "      <td>127.6557</td>\n",
       "      <td>130.7100</td>\n",
       "      <td>124.9859</td>\n",
       "      <td>130.3364</td>\n",
       "      <td>125.8346</td>\n",
       "      <td>135.9115</td>\n",
       "      <td>122.1836</td>\n",
       "      <td>125.1833</td>\n",
       "      <td>130.5341</td>\n",
       "      <td>119.7626</td>\n",
       "      <td>132.6003</td>\n",
       "      <td>126.3554</td>\n",
       "      <td>129.4091</td>\n",
       "      <td>136.1757</td>\n",
       "      <td>135.9103</td>\n",
       "      <td>146.2409</td>\n",
       "      <td>126.7153</td>\n",
       "      <td>130.9704</td>\n",
       "      <td>139.5827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20046</td>\n",
       "      <td>140.6019</td>\n",
       "      <td>122.1865</td>\n",
       "      <td>133.0878</td>\n",
       "      <td>143.0905</td>\n",
       "      <td>142.3046</td>\n",
       "      <td>158.9180</td>\n",
       "      <td>137.0935</td>\n",
       "      <td>124.9887</td>\n",
       "      <td>134.6051</td>\n",
       "      <td>137.8277</td>\n",
       "      <td>153.4532</td>\n",
       "      <td>134.1395</td>\n",
       "      <td>141.4022</td>\n",
       "      <td>117.0521</td>\n",
       "      <td>179.5318</td>\n",
       "      <td>128.8239</td>\n",
       "      <td>120.3241</td>\n",
       "      <td>141.8918</td>\n",
       "      <td>119.3901</td>\n",
       "      <td>139.8637</td>\n",
       "      <td>133.7518</td>\n",
       "      <td>127.8067</td>\n",
       "      <td>141.5816</td>\n",
       "      <td>139.7597</td>\n",
       "      <td>154.5396</td>\n",
       "      <td>134.2034</td>\n",
       "      <td>151.1898</td>\n",
       "      <td>137.1286</td>\n",
       "      <td>120.3333</td>\n",
       "      <td>165.5052</td>\n",
       "      <td>111.8281</td>\n",
       "      <td>146.3377</td>\n",
       "      <td>132.1578</td>\n",
       "      <td>145.0149</td>\n",
       "      <td>143.9410</td>\n",
       "      <td>176.6115</td>\n",
       "      <td>135.0012</td>\n",
       "      <td>126.6738</td>\n",
       "      <td>147.0107</td>\n",
       "      <td>131.0323</td>\n",
       "      <td>124.4931</td>\n",
       "      <td>131.8747</td>\n",
       "      <td>127.4627</td>\n",
       "      <td>124.6323</td>\n",
       "      <td>117.3978</td>\n",
       "      <td>124.5680</td>\n",
       "      <td>128.3347</td>\n",
       "      <td>125.3601</td>\n",
       "      <td>134.1889</td>\n",
       "      <td>151.6763</td>\n",
       "      <td>132.1660</td>\n",
       "      <td>134.7039</td>\n",
       "      <td>144.6186</td>\n",
       "      <td>140.0361</td>\n",
       "      <td>138.2352</td>\n",
       "      <td>136.8801</td>\n",
       "      <td>135.4458</td>\n",
       "      <td>133.5978</td>\n",
       "      <td>135.5044</td>\n",
       "      <td>143.8235</td>\n",
       "      <td>136.3975</td>\n",
       "      <td>142.4995</td>\n",
       "      <td>151.2028</td>\n",
       "      <td>137.3775</td>\n",
       "      <td>141.8990</td>\n",
       "      <td>139.3013</td>\n",
       "      <td>128.5965</td>\n",
       "      <td>145.5810</td>\n",
       "      <td>138.1170</td>\n",
       "      <td>141.4080</td>\n",
       "      <td>152.1455</td>\n",
       "      <td>142.7233</td>\n",
       "      <td>129.4558</td>\n",
       "      <td>131.6492</td>\n",
       "      <td>141.0321</td>\n",
       "      <td>130.8118</td>\n",
       "      <td>138.8436</td>\n",
       "      <td>134.7361</td>\n",
       "      <td>131.7339</td>\n",
       "      <td>131.6593</td>\n",
       "      <td>132.0464</td>\n",
       "      <td>137.1324</td>\n",
       "      <td>129.7760</td>\n",
       "      <td>135.3761</td>\n",
       "      <td>139.6932</td>\n",
       "      <td>140.8361</td>\n",
       "      <td>147.3288</td>\n",
       "      <td>120.9484</td>\n",
       "      <td>135.0484</td>\n",
       "      <td>132.2334</td>\n",
       "      <td>142.6214</td>\n",
       "      <td>137.0443</td>\n",
       "      <td>145.9702</td>\n",
       "      <td>138.6783</td>\n",
       "      <td>142.3589</td>\n",
       "      <td>134.4104</td>\n",
       "      <td>145.4307</td>\n",
       "      <td>141.1705</td>\n",
       "      <td>141.1012</td>\n",
       "      <td>134.8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>20047</td>\n",
       "      <td>112.7898</td>\n",
       "      <td>121.9343</td>\n",
       "      <td>133.7349</td>\n",
       "      <td>120.0080</td>\n",
       "      <td>137.7528</td>\n",
       "      <td>136.4585</td>\n",
       "      <td>152.4946</td>\n",
       "      <td>130.9987</td>\n",
       "      <td>175.7559</td>\n",
       "      <td>162.0693</td>\n",
       "      <td>166.7150</td>\n",
       "      <td>137.3730</td>\n",
       "      <td>170.0472</td>\n",
       "      <td>126.3714</td>\n",
       "      <td>126.3432</td>\n",
       "      <td>148.0920</td>\n",
       "      <td>127.9628</td>\n",
       "      <td>151.0718</td>\n",
       "      <td>139.5135</td>\n",
       "      <td>121.4570</td>\n",
       "      <td>98.5929</td>\n",
       "      <td>129.1329</td>\n",
       "      <td>121.3560</td>\n",
       "      <td>152.7853</td>\n",
       "      <td>151.9488</td>\n",
       "      <td>156.0461</td>\n",
       "      <td>179.9937</td>\n",
       "      <td>142.0007</td>\n",
       "      <td>142.7077</td>\n",
       "      <td>142.3233</td>\n",
       "      <td>127.0095</td>\n",
       "      <td>142.9073</td>\n",
       "      <td>126.8133</td>\n",
       "      <td>148.5771</td>\n",
       "      <td>148.0974</td>\n",
       "      <td>148.7994</td>\n",
       "      <td>126.9281</td>\n",
       "      <td>130.4287</td>\n",
       "      <td>126.0484</td>\n",
       "      <td>117.4828</td>\n",
       "      <td>94.4072</td>\n",
       "      <td>145.9857</td>\n",
       "      <td>149.2209</td>\n",
       "      <td>130.4001</td>\n",
       "      <td>100.3096</td>\n",
       "      <td>111.3793</td>\n",
       "      <td>149.5390</td>\n",
       "      <td>98.9186</td>\n",
       "      <td>139.7794</td>\n",
       "      <td>119.2413</td>\n",
       "      <td>120.9326</td>\n",
       "      <td>134.3420</td>\n",
       "      <td>148.7645</td>\n",
       "      <td>127.2989</td>\n",
       "      <td>124.7104</td>\n",
       "      <td>134.1805</td>\n",
       "      <td>122.8533</td>\n",
       "      <td>127.3861</td>\n",
       "      <td>113.2905</td>\n",
       "      <td>111.8986</td>\n",
       "      <td>116.0764</td>\n",
       "      <td>146.9633</td>\n",
       "      <td>200.8864</td>\n",
       "      <td>129.1961</td>\n",
       "      <td>135.0801</td>\n",
       "      <td>130.4818</td>\n",
       "      <td>135.3745</td>\n",
       "      <td>124.2038</td>\n",
       "      <td>131.7411</td>\n",
       "      <td>120.6173</td>\n",
       "      <td>124.1813</td>\n",
       "      <td>135.1484</td>\n",
       "      <td>115.6082</td>\n",
       "      <td>132.7554</td>\n",
       "      <td>135.9990</td>\n",
       "      <td>124.5474</td>\n",
       "      <td>120.8967</td>\n",
       "      <td>127.4116</td>\n",
       "      <td>129.0049</td>\n",
       "      <td>122.8388</td>\n",
       "      <td>153.5026</td>\n",
       "      <td>128.2800</td>\n",
       "      <td>120.3465</td>\n",
       "      <td>128.9377</td>\n",
       "      <td>131.4371</td>\n",
       "      <td>125.1344</td>\n",
       "      <td>125.2921</td>\n",
       "      <td>128.9779</td>\n",
       "      <td>118.8690</td>\n",
       "      <td>125.3688</td>\n",
       "      <td>125.9618</td>\n",
       "      <td>139.0647</td>\n",
       "      <td>162.8771</td>\n",
       "      <td>128.3149</td>\n",
       "      <td>150.0537</td>\n",
       "      <td>123.2969</td>\n",
       "      <td>131.1076</td>\n",
       "      <td>120.0029</td>\n",
       "      <td>131.4980</td>\n",
       "      <td>140.1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20049</td>\n",
       "      <td>148.6021</td>\n",
       "      <td>138.9062</td>\n",
       "      <td>163.7996</td>\n",
       "      <td>126.6685</td>\n",
       "      <td>156.4230</td>\n",
       "      <td>156.7728</td>\n",
       "      <td>147.2529</td>\n",
       "      <td>150.2838</td>\n",
       "      <td>203.6771</td>\n",
       "      <td>195.2277</td>\n",
       "      <td>194.2095</td>\n",
       "      <td>170.0946</td>\n",
       "      <td>207.3161</td>\n",
       "      <td>135.0765</td>\n",
       "      <td>195.1236</td>\n",
       "      <td>165.0368</td>\n",
       "      <td>137.3116</td>\n",
       "      <td>159.4769</td>\n",
       "      <td>141.8843</td>\n",
       "      <td>156.7186</td>\n",
       "      <td>154.1821</td>\n",
       "      <td>126.9049</td>\n",
       "      <td>166.0358</td>\n",
       "      <td>194.7786</td>\n",
       "      <td>194.0369</td>\n",
       "      <td>187.4241</td>\n",
       "      <td>193.2978</td>\n",
       "      <td>167.8842</td>\n",
       "      <td>163.3118</td>\n",
       "      <td>185.4265</td>\n",
       "      <td>125.0710</td>\n",
       "      <td>148.3568</td>\n",
       "      <td>138.7908</td>\n",
       "      <td>176.0684</td>\n",
       "      <td>163.5617</td>\n",
       "      <td>218.0404</td>\n",
       "      <td>158.0214</td>\n",
       "      <td>136.3463</td>\n",
       "      <td>152.3702</td>\n",
       "      <td>143.8754</td>\n",
       "      <td>151.5008</td>\n",
       "      <td>167.1265</td>\n",
       "      <td>186.0557</td>\n",
       "      <td>155.6716</td>\n",
       "      <td>140.0977</td>\n",
       "      <td>132.5957</td>\n",
       "      <td>167.5376</td>\n",
       "      <td>120.8689</td>\n",
       "      <td>151.3033</td>\n",
       "      <td>136.7757</td>\n",
       "      <td>133.1184</td>\n",
       "      <td>131.5002</td>\n",
       "      <td>149.9309</td>\n",
       "      <td>137.9380</td>\n",
       "      <td>127.4936</td>\n",
       "      <td>127.0461</td>\n",
       "      <td>135.0284</td>\n",
       "      <td>135.6991</td>\n",
       "      <td>123.4610</td>\n",
       "      <td>137.2097</td>\n",
       "      <td>128.2162</td>\n",
       "      <td>140.3445</td>\n",
       "      <td>129.1596</td>\n",
       "      <td>141.9558</td>\n",
       "      <td>136.9463</td>\n",
       "      <td>133.5013</td>\n",
       "      <td>136.1952</td>\n",
       "      <td>126.6861</td>\n",
       "      <td>132.3800</td>\n",
       "      <td>133.3875</td>\n",
       "      <td>131.9747</td>\n",
       "      <td>136.3842</td>\n",
       "      <td>137.9813</td>\n",
       "      <td>134.3898</td>\n",
       "      <td>137.1427</td>\n",
       "      <td>127.4298</td>\n",
       "      <td>138.9114</td>\n",
       "      <td>128.2748</td>\n",
       "      <td>120.6962</td>\n",
       "      <td>131.0415</td>\n",
       "      <td>147.5064</td>\n",
       "      <td>129.6064</td>\n",
       "      <td>127.4917</td>\n",
       "      <td>137.1912</td>\n",
       "      <td>141.3265</td>\n",
       "      <td>130.5686</td>\n",
       "      <td>134.2588</td>\n",
       "      <td>130.0772</td>\n",
       "      <td>126.4841</td>\n",
       "      <td>128.7874</td>\n",
       "      <td>126.7088</td>\n",
       "      <td>137.6698</td>\n",
       "      <td>137.4613</td>\n",
       "      <td>141.4993</td>\n",
       "      <td>120.6018</td>\n",
       "      <td>129.0737</td>\n",
       "      <td>132.1620</td>\n",
       "      <td>125.9416</td>\n",
       "      <td>132.2236</td>\n",
       "      <td>142.7874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>20050</td>\n",
       "      <td>113.8600</td>\n",
       "      <td>114.5453</td>\n",
       "      <td>114.8321</td>\n",
       "      <td>126.1148</td>\n",
       "      <td>136.8878</td>\n",
       "      <td>137.0167</td>\n",
       "      <td>126.0218</td>\n",
       "      <td>116.0356</td>\n",
       "      <td>118.4541</td>\n",
       "      <td>131.7124</td>\n",
       "      <td>134.5607</td>\n",
       "      <td>138.6863</td>\n",
       "      <td>127.2810</td>\n",
       "      <td>117.9250</td>\n",
       "      <td>164.2532</td>\n",
       "      <td>113.8463</td>\n",
       "      <td>106.1581</td>\n",
       "      <td>130.3423</td>\n",
       "      <td>112.1559</td>\n",
       "      <td>125.0521</td>\n",
       "      <td>117.7492</td>\n",
       "      <td>122.6452</td>\n",
       "      <td>132.2803</td>\n",
       "      <td>121.1449</td>\n",
       "      <td>153.1223</td>\n",
       "      <td>124.9410</td>\n",
       "      <td>144.5731</td>\n",
       "      <td>115.5526</td>\n",
       "      <td>108.8730</td>\n",
       "      <td>157.1893</td>\n",
       "      <td>97.1985</td>\n",
       "      <td>144.4508</td>\n",
       "      <td>122.8509</td>\n",
       "      <td>138.8349</td>\n",
       "      <td>128.4884</td>\n",
       "      <td>131.0239</td>\n",
       "      <td>106.2786</td>\n",
       "      <td>109.5915</td>\n",
       "      <td>134.3059</td>\n",
       "      <td>121.3317</td>\n",
       "      <td>110.3187</td>\n",
       "      <td>125.8736</td>\n",
       "      <td>120.7810</td>\n",
       "      <td>118.9220</td>\n",
       "      <td>98.3672</td>\n",
       "      <td>110.6573</td>\n",
       "      <td>116.1768</td>\n",
       "      <td>142.2232</td>\n",
       "      <td>122.3848</td>\n",
       "      <td>132.6603</td>\n",
       "      <td>113.7623</td>\n",
       "      <td>117.7935</td>\n",
       "      <td>113.7981</td>\n",
       "      <td>117.7125</td>\n",
       "      <td>116.1170</td>\n",
       "      <td>114.9405</td>\n",
       "      <td>122.2972</td>\n",
       "      <td>121.0639</td>\n",
       "      <td>111.1100</td>\n",
       "      <td>121.4339</td>\n",
       "      <td>115.0641</td>\n",
       "      <td>120.0667</td>\n",
       "      <td>104.2197</td>\n",
       "      <td>120.8996</td>\n",
       "      <td>109.9421</td>\n",
       "      <td>117.3860</td>\n",
       "      <td>116.6725</td>\n",
       "      <td>115.6952</td>\n",
       "      <td>126.4047</td>\n",
       "      <td>124.7182</td>\n",
       "      <td>118.5419</td>\n",
       "      <td>124.9579</td>\n",
       "      <td>108.0849</td>\n",
       "      <td>120.1405</td>\n",
       "      <td>120.5280</td>\n",
       "      <td>117.7205</td>\n",
       "      <td>115.5378</td>\n",
       "      <td>120.7477</td>\n",
       "      <td>104.8067</td>\n",
       "      <td>111.9014</td>\n",
       "      <td>109.9204</td>\n",
       "      <td>118.0198</td>\n",
       "      <td>123.6302</td>\n",
       "      <td>125.8641</td>\n",
       "      <td>124.6199</td>\n",
       "      <td>114.5497</td>\n",
       "      <td>118.9968</td>\n",
       "      <td>119.0111</td>\n",
       "      <td>107.7939</td>\n",
       "      <td>115.3392</td>\n",
       "      <td>116.5024</td>\n",
       "      <td>118.8374</td>\n",
       "      <td>114.9288</td>\n",
       "      <td>110.9291</td>\n",
       "      <td>116.7473</td>\n",
       "      <td>120.8362</td>\n",
       "      <td>103.2058</td>\n",
       "      <td>120.4437</td>\n",
       "      <td>111.4972</td>\n",
       "      <td>112.7813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20051</td>\n",
       "      <td>111.3173</td>\n",
       "      <td>104.7312</td>\n",
       "      <td>112.3026</td>\n",
       "      <td>131.3102</td>\n",
       "      <td>119.5660</td>\n",
       "      <td>141.9819</td>\n",
       "      <td>109.3200</td>\n",
       "      <td>112.7876</td>\n",
       "      <td>112.2235</td>\n",
       "      <td>124.4828</td>\n",
       "      <td>120.2231</td>\n",
       "      <td>127.5690</td>\n",
       "      <td>132.1976</td>\n",
       "      <td>103.5156</td>\n",
       "      <td>150.7373</td>\n",
       "      <td>102.9313</td>\n",
       "      <td>98.1930</td>\n",
       "      <td>123.7323</td>\n",
       "      <td>101.8809</td>\n",
       "      <td>121.3352</td>\n",
       "      <td>113.8900</td>\n",
       "      <td>104.7394</td>\n",
       "      <td>114.8879</td>\n",
       "      <td>111.3758</td>\n",
       "      <td>139.2241</td>\n",
       "      <td>117.0933</td>\n",
       "      <td>139.7410</td>\n",
       "      <td>111.1499</td>\n",
       "      <td>99.7127</td>\n",
       "      <td>146.0055</td>\n",
       "      <td>96.2754</td>\n",
       "      <td>129.9380</td>\n",
       "      <td>105.4138</td>\n",
       "      <td>115.8447</td>\n",
       "      <td>114.6335</td>\n",
       "      <td>133.1924</td>\n",
       "      <td>113.7566</td>\n",
       "      <td>98.7388</td>\n",
       "      <td>114.4755</td>\n",
       "      <td>118.6069</td>\n",
       "      <td>111.1626</td>\n",
       "      <td>118.4762</td>\n",
       "      <td>112.5369</td>\n",
       "      <td>107.5769</td>\n",
       "      <td>101.9296</td>\n",
       "      <td>109.0567</td>\n",
       "      <td>112.6448</td>\n",
       "      <td>112.7196</td>\n",
       "      <td>117.1394</td>\n",
       "      <td>131.4615</td>\n",
       "      <td>95.3864</td>\n",
       "      <td>96.1288</td>\n",
       "      <td>99.8989</td>\n",
       "      <td>102.8198</td>\n",
       "      <td>99.3878</td>\n",
       "      <td>93.0772</td>\n",
       "      <td>93.8883</td>\n",
       "      <td>99.0342</td>\n",
       "      <td>98.6369</td>\n",
       "      <td>99.6910</td>\n",
       "      <td>99.8480</td>\n",
       "      <td>100.9643</td>\n",
       "      <td>94.5917</td>\n",
       "      <td>95.6428</td>\n",
       "      <td>108.8299</td>\n",
       "      <td>102.6612</td>\n",
       "      <td>100.9267</td>\n",
       "      <td>103.8556</td>\n",
       "      <td>101.5239</td>\n",
       "      <td>104.0038</td>\n",
       "      <td>96.0305</td>\n",
       "      <td>97.3830</td>\n",
       "      <td>105.4166</td>\n",
       "      <td>95.8392</td>\n",
       "      <td>104.6544</td>\n",
       "      <td>98.0625</td>\n",
       "      <td>97.5718</td>\n",
       "      <td>104.2765</td>\n",
       "      <td>89.7319</td>\n",
       "      <td>107.0417</td>\n",
       "      <td>99.7224</td>\n",
       "      <td>98.9931</td>\n",
       "      <td>99.0742</td>\n",
       "      <td>101.6804</td>\n",
       "      <td>107.3381</td>\n",
       "      <td>98.3022</td>\n",
       "      <td>103.6784</td>\n",
       "      <td>105.7010</td>\n",
       "      <td>102.3312</td>\n",
       "      <td>97.0157</td>\n",
       "      <td>98.2122</td>\n",
       "      <td>97.0613</td>\n",
       "      <td>100.8966</td>\n",
       "      <td>99.1693</td>\n",
       "      <td>99.1236</td>\n",
       "      <td>93.1812</td>\n",
       "      <td>103.9998</td>\n",
       "      <td>94.8536</td>\n",
       "      <td>99.7219</td>\n",
       "      <td>107.9916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>20052</td>\n",
       "      <td>85.9291</td>\n",
       "      <td>96.0485</td>\n",
       "      <td>85.8676</td>\n",
       "      <td>108.2249</td>\n",
       "      <td>109.0343</td>\n",
       "      <td>113.9272</td>\n",
       "      <td>89.5873</td>\n",
       "      <td>96.8845</td>\n",
       "      <td>100.2198</td>\n",
       "      <td>98.9782</td>\n",
       "      <td>110.0235</td>\n",
       "      <td>100.6163</td>\n",
       "      <td>100.1973</td>\n",
       "      <td>90.8790</td>\n",
       "      <td>109.9354</td>\n",
       "      <td>105.5913</td>\n",
       "      <td>91.4618</td>\n",
       "      <td>106.1995</td>\n",
       "      <td>89.2990</td>\n",
       "      <td>96.6372</td>\n",
       "      <td>102.3063</td>\n",
       "      <td>96.7777</td>\n",
       "      <td>93.3656</td>\n",
       "      <td>101.9660</td>\n",
       "      <td>111.0253</td>\n",
       "      <td>103.3844</td>\n",
       "      <td>116.5651</td>\n",
       "      <td>94.2756</td>\n",
       "      <td>95.9962</td>\n",
       "      <td>107.7841</td>\n",
       "      <td>82.5281</td>\n",
       "      <td>112.3487</td>\n",
       "      <td>88.9347</td>\n",
       "      <td>92.9191</td>\n",
       "      <td>94.1223</td>\n",
       "      <td>112.4085</td>\n",
       "      <td>88.9387</td>\n",
       "      <td>88.6452</td>\n",
       "      <td>101.6811</td>\n",
       "      <td>98.2502</td>\n",
       "      <td>106.3692</td>\n",
       "      <td>105.9878</td>\n",
       "      <td>97.6585</td>\n",
       "      <td>99.2336</td>\n",
       "      <td>88.2308</td>\n",
       "      <td>91.0443</td>\n",
       "      <td>95.5763</td>\n",
       "      <td>96.3576</td>\n",
       "      <td>95.6028</td>\n",
       "      <td>103.7635</td>\n",
       "      <td>90.6227</td>\n",
       "      <td>88.3538</td>\n",
       "      <td>94.2221</td>\n",
       "      <td>94.0022</td>\n",
       "      <td>90.9735</td>\n",
       "      <td>86.8635</td>\n",
       "      <td>89.6627</td>\n",
       "      <td>96.5417</td>\n",
       "      <td>90.0844</td>\n",
       "      <td>103.0053</td>\n",
       "      <td>91.6166</td>\n",
       "      <td>104.3403</td>\n",
       "      <td>122.1363</td>\n",
       "      <td>97.8562</td>\n",
       "      <td>103.1013</td>\n",
       "      <td>94.6584</td>\n",
       "      <td>100.0860</td>\n",
       "      <td>97.3751</td>\n",
       "      <td>99.2316</td>\n",
       "      <td>96.7973</td>\n",
       "      <td>98.3368</td>\n",
       "      <td>99.2206</td>\n",
       "      <td>96.7877</td>\n",
       "      <td>98.8623</td>\n",
       "      <td>103.8621</td>\n",
       "      <td>96.1823</td>\n",
       "      <td>100.8226</td>\n",
       "      <td>87.6799</td>\n",
       "      <td>98.9420</td>\n",
       "      <td>101.0845</td>\n",
       "      <td>100.5893</td>\n",
       "      <td>95.3727</td>\n",
       "      <td>101.6741</td>\n",
       "      <td>92.8322</td>\n",
       "      <td>106.3499</td>\n",
       "      <td>93.1652</td>\n",
       "      <td>92.6238</td>\n",
       "      <td>102.7070</td>\n",
       "      <td>93.2022</td>\n",
       "      <td>88.7350</td>\n",
       "      <td>100.1725</td>\n",
       "      <td>93.9086</td>\n",
       "      <td>96.1411</td>\n",
       "      <td>92.0707</td>\n",
       "      <td>91.7463</td>\n",
       "      <td>100.2943</td>\n",
       "      <td>103.1747</td>\n",
       "      <td>93.9691</td>\n",
       "      <td>91.2034</td>\n",
       "      <td>99.1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20053</td>\n",
       "      <td>132.0416</td>\n",
       "      <td>132.7882</td>\n",
       "      <td>140.8626</td>\n",
       "      <td>135.3187</td>\n",
       "      <td>135.0616</td>\n",
       "      <td>160.6608</td>\n",
       "      <td>127.9117</td>\n",
       "      <td>128.2995</td>\n",
       "      <td>129.9756</td>\n",
       "      <td>140.8309</td>\n",
       "      <td>144.4483</td>\n",
       "      <td>131.3959</td>\n",
       "      <td>153.9606</td>\n",
       "      <td>115.6361</td>\n",
       "      <td>174.7380</td>\n",
       "      <td>138.5215</td>\n",
       "      <td>111.4641</td>\n",
       "      <td>136.1335</td>\n",
       "      <td>118.6320</td>\n",
       "      <td>134.4414</td>\n",
       "      <td>126.7995</td>\n",
       "      <td>129.0107</td>\n",
       "      <td>137.7684</td>\n",
       "      <td>142.7227</td>\n",
       "      <td>145.9864</td>\n",
       "      <td>128.3855</td>\n",
       "      <td>152.0441</td>\n",
       "      <td>121.6167</td>\n",
       "      <td>108.5741</td>\n",
       "      <td>161.4697</td>\n",
       "      <td>113.2184</td>\n",
       "      <td>156.2361</td>\n",
       "      <td>138.7272</td>\n",
       "      <td>132.2482</td>\n",
       "      <td>135.8226</td>\n",
       "      <td>157.4240</td>\n",
       "      <td>130.5868</td>\n",
       "      <td>114.9628</td>\n",
       "      <td>132.9163</td>\n",
       "      <td>131.0001</td>\n",
       "      <td>113.0482</td>\n",
       "      <td>124.2073</td>\n",
       "      <td>124.7968</td>\n",
       "      <td>113.6061</td>\n",
       "      <td>114.4457</td>\n",
       "      <td>120.0572</td>\n",
       "      <td>121.5379</td>\n",
       "      <td>135.9194</td>\n",
       "      <td>130.4676</td>\n",
       "      <td>151.9247</td>\n",
       "      <td>131.6374</td>\n",
       "      <td>123.4246</td>\n",
       "      <td>121.3720</td>\n",
       "      <td>124.1064</td>\n",
       "      <td>126.8826</td>\n",
       "      <td>141.1670</td>\n",
       "      <td>125.7933</td>\n",
       "      <td>121.0401</td>\n",
       "      <td>126.0408</td>\n",
       "      <td>133.8082</td>\n",
       "      <td>128.2991</td>\n",
       "      <td>123.1400</td>\n",
       "      <td>130.5391</td>\n",
       "      <td>139.4070</td>\n",
       "      <td>133.2731</td>\n",
       "      <td>134.3072</td>\n",
       "      <td>125.2901</td>\n",
       "      <td>142.2289</td>\n",
       "      <td>131.7277</td>\n",
       "      <td>130.9304</td>\n",
       "      <td>122.6867</td>\n",
       "      <td>128.4353</td>\n",
       "      <td>115.0190</td>\n",
       "      <td>124.1746</td>\n",
       "      <td>136.2718</td>\n",
       "      <td>130.6638</td>\n",
       "      <td>124.0992</td>\n",
       "      <td>129.5070</td>\n",
       "      <td>128.3534</td>\n",
       "      <td>127.4235</td>\n",
       "      <td>119.5607</td>\n",
       "      <td>128.0194</td>\n",
       "      <td>135.4098</td>\n",
       "      <td>126.9290</td>\n",
       "      <td>126.8239</td>\n",
       "      <td>121.3445</td>\n",
       "      <td>123.9018</td>\n",
       "      <td>130.0178</td>\n",
       "      <td>120.1516</td>\n",
       "      <td>130.4037</td>\n",
       "      <td>128.0436</td>\n",
       "      <td>130.1602</td>\n",
       "      <td>122.3631</td>\n",
       "      <td>131.9648</td>\n",
       "      <td>138.1054</td>\n",
       "      <td>127.5019</td>\n",
       "      <td>127.0401</td>\n",
       "      <td>130.4993</td>\n",
       "      <td>125.2894</td>\n",
       "      <td>131.0698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>20054</td>\n",
       "      <td>117.9040</td>\n",
       "      <td>123.2108</td>\n",
       "      <td>120.7178</td>\n",
       "      <td>130.6527</td>\n",
       "      <td>133.3665</td>\n",
       "      <td>147.0044</td>\n",
       "      <td>131.9242</td>\n",
       "      <td>122.9216</td>\n",
       "      <td>135.3021</td>\n",
       "      <td>121.3544</td>\n",
       "      <td>146.1642</td>\n",
       "      <td>129.0535</td>\n",
       "      <td>133.1986</td>\n",
       "      <td>115.7139</td>\n",
       "      <td>158.9760</td>\n",
       "      <td>115.6026</td>\n",
       "      <td>118.5695</td>\n",
       "      <td>134.8093</td>\n",
       "      <td>116.6896</td>\n",
       "      <td>118.7757</td>\n",
       "      <td>137.9322</td>\n",
       "      <td>123.8890</td>\n",
       "      <td>128.3514</td>\n",
       "      <td>130.3001</td>\n",
       "      <td>143.4286</td>\n",
       "      <td>127.3174</td>\n",
       "      <td>144.2410</td>\n",
       "      <td>124.3654</td>\n",
       "      <td>115.8465</td>\n",
       "      <td>150.7732</td>\n",
       "      <td>109.5424</td>\n",
       "      <td>151.4449</td>\n",
       "      <td>118.3913</td>\n",
       "      <td>120.9799</td>\n",
       "      <td>135.1488</td>\n",
       "      <td>158.8624</td>\n",
       "      <td>125.4758</td>\n",
       "      <td>114.8463</td>\n",
       "      <td>137.5673</td>\n",
       "      <td>127.0795</td>\n",
       "      <td>107.6005</td>\n",
       "      <td>125.9004</td>\n",
       "      <td>120.5937</td>\n",
       "      <td>120.2987</td>\n",
       "      <td>110.6240</td>\n",
       "      <td>117.5954</td>\n",
       "      <td>120.9809</td>\n",
       "      <td>130.6761</td>\n",
       "      <td>132.6544</td>\n",
       "      <td>152.2041</td>\n",
       "      <td>114.2780</td>\n",
       "      <td>111.6657</td>\n",
       "      <td>118.2952</td>\n",
       "      <td>122.0377</td>\n",
       "      <td>114.6504</td>\n",
       "      <td>115.1568</td>\n",
       "      <td>115.9043</td>\n",
       "      <td>121.8359</td>\n",
       "      <td>120.7450</td>\n",
       "      <td>131.6093</td>\n",
       "      <td>115.4693</td>\n",
       "      <td>125.9198</td>\n",
       "      <td>117.1168</td>\n",
       "      <td>123.6936</td>\n",
       "      <td>125.1949</td>\n",
       "      <td>118.6674</td>\n",
       "      <td>110.2873</td>\n",
       "      <td>114.6969</td>\n",
       "      <td>121.9140</td>\n",
       "      <td>120.1840</td>\n",
       "      <td>125.9304</td>\n",
       "      <td>114.2688</td>\n",
       "      <td>114.3269</td>\n",
       "      <td>120.6738</td>\n",
       "      <td>117.6424</td>\n",
       "      <td>121.9007</td>\n",
       "      <td>115.0596</td>\n",
       "      <td>127.5703</td>\n",
       "      <td>101.6153</td>\n",
       "      <td>122.4371</td>\n",
       "      <td>113.6709</td>\n",
       "      <td>117.1318</td>\n",
       "      <td>115.9102</td>\n",
       "      <td>120.7083</td>\n",
       "      <td>124.7338</td>\n",
       "      <td>114.8774</td>\n",
       "      <td>117.6280</td>\n",
       "      <td>123.1530</td>\n",
       "      <td>112.3609</td>\n",
       "      <td>112.7786</td>\n",
       "      <td>115.7527</td>\n",
       "      <td>125.7410</td>\n",
       "      <td>126.5696</td>\n",
       "      <td>121.2177</td>\n",
       "      <td>117.2584</td>\n",
       "      <td>126.5743</td>\n",
       "      <td>113.3522</td>\n",
       "      <td>112.4231</td>\n",
       "      <td>124.7077</td>\n",
       "      <td>109.9032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PRODUCT_ID  mlp_pred_ORIG_1  mlp_pred_ORIG_2  mlp_pred_ORIG_3  mlp_pred_ORIG_4  mlp_pred_ORIG_5  mlp_pred_ORIG_6  mlp_pred_ORIG_7  mlp_pred_ORIG_8  mlp_pred_ORIG_9  mlp_pred_ORIG_10  mlp_pred_ORIG_11  mlp_pred_ORIG_12  mlp_pred_ORIG_13  mlp_pred_ORIG_14  mlp_pred_ORIG_15  mlp_pred_ORIG_16  mlp_pred_ORIG_17  mlp_pred_ORIG_18  mlp_pred_ORIG_19  mlp_pred_ORIG_20  mlp_pred_ORIG_21  mlp_pred_ORIG_22  mlp_pred_ORIG_23  mlp_pred_ORIG_24  mlp_pred_ORIG_25  mlp_pred_ORIG_26  mlp_pred_ORIG_27  mlp_pred_ORIG_28  mlp_pred_ORIG_29  mlp_pred_ORIG_30  mlp_pred_ORIG_31  mlp_pred_ORIG_32  mlp_pred_ORIG_33  mlp_pred_ORIG_34  mlp_pred_ORIG_35  mlp_pred_ORIG_36  mlp_pred_ORIG_37  mlp_pred_ORIG_38  mlp_pred_ORIG_39  mlp_pred_ORIG_40  mlp_pred_ORIG_41  mlp_pred_ORIG_42  mlp_pred_ORIG_43  mlp_pred_ORIG_44  mlp_pred_ORIG_45  mlp_pred_ORIG_46  mlp_pred_ORIG_47  mlp_pred_ORIG_48  mlp_pred_ORIG_49  mlp_pred_ORIG_50  lgbm_pred_ORIG_1  lgbm_pred_ORIG_2  lgbm_pred_ORIG_3  lgbm_pred_ORIG_4  lgbm_pred_ORIG_5  lgbm_pred_ORIG_6  lgbm_pred_ORIG_7  lgbm_pred_ORIG_8  lgbm_pred_ORIG_9  lgbm_pred_ORIG_10  lgbm_pred_ORIG_11  lgbm_pred_ORIG_12  lgbm_pred_ORIG_13  lgbm_pred_ORIG_14  lgbm_pred_ORIG_15  lgbm_pred_ORIG_16  lgbm_pred_ORIG_17  lgbm_pred_ORIG_18  lgbm_pred_ORIG_19  lgbm_pred_ORIG_20  lgbm_pred_ORIG_21  lgbm_pred_ORIG_22  lgbm_pred_ORIG_23  lgbm_pred_ORIG_24  lgbm_pred_ORIG_25  lgbm_pred_ORIG_26  lgbm_pred_ORIG_27  lgbm_pred_ORIG_28  lgbm_pred_ORIG_29  lgbm_pred_ORIG_30  lgbm_pred_ORIG_31  lgbm_pred_ORIG_32  lgbm_pred_ORIG_33  lgbm_pred_ORIG_34  lgbm_pred_ORIG_35  lgbm_pred_ORIG_36  lgbm_pred_ORIG_37  lgbm_pred_ORIG_38  lgbm_pred_ORIG_39  lgbm_pred_ORIG_40  lgbm_pred_ORIG_41  lgbm_pred_ORIG_42  lgbm_pred_ORIG_43  lgbm_pred_ORIG_44  lgbm_pred_ORIG_45  lgbm_pred_ORIG_46  lgbm_pred_ORIG_47  lgbm_pred_ORIG_48  lgbm_pred_ORIG_49  lgbm_pred_ORIG_50\n",
       "0        20001        2499.2090        5263.2959         804.5912         727.6192        1559.3298        2911.9277        7580.3813         932.1174        2122.7937         2514.7844         4620.2842         2722.4951         4365.8901         1492.4357         4894.0991         1080.9789          728.9855         3817.6021         2046.8887         1205.3357         1795.4785          917.8847         2162.9702         2008.6067         3880.3560         7916.3921         2108.4348        14830.9072         3071.4585         3162.0374         4540.4404         4199.3940          627.3073         3711.7502        10808.7129         4010.7842         2787.4878         1962.2367         3214.1221         3062.6428         3114.7310         1005.9449         4292.7256         2789.5337          227.9098         1334.6522         1259.5654         1528.6150         2745.5435         1266.5941          935.1566          924.6894          902.1176          848.3029          847.1355         1157.0333          886.3787          840.6570          907.4833           997.5659           939.1530          1088.7173          1124.7459           960.0096           952.6442           912.2195          1110.8863           869.8642          1243.7613           897.8728           896.6645           954.6576           885.1398           913.2840           889.9761           890.7729           996.9416           879.9595          1193.1274          1006.1139           995.2010           881.5662           982.9409           833.5339           913.6344           822.5711           847.4863          1046.9595           949.3221           906.5048           845.7527           923.6794          1223.6973           906.0816          1274.6605          1021.0181          1118.0922           866.9102           870.2219          1073.2910\n",
       "1        20002         473.9576        2823.5747         600.2195         562.8087         814.6390         766.4780        1546.3629         697.9310        1351.1823          861.1847         1697.4403          871.4789          833.1289          986.6110         2509.4326          485.7554          377.7762         1358.7043          708.0589          702.3293          704.1918          505.4628         1177.1813         1257.8733         1018.1789         2839.2078          907.6165         1758.9507         1266.1028         1861.8225         1645.7792         1898.1858          374.1505         1872.4506         2554.7678         2109.0835         1151.9299         1068.7759         1398.7549          735.1407          438.5002          396.7874         1140.4252         1037.7700          721.7929          469.1016          733.5745          606.8829         1220.3689          514.1923          981.6201          939.5924          925.6344          905.8875          857.1993         1066.8393          858.0403          862.9338          933.1881           996.1841           986.4728           987.0411           990.3274           976.5534           974.2095           905.7628          1023.4710           897.3699           981.4370           918.8154           909.6917           950.0992           853.5017           940.0597           884.6668           912.2222           967.2535           929.4169          1016.0062          1004.1714           931.8859           900.4557           908.7761           864.7181           918.5266           830.9508           943.2141           990.5508           966.8951           951.4487           890.2561           935.1005          1031.1569           977.4998          1042.5057          1091.4635          1072.1501           882.6033           903.1474          1013.8543\n",
       "2        20003         883.6436        1229.2775         341.9880         209.2350         570.0003         709.4800        1465.1033         475.2508         871.2306          771.3541         1210.7092          844.2320         1276.0896          455.5036         1272.2114          446.6269          334.5750         1082.2994          741.9630          457.5691          783.3557          551.6803          694.3302          812.8923          917.5224         1999.0488          685.9417         1526.1064          879.1579          939.0835          786.8970         1354.7971          190.1784         1385.3892         1266.3289          974.0811          875.1936          683.4855         1270.4812          581.8795          615.1618          380.8811          654.5162          726.2853          215.2493          453.0035          521.6562          361.4240          925.2378          486.3553          708.5550          722.1337          674.7821          724.4121          709.4367          692.6146          684.6285          657.6063          706.7446           637.2066           705.5091           736.1898           746.9853           700.4674           711.8722           720.9611           729.5378           696.0280           724.3710           701.2526           716.6207           685.6679           675.9561           710.5982           666.4150           702.5856           739.6281           706.2956           811.9448           655.1595           711.2714           692.4726           682.4361           702.6108           745.0326           693.8825           707.0578           633.3803           705.9305           699.2798           687.3723           691.9151           692.6686           728.2671           800.5936           706.6698           666.8135           663.5753           697.2887           770.3446\n",
       "3        20004         278.6172         553.0844         234.8324         168.0436         327.3300         299.1143         578.9184         247.4313         458.3942          403.2302          510.4264          373.7491          395.3320          238.5614          652.7551          211.9489          193.3931          393.5305          360.6262          254.4985          268.7941          268.4264          382.7681          442.2457          432.5879          784.3494          415.4915          459.0695          418.1459          438.2862          333.4021          481.8074          130.5738          504.7629          583.4701          593.1573          393.1740          351.0249          522.1809          213.8898          226.8187          211.4141          351.3539          363.8842          219.0842          218.6337          279.0984          174.0665          399.5264          230.4446          535.1559          559.8694          487.0127          480.5612          510.9151          446.1754          518.0812          515.1366          510.9783           499.3051           484.9185           512.9588           515.7613           516.3664           501.8349           521.4044           496.8330           514.3620           512.5281           523.5468           530.0845           523.0726           502.9984           507.1017           505.5286           536.1644           531.2275           520.1293           452.3890           496.4564           481.8401           513.1341           503.8631           508.4100           500.0813           547.9576           518.5657           498.6645           504.4555           485.1947           502.6226           504.4424           518.0172           525.8581           424.3959           518.2056           514.8020           506.9468           486.6326           472.6493\n",
       "4        20005         429.0883         734.8737         311.1428         168.2841         453.6467         397.8544         835.8878         332.2200         607.6149          551.0172          783.7721          482.9267          459.3329          344.3236          837.6432          236.5896          237.6687          517.5449          451.0930          364.5672          295.2185          421.9180          478.5123          497.4446          658.9768         1209.5479          543.8024          561.8757          674.3246          540.5536          402.4775          668.4355          174.1039          679.2991          872.0078          937.3920          551.2717          471.9512          673.8182          229.7278          289.0652          284.1077          480.9610          471.0372          209.6224          287.6818          363.4773          226.2050          443.1060          291.9259          544.1441          583.5600          568.0413          527.3896          559.0232          546.1975          569.9907          542.7089          542.6554           552.5827           527.6042           572.3730           522.8171           573.9738           556.4121           547.5127           594.7303           533.1944           599.2221           544.0785           557.6838           552.0713           498.2050           567.1273           519.3687           558.8593           534.6386           599.9881           535.1184           533.3071           559.5628           538.4627           512.5910           561.0331           547.2715           568.3845           528.5830           571.7185           527.1043           528.0528           516.6702           562.3251           566.8520           523.2167           530.9225           563.7212           562.6040           551.5517           569.3718           534.2661\n",
       "5        20006         280.8558         297.0925         200.0112         191.8327         316.7312         341.5848         492.2798         274.4301         377.6528          421.6972          413.1416          324.3731          518.1161          217.6581          437.6769          232.0661          191.8179          348.1488          317.8459          232.0272          348.0276          263.6566          294.2041          345.0845          363.8317          487.6766          340.4414          385.0577          381.0684          350.4612          316.8833          362.4133          256.2034          473.7647          508.9326          530.5219          426.1308          272.1187          362.3022          240.4158          218.0258          204.0284          347.8195          299.4139          141.3815          249.3513          259.3560          205.5951          325.8334          241.7221          404.5213          395.3825          430.8314          400.1686          393.8954          393.7507          408.0349          404.2596          410.8109           409.8276           393.6516           390.2073           390.0506           397.2758           404.0817           418.2886           371.7756           415.7784           364.1279           403.2674           410.7887           402.7183           406.2387           428.8983           394.8739           411.9006           418.5347           401.4928           404.0425           380.6641           393.6092           404.1576           405.0446           425.0734           399.2897           414.6534           415.5160           409.0630           399.4240           403.2469           409.8919           408.7155           366.1578           417.8358           425.4475           363.5404           400.7977           396.5690           417.7821           387.5821\n",
       "6        20007         296.2708         333.1086         230.3921         172.9154         289.8467         319.6021         433.3800         305.9345         354.4533          442.3618          450.8955          354.1555          478.4842          223.8989          461.2814          265.0870          199.1672          387.2059          291.1370          260.3091          316.5783          292.1387          309.7810          404.1246          375.9898          509.4577          335.1607          368.6393          379.1519          365.4007          295.3626          344.8303          321.2924          416.0146          483.5619          468.4059          414.9564          257.1808          325.3233          243.1497          251.2310          202.3355          340.2116          296.1609          172.2334          224.0494          274.8239          207.3195          315.5432          251.8892          411.6529          397.2526          422.2391          381.1503          368.1242          357.9685          386.1497          388.8716          377.5113           343.4933           389.1583           376.3100           390.0131           369.5434           378.9110           404.0249           391.5741           375.5589           372.1809           386.7686           382.5604           388.7550           382.6798           391.2735           380.5871           397.4668           395.5004           384.0225           345.5175           371.0485           397.3732           377.7882           386.8561           402.1145           377.3998           377.5702           384.2676           385.7685           371.2544           387.6094           365.7843           387.6839           378.8600           382.0768           397.7851           400.8314           422.2712           371.9059           369.4626           396.5289\n",
       "7        20008         198.6281         284.4379         333.9463         224.3885         339.9262         307.8564         266.9429         354.9914         412.6861          364.4836          528.4730          296.9058          343.0237          280.4902          580.3459          458.2091          168.2576          433.6935          233.6079          241.0914          241.0656          298.0276          323.0456          344.4931          295.6278          559.3028          397.6200          324.9277          380.9037          559.4137          256.2604          300.1310          234.6953          355.8054          350.9953          471.3720          304.9469          268.8729          332.9715          160.3580          150.2656          198.5053          339.4487          328.4052          122.8347          141.4607          252.6100          211.9576          269.3908          234.1145          302.2056          297.4131          321.0250          294.3056          287.8162          333.3961          289.5512          292.8300          283.7474           267.5091           308.0737           323.0754           309.6527           309.2946           301.6813           301.3387           303.3863           276.7282           325.9806           290.2612           304.9485           330.4738           296.7137           292.2735           297.0934           318.0235           281.2386           263.1197           343.6680           325.0155           340.4911           283.2066           287.3470           303.8380           294.6661           281.6929           270.2542           309.3288           274.5401           279.5149           296.3370           293.9492           294.8128           286.3248           387.1135           344.3502           323.7404           300.9923           281.1303           330.6605\n",
       "8        20009         288.4441         489.4061         317.1825         288.5723         299.8247         346.9036         444.3189         291.3856         384.5548          388.0910          476.0784          361.4950          361.7348          245.0797          488.4361          250.2549          191.4568          445.3661          358.9844          251.9758          209.3933          319.4068          340.6290          306.0844          375.2443          631.3837          394.4087          341.7485          413.2034          395.8785          283.3976          458.4435          228.1709          458.4774          416.3999          529.8915          360.2247          340.0416          399.9753          161.8585          206.6255          179.0312          343.3600          310.1695          194.2325          204.9117          251.8172          233.0932          420.8718          242.0098          550.8647          527.6336          501.4489          516.5569          485.8251          523.6324          505.0479          501.2233          504.1180           510.2945           483.3552           496.1605           543.3705           509.3443           500.3376           525.5023           535.2814           492.1381           517.1385           506.5189           532.3069           512.1559           502.9536           480.6616           495.4314           488.9331           513.5672           528.8721           509.6337           489.2893           470.7552           498.4859           481.5913           496.6831           477.0782           506.9008           497.4663           513.7261           477.2397           490.2736           505.6559           508.3209           477.1259           511.8977           472.4033           540.0916           496.2674           483.5904           491.7839           487.5864\n",
       "9        20010         266.6828         322.5042         260.6862         210.4291         335.9005         297.8471         400.6422         332.7712         417.2506          472.9805          500.3463          349.7953          391.3864          255.6364          505.7065          248.6692          200.2703          421.4165          305.0933          249.6359          230.1592          320.4061          305.5734          316.1431          365.4896          533.0317          380.5413          321.7933          423.7956          438.3373          317.9659          444.5315          276.9650          492.2325          495.9268          533.6289          311.4711          286.1570          359.1872          208.0858          205.4515          196.9852          356.1472          297.4243          155.3479          201.2424          268.2568          253.0703          328.8226          274.9599          371.1073          358.8705          384.4238          329.7878          341.8712          356.5273          352.9622          372.1420          352.2015           355.5036           369.3624           342.6946           348.3828           347.6320           357.4441           353.8671           334.5460           366.7285           346.7992           352.5910           343.8249           363.1518           370.8933           380.3673           342.8132           357.3465           347.0594           355.8490           316.6055           341.4541           352.5249           350.0840           362.0490           389.4851           346.4531           360.4720           342.0493           343.1289           349.9807           340.8923           357.0990           351.5350           333.6103           339.9525           344.2737           369.0546           361.5238           373.9888           364.0245           353.2926\n",
       "10       20011         358.1515         363.5171         210.6704         224.7597         265.7324         290.0558         408.0057         285.2424         294.9691          355.2982          406.4937          324.9219          338.6895          204.4557          376.3290          240.7869          208.5515          307.0099          317.7848          246.0946          372.6419          308.3842          278.3726          279.7374          336.4323          450.4112          301.6436          417.0648          315.9605          291.2880          351.9256          351.5687          226.5748          404.8754          388.2604          416.7029          384.0909          287.7541          345.2965          234.2582          256.0314          195.8936          284.2144          259.5845          171.7628          253.5425          227.5767          198.8994          278.9482          251.5134          339.5526          343.5585          337.1693          316.7349          328.0732          364.7697          329.6051          361.7843          333.5858           322.1346           319.0740           318.0418           304.1087           328.5819           335.6796           343.1943           315.4289           333.7452           306.8825           342.1581           324.3047           342.6534           353.0988           350.8351           328.2565           348.9957           328.9253           320.1536           310.3553           308.2447           316.1441           330.8929           323.0334           352.2038           345.0770           330.9756           334.9295           323.2631           308.6554           323.1946           338.3553           331.3668           294.6330           315.7709           355.4569           312.1224           318.4558           335.1929           321.7832           340.1248\n",
       "11       20012         152.7624         215.7090         251.2873         132.2263         241.5075         239.6093         182.9111         261.3687         292.9967          281.8690          340.6937          201.2960          238.7414          190.4474          337.0681          229.5300          142.4247          282.1682          188.7193          190.4253          181.2191          192.9039          227.8955          248.9048          223.3717          340.7058          272.6329          204.8439          281.1164          298.7244          156.8392          227.1407          208.1576          260.1514          258.3961          291.8790          229.0626          194.3356          227.7295          138.1050          132.3875          167.1885          259.4132          224.1344          119.3456          130.5496          214.6899          176.7520          211.0146          153.2183          252.9333          248.7924          261.6723          243.7042          233.2065          328.8353          241.2763          260.2333          249.8184           262.6249           267.8960           282.9733           348.4836           256.6907           272.5592           252.8324           307.5003           238.9906           304.3496           247.4864           259.2143           264.1375           246.4765           247.6196           244.8284           255.9343           240.5712           244.1653           314.9363           255.4976           312.0808           241.5967           255.3192           248.2541           257.4813           234.4254           248.0974           247.0509           236.3967           248.8217           241.0563           251.3604           300.8934           266.0387           316.6324           269.5446           326.9195           244.3334           253.8105           273.7076\n",
       "12       20013         203.5723         276.7628         220.0168         165.4129         254.1183         224.2620         340.0742         273.0446         356.4384          388.2974          386.7734          263.7076          345.8524          197.8754          399.2851          217.2249          178.9970          276.9226          257.5556          221.9117          177.3840          265.4717          261.2240          300.2694          284.2005          400.2375          304.3654          280.6865          345.1459          269.4763          268.8552          313.6452          209.2873          318.2873          375.9600          463.6531          337.3481          243.7714          323.3137          176.5419          196.6377          175.2677          285.2388          251.4367          146.3316          186.4424          243.3664          166.3571          249.1559          199.4850          348.3843          371.3341          368.0367          363.9905          348.6221          342.9157          344.9277          344.1065          346.0304           362.4932           359.5920           352.6896           345.6869           343.5948           349.4741           365.3018           350.6750           337.6674           330.4016           346.1878           376.5712           364.1119           390.4145           368.8434           332.1354           374.8305           348.8912           360.3807           333.8558           365.6669           337.9571           347.1644           334.5584           361.4828           379.4414           323.5271           349.0451           346.3999           340.4211           341.8714           388.1345           368.3155           336.5977           345.1350           332.8562           362.8109           365.6833           345.7487           351.6586           375.7341\n",
       "13       20014         183.7936         285.0066         335.6780         185.3158         269.0562         263.3122         269.2075         318.2182         389.6828          373.3569          479.5167          269.8575          297.9926          230.4515          449.3620          275.2258          183.4015          364.6313          238.0948          232.6436          232.7774          237.4342          249.8011          325.7656          312.9087          500.2722          346.6465          245.7995          413.0675          378.8074          248.9028          297.0576          219.1790          291.0026          373.1500          457.9652          267.7935          255.2622          307.1646          151.8990          177.1693          180.9748          333.2203          286.8439          156.4577          164.7794          253.8020          208.8929          276.8939          223.1575          312.0953          298.7461          313.4228          287.7018          281.3126          328.8973          289.2942          294.1994          304.2267           287.2626           292.2864           328.4848           310.6166           311.8122           305.2852           303.6402           303.7448           277.5135           303.8037           303.6988           283.4085           298.7447           300.8626           296.6203           296.1587           307.6573           296.0689           302.5408           319.5431           316.3471           332.4023           286.4057           299.4545           316.2545           309.0601           275.0566           297.8221           268.8218           265.5662           283.7745           300.0795           296.7173           290.7662           288.1087           313.9839           306.7648           321.0337           288.2582           306.8109           327.2008\n",
       "14       20015         246.8527         249.8953         231.7329         163.6028         272.7079         271.3721         327.3424         246.0096         331.5491          346.5272          374.9778          276.1705          344.3305          195.4508          377.2167          229.8274          199.2773          329.3021          239.9137          234.7773          229.3601          254.6186          275.0423          290.6320          313.7548          393.1445          306.4591          292.8059          315.2889          295.1258          245.1120          302.4919          212.6149          355.3387          374.1417          404.7961          297.3698          231.6110          306.5431          203.1491          196.1242          176.7333          333.8192          255.0990          182.0360          195.5681          237.6357          200.7593          298.6372          251.3197          303.1101          302.9817          283.8931          288.9779          278.9329          275.7628          289.4160          291.7311          299.8438           263.6394           288.7189           311.7105           315.9326           299.4993           297.9475           294.1146           285.8324           285.8009           287.1232           287.3833           282.2822           297.6469           302.6973           287.7439           292.0352           277.0015           294.9486           299.7024           293.6153           294.6836           290.3659           283.5177           280.2875           289.8427           277.4487           298.1681           286.9127           293.9831           296.1117           273.9433           281.6906           284.9567           273.6435           281.7536           291.4132           303.4870           305.2968           292.0608           286.9406           303.2368\n",
       "15       20016         266.6426         228.8782         237.3260         214.4464         231.3451         228.1676         261.4848         222.1040         266.9767          314.4025          300.7541          260.6566          287.0256          171.8345          341.1310          233.6638          186.7841          262.0691          209.8882          207.9468          275.8710          215.2436          261.8897          260.6083          285.4200          299.2360          269.7359          277.5331          236.5473          263.3135          202.7196          273.1141          234.4125          305.9764          338.6490          319.3119          243.8154          214.0019          262.7527          214.4642          210.0442          189.9209          265.7623          216.5580          164.8940          192.4227          211.5972          182.2968          236.4318          215.4122          257.7431          250.1832          236.9269          233.1023          222.8216          226.8299          241.6387          233.4437          253.2596           227.3268           237.7536           235.9877           220.1904           256.8346           239.4916           236.6657           227.4562           236.2756           222.8971           247.7284           234.1190           242.3980           237.8890           226.4333           231.1324           237.9142           246.3950           226.8603           226.7440           234.9632           229.2556           231.4324           232.8846           251.1509           243.1565           232.5724           220.1743           225.5598           217.6012           236.4127           224.7922           237.5574           218.3215           232.7664           226.3217           237.7535           233.3358           238.8180           228.1093           234.6848\n",
       "16       20017         243.2464         200.8084         179.5604         211.4150         235.8199         244.8614         271.7946         205.5155         266.2471          319.6903          293.5272          242.8428          318.0764          167.4097          317.4213          263.5899          174.2771          226.4587          193.6739          198.1180          209.1451          220.4877          225.7672          244.7135          254.0255          254.2892          276.5022          278.2473          223.7553          260.6682          193.8855          255.4921          224.4456          302.7612          335.8323          345.5919          261.2160          199.7535          225.2262          212.1377          202.6634          196.3146          258.2674          205.4587          165.1194          176.2527          205.7049          170.6700          224.4894          202.9740          247.5417          244.9961          272.0844          251.9825          231.0031          295.2462          243.1884          247.6595          241.4108           256.0950           253.7249           263.1696           273.1190           258.6289           257.8021           249.4350           266.3261           247.0410           258.3509           243.6188           271.2289           255.8925           244.1262           249.9372           240.4596           248.9844           242.0144           226.7629           281.1349           258.6830           273.8358           242.5474           243.7231           254.8829           235.5841           230.7175           236.0453           246.8413           236.8873           254.3229           266.1317           260.7343           278.9049           254.9065           306.3779           265.2905           277.0872           233.6228           247.2711           259.0181\n",
       "17       20018         146.8092         173.5994         183.7167         127.8062         201.9778         178.0499         179.7174         185.8828         260.1848          268.0592          241.7156          184.4933          220.9180          153.0389          270.0186          207.4811          138.7725          227.9469          176.1756          169.8983          148.6796          188.9097          183.3200          206.8537          195.0901          235.2041          255.0103          212.0715          205.0110          265.0485          149.0026          195.9894          151.3649          240.1951          232.9833          244.0610          185.1324          158.0910          197.8043          144.7892          120.9814          168.2998          203.3011          183.6302          130.0967          126.0358          186.8956          133.1093          162.1756          139.8946          234.6080          232.4463          238.4623          239.2159          213.2384          263.9108          224.2107          232.1559          230.2469           240.9964           249.3047           248.0594           253.5145           251.0162           259.5062           230.6581           255.3788           226.3872           260.6573           231.7550           261.9050           239.9854           231.8995           237.7901           223.9133           233.4132           228.1611           228.8844           240.3058           237.4203           251.9219           224.5933           232.3907           242.3174           221.6363           219.0845           226.0891           248.3749           212.1899           227.3979           244.1002           235.1446           288.8717           247.8035           266.4924           236.7673           246.2668           228.4297           240.7052           243.9670\n",
       "18       20019         333.7240         298.5388         217.2804         219.2756         232.6180         235.8926         345.1527         236.3890         316.0638          375.9659          344.1932          270.2515          364.7344          178.2398          338.5114          214.2638          198.4787          292.2658          257.3390          209.7615          298.6723          229.6966          222.0267          332.6259          279.7207          391.6745          273.0007          319.3423          291.6534          283.1917          243.3550          284.4539          183.8125          376.4846          297.8148          340.8358          347.1460          235.0005          291.9161          218.0198          195.8167          180.4761          238.8743          230.0132          193.9374          214.4140          229.5230          157.8925          274.1786          220.4323          240.3388          256.2993          257.1513          258.8263          258.1109          229.3509          266.9618          265.7363          257.1720           258.3042           263.0757           266.7926           271.6118           250.1527           234.4701           258.4594           232.5545           255.2572           271.1297           255.3679           267.6658           251.8240           255.5083           267.8983           256.5400           251.0364           252.4101           272.0752           258.9598           244.0611           254.8120           253.8101           241.4689           273.7900           256.8817           256.9428           253.1288           242.3638           245.0456           230.6809           247.1403           255.6287           257.6972           249.0658           254.5104           268.7801           276.3046           248.5835           251.8889           284.9751\n",
       "19       20020         250.1889         236.0363         201.2217         203.7375         238.8985         205.7505         265.0449         212.0956         253.8010          236.5114          276.0221          243.8452          238.9370          173.7466          289.8298          229.9689          183.3561          247.5479          192.2135          212.8287          220.7552          222.4062          199.0481          233.6088          264.1161          276.8528          254.3776          271.2151          211.7924          257.2044          207.8034          247.6765          192.7023          317.9188          296.2874          290.7239          221.0001          213.8420          249.1900          217.3087          214.2203          183.7230          239.0018          200.0947          164.4724          191.9957          204.4753          190.7695          224.1274          210.0432          249.0808          257.7846          253.3750          251.7753          227.9519          327.8557          259.7601          266.1178          250.6958           259.7081           262.7161           283.4750           280.3413           267.5752           247.5188           262.3790           270.7496           251.8425           268.8173           267.0324           257.8770           280.6467           260.7018           271.3517           254.6701           266.0238           251.6134           265.4634           283.0759           266.6378           280.2208           251.9611           253.4478           278.8942           249.4346           253.3386           263.8619           270.0034           236.5778           251.8727           262.9104           258.9364           241.2694           263.1436           318.9176           284.6452           278.6710           257.2354           254.3079           269.5311\n",
       "20       20021         206.7702         204.0871         203.7863         186.0510         212.7186         200.2862         226.5852         196.8774         251.9747          265.6888          263.0918          228.1276          253.1646          158.3613          303.1847          228.5077          170.1047          248.5453          174.8180          189.7979          205.7954          203.8327          207.8907          232.6257          237.5254          250.8132          255.4241          247.3859          200.4808          256.5458          184.0125          259.6241          197.6077          284.8824          265.2211          293.3830          199.7913          181.3211          224.5322          195.8175          184.6268          181.6390          236.8113          189.8426          153.8181          163.5551          192.3511          179.8380          208.8595          187.0916          243.5035          224.2617          242.2694          240.4688          221.1545          252.9762          227.6605          238.4102          228.5842           255.2341           246.3959           248.0023           232.9677           252.7705           236.7849           234.7719           259.1571           234.3625           242.6581           230.7865           258.5150           250.9109           233.5472           238.7637           236.6973           228.3477           234.0382           234.3605           244.6056           256.2115           251.6724           227.1327           231.6205           237.1501           227.2787           228.6161           228.6585           233.1159           216.9694           229.0769           240.2588           244.3884           238.6540           233.9305           248.5294           247.7609           251.1889           232.2856           234.8131           254.1705\n",
       "21       20022         204.3186         195.5432         147.7194         153.2001         224.6390         185.1861         281.4965         203.0135         252.8136          254.8137          296.2015          208.7445          190.0431          170.8985          244.9019          192.6418          164.1596          223.6768          183.4022          181.7544          156.4992          218.1961          164.8275          206.7107          227.9048          275.1958          243.4991          243.7484          221.6953          235.4086          209.0647          207.8151          197.1470          324.6418          283.7852          315.5475          211.7318          201.8108          254.6193          177.4630          159.1949          172.1994          261.9374          187.7452          144.9734          157.5455          194.8225          166.1870          221.2229          167.2166          207.8845          227.2167          211.4826          204.1718          204.2903          213.9242          206.9639          207.6323          196.2679           214.6273           219.2804           235.6299           255.6617           222.2275           212.5497           212.7485           233.9722           211.6037           225.6409           203.5099           209.9220           207.1628           212.0513           220.8605           202.5719           210.5072           216.0306           219.0675           198.8880           225.4129           246.1837           205.8427           199.3638           215.2501           218.8605           199.9041           201.2129           224.9609           206.4833           194.0175           204.4173           207.1302           228.1789           207.6221           226.3187           208.0615           244.6846           205.9263           211.2751           240.5112\n",
       "22       20023         154.7739         189.8394         190.7063         152.9794         176.4942         167.2994         190.8615         201.5220         241.2120          237.9286          242.5268          198.8588          207.4656          158.5059          230.9552          204.7216          162.2477          208.9885          179.4072          172.6681          135.2368          167.0702          166.6089          214.7499          215.0936          254.9729          245.3691          210.3238          218.6446          202.5740          187.5085          195.9113          146.3852          227.4288          223.5308          271.3043          171.2497          180.1453          208.9044          152.9818          137.9991          163.4818          193.2145          181.3427          155.7369          148.0953          188.1926          145.0883          198.5445          148.2832          173.1491          188.5196          192.0669          192.9574          179.7390          196.3033          174.7519          194.2208          181.0947           188.2932           197.5973           204.8207           212.2163           192.2057           188.2158           198.2343           191.0649           192.7913           194.2718           186.3374           192.7077           190.8062           187.3523           187.6778           185.1813           189.9220           185.9377           197.4658           200.1503           179.5375           197.0140           186.6268           206.1380           192.2795           198.4527           180.9501           186.0529           191.1680           188.6187           183.5219           188.4692           190.4764           183.6228           184.9470           202.2642           175.7830           202.9636           181.8300           180.5297           198.9536\n",
       "23       20024         196.9871         156.4449         160.7994         144.7862         188.3029         172.1950         208.4146         164.7698         157.1953          194.0513          215.9570          214.1749          181.7366          149.9376          203.5142          148.9879          153.6296          187.1028          171.5111          179.8085          177.3112          186.1347          155.7732          153.6751          222.7785          206.0609          206.1670          242.5574          162.8626          181.1994          222.3933          201.1889          166.1943          238.0730          222.5297          206.5183          203.1564          159.3543          182.6046          176.1098          200.2392          162.1997          175.7723          171.0054          125.2981          170.6263          163.8259          141.8834          173.9342          158.7463          182.8201          201.7316          191.8966          192.5946          180.4501          198.2704          190.7530          183.7925          183.0519           190.8219           190.1079           194.8258           204.3025           194.4530           189.8451           192.1940           187.5720           186.6807           179.7751           187.4232           191.0067           185.5486           202.4988           181.6377           185.2015           191.5141           195.4827           189.1234           183.0305           190.4109           191.1020           187.3794           190.0439           190.8034           184.2814           183.3200           187.6816           190.7123           189.8471           179.5776           184.4081           187.8047           168.9537           186.5142           175.8858           188.7178           179.4281           181.6267           191.8945           199.3252\n",
       "24       20025         201.7556         173.6738         181.2639         165.7191         200.9400         194.9510         195.6301         169.5098         186.8392          219.9137          224.9564          216.5952          198.9951          151.9683          260.1266          189.5292          158.9270          208.8615          167.6308          188.6310          213.8370          195.5070          205.5137          184.2327          235.7895          213.4122          232.6076          215.3630          163.1975          233.6811          159.0175          245.6071          181.5168          261.0289          217.4312          246.3075          179.9082          166.5412          196.7020          175.6000          192.8822          171.7903          196.9897          172.2155          146.8083          166.5977          166.1239          182.5636          193.6123          199.9544          180.4972          200.4107          188.1480          182.1181          181.1941          194.1809          182.8368          200.5765          178.6984           196.0340           193.6315           197.9958           209.5999           204.8867           191.0695           191.5946           179.8772           195.6026           189.0002           188.6341           181.4438           191.4342           183.3453           180.8001           187.1319           191.7446           188.1347           178.7979           202.4864           197.8702           200.1934           193.1235           182.8550           190.8551           185.3785           186.9336           187.3188           195.5642           177.3376           188.3717           186.2788           188.8161           197.5693           187.9844           204.8362           193.2576           220.2527           194.9981           186.3425           211.5309\n",
       "25       20026         222.4757         188.2728         180.7988         196.8964         193.0990         185.7327         210.5284         191.4829         228.5465          255.8821          244.4364          210.6821          236.9547          155.6008          278.2791          186.2167          176.2196          232.3537          172.6164          193.7608          200.9583          193.9479          200.9692          225.5047          231.6045          235.6431          240.6661          228.3780          189.8014          234.6398          189.4548          208.4728          177.5191          237.6749          253.3883          254.0898          197.2449          177.2830          215.0225          195.3503          164.1656          165.8256          202.3752          184.9746          173.6719          174.7483          186.0022          157.2683          214.0684          179.4345          190.8153          192.5578          182.4458          186.8770          191.5965          187.5576          190.7332          203.3693          203.0208           183.2128           196.7162           193.3122           191.0769           202.4683           196.8036           191.7657           168.9156           196.9417           203.6723           197.9112           200.3313           177.9448           177.9914           185.0922           202.2777           189.1094           188.3963           173.4599           203.0107           194.5953           196.4094           188.8118           185.2894           188.0799           187.4962           185.8885           205.3672           196.1646           182.7661           185.9377           184.6391           192.9258           185.1989           187.5488           199.3900           192.7880           202.9658           199.9068           188.2445           206.3556\n",
       "26       20027         204.7608         172.4065         195.6333         184.3229         195.6511         178.7893         213.3148         183.0247         247.6133          244.3981          230.1248          200.4477          241.8817          155.5597          277.7380          215.3830          161.5149          219.1905          174.3703          183.0352          170.3108          168.6970          178.3614          220.0963          231.1775          223.9741          236.2650          237.6187          189.7766          241.6551          167.9886          228.5193          187.1740          235.6314          239.3994          262.7574          186.2386          175.5962          200.3568          173.2937          163.9376          190.3548          216.8224          181.0292          154.6323          150.4325          196.5087          170.7045          178.6544          183.5594          205.5056          203.1725          201.1727          213.3744          203.4035          190.5596          203.2466          202.8692          199.4680           200.9211           220.2669           199.5130           188.9622           220.6252           208.0071           209.1899           198.3902           204.6120           195.3398           208.6758           214.7945           223.7071           206.1927           214.2165           200.6769           202.3346           216.7990           195.0555           195.4341           211.9662           209.6077           198.0541           199.7941           208.4376           203.0989           193.8628           195.3484           211.3170           196.3968           200.6615           196.6780           205.8097           186.6006           209.1679           217.4850           210.5829           190.9985           204.2205           205.7524           204.4006\n",
       "27       20028         149.0241         150.7628         160.0666         131.8998         175.9729         144.3996         193.6075         177.0878         228.3310          214.6624          227.9122          172.0022          168.8715          143.5322          212.1931          199.7673          139.1575          175.2804          159.9554          155.4294          122.8522          191.1388          181.7122          196.2915          185.7934          216.6078          219.5240          216.0235          175.4108          213.6347          156.1745          218.4753          148.9964          214.4248          197.3648          224.6489          160.8052          158.0359          173.3082          129.8645          124.6533          158.1649          194.2608          154.0205          125.0561          119.6689          167.3328          118.1868          164.2632          129.8396          161.9298          168.3757          161.0553          176.9267          155.3122          158.8228          173.3927          165.9532          156.8536           174.7564           170.5007           169.5987           162.6755           161.8589           166.7904           171.1117           160.8149           162.9085           165.2477           163.9082           164.7386           170.7662           169.5804           156.4031           172.8289           168.2795           168.9028           168.6903           150.7001           172.1648           165.7515           168.6835           177.9942           170.9547           166.5719           173.9749           171.5150           169.6386           151.0595           165.5343           159.8534           173.9284           189.8815           163.1335           140.2785           154.6837           168.9486           168.7375           165.9787           170.9215\n",
       "28       20029         161.3855         138.2247         128.5308         152.4532         171.8926         191.4201         159.4756         142.9141         153.4922          167.7637          164.4575          166.4948          182.1825          131.3469          225.5374          147.3836          125.1048          163.5588          142.6067          152.6889          149.0735          151.6318          172.8207          159.7892          179.9001          156.9738          201.2384          144.5189          130.1391          208.8112          124.7660          176.4053          147.8783          193.4897          170.6870          187.0580          142.2234          134.4911          155.8109          144.3076          143.5409          146.8772          156.8738          140.9689          104.2430          137.8999          142.9662          150.5483          150.4303          175.4655          153.4266          157.3747          159.9828          161.2484          154.6130          150.9601          163.9516          164.1916          164.8137           173.1691           170.8680           159.7416           168.4870           173.4338           157.0724           161.9402           168.4999           170.9995           157.2673           163.1237           164.4243           153.4433           175.5446           162.0389           173.0614           168.6940           161.3936           164.1024           135.4664           160.3269           143.5543           161.0234           174.2537           165.2522           157.8482           157.3934           164.5414           161.2702           153.7216           165.8208           159.2868           161.8892           158.8796           159.2497           151.5982           169.8101           180.9886           160.8167           166.4131           157.4406\n",
       "29       20030          93.9724         110.1499          91.1880          92.8479         119.7675         113.0949         102.9308         105.1892          98.2036          123.5401          115.7651          116.9693          119.4522          107.6014          106.4437          101.7516           92.3532          120.2847           91.3352          102.0478           87.2728           97.6167           91.6074          104.6411          123.2338          116.5608          120.8807          102.3065           94.2406          110.7323           85.7984          126.0250           97.2901          125.9381          108.0583          113.5843           86.4895           92.4885           99.0431          105.5050          119.6268          113.3322          114.4529           97.7112           77.3268           98.0522           98.8708           97.7567           92.8996          117.8156          117.6203          141.7923          113.8913          124.8845          127.2723          124.1096          129.3386          126.6222          113.9999           129.6498           131.2299           113.8999           122.7629           128.8505           136.9892           130.2413           134.2951           126.0730           118.1757           122.5079           131.4391           125.5049           131.3727           128.4164           128.1179           130.4106           125.9034           127.7948           107.8309           130.3470           123.1481           126.5420           126.6739           126.5410           119.4832           131.1901           123.1050           128.0390           135.3104           122.6758           130.7518           132.4906           117.5598           122.7631           123.0540           125.3133           127.3516           127.9951           129.5071           117.4723\n",
       "30       20031         149.5538         131.8266         137.4409         136.5184         169.5321         137.3079         173.4217         156.0653         177.9780          185.4482          204.3158          164.5594          142.2590          142.7616          179.8644          165.9944          142.7045          172.2354          146.9605          151.3652          126.4088          146.3604          135.3381          164.0225          186.1351          188.9631          208.6886          188.1834          151.3405          172.2497          150.0432          163.0929          154.5921          232.5879          189.6538          192.9857          153.9412          144.4969          171.6511          142.4617          114.6349          155.9208          183.7528          150.4608          124.6302          133.0132          162.1536          130.1334          163.1354          122.9517          159.6235          170.5202          155.1799          172.1941          163.6023          164.0670          169.5248          166.9896          167.6808           176.4302           170.6967           178.3387           183.3684           178.1217           168.6252           167.8783           195.9046           179.1960           173.8303           167.9997           176.0619           157.8743           182.5080           169.5152           176.0296           176.0653           169.6586           169.7925           170.4700           180.3523           186.1322           169.1157           172.2018           170.9430           174.4274           165.0365           177.9984           168.8361           161.5027           167.7327           165.7992           179.8213           186.3055           173.2551           179.5217           180.7552           197.5646           163.4244           171.8347           190.5813\n",
       "31       20032         293.8077         221.5754         306.7467         278.7379         290.7786         306.9963         409.9538         374.5432         480.4341          401.3495          517.0447          401.2173          258.9606          342.6221          713.7483          331.6915          206.6797          523.1787          326.8314          288.7405          359.8306          276.1968          327.2204          319.0630          395.6288          521.3525          322.3907          557.8931          458.4570          557.8550          395.9073          324.7355          229.6913          253.1717          339.8326          390.5334          306.5388          304.4779          391.0621          134.9783          293.2355          205.0012          380.8655          287.6712          229.3710          241.8626          297.9864          218.7566          437.0433          236.7548          360.6154          380.8710          361.1404          370.9860          397.4942          289.6452          365.6733          346.8838          397.5960           339.2564           339.2108           311.0655           340.7786           331.3242           357.4355           386.1326           314.1153           365.6909           282.2910           360.6281           377.9049           351.4260           343.4717           357.3763           364.5757           353.0154           334.8573           415.7653           292.5078           346.9949           300.1604           375.1366           333.2039           360.8628           380.3383           377.5653           396.1262           329.7908           365.6472           360.0468           377.4798           384.2602           279.4871           349.1280           289.9366           318.2822           281.0026           360.5740           335.0682           325.6003\n",
       "32       20033         147.0707         139.2879         141.9818         129.9245         152.5105         156.0215         146.8749         137.8159         204.0022          152.5423          174.3877          169.4315          173.5629          133.6483          200.7517          150.9508          136.7839          176.4219          135.3735          153.6187          132.0650          131.9070          144.7022          191.5445          183.2472          172.5116          197.9057          164.1146          157.3339          183.3693          128.1466          153.2623          120.7799          158.2563          166.8435          202.5654          143.2403          126.5126          150.6224          136.9909          127.5857          162.8592          150.5226          152.0828          107.7393          117.7514          161.9576          115.5054          150.6046          132.3869          152.6740          161.0171          154.0657          161.1739          134.6203          158.9357          153.6290          152.5943          145.3410           145.9474           155.5663           161.4639           163.8484           159.4729           158.4050           143.8312           161.9339           146.5934           155.9855           148.4028           152.0356           150.1118           146.0164           151.9261           152.6968           162.2125           161.3070           153.5735           143.2479           163.6287           162.9206           149.1253           162.5769           157.4079           159.4977           148.1175           148.5178           140.9751           145.0534           150.3364           151.3664           157.1866           179.5706           149.5516           153.0176           154.8152           150.6167           145.5199           159.8416           152.4497\n",
       "33       20035         143.4371         142.8364         146.1861         140.7676         163.7396         169.2113         147.2383         138.3887         138.4473          164.3530          161.6685          168.8474          147.3448          127.8374          201.6550          138.3497          125.4764          163.0695          126.9716          150.1888          146.2881          150.1362          166.7038          141.9548          185.2390          154.3720          178.5173          152.5718          121.2555          197.2472          118.3496          194.5986          139.8323          172.1453          149.7245          166.7070          125.6850          131.0723          151.9287          143.0878          144.0166          139.9561          136.2970          139.4698          128.4745          136.8866          133.9539          168.4924          144.5071          178.5392          169.8240          184.7186          173.9340          177.6883          173.2986          180.1557          176.7811          185.3754          166.7029           179.2706           180.9199           161.6526           192.0314           191.3981           180.1943           179.6839           176.4440           187.8086           172.0887           169.5977           181.6283           173.9056           182.3582           165.3770           181.9295           187.0482           165.9020           182.4737           161.4001           183.8603           171.3620           172.6371           187.7009           174.4993           180.5801           172.2739           173.5679           179.8597           164.2872           175.0192           179.2498           183.8127           170.4757           169.4707           194.8038           188.0344           183.5897           176.9232           177.8582           179.1412\n",
       "34       20037          99.8669         116.5094          95.2026          99.7441         124.3372         123.3444         108.0891         117.9680         138.1783          125.6735          132.0409          122.8899          151.8933          116.6103          133.0978          120.8281          108.2332          116.5078          106.8207          115.9274           84.3842          108.0805          111.1125          119.8159          131.5833          129.3511          160.3539          106.9095          124.1121          129.2110           95.2042          143.9558           97.5699          147.2337          126.6272          142.2823          107.1593          113.4130          109.7311          105.2100          101.2379          128.9412          133.4731          118.2429           67.0356           96.3697          126.6978           87.5188          106.9121          108.5374           82.0805           91.2024          102.6056           98.8600           96.6082          101.7833           92.2282           85.4809           84.4405            88.4554            91.8238           103.0833           123.2556            82.0875            91.2302            95.0121            89.4963            90.1772            80.2903            82.8869            95.4519            92.0316           100.9470            91.8333            98.8745            90.8852            91.7316            98.1267           106.3843            87.1880           104.9997            87.9972            83.2134            91.3137            97.7382            88.0808            95.0126            86.7734            93.0855            87.1346            88.3346            90.2364            92.4915            89.0421           106.2461            92.7213            91.4649            94.0510            96.7610            94.4856\n",
       "35       20038         144.7399         138.2283         139.4684         150.3530         156.0508         152.5462         162.7816         142.5520         142.6426          160.4717          165.4502          160.7731          149.4334          129.6129          174.4251          145.6009          122.8172          157.5076          128.0863          151.4524          133.1522          150.8343          140.5670          146.1563          180.5960          147.6876          168.1225          139.2202          126.0470          172.2079          122.2128          195.5916          143.8683          165.9235          162.1844          161.2547          133.9495          129.0941          145.8009          143.7886          141.9816          133.5973          139.2274          133.1335          129.2899          133.6828          136.4994          149.3817          144.5341          147.3884          126.7722          127.9022          135.8337          123.0386          133.7974          149.0318          124.8588          130.6720          123.9969           135.9127           134.6636           129.2286           150.2301           137.5690           134.6355           130.2049           133.4851           141.1923           137.6484           124.7656           134.9871           128.0318           123.0293           132.4739           126.7291           128.1731           137.0092           121.0921           142.7866           130.2934           148.6982           133.0732           137.9582           129.2926           128.8121           121.1073           138.4083           136.5179           122.4147           138.0092           130.8860           133.5628           133.4755           138.1780           163.2792           136.1106           146.3470           123.2269           128.7197           138.2303\n",
       "36       20039         110.3789         108.9284         104.5499         124.2283         125.0757         141.7249         113.3660         103.9642         107.5785          111.6985          131.1115          125.9840          124.8481          102.6389          155.3345          115.0663          103.0565          128.6975           99.4725          114.6393          110.0601          114.5994          120.0077          111.3927          140.1224          116.5938          140.5085          112.4132          104.2110          158.0055          101.4026          127.6344          111.1005          111.6860          120.6298          140.3429          122.1249          104.6693          126.4792          118.6021          109.6189          117.2344          115.8060          111.1877          107.1197          109.8603          107.4721          119.2870          113.0822          144.6469          125.4597          125.1881          123.9174          118.4792          117.4450          137.7511          127.9736          127.8227          113.1887           131.2800           118.9790           127.5017           139.6781           130.6146           132.7026           119.3582           126.2601           124.3176           125.2898           122.3137           126.3856           119.9864           112.1625           124.5754           124.4437           125.6594           126.5154           121.6095           133.4891           124.7810           136.5167           122.4327           117.0441           124.6958           112.0019           121.7805           121.5130           133.7806           116.8150           128.2109           129.5751           125.5405           129.9859           127.6204           127.7045           128.8667           136.6615           121.9893           119.7872           129.0150\n",
       "37       20041         116.6943         113.2528         115.4239         129.6136         128.2218         141.9884         118.5737         115.5773         107.6348          129.9765          132.4853          137.9937          127.3719          114.4955          159.8279          128.3643          102.4466          138.3189          107.2258          126.5717          110.7823          118.6859          131.1102          118.0458          153.5787          126.0750          146.3040          114.5976          102.0287          163.9224           96.3636          142.0979          110.7196          131.8866          125.5708          144.3454          114.0227          107.6500          123.3211          114.2257          115.9034          127.0972          130.9606          121.2551           96.8991          111.9013          110.8084          132.6254          115.6802          142.7584          109.7369          114.1631          120.3225          117.4432          110.2971          120.8008          113.5941          114.1584          109.7372           123.9470           114.6036           115.9419           148.4755           126.3713           116.6357           114.0014           114.4256           119.7452           110.6857           117.7073           120.0833           111.4837           120.1851           121.2393           122.3760           117.2666           127.6008           117.1966           108.0432           114.1732           129.0926           115.1785           114.8752           118.0085           118.3207           116.6613           116.0024           113.2249           114.4562           122.5673           112.1269           120.4343           129.1037           121.0256           119.3263           109.7617           131.4555           111.5880           118.5640           109.6508\n",
       "38       20042         159.9865         148.8530         146.5676         153.6497         157.7733         166.1177         150.4509         144.6221         197.6780          174.9880          178.0265          161.5965          157.7426          140.7046          184.7221          174.3629          142.4898          164.5987          139.3142          161.7037          151.5219          143.5305          151.6303          181.8778          176.7753          168.7838          190.8953          173.6293          150.3520          172.6195          135.1622          170.9543          145.2398          183.2045          169.9793          197.1689          143.4695          141.4785          151.8205          148.7818          147.2323          149.8004          154.8309          145.3748          135.3343          133.8757          154.7262          144.2402          155.1571          153.9082          141.9906          140.1508          151.3614          145.9008          139.5827          141.4578          160.4123          151.8647          144.4574           142.8151           145.8115           147.8484           154.0840           154.9397           147.7575           145.4863           152.3641           142.4368           140.5887           142.0334           146.7145           148.9789           143.0665           155.9237           147.8032           145.0917           148.0746           139.9452           131.5185           151.8284           147.4619           148.2473           147.0051           150.8811           150.5424           135.2271           143.2361           152.4308           134.4564           141.4033           146.2342           151.0918           145.6117           140.5945           124.2010           145.7154           151.7642           139.0290           149.9572           155.0182\n",
       "39       20043         111.7149         117.4667         113.7577         124.8901         138.1347         127.9820         119.8007         115.0544         128.8901          136.9164          139.7854          138.6323          133.4297          124.9072          166.9571          124.9850          106.1039          144.8774          109.1230          132.4126          110.8035          116.9261          115.7571          124.8778          155.4358          135.4146          151.0351          122.3160          110.0924          158.6599           99.6517          139.9108          121.3831          132.2557          131.7651          134.5080          103.6642          107.3088          132.9827          119.7772          118.4612          131.2399          128.1320          127.0353          103.7317          104.3203          119.1684          124.2115          119.7640          115.0307          120.8504          112.0642          132.9710          125.6856          117.5572          128.2544          131.3250          131.3921          112.0700           128.3590           119.2013           128.6890           136.2385           137.3303           131.1523           117.0530           134.3293           117.3966           134.5886           125.2923           131.9566           126.0413           119.3996           126.6011           130.6147           122.7916           124.9323           115.6650           122.8478           130.1197           127.0007           118.3631           140.1864           131.6452           135.1641           119.8297           122.6380           124.1217           117.4798           124.5238           129.5584           124.4200           121.0740           127.9312           118.6947           120.0195           117.3686           120.7703           121.4383           128.4083\n",
       "40       20044          98.2742         123.0857         152.7743         132.1182         130.2750         139.4480         141.7809         134.2030         189.6644          157.1923          169.6821          139.6038          180.2266          120.2961          127.5143          144.5782          121.4368          156.0736          138.3712          124.3114          108.0032          132.0442          119.9837          143.7612          148.1151          170.2813          181.3060          146.3005          150.1703          149.4258          132.2264          138.4821          116.3148          120.6318          141.6637          163.2272          128.3400          125.9149          136.2038          109.8446          106.1540          148.0516          144.4605          133.5214           99.8351          107.1362          153.3463          101.7502          143.5213          116.1908          112.1055          128.1485          146.8683          118.6127          121.4749          137.4110          123.8984          119.0080          110.9129           108.6261           114.9579           131.3961           143.1553           124.6046           119.1289           124.5161           124.1823           119.5163           127.1172           114.0531           120.1379           121.1377           118.7033           134.8957           126.6791           130.4721           120.2436           116.2238           135.4753           119.0525           146.8516           120.5690           113.9354           129.8898           122.5103           117.6289           113.9200           115.9493           116.3527           120.6736           113.6537           134.0726           150.1549           111.6192           139.0059           125.4890           145.7708           119.1710           125.0577           135.0803\n",
       "41       20045         129.1256         135.0495         129.3313         141.4520         146.9566         159.2517         141.5625         131.2389         125.5955          148.3171          157.9689          144.9264          144.1110          117.0766          171.4623          140.8984          116.8304          159.1907          126.0840          148.4241          146.0917          147.3873          170.6536          139.3944          163.9376          140.4694          159.7523          129.7696          115.1425          170.8185          108.3640          168.4754          150.6828          145.2205          152.4082          144.6310          136.9914          123.8105          142.2598          136.0004          126.9364          132.0378          133.6658          132.9411          128.6526          133.8551          125.0320          149.9517          137.3138          160.4400          129.3702          130.2229          136.5659          130.6829          126.4945          136.3831          130.8027          134.8438          128.1850           138.3470           131.4334           125.9621           146.9784           128.0966           128.6594           126.4036           130.5360           134.4693           130.2593           134.7523           122.7645           123.6030           136.8738           129.1278           133.2368           125.8634           130.2669           118.3637           146.4860           132.2060           133.4628           127.6557           130.7100           124.9859           130.3364           125.8346           135.9115           122.1836           125.1833           130.5341           119.7626           132.6003           126.3554           129.4091           136.1757           135.9103           146.2409           126.7153           130.9704           139.5827\n",
       "42       20046         140.6019         122.1865         133.0878         143.0905         142.3046         158.9180         137.0935         124.9887         134.6051          137.8277          153.4532          134.1395          141.4022          117.0521          179.5318          128.8239          120.3241          141.8918          119.3901          139.8637          133.7518          127.8067          141.5816          139.7597          154.5396          134.2034          151.1898          137.1286          120.3333          165.5052          111.8281          146.3377          132.1578          145.0149          143.9410          176.6115          135.0012          126.6738          147.0107          131.0323          124.4931          131.8747          127.4627          124.6323          117.3978          124.5680          128.3347          125.3601          134.1889          151.6763          132.1660          134.7039          144.6186          140.0361          138.2352          136.8801          135.4458          133.5978          135.5044           143.8235           136.3975           142.4995           151.2028           137.3775           141.8990           139.3013           128.5965           145.5810           138.1170           141.4080           152.1455           142.7233           129.4558           131.6492           141.0321           130.8118           138.8436           134.7361           131.7339           131.6593           132.0464           137.1324           129.7760           135.3761           139.6932           140.8361           147.3288           120.9484           135.0484           132.2334           142.6214           137.0443           145.9702           138.6783           142.3589           134.4104           145.4307           141.1705           141.1012           134.8299\n",
       "43       20047         112.7898         121.9343         133.7349         120.0080         137.7528         136.4585         152.4946         130.9987         175.7559          162.0693          166.7150          137.3730          170.0472          126.3714          126.3432          148.0920          127.9628          151.0718          139.5135          121.4570           98.5929          129.1329          121.3560          152.7853          151.9488          156.0461          179.9937          142.0007          142.7077          142.3233          127.0095          142.9073          126.8133          148.5771          148.0974          148.7994          126.9281          130.4287          126.0484          117.4828           94.4072          145.9857          149.2209          130.4001          100.3096          111.3793          149.5390           98.9186          139.7794          119.2413          120.9326          134.3420          148.7645          127.2989          124.7104          134.1805          122.8533          127.3861          113.2905           111.8986           116.0764           146.9633           200.8864           129.1961           135.0801           130.4818           135.3745           124.2038           131.7411           120.6173           124.1813           135.1484           115.6082           132.7554           135.9990           124.5474           120.8967           127.4116           129.0049           122.8388           153.5026           128.2800           120.3465           128.9377           131.4371           125.1344           125.2921           128.9779           118.8690           125.3688           125.9618           139.0647           162.8771           128.3149           150.0537           123.2969           131.1076           120.0029           131.4980           140.1058\n",
       "44       20049         148.6021         138.9062         163.7996         126.6685         156.4230         156.7728         147.2529         150.2838         203.6771          195.2277          194.2095          170.0946          207.3161          135.0765          195.1236          165.0368          137.3116          159.4769          141.8843          156.7186          154.1821          126.9049          166.0358          194.7786          194.0369          187.4241          193.2978          167.8842          163.3118          185.4265          125.0710          148.3568          138.7908          176.0684          163.5617          218.0404          158.0214          136.3463          152.3702          143.8754          151.5008          167.1265          186.0557          155.6716          140.0977          132.5957          167.5376          120.8689          151.3033          136.7757          133.1184          131.5002          149.9309          137.9380          127.4936          127.0461          135.0284          135.6991          123.4610           137.2097           128.2162           140.3445           129.1596           141.9558           136.9463           133.5013           136.1952           126.6861           132.3800           133.3875           131.9747           136.3842           137.9813           134.3898           137.1427           127.4298           138.9114           128.2748           120.6962           131.0415           147.5064           129.6064           127.4917           137.1912           141.3265           130.5686           134.2588           130.0772           126.4841           128.7874           126.7088           137.6698           137.4613           141.4993           120.6018           129.0737           132.1620           125.9416           132.2236           142.7874\n",
       "45       20050         113.8600         114.5453         114.8321         126.1148         136.8878         137.0167         126.0218         116.0356         118.4541          131.7124          134.5607          138.6863          127.2810          117.9250          164.2532          113.8463          106.1581          130.3423          112.1559          125.0521          117.7492          122.6452          132.2803          121.1449          153.1223          124.9410          144.5731          115.5526          108.8730          157.1893           97.1985          144.4508          122.8509          138.8349          128.4884          131.0239          106.2786          109.5915          134.3059          121.3317          110.3187          125.8736          120.7810          118.9220           98.3672          110.6573          116.1768          142.2232          122.3848          132.6603          113.7623          117.7935          113.7981          117.7125          116.1170          114.9405          122.2972          121.0639          111.1100           121.4339           115.0641           120.0667           104.2197           120.8996           109.9421           117.3860           116.6725           115.6952           126.4047           124.7182           118.5419           124.9579           108.0849           120.1405           120.5280           117.7205           115.5378           120.7477           104.8067           111.9014           109.9204           118.0198           123.6302           125.8641           124.6199           114.5497           118.9968           119.0111           107.7939           115.3392           116.5024           118.8374           114.9288           110.9291           116.7473           120.8362           103.2058           120.4437           111.4972           112.7813\n",
       "46       20051         111.3173         104.7312         112.3026         131.3102         119.5660         141.9819         109.3200         112.7876         112.2235          124.4828          120.2231          127.5690          132.1976          103.5156          150.7373          102.9313           98.1930          123.7323          101.8809          121.3352          113.8900          104.7394          114.8879          111.3758          139.2241          117.0933          139.7410          111.1499           99.7127          146.0055           96.2754          129.9380          105.4138          115.8447          114.6335          133.1924          113.7566           98.7388          114.4755          118.6069          111.1626          118.4762          112.5369          107.5769          101.9296          109.0567          112.6448          112.7196          117.1394          131.4615           95.3864           96.1288           99.8989          102.8198           99.3878           93.0772           93.8883           99.0342           98.6369            99.6910            99.8480           100.9643            94.5917            95.6428           108.8299           102.6612           100.9267           103.8556           101.5239           104.0038            96.0305            97.3830           105.4166            95.8392           104.6544            98.0625            97.5718           104.2765            89.7319           107.0417            99.7224            98.9931            99.0742           101.6804           107.3381            98.3022           103.6784           105.7010           102.3312            97.0157            98.2122            97.0613           100.8966            99.1693            99.1236            93.1812           103.9998            94.8536            99.7219           107.9916\n",
       "47       20052          85.9291          96.0485          85.8676         108.2249         109.0343         113.9272          89.5873          96.8845         100.2198           98.9782          110.0235          100.6163          100.1973           90.8790          109.9354          105.5913           91.4618          106.1995           89.2990           96.6372          102.3063           96.7777           93.3656          101.9660          111.0253          103.3844          116.5651           94.2756           95.9962          107.7841           82.5281          112.3487           88.9347           92.9191           94.1223          112.4085           88.9387           88.6452          101.6811           98.2502          106.3692          105.9878           97.6585           99.2336           88.2308           91.0443           95.5763           96.3576           95.6028          103.7635           90.6227           88.3538           94.2221           94.0022           90.9735           86.8635           89.6627           96.5417           90.0844           103.0053            91.6166           104.3403           122.1363            97.8562           103.1013            94.6584           100.0860            97.3751            99.2316            96.7973            98.3368            99.2206            96.7877            98.8623           103.8621            96.1823           100.8226            87.6799            98.9420           101.0845           100.5893            95.3727           101.6741            92.8322           106.3499            93.1652            92.6238           102.7070            93.2022            88.7350           100.1725            93.9086            96.1411            92.0707            91.7463           100.2943           103.1747            93.9691            91.2034            99.1341\n",
       "48       20053         132.0416         132.7882         140.8626         135.3187         135.0616         160.6608         127.9117         128.2995         129.9756          140.8309          144.4483          131.3959          153.9606          115.6361          174.7380          138.5215          111.4641          136.1335          118.6320          134.4414          126.7995          129.0107          137.7684          142.7227          145.9864          128.3855          152.0441          121.6167          108.5741          161.4697          113.2184          156.2361          138.7272          132.2482          135.8226          157.4240          130.5868          114.9628          132.9163          131.0001          113.0482          124.2073          124.7968          113.6061          114.4457          120.0572          121.5379          135.9194          130.4676          151.9247          131.6374          123.4246          121.3720          124.1064          126.8826          141.1670          125.7933          121.0401          126.0408           133.8082           128.2991           123.1400           130.5391           139.4070           133.2731           134.3072           125.2901           142.2289           131.7277           130.9304           122.6867           128.4353           115.0190           124.1746           136.2718           130.6638           124.0992           129.5070           128.3534           127.4235           119.5607           128.0194           135.4098           126.9290           126.8239           121.3445           123.9018           130.0178           120.1516           130.4037           128.0436           130.1602           122.3631           131.9648           138.1054           127.5019           127.0401           130.4993           125.2894           131.0698\n",
       "49       20054         117.9040         123.2108         120.7178         130.6527         133.3665         147.0044         131.9242         122.9216         135.3021          121.3544          146.1642          129.0535          133.1986          115.7139          158.9760          115.6026          118.5695          134.8093          116.6896          118.7757          137.9322          123.8890          128.3514          130.3001          143.4286          127.3174          144.2410          124.3654          115.8465          150.7732          109.5424          151.4449          118.3913          120.9799          135.1488          158.8624          125.4758          114.8463          137.5673          127.0795          107.6005          125.9004          120.5937          120.2987          110.6240          117.5954          120.9809          130.6761          132.6544          152.2041          114.2780          111.6657          118.2952          122.0377          114.6504          115.1568          115.9043          121.8359          120.7450           131.6093           115.4693           125.9198           117.1168           123.6936           125.1949           118.6674           110.2873           114.6969           121.9140           120.1840           125.9304           114.2688           114.3269           120.6738           117.6424           121.9007           115.0596           127.5703           101.6153           122.4371           113.6709           117.1318           115.9102           120.7083           124.7338           114.8774           117.6280           123.1530           112.3609           112.7786           115.7527           125.7410           126.5696           121.2177           117.2584           126.5743           113.3522           112.4231           124.7077           109.9032"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "display(df_preds_final.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "809017cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_656/1127774767.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['mlp_median'] = df_preds_final[mlp_cols].median(axis=1)\n",
      "/tmp/ipykernel_656/1127774767.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['lgbm_median'] = df_preds_final[lgbm_cols].median(axis=1)\n",
      "/tmp/ipykernel_656/1127774767.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['ensemble_median'] = df_preds_final[mlp_cols + lgbm_cols].median(axis=1)\n",
      "/tmp/ipykernel_656/1127774767.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['mlp_mean'] = df_preds_final[mlp_cols].mean(axis=1)\n",
      "/tmp/ipykernel_656/1127774767.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['lgbm_mean'] = df_preds_final[lgbm_cols].mean(axis=1)\n",
      "/tmp/ipykernel_656/1127774767.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['ensemble_mean'] = df_preds_final[mlp_cols + lgbm_cols].mean(axis=1)\n",
      "/tmp/ipykernel_656/1127774767.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['mlp_std'] = df_preds_final[mlp_cols].std(axis=1)\n",
      "/tmp/ipykernel_656/1127774767.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['lgbm_std'] = df_preds_final[lgbm_cols].std(axis=1)\n",
      "/tmp/ipykernel_656/1127774767.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_preds_final['ensemble_std'] = df_preds_final[mlp_cols + lgbm_cols].std(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Columnas de predicciÃ³n\n",
    "\n",
    "mlp_cols = [c for c in df_preds_final.columns if c.startswith('mlp_pred_ORIG_')]\n",
    "lgbm_cols = [c for c in df_preds_final.columns if c.startswith('lgbm_pred_ORIG_')]\n",
    "\n",
    "df_preds_final['mlp_median'] = df_preds_final[mlp_cols].median(axis=1)\n",
    "df_preds_final['lgbm_median'] = df_preds_final[lgbm_cols].median(axis=1)\n",
    "df_preds_final['ensemble_median'] = df_preds_final[mlp_cols + lgbm_cols].median(axis=1)\n",
    "\n",
    "# Medias (nuevo)\n",
    "df_preds_final['mlp_mean'] = df_preds_final[mlp_cols].mean(axis=1)\n",
    "df_preds_final['lgbm_mean'] = df_preds_final[lgbm_cols].mean(axis=1)\n",
    "df_preds_final['ensemble_mean'] = df_preds_final[mlp_cols + lgbm_cols].mean(axis=1)\n",
    "\n",
    "# DesvÃ­o estÃ¡ndar\n",
    "df_preds_final['mlp_std'] = df_preds_final[mlp_cols].std(axis=1)\n",
    "df_preds_final['lgbm_std'] = df_preds_final[lgbm_cols].std(axis=1)\n",
    "df_preds_final['ensemble_std'] = df_preds_final[mlp_cols + lgbm_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3ac36d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PRODUCT_ID  mlp_pred_ORIG_1  mlp_pred_ORIG_2  mlp_pred_ORIG_3  mlp_pred_ORIG_4  mlp_pred_ORIG_5  mlp_pred_ORIG_6  mlp_pred_ORIG_7  mlp_pred_ORIG_8  mlp_pred_ORIG_9  mlp_pred_ORIG_10  mlp_pred_ORIG_11  mlp_pred_ORIG_12  mlp_pred_ORIG_13  mlp_pred_ORIG_14  mlp_pred_ORIG_15  mlp_pred_ORIG_16  mlp_pred_ORIG_17  mlp_pred_ORIG_18  mlp_pred_ORIG_19  mlp_pred_ORIG_20  mlp_pred_ORIG_21  mlp_pred_ORIG_22  mlp_pred_ORIG_23  mlp_pred_ORIG_24  mlp_pred_ORIG_25  mlp_pred_ORIG_26  mlp_pred_ORIG_27  mlp_pred_ORIG_28  mlp_pred_ORIG_29  mlp_pred_ORIG_30  mlp_pred_ORIG_31  mlp_pred_ORIG_32  mlp_pred_ORIG_33  mlp_pred_ORIG_34  mlp_pred_ORIG_35  mlp_pred_ORIG_36  mlp_pred_ORIG_37  mlp_pred_ORIG_38  mlp_pred_ORIG_39  mlp_pred_ORIG_40  mlp_pred_ORIG_41  mlp_pred_ORIG_42  mlp_pred_ORIG_43  mlp_pred_ORIG_44  mlp_pred_ORIG_45  mlp_pred_ORIG_46  mlp_pred_ORIG_47  mlp_pred_ORIG_48  mlp_pred_ORIG_49  mlp_pred_ORIG_50  lgbm_pred_ORIG_1  lgbm_pred_ORIG_2  lgbm_pred_ORIG_3  lgbm_pred_ORIG_4  lgbm_pred_ORIG_5  lgbm_pred_ORIG_6  lgbm_pred_ORIG_7  lgbm_pred_ORIG_8  lgbm_pred_ORIG_9  lgbm_pred_ORIG_10  lgbm_pred_ORIG_11  lgbm_pred_ORIG_12  lgbm_pred_ORIG_13  lgbm_pred_ORIG_14  lgbm_pred_ORIG_15  lgbm_pred_ORIG_16  lgbm_pred_ORIG_17  lgbm_pred_ORIG_18  lgbm_pred_ORIG_19  lgbm_pred_ORIG_20  lgbm_pred_ORIG_21  lgbm_pred_ORIG_22  lgbm_pred_ORIG_23  lgbm_pred_ORIG_24  lgbm_pred_ORIG_25  lgbm_pred_ORIG_26  lgbm_pred_ORIG_27  lgbm_pred_ORIG_28  lgbm_pred_ORIG_29  lgbm_pred_ORIG_30  lgbm_pred_ORIG_31  lgbm_pred_ORIG_32  lgbm_pred_ORIG_33  lgbm_pred_ORIG_34  lgbm_pred_ORIG_35  lgbm_pred_ORIG_36  lgbm_pred_ORIG_37  lgbm_pred_ORIG_38  lgbm_pred_ORIG_39  lgbm_pred_ORIG_40  lgbm_pred_ORIG_41  lgbm_pred_ORIG_42  lgbm_pred_ORIG_43  lgbm_pred_ORIG_44  lgbm_pred_ORIG_45  lgbm_pred_ORIG_46  lgbm_pred_ORIG_47  lgbm_pred_ORIG_48  lgbm_pred_ORIG_49  lgbm_pred_ORIG_50  mlp_median  lgbm_median  ensemble_median  mlp_mean  lgbm_mean  ensemble_mean   mlp_std  lgbm_std  ensemble_std\n",
      "0         20001        2499.2090        5263.2959         804.5912         727.6192        1559.3298        2911.9277        7580.3813         932.1174        2122.7937         2514.7844         4620.2842         2722.4951         4365.8901         1492.4357         4894.0991         1080.9789          728.9855         3817.6021         2046.8887         1205.3357         1795.4785          917.8847         2162.9702         2008.6067         3880.3560         7916.3921         2108.4348        14830.9072         3071.4585         3162.0374         4540.4404         4199.3940          627.3073         3711.7502        10808.7129         4010.7842         2787.4878         1962.2367         3214.1221         3062.6428         3114.7310         1005.9449         4292.7256         2789.5337          227.9098         1334.6522         1259.5654         1528.6150         2745.5435         1266.5941          935.1566          924.6894          902.1176          848.3029          847.1355         1157.0333          886.3787          840.6570          907.4833           997.5659           939.1530          1088.7173          1124.7459           960.0096           952.6442           912.2195          1110.8863           869.8642          1243.7613           897.8728           896.6645           954.6576           885.1398           913.2840           889.9761           890.7729           996.9416           879.9595          1193.1274          1006.1139           995.2010           881.5662           982.9409           833.5339           913.6344           822.5711           847.4863          1046.9595           949.3221           906.5048           845.7527           923.6794          1223.6973           906.0816          1274.6605          1021.0181          1118.0922           866.9102           870.2219          1073.2910   2618.6396     918.6569        1084.8481 3084.7244   963.1231      2023.9242 2627.0339  115.3843     2135.1941\n",
      "1         20002         473.9576        2823.5747         600.2195         562.8087         814.6390         766.4780        1546.3629         697.9310        1351.1823          861.1847         1697.4403          871.4789          833.1289          986.6110         2509.4326          485.7554          377.7762         1358.7043          708.0589          702.3293          704.1918          505.4628         1177.1813         1257.8733         1018.1789         2839.2078          907.6165         1758.9507         1266.1028         1861.8225         1645.7792         1898.1858          374.1505         1872.4506         2554.7678         2109.0835         1151.9299         1068.7759         1398.7549          735.1407          438.5002          396.7874         1140.4252         1037.7700          721.7929          469.1016          733.5745          606.8829         1220.3689          514.1923          981.6201          939.5924          925.6344          905.8875          857.1993         1066.8393          858.0403          862.9338          933.1881           996.1841           986.4728           987.0411           990.3274           976.5534           974.2095           905.7628          1023.4710           897.3699           981.4370           918.8154           909.6917           950.0992           853.5017           940.0597           884.6668           912.2222           967.2535           929.4169          1016.0062          1004.1714           931.8859           900.4557           908.7761           864.7181           918.5266           830.9508           943.2141           990.5508           966.8951           951.4487           890.2561           935.1005          1031.1569           977.4998          1042.5057          1091.4635          1072.1501           882.6033           903.1474          1013.8543    947.1138     939.8261         939.8261 1128.2811   947.6566      1037.9689  648.0220   60.7561      466.8095\n",
      "2         20003         883.6436        1229.2775         341.9880         209.2350         570.0003         709.4800        1465.1033         475.2508         871.2306          771.3541         1210.7092          844.2320         1276.0896          455.5036         1272.2114          446.6269          334.5750         1082.2994          741.9630          457.5691          783.3557          551.6803          694.3302          812.8923          917.5224         1999.0488          685.9417         1526.1064          879.1579          939.0835          786.8970         1354.7971          190.1784         1385.3892         1266.3289          974.0811          875.1936          683.4855         1270.4812          581.8795          615.1618          380.8811          654.5162          726.2853          215.2493          453.0035          521.6562          361.4240          925.2378          486.3553          708.5550          722.1337          674.7821          724.4121          709.4367          692.6146          684.6285          657.6063          706.7446           637.2066           705.5091           736.1898           746.9853           700.4674           711.8722           720.9611           729.5378           696.0280           724.3710           701.2526           716.6207           685.6679           675.9561           710.5982           666.4150           702.5856           739.6281           706.2956           811.9448           655.1595           711.2714           692.4726           682.4361           702.6108           745.0326           693.8825           707.0578           633.3803           705.9305           699.2798           687.3723           691.9151           692.6686           728.2671           800.5936           706.6698           666.8135           663.5753           697.2887           770.3446    756.6586     704.0600         706.4827  802.9189   704.8206       753.8697  388.0083   33.9816      278.4179\n",
      "3         20004         278.6172         553.0844         234.8324         168.0436         327.3300         299.1143         578.9184         247.4313         458.3942          403.2302          510.4264          373.7491          395.3320          238.5614          652.7551          211.9489          193.3931          393.5305          360.6262          254.4985          268.7941          268.4264          382.7681          442.2457          432.5879          784.3494          415.4915          459.0695          418.1459          438.2862          333.4021          481.8074          130.5738          504.7629          583.4701          593.1573          393.1740          351.0249          522.1809          213.8898          226.8187          211.4141          351.3539          363.8842          219.0842          218.6337          279.0984          174.0665          399.5264          230.4446          535.1559          559.8694          487.0127          480.5612          510.9151          446.1754          518.0812          515.1366          510.9783           499.3051           484.9185           512.9588           515.7613           516.3664           501.8349           521.4044           496.8330           514.3620           512.5281           523.5468           530.0845           523.0726           502.9984           507.1017           505.5286           536.1644           531.2275           520.1293           452.3890           496.4564           481.8401           513.1341           503.8631           508.4100           500.0813           547.9576           518.5657           498.6645           504.4555           485.1947           502.6226           504.4424           518.0172           525.8581           424.3959           518.2056           514.8020           506.9468           486.6326           472.6493    362.2552     509.6626         486.8227  364.5149   506.1119       435.3135  140.8418   23.7860      123.1304\n",
      "4         20005         429.0883         734.8737         311.1428         168.2841         453.6467         397.8544         835.8878         332.2200         607.6149          551.0172          783.7721          482.9267          459.3329          344.3236          837.6432          236.5896          237.6687          517.5449          451.0930          364.5672          295.2185          421.9180          478.5123          497.4446          658.9768         1209.5479          543.8024          561.8757          674.3246          540.5536          402.4775          668.4355          174.1039          679.2991          872.0078          937.3920          551.2717          471.9512          673.8182          229.7278          289.0652          284.1077          480.9610          471.0372          209.6224          287.6818          363.4773          226.2050          443.1060          291.9259          544.1441          583.5600          568.0413          527.3896          559.0232          546.1975          569.9907          542.7089          542.6554           552.5827           527.6042           572.3730           522.8171           573.9738           556.4121           547.5127           594.7303           533.1944           599.2221           544.0785           557.6838           552.0713           498.2050           567.1273           519.3687           558.8593           534.6386           599.9881           535.1184           533.3071           559.5628           538.4627           512.5910           561.0331           547.2715           568.3845           528.5830           571.7185           527.1043           528.0528           516.6702           562.3251           566.8520           523.2167           530.9225           563.7212           562.6040           551.5517           569.3718           534.2661    465.1850     549.5322         539.5082  488.5388   549.7769       519.1579  218.3652   22.4186      157.4694\n",
      "..          ...              ...              ...              ...              ...              ...              ...              ...              ...              ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...               ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...                ...         ...          ...              ...       ...        ...            ...       ...       ...           ...\n",
      "775       21263          -0.0595          -0.0733          -0.1052          -0.0467          -0.0761           0.1612          -0.1122          -0.0046          -0.0760           -0.1786           -0.0336           -0.0441            0.0246            0.2045           -0.0634           -0.0923            0.0231            0.0491            0.0973           -0.0112            0.0543           -0.1182           -0.0851           -0.1074           -0.0651           -0.0831           -0.0901            0.4045            0.2320           -0.0954           -0.0076           -0.0073            0.0776           -0.1391            0.0307           -0.1195            0.0144            0.0062            0.0368           -0.0777           -0.0308           -0.0125            0.1315            0.0341            0.0320            0.1199           -0.1719           -0.1746            0.0530            0.1674           -0.0016            0.0071            0.0066            0.0596            0.0504           -0.0801            0.0225            0.0439            0.0105             0.0204             0.0096             0.0287            -0.0148             0.0268             0.0425             0.0424             0.0712             0.0412             0.0004             0.0298             0.0391             0.0300             0.0374            -0.0079             0.0550             0.0603             0.0430             0.0470             0.0107             0.0356             0.0269             0.0480             0.0701             0.0144             0.0252             0.0498             0.0467             0.0159             0.0128             0.0435             0.0240             0.0433             0.0132             0.0136            -0.0268             0.0343             0.0268             0.0157             0.0461             0.0212     -0.0217       0.0278           0.0208   -0.0082     0.0266         0.0092    0.1131    0.0260        0.0835\n",
      "776       21265           0.1242           0.1121           0.0848           0.1317          -0.0130           0.2923           0.1583           0.1167           0.1512           -0.0054            0.0658            0.0987            0.2951            0.2403           -0.0230            0.2306            0.0994            0.1599            0.3129            0.1841            0.1534            0.0903            0.2398            0.2516            0.0115            0.0481            0.0347            0.7141            0.2884           -0.0912            0.1193           -0.0269            0.2381            0.0113            0.3446           -0.0205            0.2087            0.1710            0.1174            0.0247            0.0599            0.1297            0.3619            0.0870            0.1846            0.2354           -0.0369           -0.0168            0.2609            0.2940            0.0361            0.0723            0.0559            0.0582            0.1033            0.0305            0.0539            0.0477            0.0555             0.0394             0.0403             0.0404             0.0354             0.0442             0.0504             0.0430             0.0263             0.0645            -0.0135             0.0487             0.0366             0.0334             0.0487             0.0321             0.0827             0.0383             0.0622             0.0411             0.0353             0.0326            -0.0022             0.0490             0.0689             0.0353             0.0480             0.0703             0.0761             0.0438             0.0336             0.0646             0.0711             0.0437             0.0437             0.0531             0.0228             0.0432             0.0238             0.0208             0.0653             0.0169      0.1270       0.0437           0.0557    0.1461     0.0453         0.0957    0.1394    0.0203        0.1113\n",
      "777       21266           0.1030           0.0853           0.0548           0.1231          -0.0317           0.2401           0.0987           0.0924           0.1349           -0.0587            0.0452            0.0590            0.2716            0.2284           -0.0398            0.1786            0.0693            0.1314            0.2947            0.1409            0.0911            0.0496            0.1720            0.2033           -0.0217            0.0271           -0.0009            0.6730            0.2635           -0.1135            0.1019           -0.0492            0.2070            0.0022            0.2980           -0.0549            0.1751            0.1556            0.1053            0.0131            0.0559            0.1056            0.3023            0.0601            0.1656            0.2086           -0.0577           -0.0641            0.2432            0.2367            0.0267            0.0693            0.0503            0.0509            0.0996            0.0587            0.0511            0.0538            0.0541             0.0340             0.0417             0.0172             0.0350             0.0397             0.0525             0.0352             0.0371             0.0623            -0.0042             0.0422             0.0290             0.0318             0.0487             0.0220             0.0645             0.0361             0.0588             0.0482             0.0343             0.0338             0.0080             0.0476             0.0625             0.0355             0.0466             0.0695             0.0693             0.0419             0.0320             0.0622             0.0528             0.0469             0.0570             0.0481             0.0615             0.0333             0.0251             0.0238             0.0620             0.0193      0.1024       0.0468           0.0533    0.1155     0.0444         0.0799    0.1352    0.0181        0.1024\n",
      "778       21267           0.2364           0.2701           0.0396           0.1421          -0.0255           0.2471           0.1036           0.2071           0.1486           -0.0126            0.0325            0.0128            0.3375            0.2460            0.0507            0.3060            0.0583            0.1393            0.2240            0.1194            0.0783            0.0239            0.0679            0.2683           -0.0384            0.0286            0.0159            0.6643            0.2247            0.0201            0.1627           -0.0099            0.3317           -0.0106            0.3946           -0.0198            0.2358            0.2355            0.1481            0.0353            0.2272            0.0634            0.4064            0.0861            0.3061            0.3417           -0.0476            0.0830            0.3368            0.2207            0.0210            0.0512            0.0252            0.0368            0.0522            0.0048            0.0296            0.0405            0.0167             0.0241             0.0298             0.0214            -0.0113             0.0208             0.0403             0.0271             0.0356             0.0556            -0.0138             0.0367             0.0375             0.0283             0.0229            -0.0044             0.0420             0.0443             0.0387             0.0053             0.0306             0.0238             0.0068             0.0355             0.0351             0.0179             0.0287             0.0561             0.0603            -0.0182            -0.0017             0.0304             0.0304             0.0354             0.0298             0.0097            -0.0202             0.0170             0.0441             0.0099             0.0498             0.0312      0.1407       0.0297           0.0367    0.1553     0.0260         0.0907    0.1469    0.0192        0.1228\n",
      "779       21276           0.2866           0.2802           0.0510           0.1580          -0.0126           0.2714           0.1457           0.2416           0.1623           -0.0041            0.0457            0.0346            0.3450            0.2644            0.0705            0.2674            0.0906            0.1692            0.2481            0.1541            0.0824            0.0459            0.0967            0.2966            0.0028            0.0683            0.0454            0.6990            0.2643            0.0246            0.1923            0.0074            0.3128            0.0258            0.3926            0.0106            0.2627            0.2372            0.1863            0.0644            0.2571            0.0919            0.4601            0.1087            0.3096            0.3613           -0.0405            0.0572            0.3163            0.2285            0.0332            0.0591            0.0175            0.0297            0.0564            0.0339            0.0292            0.0560            0.0223             0.0236             0.0357             0.0337            -0.0052             0.0185             0.0336             0.0294             0.0198             0.0495            -0.0224             0.0389             0.0366             0.0262             0.0288            -0.0022             0.0422             0.0427             0.0318             0.0293             0.0796             0.0187             0.0167             0.0399             0.0366             0.0151             0.0296             0.0538             0.0536            -0.0013             0.0184             0.0377             0.0355             0.0350             0.0086             0.0151            -0.0018             0.0177             0.0238             0.0169             0.0463             0.0310      0.1602       0.0297           0.0425    0.1748     0.0291         0.1019    0.1456    0.0183        0.1266\n",
      "\n",
      "[780 rows x 110 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_preds_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e634a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_final.to_csv('predicciones_finales.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
