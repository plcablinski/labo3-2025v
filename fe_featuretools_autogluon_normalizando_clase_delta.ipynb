{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41eccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import optuna\n",
    "import sqlite3\n",
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cd76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducir memoria autom√°ticamente\n",
    "def optimizar_memoria(df):\n",
    "    for col in df.select_dtypes(include=['int64', 'int32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    for col in df.select_dtypes(include=['float64', 'float32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd72eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar en df_full los product_id, customer_id que solo tienen ceros en TN\n",
    "def buscar_productos_solo_ceros(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    grouped = df.groupby(['PRODUCT_ID', 'CUSTOMER_ID'])['TN'].sum().reset_index()\n",
    "    productos_solo_ceros = grouped[grouped['TN'] == 0]\n",
    "    return productos_solo_ceros\n",
    "\n",
    "productos_solo_ceros = buscar_productos_solo_ceros(df_full)\n",
    "print(f\"üîç Combinaciones PRODUCT_ID + CUSTOMER_ID con TN = 0 en todos sus registros: {len(productos_solo_ceros)}\")\n",
    "\n",
    "# Eliminar del df_full los product_id, customer_id que solo tienen ceros en TN\n",
    "def eliminar_productos_solo_ceros(df: pd.DataFrame, productos_solo_ceros: pd.DataFrame) -> pd.DataFrame:\n",
    "    productos_set = set(zip(productos_solo_ceros['PRODUCT_ID'], productos_solo_ceros['CUSTOMER_ID']))\n",
    "    mask = df.set_index(['PRODUCT_ID', 'CUSTOMER_ID']).index.isin(productos_set)\n",
    "    \n",
    "    cantidad_eliminada = mask.sum()\n",
    "    print(f\"üóëÔ∏è Filas eliminadas de df_full: {cantidad_eliminada:,}\")\n",
    "    \n",
    "    df_filtrado = df[~mask]\n",
    "    return df_filtrado\n",
    "\n",
    "df_full = eliminar_productos_solo_ceros(df_full, productos_solo_ceros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar de df_full las filas donde la columna A_PREDECIR sea 'N'\n",
    "df_full = df_full[df_full['A_PREDECIR'] != 'N']\n",
    "df_full = df_full.drop(columns=['A_PREDECIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd918b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservar las siguientes columnas\n",
    "columns_to_keep = ['PERIODO', 'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE', 'ID_CAT1',\n",
    "       'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID',\n",
    "       'PRODUCT_ID', 'PLAN_PRECIOS_CUIDADOS', 'CUST_REQUEST_QTY',\n",
    "       'CUST_REQUEST_TN', 'TN', 'CLASE', 'CLASE_DELTA',\n",
    "       'ORDINAL', 'ANTIG_CLIENTE',\n",
    "       'ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER']\n",
    "# Filtrar el DataFrame para conservar solo las columnas deseadas \n",
    "df_full = df_full[columns_to_keep]\n",
    "df_full['DIAS_EN_MES'] = pd.to_datetime(df_full['PERIODO'], format='%Y%m').dt.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = optimizar_memoria(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que df_full es un DataFrame de pandas\n",
    "if not isinstance(df_full, pd.DataFrame):\n",
    "    df_full = df_full.to_pandas()\n",
    "\n",
    "# 1. Ordenar correctamente\n",
    "df_full = df_full.sort_values(by=['PRODUCT_ID', 'CUSTOMER_ID', 'ORDINAL'], ascending=True)\n",
    "\n",
    "meses_a_tomar = 24  # N√∫mero de meses para crear los LAGs y DELTAs\n",
    "\n",
    "# 2. Crear los LAGs y DELTAs\n",
    "for lag in range(1, meses_a_tomar):\n",
    "    lag_col = f'TN_LAG_{lag:02d}'\n",
    "    delta_col = f'TN_DELTA_{lag:02d}'\n",
    "\n",
    "    df_full[lag_col] = df_full.groupby(['PRODUCT_ID', 'CUSTOMER_ID'])['TN'].shift(lag)\n",
    "    df_full[delta_col] = df_full['TN'] - df_full[lag_col]\n",
    "\n",
    "# 3. Calcular los porcentajes de cambio como la divisi√≥n de df_full['TN'] / df_full[lag_col]\n",
    "# si df_full[lag_col] = 0, reemplazar por el m√≠nimo valor de TN en el grupo (scalar, no dict)\n",
    "minimos_tn = df_full.groupby(['PRODUCT_ID', 'CUSTOMER_ID'])['TN'].transform('min')\n",
    "delta_pct_cols = {}\n",
    "for lag in range(1, meses_a_tomar):\n",
    "    lag_col = f'TN_LAG_{lag:02d}'\n",
    "    delta_pct_col = f'TN_DELTA_PCT_{lag:02d}'\n",
    "\n",
    "    # Construir la serie de lag reemplazando 0 por el m√≠nimo del grupo (elementwise)\n",
    "    lag_values = df_full[lag_col]\n",
    "    lag_reemplazo = np.where(lag_values == 0, minimos_tn, lag_values)\n",
    "    # Calcular el porcentaje de cambio\n",
    "    df_full[delta_pct_col] = df_full['TN'] / lag_reemplazo\n",
    "\n",
    "    # Reemplazar infinitos y NaN por 0\n",
    "    df_full[delta_pct_col] = df_full[delta_pct_col].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    # Guardar en el diccionario\n",
    "    delta_pct_cols[delta_pct_col] = df_full[delta_pct_col]\n",
    "\n",
    "# 4. Defragmentar para mejorar rendimiento\n",
    "df_full = df_full.copy()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar una columna que indique la diferencia en ORDINAL entre el ORDINAL actual y el ORDINAL anterior donde TN sea mayor a 0\n",
    "# para ese CUSTOMER_ID y PRODUCT_ID\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_grupo(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_parallel(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = 36  # valor inicial\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'].astype('int16')\n",
    "\n",
    "    # Agrupar por cliente y producto\n",
    "    grupos = list(df.groupby(['CUSTOMER_ID', 'PRODUCT_ID']))\n",
    "\n",
    "    # Procesar en paralelo\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_grupo)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    # Concatenar todos los resultados\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_parallel(df_full, n_jobs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b68387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_producto(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_PRODUCT_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_por_producto(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_ID'] = 36\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_ID'] = df['MESES_SIN_COMPRAR_PRODUCT_ID'].astype('int16')\n",
    "\n",
    "    # Agrupar solo por PRODUCT_ID\n",
    "    grupos = list(df.groupby('PRODUCT_ID'))\n",
    "\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_producto)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_por_producto(df_full, n_jobs=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_cliente(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_CUSTOMER_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_por_cliente(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_CUSTOMER_ID'] = 36\n",
    "    df['MESES_SIN_COMPRAR_CUSTOMER_ID'] = df['MESES_SIN_COMPRAR_CUSTOMER_ID'].astype('int16')\n",
    "\n",
    "    grupos = list(df.groupby('CUSTOMER_ID'))\n",
    "\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_cliente)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_por_cliente(df_full, n_jobs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfb7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Lista de campos a analizar\n",
    "campos = [\n",
    "    'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID',\n",
    "    'MESES_SIN_COMPRAR_PRODUCT_ID',\n",
    "    'MESES_SIN_COMPRAR_CUSTOMER_ID'\n",
    "]\n",
    "\n",
    "# Mostrar estad√≠sticas descriptivas\n",
    "print(\"üîç Estad√≠sticas Descriptivas:\\n\")\n",
    "print(df_full[campos].describe().astype(int))\n",
    "\n",
    "# Contar cu√°ntos tienen valor 36 (nunca compr√≥ antes)\n",
    "print(\"\\nüìä Conteo de 36 (sin compra previa):\\n\")\n",
    "for campo in campos:\n",
    "    total = len(df_full)\n",
    "    conteo = (df_full[campo] == 36).sum()\n",
    "    porc = 100 * conteo / total\n",
    "    print(f\"{campo}: {conteo:,} ({porc:.2f}%)\")\n",
    "\n",
    "# Histograma de cada campo\n",
    "for campo in campos:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df_full[campo], bins=range(0, 38), kde=False)\n",
    "    plt.title(f\"Distribuci√≥n de {campo}\")\n",
    "    plt.xlabel(\"Meses desde la √∫ltima compra\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Boxplot comparativo\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_melted = df_full[campos].melt(var_name='campo', value_name='meses')\n",
    "sns.boxplot(data=df_melted, x='campo', y='meses')\n",
    "plt.title(\"Boxplot comparativo de los campos de meses sin compra\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = optimizar_memoria(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar columnas num√©ricas recientes sobre las que vale la pena transformar\n",
    "cols_base = [col for col in df_full.columns if (\n",
    "    (col.startswith('TN_DELTA_') and not col.endswith('_PORC'))\n",
    "    or col.startswith('TN_LAG_')\n",
    "    or col == 'TN'\n",
    "    or col == 'CUST_REQUEST_TN'   \n",
    "    or col == 'CUST_REQUEST_QTY'\n",
    "    or col in ['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID', 'MESES_SIN_COMPRAR_PRODUCT_ID', 'MESES_SIN_COMPRAR_CUSTOMER_ID']\n",
    "    or col in ['ORDINAL', 'ANTIG_CLIENTE', 'ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER']\n",
    ")]\n",
    "print(f\"Columnas base para transformaci√≥n: {cols_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_columna_en_memoria(df, col):\n",
    "    if col not in df.columns:\n",
    "        return None\n",
    "\n",
    "    col_data = df[col].values  # Evita overhead de pandas internamente\n",
    "\n",
    "    # Calcular cada transformaci√≥n usando arrays\n",
    "    df[f'{col}_SQ'] = np.square(col_data)\n",
    "    df[f'{col}_SQRT'] = np.sqrt(np.abs(col_data)) * np.sign(col_data)\n",
    "    df[f'{col}_LOG1P'] = np.log1p(np.abs(col_data)) * np.sign(col_data)\n",
    "\n",
    "    # Liberar arrays intermedios (aunque pandas puede retenerlos por un tiempo)\n",
    "    del col_data\n",
    "    gc.collect()\n",
    "\n",
    "# Procesar columna por columna (sin crear estructuras innecesarias)\n",
    "for col in cols_base:\n",
    "    transformar_columna_en_memoria(df_full, col)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a515075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = optimizar_memoria(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9af34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols_base[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f47e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "#columnas_a_combinar = cols_base[5:]  # Use a list, not a single string\n",
    "# Agregar a columnas_a_combinar PERIODO, ANIO, MES, MES_SIN, MES_COS, TRIMESTRE\n",
    "columnas_a_combinar = ['CUST_REQUEST_QTY','TN','ANTIG_CLIENTE','ANTIG_PRODUCTO','ORDINAL', \n",
    "                       'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE','MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID',\n",
    "                       'MESES_SIN_COMPRAR_PRODUCT_ID', 'MESES_SIN_COMPRAR_CUSTOMER_ID','TN_LAG_01', 'TN_DELTA_01', 'TN_LAG_02', 'TN_DELTA_02',\n",
    "                       'TN_LAG_06', 'TN_DELTA_06','TN_LAG_11', 'TN_DELTA_11', 'TN_LAG_12', 'TN_DELTA_12', 'TN_LAG_13', 'TN_DELTA_13','TN_DELTA_PCT_12']\n",
    "\n",
    "for col1, col2 in combinations(columnas_a_combinar, 2): \n",
    "    df_full[f'{col1}_x_{col2}'] = df_full[col1] * df_full[col2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf402cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = optimizar_memoria(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Columnas despu√©s de transformaciones: {df_full.columns.tolist()}\")\n",
    "print(f\"Forma del DataFrame: {df_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b134a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame a un DataFrame de Polars\n",
    "df_full = pl.from_pandas(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987bf055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrar columnas relevantes\n",
    "columnas_a_normalizar = [\n",
    "    col for col in df_full.columns\n",
    "    if ('TN' in col or 'DELTA' in col or 'CLASE' in col or 'LAG' in col)\n",
    "]\n",
    "\n",
    "# 2. Inicializar DataFrame con combinaciones √∫nicas\n",
    "medias_y_desvios = df_full.select(['PRODUCT_ID', 'CUSTOMER_ID']).unique()\n",
    "\n",
    "# 3. Calcular medias y desv√≠os por columna\n",
    "resultados = []\n",
    "for col in columnas_a_normalizar:\n",
    "    if col in df_full.columns:\n",
    "        resumen = (\n",
    "            df_full\n",
    "            .select(['PRODUCT_ID', 'CUSTOMER_ID', col])\n",
    "            .group_by(['PRODUCT_ID', 'CUSTOMER_ID'])\n",
    "            .agg([\n",
    "                pl.col(col).mean().alias(f'{col}_MEDIA'),\n",
    "                pl.col(col).std().alias(f'{col}_DESVIO')\n",
    "            ])\n",
    "        )\n",
    "        resultados.append(resumen)\n",
    "\n",
    "# 4. Combinar todos los resultados\n",
    "medias_y_desvios = reduce(\n",
    "    lambda df1, df2: df1.join(df2, on=['PRODUCT_ID', 'CUSTOMER_ID'], how='left'), \n",
    "    resultados\n",
    ")\n",
    "# Convertir los nulos en ceros\n",
    "medias_y_desvios = medias_y_desvios.fill_null(0)\n",
    "\n",
    "del resumen, resultados\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef578c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_zscore_con_join(\n",
    "    df_full: pl.DataFrame,\n",
    "    medias_y_desvios: pl.DataFrame,\n",
    "    columnas_a_normalizar: List[str],\n",
    "    conservar_auxiliares: bool = False,\n",
    "    epsilon: float = 1e-6,\n",
    "    clip_value: float = 5.0,\n",
    "    agregar_clip: bool = False\n",
    ") -> pl.DataFrame:\n",
    "    # 1. Armar columnas necesarias\n",
    "    columnas_media = [f'{col}_MEDIA' for col in columnas_a_normalizar if f'{col}_MEDIA' in medias_y_desvios.columns]\n",
    "    columnas_desvio = [f'{col}_DESVIO' for col in columnas_a_normalizar if f'{col}_DESVIO' in medias_y_desvios.columns]\n",
    "    columnas_join = ['PRODUCT_ID', 'CUSTOMER_ID'] + columnas_media + columnas_desvio\n",
    "\n",
    "    # 2. Join\n",
    "    df_aux = medias_y_desvios.select(columnas_join)\n",
    "    df_full = df_full.join(df_aux, on=['PRODUCT_ID', 'CUSTOMER_ID'], how='left')\n",
    "\n",
    "    # 3. Calcular ZSCOREs\n",
    "    zscore_exprs = []\n",
    "    for col in columnas_a_normalizar:\n",
    "        media_col = f\"{col}_MEDIA\"\n",
    "        desvio_col = f\"{col}_DESVIO\"\n",
    "        z_col = f\"{col}_ZSCORE\"\n",
    "        if media_col in df_full.columns and desvio_col in df_full.columns:\n",
    "            expr = (\n",
    "                (pl.col(col) - pl.col(media_col)) /\n",
    "                (pl.col(desvio_col) + epsilon)\n",
    "            ).alias(z_col)\n",
    "            zscore_exprs.append(expr)\n",
    "            print(f\"‚úÖ Normalizando: {col} -> {z_col}\")\n",
    "\n",
    "    df_full = df_full.with_columns(zscore_exprs)\n",
    "\n",
    "    # 4. Clipping (despu√©s de que los zscores existen)\n",
    "    if agregar_clip:\n",
    "        clip_exprs = [\n",
    "            pl.col(f\"{col}_ZSCORE\").clip(-clip_value, clip_value).alias(f\"{col}_ZSCORE_CLIP\")\n",
    "            for col in columnas_a_normalizar\n",
    "            if f\"{col}_ZSCORE\" in df_full.columns\n",
    "        ]\n",
    "        df_full = df_full.with_columns(clip_exprs)\n",
    "\n",
    "    # 5. Eliminar auxiliares si no se quieren\n",
    "    if not conservar_auxiliares:\n",
    "        df_full = df_full.drop(columnas_media + columnas_desvio)\n",
    "\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = normalizar_zscore_con_join(\n",
    "    df_full=df_full,\n",
    "    medias_y_desvios=medias_y_desvios,\n",
    "    columnas_a_normalizar=columnas_a_normalizar,\n",
    "    conservar_auxiliares=False,\n",
    "    epsilon=1e-6,\n",
    "    clip_value=5.0,\n",
    "    agregar_clip=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo el DataFrame resultante de medias y desvios en un archivo parquet para utilizarlo en el futuro\n",
    "# (esto es √∫til para no tener que recalcularlo cada vez)\n",
    "# y para que otros scripts puedan usarlo sin recalcularlo.\n",
    "# Libero memoria de medias_y_desvios \n",
    "medias_y_desvios_pd = medias_y_desvios.to_pandas()\n",
    "medias_y_desvios_pd.to_parquet('./data/medias_y_desvios.parquet', engine='fastparquet', index=False)\n",
    "del medias_y_desvios_pd, medias_y_desvios\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449684fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eliminar las columnas originales de df_full que fueron normalizadas\n",
    "# son las que est√°n en columnas_a_normalizar\n",
    "df_full = df_full.drop(columnas_a_normalizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir de nuevo a DataFrame de Pandas\n",
    "df_full = df_full.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80495a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = optimizar_memoria(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_parquet('./data/tmp_intermedio.parquet', engine='fastparquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e39b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuar desde aqu√≠ si falla\n",
    "df_full = pd.read_parquet('./data/tmp_intermedio.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = optimizar_memoria(df_full)\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C√°lculo de features por grupo ---\n",
    "def calcular_pendientes_grupo(group, periodos_list):\n",
    "    group = group.sort_values(by='PERIODO').copy()\n",
    "    n = len(group)\n",
    "    y_series = pd.Series(group['TN_ZSCORE'].values)\n",
    "\n",
    "    new_cols = {}\n",
    "\n",
    "    for cant in periodos_list:\n",
    "        x = np.arange(cant)\n",
    "        rolling = y_series.rolling(window=cant, min_periods=1)\n",
    "\n",
    "        # Medidas estad√≠sticas\n",
    "        mean_vals = rolling.mean().values\n",
    "        std_vals = rolling.std().values\n",
    "        median_vals = rolling.median().values\n",
    "        min_vals = rolling.min().values\n",
    "        max_vals = rolling.max().values\n",
    "        ewma_vals = y_series.ewm(span=cant, adjust=False).mean().values\n",
    "\n",
    "        new_cols[f'TN_MEAN_ZSCORE_{cant}'] = mean_vals\n",
    "        new_cols[f'TN_STD_ZSCORE_{cant}'] = std_vals\n",
    "        new_cols[f'TN_MEDIAN_ZSCORE_{str(cant).zfill(2)}'] = median_vals\n",
    "        new_cols[f'TN_MIN_ZSCORE_{str(cant).zfill(2)}'] = min_vals\n",
    "        new_cols[f'TN_MAX_ZSCORE_{str(cant).zfill(2)}'] = max_vals\n",
    "        new_cols[f'TN_EWMA_ZSCORE_{str(cant).zfill(2)}'] = ewma_vals\n",
    "\n",
    "        # Pendiente de regresi√≥n lineal\n",
    "        if n >= cant:\n",
    "            y_rolling = np.lib.stride_tricks.sliding_window_view(y_series.values, window_shape=cant)\n",
    "            X = np.vstack([x, np.ones(cant)]).T\n",
    "            XTX_inv_XT = np.linalg.pinv(X)\n",
    "            betas = XTX_inv_XT @ y_rolling.T\n",
    "            pendientes = np.full(n, np.nan)\n",
    "            pendientes[cant - 1:] = betas[0]\n",
    "        else:\n",
    "            pendientes = np.full(n, np.nan)\n",
    "        new_cols[f'PENDIENTE_TENDENCIA_ZSCORE_{cant}'] = pendientes\n",
    "\n",
    "        # Medidas de variabilidad respecto a la media\n",
    "        abs_diff = np.abs(y_series.values - mean_vals)\n",
    "        cv_vals = std_vals / np.where(mean_vals == 0, np.nan, mean_vals)\n",
    "\n",
    "        new_cols[f'TN_ABS_DIFF_MEAN_ZSCORE_{cant}'] = abs_diff\n",
    "        new_cols[f'TN_CV_ZSCORE_{cant}'] = cv_vals\n",
    "\n",
    "    df_features = pd.DataFrame(new_cols, index=group.index)\n",
    "    group = pd.concat([group, df_features], axis=1)\n",
    "    return group\n",
    "\n",
    "# --- Procesar un chunk de grupos ---\n",
    "def procesar_chunk(chunk, periodos_list):\n",
    "    return pd.concat([calcular_pendientes_grupo(g, periodos_list) for g in chunk], ignore_index=True)\n",
    "\n",
    "# --- Paralelizaci√≥n eficiente ---\n",
    "def calcular_pendientes_parallel_optimizado(df, periodos_list, n_jobs=28, chunk_size=100):\n",
    "    df = df.copy()  # conserva todas las columnas originales\n",
    "    grupos = [group for _, group in df.groupby(['PRODUCT_ID', 'CUSTOMER_ID'])]\n",
    "    chunks = list(chunked(grupos, chunk_size))\n",
    "\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', verbose=10)(\n",
    "        delayed(procesar_chunk)(chunk, periodos_list) for chunk in chunks\n",
    "    )\n",
    "\n",
    "    df_final = pd.concat(resultados, ignore_index=True)\n",
    "    return df_final\n",
    "\n",
    "# --- Script principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    df_full = calcular_pendientes_parallel_optimizado(\n",
    "        df_full,\n",
    "        periodos_list=[2, 3, 6, 9, 12, 13,15,18],\n",
    "        n_jobs=28,\n",
    "        chunk_size=200\n",
    "    )\n",
    "\n",
    "    print(f\"Tiempo total: {time.time() - start:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6406924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_parquet('./data/tmp_intermedio.parquet', engine='fastparquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661a19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuar desde aqu√≠ si falla\n",
    "df_full = pd.read_parquet('./data/tmp_intermedio.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57adf11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se eliminar√°n 64 columnas con m√°s del 80% de valores faltantes o infinitos:\n",
      "- TN_LAG_28_ZSCORE: 81.99% (nulls=6379955, infs=0)\n",
      "- TN_DELTA_28_ZSCORE: 81.99% (nulls=6379955, infs=0)\n",
      "- TN_LAG_29_ZSCORE: 84.40% (nulls=6567729, infs=0)\n",
      "- TN_DELTA_29_ZSCORE: 84.40% (nulls=6567729, infs=0)\n",
      "- TN_LAG_30_ZSCORE: 86.77% (nulls=6752113, infs=0)\n",
      "- TN_DELTA_30_ZSCORE: 86.77% (nulls=6752113, infs=0)\n",
      "- TN_LAG_31_ZSCORE: 89.08% (nulls=6932022, infs=0)\n",
      "- TN_DELTA_31_ZSCORE: 89.08% (nulls=6932022, infs=0)\n",
      "- TN_LAG_32_ZSCORE: 91.36% (nulls=7109220, infs=0)\n",
      "- TN_DELTA_32_ZSCORE: 91.36% (nulls=7109220, infs=0)\n",
      "- TN_LAG_33_ZSCORE: 93.62% (nulls=7284849, infs=0)\n",
      "- TN_DELTA_33_ZSCORE: 93.62% (nulls=7284849, infs=0)\n",
      "- TN_LAG_34_ZSCORE: 95.85% (nulls=7458821, infs=0)\n",
      "- TN_DELTA_34_ZSCORE: 95.85% (nulls=7458821, infs=0)\n",
      "- TN_LAG_35_ZSCORE: 98.03% (nulls=7628113, infs=0)\n",
      "- TN_DELTA_35_ZSCORE: 98.03% (nulls=7628113, infs=0)\n",
      "- TN_LAG_28_SQ_ZSCORE: 81.99% (nulls=6379955, infs=0)\n",
      "- TN_LAG_28_SQRT_ZSCORE: 81.99% (nulls=6379955, infs=0)\n",
      "- TN_LAG_28_LOG1P_ZSCORE: 81.99% (nulls=6379955, infs=0)\n",
      "- TN_DELTA_28_SQ_ZSCORE: 81.99% (nulls=6379955, infs=0)\n",
      "- TN_DELTA_28_SQRT_ZSCORE: 81.99% (nulls=6379955, infs=0)\n",
      "- TN_DELTA_28_LOG1P_ZSCORE: 81.99% (nulls=6379955, infs=0)\n",
      "- TN_LAG_29_SQ_ZSCORE: 84.40% (nulls=6567729, infs=0)\n",
      "- TN_LAG_29_SQRT_ZSCORE: 84.40% (nulls=6567729, infs=0)\n",
      "- TN_LAG_29_LOG1P_ZSCORE: 84.40% (nulls=6567729, infs=0)\n",
      "- TN_DELTA_29_SQ_ZSCORE: 84.40% (nulls=6567729, infs=0)\n",
      "- TN_DELTA_29_SQRT_ZSCORE: 84.40% (nulls=6567729, infs=0)\n",
      "- TN_DELTA_29_LOG1P_ZSCORE: 84.40% (nulls=6567729, infs=0)\n",
      "- TN_LAG_30_SQ_ZSCORE: 86.77% (nulls=6752113, infs=0)\n",
      "- TN_LAG_30_SQRT_ZSCORE: 86.77% (nulls=6752113, infs=0)\n",
      "- TN_LAG_30_LOG1P_ZSCORE: 86.77% (nulls=6752113, infs=0)\n",
      "- TN_DELTA_30_SQ_ZSCORE: 86.77% (nulls=6752113, infs=0)\n",
      "- TN_DELTA_30_SQRT_ZSCORE: 86.77% (nulls=6752113, infs=0)\n",
      "- TN_DELTA_30_LOG1P_ZSCORE: 86.77% (nulls=6752113, infs=0)\n",
      "- TN_LAG_31_SQ_ZSCORE: 89.08% (nulls=6932022, infs=0)\n",
      "- TN_LAG_31_SQRT_ZSCORE: 89.08% (nulls=6932022, infs=0)\n",
      "- TN_LAG_31_LOG1P_ZSCORE: 89.08% (nulls=6932022, infs=0)\n",
      "- TN_DELTA_31_SQ_ZSCORE: 89.08% (nulls=6932022, infs=0)\n",
      "- TN_DELTA_31_SQRT_ZSCORE: 89.08% (nulls=6932022, infs=0)\n",
      "- TN_DELTA_31_LOG1P_ZSCORE: 89.08% (nulls=6932022, infs=0)\n",
      "- TN_LAG_32_SQ_ZSCORE: 91.36% (nulls=7109220, infs=0)\n",
      "- TN_LAG_32_SQRT_ZSCORE: 91.36% (nulls=7109220, infs=0)\n",
      "- TN_LAG_32_LOG1P_ZSCORE: 91.36% (nulls=7109220, infs=0)\n",
      "- TN_DELTA_32_SQ_ZSCORE: 91.36% (nulls=7109220, infs=0)\n",
      "- TN_DELTA_32_SQRT_ZSCORE: 91.36% (nulls=7109220, infs=0)\n",
      "- TN_DELTA_32_LOG1P_ZSCORE: 91.36% (nulls=7109220, infs=0)\n",
      "- TN_LAG_33_SQ_ZSCORE: 93.62% (nulls=7284849, infs=0)\n",
      "- TN_LAG_33_SQRT_ZSCORE: 93.62% (nulls=7284849, infs=0)\n",
      "- TN_LAG_33_LOG1P_ZSCORE: 93.62% (nulls=7284849, infs=0)\n",
      "- TN_DELTA_33_SQ_ZSCORE: 93.62% (nulls=7284849, infs=0)\n",
      "- TN_DELTA_33_SQRT_ZSCORE: 93.62% (nulls=7284849, infs=0)\n",
      "- TN_DELTA_33_LOG1P_ZSCORE: 93.62% (nulls=7284849, infs=0)\n",
      "- TN_LAG_34_SQ_ZSCORE: 95.85% (nulls=7458821, infs=0)\n",
      "- TN_LAG_34_SQRT_ZSCORE: 95.85% (nulls=7458821, infs=0)\n",
      "- TN_LAG_34_LOG1P_ZSCORE: 95.85% (nulls=7458821, infs=0)\n",
      "- TN_DELTA_34_SQ_ZSCORE: 95.85% (nulls=7458821, infs=0)\n",
      "- TN_DELTA_34_SQRT_ZSCORE: 95.85% (nulls=7458821, infs=0)\n",
      "- TN_DELTA_34_LOG1P_ZSCORE: 95.85% (nulls=7458821, infs=0)\n",
      "- TN_LAG_35_SQ_ZSCORE: 98.03% (nulls=7628113, infs=0)\n",
      "- TN_LAG_35_SQRT_ZSCORE: 98.03% (nulls=7628113, infs=0)\n",
      "- TN_LAG_35_LOG1P_ZSCORE: 98.03% (nulls=7628113, infs=0)\n",
      "- TN_DELTA_35_SQ_ZSCORE: 98.03% (nulls=7628113, infs=0)\n",
      "- TN_DELTA_35_SQRT_ZSCORE: 98.03% (nulls=7628113, infs=0)\n",
      "- TN_DELTA_35_LOG1P_ZSCORE: 98.03% (nulls=7628113, infs=0)\n"
     ]
    }
   ],
   "source": [
    "# Umbral: proporci√≥n m√°xima permitida (ej. 0.4 = 40%)\n",
    "umbral_faltantes = 0.8\n",
    "\n",
    "# Diccionario para almacenar estad√≠sticas\n",
    "estadisticas_columnas = []\n",
    "\n",
    "# Recorremos las columnas del DataFrame\n",
    "for col in df_full.columns:\n",
    "    total = len(df_full[col])\n",
    "    nulls = df_full[col].isnull().sum()\n",
    "    nans = df_full[col].isna().sum()\n",
    "    infs = np.isinf(df_full[col]).sum()\n",
    "    \n",
    "    total_faltantes = nulls + infs  # NaN est√° incluido en nulls/isna\n",
    "    \n",
    "    porcentaje = total_faltantes / total\n",
    "    \n",
    "    estadisticas_columnas.append({\n",
    "        'columna': col,\n",
    "        'nulls': nulls,\n",
    "        'NaNs': nans,\n",
    "        'infs': infs,\n",
    "        'porcentaje_faltantes': porcentaje\n",
    "    })\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_faltantes = pd.DataFrame(estadisticas_columnas)\n",
    "\n",
    "# Filtrar columnas que superen el umbral\n",
    "columnas_a_eliminar = df_faltantes[df_faltantes['porcentaje_faltantes'] > umbral_faltantes]\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\nSe eliminar√°n {len(columnas_a_eliminar)} columnas con m√°s del {umbral_faltantes*100:.0f}% de valores faltantes o infinitos:\")\n",
    "for _, row in columnas_a_eliminar.iterrows():\n",
    "    print(f\"- {row['columna']}: {row['porcentaje_faltantes']*100:.2f}% (nulls={row['nulls']}, infs={row['infs']})\")\n",
    "\n",
    "# Eliminar columnas del DataFrame\n",
    "df_full = df_full.drop(columns=columnas_a_eliminar['columna'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2761d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar a df_resultado una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "df_full['MES_PROBLEMATICO'] = df_full['PERIODO'].apply(lambda x: True if x in [201906, 201908] else False)\n",
    "df_full['PLAN_PRECIOS_CUIDADOS'] = df_full['PLAN_PRECIOS_CUIDADOS'].map({1 : True, 0: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d7c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = optimizar_memoria(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5652d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame resultante en un archivo parquet\n",
    "df_full.to_parquet('./data/l_vm_completa_normalizada_fe.parquet', engine='fastparquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaboIII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
