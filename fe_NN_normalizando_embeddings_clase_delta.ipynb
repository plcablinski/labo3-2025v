{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a41eccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9628af",
   "metadata": {},
   "source": [
    "El objetivo de este Notebook es preparar los datos para modelar con redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9cd76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducir memoria automáticamente\n",
    "def optimizar_memoria(df):\n",
    "    for col in df.select_dtypes(include=['int64', 'int32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    for col in df.select_dtypes(include=['float64', 'float32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d1dd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88dd72eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17021654, 76)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ccba47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Combinaciones PRODUCT_ID + CUSTOMER_ID con TN = 0 en todos sus registros: 327068\n",
      "🗑️ Filas eliminadas de df_full: 6,594,430\n"
     ]
    }
   ],
   "source": [
    "# Buscar en df_full los product_id, customer_id que solo tienen ceros en TN\n",
    "def buscar_productos_solo_ceros(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    grouped = df.groupby(['PRODUCT_ID', 'CUSTOMER_ID'])['TN'].sum().reset_index()\n",
    "    productos_solo_ceros = grouped[grouped['TN'] == 0]\n",
    "    return productos_solo_ceros\n",
    "\n",
    "productos_solo_ceros = buscar_productos_solo_ceros(df_full)\n",
    "print(f\"🔍 Combinaciones PRODUCT_ID + CUSTOMER_ID con TN = 0 en todos sus registros: {len(productos_solo_ceros)}\")\n",
    "\n",
    "# Eliminar del df_full los product_id, customer_id que solo tienen ceros en TN\n",
    "def eliminar_productos_solo_ceros(df: pd.DataFrame, productos_solo_ceros: pd.DataFrame) -> pd.DataFrame:\n",
    "    productos_set = set(zip(productos_solo_ceros['PRODUCT_ID'], productos_solo_ceros['CUSTOMER_ID']))\n",
    "    mask = df.set_index(['PRODUCT_ID', 'CUSTOMER_ID']).index.isin(productos_set)\n",
    "    \n",
    "    cantidad_eliminada = mask.sum()\n",
    "    print(f\"🗑️ Filas eliminadas de df_full: {cantidad_eliminada:,}\")\n",
    "    \n",
    "    df_filtrado = df[~mask]\n",
    "    return df_filtrado\n",
    "\n",
    "df_full = eliminar_productos_solo_ceros(df_full, productos_solo_ceros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d03193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar de df_full las filas donde la columna A_PREDECIR sea 'N'\n",
    "df_full = df_full[df_full['A_PREDECIR'] != 'N']\n",
    "df_full = df_full.drop(columns=['A_PREDECIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd918b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7781619, 75)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e0fd77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Cantidad de filas por PERIODO:\n",
      "PERIODO\n",
      "201701    153506\n",
      "201702    169292\n",
      "201703    173972\n",
      "201704    175629\n",
      "201705    177198\n",
      "201706    179909\n",
      "201707    184384\n",
      "201708    187774\n",
      "201709    190994\n",
      "201710    195034\n",
      "201711    201377\n",
      "201712    201427\n",
      "201801    201965\n",
      "201802    202278\n",
      "201803    203591\n",
      "201804    207881\n",
      "201805    212721\n",
      "201806    213622\n",
      "201807    215649\n",
      "201808    216883\n",
      "201809    222730\n",
      "201810    226473\n",
      "201811    229728\n",
      "201812    230059\n",
      "201901    230063\n",
      "201902    230976\n",
      "201903    235674\n",
      "201904    244108\n",
      "201905    247448\n",
      "201906    251593\n",
      "201907    256940\n",
      "201908    260099\n",
      "201909    262500\n",
      "201910    262616\n",
      "201911    262721\n",
      "201912    262805\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Por cada PERIODO mostrar la cantidad de filas\n",
    "periodo_counts = df_full['PERIODO'].value_counts().sort_index()\n",
    "print(\"📅 Cantidad de filas por PERIODO:\")\n",
    "print(periodo_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "355a135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar una columna que indique la diferencia en ORDINAL entre el ORDINAL actual y el ORDINAL anterior donde TN sea mayor a 0\n",
    "# para ese CUSTOMER_ID y PRODUCT_ID\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_grupo(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_parallel(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = 36  # valor inicial\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'].astype('int16')\n",
    "\n",
    "    # Agrupar por cliente y producto\n",
    "    grupos = list(df.groupby(['CUSTOMER_ID', 'PRODUCT_ID']))\n",
    "\n",
    "    # Procesar en paralelo\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_grupo)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    # Concatenar todos los resultados\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_parallel(df_full, n_jobs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8275d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_producto(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_PRODUCT_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_por_producto(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_ID'] = 36\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_ID'] = df['MESES_SIN_COMPRAR_PRODUCT_ID'].astype('int16')\n",
    "\n",
    "    # Agrupar solo por PRODUCT_ID\n",
    "    grupos = list(df.groupby('PRODUCT_ID'))\n",
    "\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_producto)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_por_producto(df_full, n_jobs=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "651a8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_cliente(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_CUSTOMER_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_por_cliente(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_CUSTOMER_ID'] = 36\n",
    "    df['MESES_SIN_COMPRAR_CUSTOMER_ID'] = df['MESES_SIN_COMPRAR_CUSTOMER_ID'].astype('int16')\n",
    "\n",
    "    grupos = list(df.groupby('CUSTOMER_ID'))\n",
    "\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_cliente)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_por_cliente(df_full, n_jobs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba02f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "# Calcular los días del mes usando las columnas ANIO y MES\n",
    "\n",
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si ANIO==2019 y MES en [6, 8, 10], y 0 en caso contrario\n",
    "df_full['MES_PROBLEMATICO'] = np.where(\n",
    "       (df_full['ANIO'] == 2019) & (df_full['MES'].isin([6, 8, 10])),\n",
    "       1., 0.0\n",
    ")\n",
    "df_full['MES_PROBLEMATICO'] = df_full['MES_PROBLEMATICO'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f26846e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame en parquet\n",
    "df_full.to_parquet('./data/interm_NN_TORCH.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c59f2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conservar las siguientes columnas\n",
    "columns_to_keep = ['MES_SIN', 'MES_COS', 'ID_CAT1',\n",
    "       'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID',\n",
    "       'PRODUCT_ID', 'CUST_REQUEST_QTY',\n",
    "       'CUST_REQUEST_TN', 'TN', 'CLASE_DELTA',\n",
    "       'ANTIG_CLIENTE','ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER',\n",
    "       'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID','MESES_SIN_COMPRAR_PRODUCT_ID','MESES_SIN_COMPRAR_CUSTOMER_ID',\n",
    "       'MES_PROBLEMATICO','PERIODO','ORDINAL']\n",
    "# Filtrar el DataFrame para conservar solo las columnas deseadas \n",
    "df_full = df_full[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3398b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_signed(x):\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "df_full['CLASE_DELTA_LOG1P'] = log_transform_signed(df_full['CLASE_DELTA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0b395843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de CLASE_DELTA_LOG1P_Z: -0.0002512964473318228\n",
      "Desviación estándar de CLASE_DELTA_LOG1P_Z: 0.2535427832171512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "df_full['CLASE_DELTA_LOG1P_Z'] = scaler_y.fit_transform(df_full[['CLASE_DELTA_LOG1P']])\n",
    "\n",
    "media_y = scaler_y.mean_[0]\n",
    "std_y = scaler_y.scale_[0]\n",
    "\n",
    "print(f\"Media de CLASE_DELTA_LOG1P_Z: {media_y}\")\n",
    "print(f\"Desviación estándar de CLASE_DELTA_LOG1P_Z: {std_y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "597da50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_clase_delta_log1p_z(pred_z, media = media_y, std = std_y):\n",
    "    pred_log1p = pred_z * std + media\n",
    "    return np.sign(pred_log1p) * (np.expm1(np.abs(pred_log1p)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca8671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas CLASE_DELTA y CLASE_DELTA_LOG1P para evitar confusiones\n",
    "df_full.drop(columns=['CLASE_DELTA', 'CLASE_DELTA_LOG1P'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc77790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE']\n",
    "\n",
    "for col in cat_cols:\n",
    "    if df_full[col].isnull().any():\n",
    "        df_full[col] = df_full[col].fillna(\"missing\")\n",
    "\n",
    "\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_full.loc[:, col] = le.fit_transform(df_full[col]).astype(int)\n",
    "    encoders[col] = le  # para guardar los mapeos por si necesitás revertirlos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9299b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo los encoders en archivos .pkl\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('encoders', exist_ok=True)\n",
    "\n",
    "for col, le in encoders.items():\n",
    "    joblib.dump(le, f'encoders/{col}_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "726476f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generar_lags_deltas_pct(\n",
    "    df: pd.DataFrame,\n",
    "    columnas: list,\n",
    "    lags: list = list(range(1, 4)),  # por default lag 1, 2, 3\n",
    "    group_cols: list = ['PRODUCT_ID', 'CUSTOMER_ID'],\n",
    "    time_col: str = 'ORDINAL'\n",
    "):\n",
    "    df = df.sort_values(by=group_cols + [time_col])\n",
    "    nuevas_columnas = []\n",
    "\n",
    "    for col in columnas:\n",
    "        # Para evitar división por 0: obtener mínimo positivo por grupo\n",
    "        minimos = df.groupby(group_cols)[col].transform('min')\n",
    "\n",
    "        for lag in lags:\n",
    "            lag_col = f'{col}_LAG_{lag:02d}'\n",
    "            delta_col = f'{col}_DELTA_{lag:02d}'\n",
    "            delta_pct_col = f'{col}_DELTA_PCT_{lag:02d}'\n",
    "\n",
    "            # LAG\n",
    "            df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
    "            nuevas_columnas.append(lag_col)\n",
    "\n",
    "            # DELTA\n",
    "            df[delta_col] = df[col] - df[lag_col]\n",
    "            nuevas_columnas.append(delta_col)\n",
    "\n",
    "            # DELTA PCT\n",
    "            reemplazo = np.where(df[lag_col] == 0, minimos, df[lag_col])\n",
    "            df[delta_pct_col] = df[col] / reemplazo\n",
    "            df[delta_pct_col] = df[delta_pct_col].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "            nuevas_columnas.append(delta_pct_col)\n",
    "\n",
    "    return df, nuevas_columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a956a4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n",
      "/tmp/ipykernel_377801/3165156112.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[lag_col] = df.groupby(group_cols)[col].shift(lag).fillna(0)\n",
      "/tmp/ipykernel_377801/3165156112.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_col] = df[col] - df[lag_col]\n",
      "/tmp/ipykernel_377801/3165156112.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[delta_pct_col] = df[col] / reemplazo\n"
     ]
    }
   ],
   "source": [
    "columnas_lags_deltas = [\n",
    "    'TN',\n",
    "    'CUST_REQUEST_QTY',\n",
    "    'CUST_REQUEST_TN',\n",
    "    'CANT_PROD_CLI_PER'\n",
    "]\n",
    "\n",
    "df_full, nuevas_col_lags_deltas = generar_lags_deltas_pct(\n",
    "    df=df_full,\n",
    "    columnas=columnas_lags_deltas,\n",
    "    lags=list(range(1, 19))  # como hacías con TN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92f799e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Excluir la variable de clase\n",
    "cols_to_check = df_full.columns.difference(['CLASE_DELTA_LOG1P_Z'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_full[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae6c73c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DataFrame final con 7,781,619 filas y 238 columnas:\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 DataFrame final con {df_full.shape[0]:,} filas y {df_full.shape[1]} columnas:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c38a24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia de seguridad\n",
    "df = df_full.copy()\n",
    "\n",
    "# === Normalización ===\n",
    "scaler_customer = StandardScaler()\n",
    "df['CUSTOMER_RANK_NORM'] = scaler_customer.fit_transform(df[['CUSTOMER_ID']])\n",
    "joblib.dump(scaler_customer, './encoders/scaler_customer_id.pkl')\n",
    "\n",
    "# === Binning (en deciles) ===\n",
    "df['CUSTOMER_RANK_BIN'] = pd.qcut(df['CUSTOMER_ID'], q=10, labels=False)\n",
    "df['CUSTOMER_RANK_BIN'] = df['CUSTOMER_RANK_BIN'].astype('category')\n",
    "\n",
    "# Validación opcional\n",
    "assert df['CUSTOMER_RANK_BIN'].isna().sum() == 0, \"NaNs en qcut\"\n",
    "\n",
    "# Reemplazar en df_full\n",
    "df_full = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bbc780ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia de seguridad del DataFrame\n",
    "df = df_full.copy()\n",
    "\n",
    "# === Normalización ===\n",
    "scaler_product = StandardScaler()\n",
    "df['PRODUCT_RANK_NORM'] = scaler_product.fit_transform(df[['PRODUCT_ID']])\n",
    "joblib.dump(scaler_product, './encoders/scaler_product_id.pkl')\n",
    "\n",
    "# === Binning (en deciles) ===\n",
    "df['PRODUCT_RANK_BIN'] = pd.qcut(df['PRODUCT_ID'], q=10, labels=False)\n",
    "df['PRODUCT_RANK_BIN'] = df['PRODUCT_RANK_BIN'].astype('category')\n",
    "\n",
    "# Validación opcional\n",
    "assert df['PRODUCT_RANK_BIN'].isna().sum() == 0, \"NaNs en qcut de PRODUCT_ID\"\n",
    "\n",
    "# Reemplazar en el DataFrame principal\n",
    "df_full = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62d8602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MES_SIN', 'MES_COS', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID', 'PRODUCT_ID', 'CUST_REQUEST_QTY', 'CUST_REQUEST_TN', 'TN', 'ANTIG_CLIENTE', 'ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER', 'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID', 'MESES_SIN_COMPRAR_PRODUCT_ID', 'MESES_SIN_COMPRAR_CUSTOMER_ID', 'MES_PROBLEMATICO', 'PERIODO', 'ORDINAL', 'CLASE_DELTA_LOG1P_Z', 'TN_LAG_01', 'TN_DELTA_01', 'TN_DELTA_PCT_01', 'TN_LAG_02', 'TN_DELTA_02', 'TN_DELTA_PCT_02', 'TN_LAG_03', 'TN_DELTA_03', 'TN_DELTA_PCT_03', 'TN_LAG_04', 'TN_DELTA_04', 'TN_DELTA_PCT_04', 'TN_LAG_05', 'TN_DELTA_05', 'TN_DELTA_PCT_05', 'TN_LAG_06', 'TN_DELTA_06', 'TN_DELTA_PCT_06', 'TN_LAG_07', 'TN_DELTA_07', 'TN_DELTA_PCT_07', 'TN_LAG_08', 'TN_DELTA_08', 'TN_DELTA_PCT_08', 'TN_LAG_09', 'TN_DELTA_09', 'TN_DELTA_PCT_09', 'TN_LAG_10', 'TN_DELTA_10', 'TN_DELTA_PCT_10', 'TN_LAG_11', 'TN_DELTA_11', 'TN_DELTA_PCT_11', 'TN_LAG_12', 'TN_DELTA_12', 'TN_DELTA_PCT_12', 'TN_LAG_13', 'TN_DELTA_13', 'TN_DELTA_PCT_13', 'TN_LAG_14', 'TN_DELTA_14', 'TN_DELTA_PCT_14', 'TN_LAG_15', 'TN_DELTA_15', 'TN_DELTA_PCT_15', 'TN_LAG_16', 'TN_DELTA_16', 'TN_DELTA_PCT_16', 'TN_LAG_17', 'TN_DELTA_17', 'TN_DELTA_PCT_17', 'TN_LAG_18', 'TN_DELTA_18', 'TN_DELTA_PCT_18', 'CUST_REQUEST_QTY_LAG_01', 'CUST_REQUEST_QTY_DELTA_01', 'CUST_REQUEST_QTY_DELTA_PCT_01', 'CUST_REQUEST_QTY_LAG_02', 'CUST_REQUEST_QTY_DELTA_02', 'CUST_REQUEST_QTY_DELTA_PCT_02', 'CUST_REQUEST_QTY_LAG_03', 'CUST_REQUEST_QTY_DELTA_03', 'CUST_REQUEST_QTY_DELTA_PCT_03', 'CUST_REQUEST_QTY_LAG_04', 'CUST_REQUEST_QTY_DELTA_04', 'CUST_REQUEST_QTY_DELTA_PCT_04', 'CUST_REQUEST_QTY_LAG_05', 'CUST_REQUEST_QTY_DELTA_05', 'CUST_REQUEST_QTY_DELTA_PCT_05', 'CUST_REQUEST_QTY_LAG_06', 'CUST_REQUEST_QTY_DELTA_06', 'CUST_REQUEST_QTY_DELTA_PCT_06', 'CUST_REQUEST_QTY_LAG_07', 'CUST_REQUEST_QTY_DELTA_07', 'CUST_REQUEST_QTY_DELTA_PCT_07', 'CUST_REQUEST_QTY_LAG_08', 'CUST_REQUEST_QTY_DELTA_08', 'CUST_REQUEST_QTY_DELTA_PCT_08', 'CUST_REQUEST_QTY_LAG_09', 'CUST_REQUEST_QTY_DELTA_09', 'CUST_REQUEST_QTY_DELTA_PCT_09', 'CUST_REQUEST_QTY_LAG_10', 'CUST_REQUEST_QTY_DELTA_10', 'CUST_REQUEST_QTY_DELTA_PCT_10', 'CUST_REQUEST_QTY_LAG_11', 'CUST_REQUEST_QTY_DELTA_11', 'CUST_REQUEST_QTY_DELTA_PCT_11', 'CUST_REQUEST_QTY_LAG_12', 'CUST_REQUEST_QTY_DELTA_12', 'CUST_REQUEST_QTY_DELTA_PCT_12', 'CUST_REQUEST_QTY_LAG_13', 'CUST_REQUEST_QTY_DELTA_13', 'CUST_REQUEST_QTY_DELTA_PCT_13', 'CUST_REQUEST_QTY_LAG_14', 'CUST_REQUEST_QTY_DELTA_14', 'CUST_REQUEST_QTY_DELTA_PCT_14', 'CUST_REQUEST_QTY_LAG_15', 'CUST_REQUEST_QTY_DELTA_15', 'CUST_REQUEST_QTY_DELTA_PCT_15', 'CUST_REQUEST_QTY_LAG_16', 'CUST_REQUEST_QTY_DELTA_16', 'CUST_REQUEST_QTY_DELTA_PCT_16', 'CUST_REQUEST_QTY_LAG_17', 'CUST_REQUEST_QTY_DELTA_17', 'CUST_REQUEST_QTY_DELTA_PCT_17', 'CUST_REQUEST_QTY_LAG_18', 'CUST_REQUEST_QTY_DELTA_18', 'CUST_REQUEST_QTY_DELTA_PCT_18', 'CUST_REQUEST_TN_LAG_01', 'CUST_REQUEST_TN_DELTA_01', 'CUST_REQUEST_TN_DELTA_PCT_01', 'CUST_REQUEST_TN_LAG_02', 'CUST_REQUEST_TN_DELTA_02', 'CUST_REQUEST_TN_DELTA_PCT_02', 'CUST_REQUEST_TN_LAG_03', 'CUST_REQUEST_TN_DELTA_03', 'CUST_REQUEST_TN_DELTA_PCT_03', 'CUST_REQUEST_TN_LAG_04', 'CUST_REQUEST_TN_DELTA_04', 'CUST_REQUEST_TN_DELTA_PCT_04', 'CUST_REQUEST_TN_LAG_05', 'CUST_REQUEST_TN_DELTA_05', 'CUST_REQUEST_TN_DELTA_PCT_05', 'CUST_REQUEST_TN_LAG_06', 'CUST_REQUEST_TN_DELTA_06', 'CUST_REQUEST_TN_DELTA_PCT_06', 'CUST_REQUEST_TN_LAG_07', 'CUST_REQUEST_TN_DELTA_07', 'CUST_REQUEST_TN_DELTA_PCT_07', 'CUST_REQUEST_TN_LAG_08', 'CUST_REQUEST_TN_DELTA_08', 'CUST_REQUEST_TN_DELTA_PCT_08', 'CUST_REQUEST_TN_LAG_09', 'CUST_REQUEST_TN_DELTA_09', 'CUST_REQUEST_TN_DELTA_PCT_09', 'CUST_REQUEST_TN_LAG_10', 'CUST_REQUEST_TN_DELTA_10', 'CUST_REQUEST_TN_DELTA_PCT_10', 'CUST_REQUEST_TN_LAG_11', 'CUST_REQUEST_TN_DELTA_11', 'CUST_REQUEST_TN_DELTA_PCT_11', 'CUST_REQUEST_TN_LAG_12', 'CUST_REQUEST_TN_DELTA_12', 'CUST_REQUEST_TN_DELTA_PCT_12', 'CUST_REQUEST_TN_LAG_13', 'CUST_REQUEST_TN_DELTA_13', 'CUST_REQUEST_TN_DELTA_PCT_13', 'CUST_REQUEST_TN_LAG_14', 'CUST_REQUEST_TN_DELTA_14', 'CUST_REQUEST_TN_DELTA_PCT_14', 'CUST_REQUEST_TN_LAG_15', 'CUST_REQUEST_TN_DELTA_15', 'CUST_REQUEST_TN_DELTA_PCT_15', 'CUST_REQUEST_TN_LAG_16', 'CUST_REQUEST_TN_DELTA_16', 'CUST_REQUEST_TN_DELTA_PCT_16', 'CUST_REQUEST_TN_LAG_17', 'CUST_REQUEST_TN_DELTA_17', 'CUST_REQUEST_TN_DELTA_PCT_17', 'CUST_REQUEST_TN_LAG_18', 'CUST_REQUEST_TN_DELTA_18', 'CUST_REQUEST_TN_DELTA_PCT_18', 'CANT_PROD_CLI_PER_LAG_01', 'CANT_PROD_CLI_PER_DELTA_01', 'CANT_PROD_CLI_PER_DELTA_PCT_01', 'CANT_PROD_CLI_PER_LAG_02', 'CANT_PROD_CLI_PER_DELTA_02', 'CANT_PROD_CLI_PER_DELTA_PCT_02', 'CANT_PROD_CLI_PER_LAG_03', 'CANT_PROD_CLI_PER_DELTA_03', 'CANT_PROD_CLI_PER_DELTA_PCT_03', 'CANT_PROD_CLI_PER_LAG_04', 'CANT_PROD_CLI_PER_DELTA_04', 'CANT_PROD_CLI_PER_DELTA_PCT_04', 'CANT_PROD_CLI_PER_LAG_05', 'CANT_PROD_CLI_PER_DELTA_05', 'CANT_PROD_CLI_PER_DELTA_PCT_05', 'CANT_PROD_CLI_PER_LAG_06', 'CANT_PROD_CLI_PER_DELTA_06', 'CANT_PROD_CLI_PER_DELTA_PCT_06', 'CANT_PROD_CLI_PER_LAG_07', 'CANT_PROD_CLI_PER_DELTA_07', 'CANT_PROD_CLI_PER_DELTA_PCT_07', 'CANT_PROD_CLI_PER_LAG_08', 'CANT_PROD_CLI_PER_DELTA_08', 'CANT_PROD_CLI_PER_DELTA_PCT_08', 'CANT_PROD_CLI_PER_LAG_09', 'CANT_PROD_CLI_PER_DELTA_09', 'CANT_PROD_CLI_PER_DELTA_PCT_09', 'CANT_PROD_CLI_PER_LAG_10', 'CANT_PROD_CLI_PER_DELTA_10', 'CANT_PROD_CLI_PER_DELTA_PCT_10', 'CANT_PROD_CLI_PER_LAG_11', 'CANT_PROD_CLI_PER_DELTA_11', 'CANT_PROD_CLI_PER_DELTA_PCT_11', 'CANT_PROD_CLI_PER_LAG_12', 'CANT_PROD_CLI_PER_DELTA_12', 'CANT_PROD_CLI_PER_DELTA_PCT_12', 'CANT_PROD_CLI_PER_LAG_13', 'CANT_PROD_CLI_PER_DELTA_13', 'CANT_PROD_CLI_PER_DELTA_PCT_13', 'CANT_PROD_CLI_PER_LAG_14', 'CANT_PROD_CLI_PER_DELTA_14', 'CANT_PROD_CLI_PER_DELTA_PCT_14', 'CANT_PROD_CLI_PER_LAG_15', 'CANT_PROD_CLI_PER_DELTA_15', 'CANT_PROD_CLI_PER_DELTA_PCT_15', 'CANT_PROD_CLI_PER_LAG_16', 'CANT_PROD_CLI_PER_DELTA_16', 'CANT_PROD_CLI_PER_DELTA_PCT_16', 'CANT_PROD_CLI_PER_LAG_17', 'CANT_PROD_CLI_PER_DELTA_17', 'CANT_PROD_CLI_PER_DELTA_PCT_17', 'CANT_PROD_CLI_PER_LAG_18', 'CANT_PROD_CLI_PER_DELTA_18', 'CANT_PROD_CLI_PER_DELTA_PCT_18', 'CUSTOMER_RANK_NORM', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_NORM', 'PRODUCT_RANK_BIN']\n"
     ]
    }
   ],
   "source": [
    "print(df_full.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "26e24452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia del DataFrame original\n",
    "df = df_full.copy()\n",
    "\n",
    "# Columnas categóricas\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE','CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN']\n",
    "\n",
    "# Columnas a excluir\n",
    "excluir = ['PERIODO','CLASE_DELTA_LOG1P_Z', 'CUSTOMER_RANK_NORM', 'PRODUCT_RANK_NORM', \n",
    "           'MES_SIN', 'MES_COS', 'MES_PROBLEMATICO', 'CUSTOMER_ID','PRODUCT_ID'] + cat_cols\n",
    "excluir += [col for col in df.columns if col.endswith('_BIN')]\n",
    "\n",
    "# Columnas numéricas a escalar\n",
    "cols_a_escalar = [col for col in df.columns if col not in excluir and pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "# Entrenamiento del scaler SOLO con datos de entrenamiento\n",
    "df_entrenamiento = df[df['PERIODO'] <= 201910].copy()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_entrenamiento[cols_a_escalar])\n",
    "\n",
    "# Aplicación del scaler a TODO el dataset\n",
    "valores_escalados = scaler.transform(df[cols_a_escalar])\n",
    "df_scaled = pd.DataFrame(valores_escalados, columns=[col + '_Z' for col in cols_a_escalar], index=df.index)\n",
    "\n",
    "# Reemplazo de columnas originales\n",
    "df.drop(columns=cols_a_escalar, inplace=True)\n",
    "df = pd.concat([df, df_scaled], axis=1)\n",
    "\n",
    "# Guardar scaler\n",
    "joblib.dump(scaler, './encoders/scaler_features_numericas.pkl')\n",
    "\n",
    "# Actualizar df_full\n",
    "df_full = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "33c9a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MES_SIN', 'MES_COS', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID', 'PRODUCT_ID', 'MES_PROBLEMATICO', 'PERIODO', 'CLASE_DELTA_LOG1P_Z', 'CUSTOMER_RANK_NORM', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_NORM', 'PRODUCT_RANK_BIN', 'CUST_REQUEST_QTY_Z', 'CUST_REQUEST_TN_Z', 'TN_Z', 'ANTIG_CLIENTE_Z', 'ANTIG_PRODUCTO_Z', 'CANT_PROD_CLI_PER_Z', 'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID_Z', 'MESES_SIN_COMPRAR_PRODUCT_ID_Z', 'MESES_SIN_COMPRAR_CUSTOMER_ID_Z', 'ORDINAL_Z', 'TN_LAG_01_Z', 'TN_DELTA_01_Z', 'TN_DELTA_PCT_01_Z', 'TN_LAG_02_Z', 'TN_DELTA_02_Z', 'TN_DELTA_PCT_02_Z', 'TN_LAG_03_Z', 'TN_DELTA_03_Z', 'TN_DELTA_PCT_03_Z', 'TN_LAG_04_Z', 'TN_DELTA_04_Z', 'TN_DELTA_PCT_04_Z', 'TN_LAG_05_Z', 'TN_DELTA_05_Z', 'TN_DELTA_PCT_05_Z', 'TN_LAG_06_Z', 'TN_DELTA_06_Z', 'TN_DELTA_PCT_06_Z', 'TN_LAG_07_Z', 'TN_DELTA_07_Z', 'TN_DELTA_PCT_07_Z', 'TN_LAG_08_Z', 'TN_DELTA_08_Z', 'TN_DELTA_PCT_08_Z', 'TN_LAG_09_Z', 'TN_DELTA_09_Z', 'TN_DELTA_PCT_09_Z', 'TN_LAG_10_Z', 'TN_DELTA_10_Z', 'TN_DELTA_PCT_10_Z', 'TN_LAG_11_Z', 'TN_DELTA_11_Z', 'TN_DELTA_PCT_11_Z', 'TN_LAG_12_Z', 'TN_DELTA_12_Z', 'TN_DELTA_PCT_12_Z', 'TN_LAG_13_Z', 'TN_DELTA_13_Z', 'TN_DELTA_PCT_13_Z', 'TN_LAG_14_Z', 'TN_DELTA_14_Z', 'TN_DELTA_PCT_14_Z', 'TN_LAG_15_Z', 'TN_DELTA_15_Z', 'TN_DELTA_PCT_15_Z', 'TN_LAG_16_Z', 'TN_DELTA_16_Z', 'TN_DELTA_PCT_16_Z', 'TN_LAG_17_Z', 'TN_DELTA_17_Z', 'TN_DELTA_PCT_17_Z', 'TN_LAG_18_Z', 'TN_DELTA_18_Z', 'TN_DELTA_PCT_18_Z', 'CUST_REQUEST_QTY_LAG_01_Z', 'CUST_REQUEST_QTY_DELTA_01_Z', 'CUST_REQUEST_QTY_DELTA_PCT_01_Z', 'CUST_REQUEST_QTY_LAG_02_Z', 'CUST_REQUEST_QTY_DELTA_02_Z', 'CUST_REQUEST_QTY_DELTA_PCT_02_Z', 'CUST_REQUEST_QTY_LAG_03_Z', 'CUST_REQUEST_QTY_DELTA_03_Z', 'CUST_REQUEST_QTY_DELTA_PCT_03_Z', 'CUST_REQUEST_QTY_LAG_04_Z', 'CUST_REQUEST_QTY_DELTA_04_Z', 'CUST_REQUEST_QTY_DELTA_PCT_04_Z', 'CUST_REQUEST_QTY_LAG_05_Z', 'CUST_REQUEST_QTY_DELTA_05_Z', 'CUST_REQUEST_QTY_DELTA_PCT_05_Z', 'CUST_REQUEST_QTY_LAG_06_Z', 'CUST_REQUEST_QTY_DELTA_06_Z', 'CUST_REQUEST_QTY_DELTA_PCT_06_Z', 'CUST_REQUEST_QTY_LAG_07_Z', 'CUST_REQUEST_QTY_DELTA_07_Z', 'CUST_REQUEST_QTY_DELTA_PCT_07_Z', 'CUST_REQUEST_QTY_LAG_08_Z', 'CUST_REQUEST_QTY_DELTA_08_Z', 'CUST_REQUEST_QTY_DELTA_PCT_08_Z', 'CUST_REQUEST_QTY_LAG_09_Z', 'CUST_REQUEST_QTY_DELTA_09_Z', 'CUST_REQUEST_QTY_DELTA_PCT_09_Z', 'CUST_REQUEST_QTY_LAG_10_Z', 'CUST_REQUEST_QTY_DELTA_10_Z', 'CUST_REQUEST_QTY_DELTA_PCT_10_Z', 'CUST_REQUEST_QTY_LAG_11_Z', 'CUST_REQUEST_QTY_DELTA_11_Z', 'CUST_REQUEST_QTY_DELTA_PCT_11_Z', 'CUST_REQUEST_QTY_LAG_12_Z', 'CUST_REQUEST_QTY_DELTA_12_Z', 'CUST_REQUEST_QTY_DELTA_PCT_12_Z', 'CUST_REQUEST_QTY_LAG_13_Z', 'CUST_REQUEST_QTY_DELTA_13_Z', 'CUST_REQUEST_QTY_DELTA_PCT_13_Z', 'CUST_REQUEST_QTY_LAG_14_Z', 'CUST_REQUEST_QTY_DELTA_14_Z', 'CUST_REQUEST_QTY_DELTA_PCT_14_Z', 'CUST_REQUEST_QTY_LAG_15_Z', 'CUST_REQUEST_QTY_DELTA_15_Z', 'CUST_REQUEST_QTY_DELTA_PCT_15_Z', 'CUST_REQUEST_QTY_LAG_16_Z', 'CUST_REQUEST_QTY_DELTA_16_Z', 'CUST_REQUEST_QTY_DELTA_PCT_16_Z', 'CUST_REQUEST_QTY_LAG_17_Z', 'CUST_REQUEST_QTY_DELTA_17_Z', 'CUST_REQUEST_QTY_DELTA_PCT_17_Z', 'CUST_REQUEST_QTY_LAG_18_Z', 'CUST_REQUEST_QTY_DELTA_18_Z', 'CUST_REQUEST_QTY_DELTA_PCT_18_Z', 'CUST_REQUEST_TN_LAG_01_Z', 'CUST_REQUEST_TN_DELTA_01_Z', 'CUST_REQUEST_TN_DELTA_PCT_01_Z', 'CUST_REQUEST_TN_LAG_02_Z', 'CUST_REQUEST_TN_DELTA_02_Z', 'CUST_REQUEST_TN_DELTA_PCT_02_Z', 'CUST_REQUEST_TN_LAG_03_Z', 'CUST_REQUEST_TN_DELTA_03_Z', 'CUST_REQUEST_TN_DELTA_PCT_03_Z', 'CUST_REQUEST_TN_LAG_04_Z', 'CUST_REQUEST_TN_DELTA_04_Z', 'CUST_REQUEST_TN_DELTA_PCT_04_Z', 'CUST_REQUEST_TN_LAG_05_Z', 'CUST_REQUEST_TN_DELTA_05_Z', 'CUST_REQUEST_TN_DELTA_PCT_05_Z', 'CUST_REQUEST_TN_LAG_06_Z', 'CUST_REQUEST_TN_DELTA_06_Z', 'CUST_REQUEST_TN_DELTA_PCT_06_Z', 'CUST_REQUEST_TN_LAG_07_Z', 'CUST_REQUEST_TN_DELTA_07_Z', 'CUST_REQUEST_TN_DELTA_PCT_07_Z', 'CUST_REQUEST_TN_LAG_08_Z', 'CUST_REQUEST_TN_DELTA_08_Z', 'CUST_REQUEST_TN_DELTA_PCT_08_Z', 'CUST_REQUEST_TN_LAG_09_Z', 'CUST_REQUEST_TN_DELTA_09_Z', 'CUST_REQUEST_TN_DELTA_PCT_09_Z', 'CUST_REQUEST_TN_LAG_10_Z', 'CUST_REQUEST_TN_DELTA_10_Z', 'CUST_REQUEST_TN_DELTA_PCT_10_Z', 'CUST_REQUEST_TN_LAG_11_Z', 'CUST_REQUEST_TN_DELTA_11_Z', 'CUST_REQUEST_TN_DELTA_PCT_11_Z', 'CUST_REQUEST_TN_LAG_12_Z', 'CUST_REQUEST_TN_DELTA_12_Z', 'CUST_REQUEST_TN_DELTA_PCT_12_Z', 'CUST_REQUEST_TN_LAG_13_Z', 'CUST_REQUEST_TN_DELTA_13_Z', 'CUST_REQUEST_TN_DELTA_PCT_13_Z', 'CUST_REQUEST_TN_LAG_14_Z', 'CUST_REQUEST_TN_DELTA_14_Z', 'CUST_REQUEST_TN_DELTA_PCT_14_Z', 'CUST_REQUEST_TN_LAG_15_Z', 'CUST_REQUEST_TN_DELTA_15_Z', 'CUST_REQUEST_TN_DELTA_PCT_15_Z', 'CUST_REQUEST_TN_LAG_16_Z', 'CUST_REQUEST_TN_DELTA_16_Z', 'CUST_REQUEST_TN_DELTA_PCT_16_Z', 'CUST_REQUEST_TN_LAG_17_Z', 'CUST_REQUEST_TN_DELTA_17_Z', 'CUST_REQUEST_TN_DELTA_PCT_17_Z', 'CUST_REQUEST_TN_LAG_18_Z', 'CUST_REQUEST_TN_DELTA_18_Z', 'CUST_REQUEST_TN_DELTA_PCT_18_Z', 'CANT_PROD_CLI_PER_LAG_01_Z', 'CANT_PROD_CLI_PER_DELTA_01_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_01_Z', 'CANT_PROD_CLI_PER_LAG_02_Z', 'CANT_PROD_CLI_PER_DELTA_02_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_02_Z', 'CANT_PROD_CLI_PER_LAG_03_Z', 'CANT_PROD_CLI_PER_DELTA_03_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_03_Z', 'CANT_PROD_CLI_PER_LAG_04_Z', 'CANT_PROD_CLI_PER_DELTA_04_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_04_Z', 'CANT_PROD_CLI_PER_LAG_05_Z', 'CANT_PROD_CLI_PER_DELTA_05_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_05_Z', 'CANT_PROD_CLI_PER_LAG_06_Z', 'CANT_PROD_CLI_PER_DELTA_06_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_06_Z', 'CANT_PROD_CLI_PER_LAG_07_Z', 'CANT_PROD_CLI_PER_DELTA_07_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_07_Z', 'CANT_PROD_CLI_PER_LAG_08_Z', 'CANT_PROD_CLI_PER_DELTA_08_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_08_Z', 'CANT_PROD_CLI_PER_LAG_09_Z', 'CANT_PROD_CLI_PER_DELTA_09_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_09_Z', 'CANT_PROD_CLI_PER_LAG_10_Z', 'CANT_PROD_CLI_PER_DELTA_10_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_10_Z', 'CANT_PROD_CLI_PER_LAG_11_Z', 'CANT_PROD_CLI_PER_DELTA_11_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_11_Z', 'CANT_PROD_CLI_PER_LAG_12_Z', 'CANT_PROD_CLI_PER_DELTA_12_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_12_Z', 'CANT_PROD_CLI_PER_LAG_13_Z', 'CANT_PROD_CLI_PER_DELTA_13_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_13_Z', 'CANT_PROD_CLI_PER_LAG_14_Z', 'CANT_PROD_CLI_PER_DELTA_14_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_14_Z', 'CANT_PROD_CLI_PER_LAG_15_Z', 'CANT_PROD_CLI_PER_DELTA_15_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_15_Z', 'CANT_PROD_CLI_PER_LAG_16_Z', 'CANT_PROD_CLI_PER_DELTA_16_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_16_Z', 'CANT_PROD_CLI_PER_LAG_17_Z', 'CANT_PROD_CLI_PER_DELTA_17_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_17_Z', 'CANT_PROD_CLI_PER_LAG_18_Z', 'CANT_PROD_CLI_PER_DELTA_18_Z', 'CANT_PROD_CLI_PER_DELTA_PCT_18_Z']\n"
     ]
    }
   ],
   "source": [
    "print(df_full.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d7910d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Excluir la variable de clase\n",
    "cols_to_check = df_full.columns.difference(['CLASE_DELTA_LOG1P_Z'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_full[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed5652d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7781619, 242)\n"
     ]
    }
   ],
   "source": [
    "print(df_full.shape)\n",
    "# Guardar el DataFrame resultante en un archivo parquet\n",
    "df_full.to_parquet('./data/train_val_NN_TORCH.parquet', engine='fastparquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaboIII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
