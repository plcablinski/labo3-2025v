{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer en df_full ./data/l_vm_completa_train.parquet\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet')\n",
    "print(df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categóricas\n",
    "categorical_features = ['ANIO','MES','TRIMESTRE','ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','SKU_SIZE','CUSTOMER_ID','PRODUCT_ID','PLAN_PRECIOS_CUIDADOS']\n",
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_full[col] = df_full[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e67c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtrar que en X el periodo sea menor o igual a 201910\n",
    "# Variables predictoras y objetivo\n",
    "# En x eliminar la columna 'CLASE' y 'CLASE_DELTA'\n",
    "X = df_full[df_full['PERIODO'] <= 201910].drop(columns=['CLASE', 'CLASE_DELTA']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar en y que el periodo sea menor o igual a 201910\n",
    "y = df_full[df_full['PERIODO'] <= 201910]['CLASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116664bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los periodos de validación 201909, 201910\n",
    "periodos_valid = [201909,201910]\n",
    "\n",
    "\n",
    "# Separar train y cinco conjuntos de validación respetando la secuencia temporal\n",
    "X_train = X[X['PERIODO'] < periodos_valid[0]]\n",
    "y_train = y[X['PERIODO'] < periodos_valid[0]]\n",
    "\n",
    "X_val_list = [X[X['PERIODO'] == p] for p in periodos_valid]\n",
    "y_val_list = [y[X['PERIODO'] == p] for p in periodos_valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df_full['CLASE'], bins=50)\n",
    "plt.title('Distribución del target (CLASE)')\n",
    "plt.show()\n",
    "print(df_full['CLASE'].describe())\n",
    "# Imprimir la cantidad de ejemplos por clase de los primeros 30 con mas ejemplos\n",
    "print(df_full['CLASE'].value_counts().head(30))\n",
    "# Calcular los ceros que porcentaje son del total\n",
    "total = df_full['CLASE'].count()\n",
    "ceros = df_full['CLASE'].value_counts().get(0, 0)\n",
    "porcentaje_ceros = (ceros / total) * 100\n",
    "print(f\"Porcentaje de ceros: {porcentaje_ceros:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ETAPA 1: CLASIFICACIÓN ---\n",
    "# Crear variable binaria: 0 si CLASE==0, 1 si CLASE>0\n",
    "df_full['CLASE_BIN'] = (df_full['CLASE'] > 0).astype(int)\n",
    "\n",
    "# Separar train/val como antes\n",
    "X_train = X[X['PERIODO'] < periodos_valid[0]]\n",
    "y_train_bin = df_full.loc[X_train.index, 'CLASE_BIN']\n",
    "X_val_list = [X[X['PERIODO'] == p] for p in periodos_valid]\n",
    "y_val_bin_list = [df_full.loc[X_val.index, 'CLASE_BIN'] for X_val in X_val_list]\n",
    "\n",
    "# Entrenar clasificador\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=64,\n",
    "    n_jobs=-1\n",
    ")\n",
    "clf.fit(\n",
    "    X_train,\n",
    "    y_train_bin,\n",
    "    eval_set=[(X_val_list[0], y_val_bin_list[0])],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(period=300)],\n",
    "    categorical_feature=categorical_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503818bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ETAPA 2: REGRESIÓN SOLO PARA NO-CERO ---\n",
    "# Filtrar solo donde CLASE > 0\n",
    "mask_train = y_train_bin == 1\n",
    "X_train_reg = X_train[mask_train]\n",
    "y_train_reg = y_train[mask_train]\n",
    "\n",
    "X_val_reg_list = []\n",
    "y_val_reg_list = []\n",
    "for X_val, y_val_bin, y_val in zip(X_val_list, y_val_bin_list, y_val_list):\n",
    "    mask_val = y_val_bin == 1\n",
    "    X_val_reg_list.append(X_val[mask_val])\n",
    "    y_val_reg_list.append(y_val[mask_val])\n",
    "\n",
    "# Entrenar regresor\n",
    "reg = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=64,\n",
    "    n_jobs=-1\n",
    ")\n",
    "reg.fit(\n",
    "    X_train_reg,\n",
    "    y_train_reg,\n",
    "    eval_set=[(X_val_reg_list[0], y_val_reg_list[0])],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(period=300)],\n",
    "    categorical_feature=categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be730957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREDICCIÓN EN VALIDACIÓN ---\n",
    "# 1. Predice probabilidad de no-cero\n",
    "proba_no_cero = clf.predict_proba(X_val_list[0])[:, 1]\n",
    "# 2. Predice valor solo donde proba_no_cero > umbral (ejemplo: 0.5)\n",
    "umbral = 0.5\n",
    "pred_bin = (proba_no_cero > umbral)\n",
    "pred_reg = np.zeros(len(X_val_list[0]))\n",
    "if pred_bin.sum() > 0:\n",
    "    pred_reg[pred_bin] = reg.predict(X_val_list[0][pred_bin])\n",
    "\n",
    "# 3. Calcula WAPE en validación\n",
    "y_val_real = y_val_list[0].values\n",
    "wape = np.sum(np.abs(y_val_real - pred_reg)) / np.sum(np.abs(y_val_real))\n",
    "print(f\"WAPE validación (dos etapas): {wape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bfa617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los datasets de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "val_data_list = [lgb.Dataset(X_val_list[i], label=y_val_list[i], categorical_feature=categorical_features) for i in range(len(periodos_valid))]\n",
    "\n",
    "\n",
    "params = { \n",
    "    'objective': 'tweedie',\n",
    "    'tweedie_variance_power': 1.3,\n",
    "    #'metric': ['mape','mae','rmse'],\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 511,\n",
    "    'learning_rate': 0.005,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_bin': 255,               # Reduce memoria y acelera el entrenamiento\n",
    "    'force_col_wise': True,       # Más eficiente en CPU para datasets anchos\n",
    "    'n_jobs': -1,                 # Usa todos los núcleos disponibles\n",
    "    'verbose': -1 }\n",
    "\n",
    "def wape_metric(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    wape = np.sum(np.abs(y_true - preds)) / np.sum(np.abs(y_true))\n",
    "    return 'wape', wape, False  # False: menor es mejor\n",
    "\n",
    "# Entrenar el modelo con validación múltiple y early stopping\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=50000,\n",
    "    valid_sets=val_data_list,\n",
    "    valid_names=[f'validation_{p}' for p in periodos_valid],\n",
    "    feval=wape_metric,  # <-- agrega la métrica personalizada\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(period=300)]\n",
    ")\n",
    "\n",
    "print(\"Modelo de regresión entrenado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la importancia de cada variable\n",
    "importancia = model.feature_importance(importance_type='gain')\n",
    "nombres = X_train.columns\n",
    "\n",
    "# Crear un DataFrame ordenado por importancia\n",
    "import pandas as pd\n",
    "df_importancia = pd.DataFrame({'feature': nombres, 'importance': importancia})\n",
    "df_importancia = df_importancia.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Mostrar las variables más importantes\n",
    "print(df_importancia)\n",
    "\n",
    "# Si quieres visualizarlo gráficamente:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(df_importancia['feature'], df_importancia['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Importancia de variables LightGBM')\n",
    "plt.xlabel('Importancia')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaboIII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
