{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/train_val_NN_TORCH.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3938cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar conjuntos\n",
    "df_train = df_full[df_full['PERIODO'] <= 201908].copy()\n",
    "df_val = df_full[(df_full['PERIODO'] >= 201909) & (df_full['PERIODO'] <= 201910)].copy()\n",
    "df_pred = df_full[df_full['PERIODO'] == 201912].copy()\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ffc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "\n",
    "# Columnas categ√≥ricas a embeddings\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', \n",
    "            'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', \n",
    "            'PRODUCT_RANK_BIN']\n",
    "\n",
    "# Codificaci√≥n para embeddings\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "    clases_entrenadas = set(le.classes_)\n",
    "    df_val[col] = df_val[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    df_pred[col] = df_pred[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "embedding_sizes = [\n",
    "    (df_train[col].nunique() + 1, min(50, (df_train[col].nunique() + 1) // 2))\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Excluir columnas que no deben ir al modelo\n",
    "excluir = ['PERIODO', 'CUSTOMER_ID', 'PRODUCT_ID', 'CLASE_LOG1P_Z', 'ORDINAL']\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in excluir and col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No los incluyas en ninguna de estas dos listas\n",
    "assert 'CUSTOMER_ID' not in feature_cols\n",
    "assert 'CUSTOMER_ID' not in cat_cols\n",
    "assert 'PRODUCT_ID' not in feature_cols\n",
    "assert 'PRODUCT_ID' not in cat_cols\n",
    "assert 'PERIODO' not in feature_cols\n",
    "assert 'PERIODO' not in cat_cols\n",
    "assert 'CLASE_LOG1P_Z' not in feature_cols\n",
    "assert 'CLASE_LOG1P_Z' not in cat_cols\n",
    "assert 'ORDINAL' not in feature_cols\n",
    "assert 'ORDINAL' not in cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col=None):\n",
    "        self.cat_data = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
    "        self.num_data = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
    "        self.has_target = target_col is not None\n",
    "        if self.has_target:\n",
    "            self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.cat_data[idx], self.num_data[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.cat_data[idx], self.num_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightedMSELossMultiStable(nn.Module):\n",
    "    def __init__(self, penalty_indices, coefficients, alpha=0.1, debug=False):\n",
    "        \"\"\"\n",
    "        penalty_indices: lista de √≠ndices de columnas en x_num (por ejemplo, [10, 11, ..., 20])\n",
    "        coefficients: lista de coeficientes (ordenados igual que penalty_indices)\n",
    "        alpha: fuerza de penalizaci√≥n (m√°s bajo por estabilidad en redes grandes)\n",
    "        debug: si True, imprime el promedio del peso de penalizaci√≥n ocasionalmente\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.penalty_indices = penalty_indices\n",
    "        self.coefficients = coefficients\n",
    "        self.alpha = alpha\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, preds, targets, x_num):\n",
    "        # Penalizaci√≥n por muestra (shape: [batch_size, 1])\n",
    "        penalty = torch.ones_like(targets).float()\n",
    "\n",
    "        for idx, coef in zip(self.penalty_indices, self.coefficients):\n",
    "            val = x_num[:, idx:idx+1]  # shape: [batch_size, 1]\n",
    "            safe_val = torch.tanh(val)  # limitar la magnitud para evitar outliers\n",
    "            penalty += self.alpha * coef * safe_val.abs()  # siempre positivo\n",
    "\n",
    "        if self.debug and torch.rand(1).item() < 0.01:\n",
    "            print(f\"[LossDebug] Mean penalty: {penalty.mean().item():.4f}\")\n",
    "\n",
    "        error = (preds - targets) ** 2\n",
    "        weighted_error = penalty * error\n",
    "\n",
    "        return weighted_error.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad019ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_cols = ['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z','TN_LAG_05_Z','TN_LAG_06_Z','TN_LAG_07_Z',\n",
    "'TN_LAG_08_Z','TN_LAG_09_Z','TN_LAG_10_Z','TN_LAG_11_Z']\n",
    "penalty_indices = [feature_cols.index(col) for col in penalty_cols]\n",
    "print(penalty_cols)\n",
    "print(penalty_indices)\n",
    "coefficients = [\n",
    "    0.236558,\n",
    "    0.178208,\n",
    "   -0.060031,\n",
    "   -0.161875,\n",
    "   -0.007775,\n",
    "    0.151936,\n",
    "    0.043933,\n",
    "    0.142839,\n",
    "    0.103804,\n",
    "    0.119211,\n",
    "    0.073671\n",
    "]\n",
    "# loss_fn = WeightedMSELossMulti(penalty_indices, coefficients, alpha=0.5)\n",
    "\n",
    "loss_fn = WeightedMSELossMultiStable(\n",
    "    penalty_indices=penalty_indices,\n",
    "    coefficients=coefficients,\n",
    "    alpha=0.1,      # m√°s suave\n",
    "    debug=True      # activalo si quer√©s monitorear internamente\n",
    ")\n",
    "\n",
    "#loss_fn = WeightedMSELossMulti(penalty_indices=penalty_cols, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(col in df_train.columns for col in cat_cols), \"Faltan columnas categ√≥ricas\"\n",
    "assert all(col in df_train.columns for col in feature_cols), \"Faltan columnas num√©ricas\"\n",
    "assert target_col in df_train.columns, \"Falta la variable objetivo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd112c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "\n",
    "train_dataset = TabularDataset(df_train, cat_cols, feature_cols, target_col)\n",
    "val_dataset = TabularDataset(df_val, cat_cols, feature_cols, target_col)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d04175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_cols)\n",
    "print(feature_cols)\n",
    "print(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TabularNNImproved(nn.Module):\n",
    "    def __init__(self, embedding_sizes, num_numerical, hidden_sizes=[512, 512, 256, 128], dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(ni, nf) for ni, nf in embedding_sizes\n",
    "        ])\n",
    "        embedding_dim = sum([nf for _, nf in embedding_sizes])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Total input size after embedding + numerical\n",
    "        input_size = embedding_dim + num_numerical\n",
    "\n",
    "        # Hidden layers\n",
    "        layers = []\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = h\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        x = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = torch.cat([x, x_num], dim=1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Detectar si hay GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Crear el modelo\n",
    "model = TabularNNImproved(\n",
    "    embedding_sizes=embedding_sizes,\n",
    "    num_numerical=len(feature_cols),\n",
    "    hidden_sizes=[4096,2048,1024,512, 512, 256, 128],\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8314c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader=None, n_epochs=20, lr=1e-3, alpha=0.5, patience=3,\n",
    "    penalty_indices=None, coefficients=None\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = WeightedMSELossMultiStable(\n",
    "        penalty_indices=penalty_indices, coefficients=coefficients, alpha=alpha\n",
    "    )\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for cats, conts, y in train_loader:\n",
    "            cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(cats, conts)\n",
    "            loss = criterion(y_pred, y, conts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validaci√≥n (solo si hay val_loader)\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            y_true_list = []\n",
    "            y_pred_list = []\n",
    "            with torch.no_grad():\n",
    "                for cats, conts, y in val_loader:\n",
    "                    cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "                    y_pred = model(cats, conts)\n",
    "                    loss = criterion(y_pred, y, conts)\n",
    "                    val_loss += loss.item() * y.size(0)\n",
    "                    y_true_list.append(y.cpu().numpy())\n",
    "                    y_pred_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            y_true = np.concatenate(y_true_list)\n",
    "            y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | MAE: {mae:.4f} | R¬≤: {r2:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if patience is not None and epochs_without_improvement >= patience:\n",
    "                    print(\"üî¥ Early stopping triggered\")\n",
    "                    break\n",
    "        else:\n",
    "            # Si no hay val_loader, solo mostrar train_loss\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Restaurar el mejor modelo solo si hubo validaci√≥n\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Retornar resultados solo si hubo validaci√≥n, si no, devolver None\n",
    "    if val_loader is not None:\n",
    "        return y_true, y_pred\n",
    "    else:\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b6b5a",
   "metadata": {},
   "source": [
    "# B√∫squeda de hiperpar√°metros (Grid Search)\n",
    "Probamos distintas combinaciones de hiperpar√°metros y seleccionamos la que da mejor MAE en validaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040764c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Definir el espacio de b√∫squeda\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'hidden_sizes': [\n",
    "        [1024, 512, 256],\n",
    "        [2048, 1024, 512, 256]\n",
    "    ],\n",
    "    'alpha': [0,0.1,0.3, 0.5, 0.7,0.9]\n",
    "}\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "param_combinations = list(product(\n",
    "    param_grid['lr'],\n",
    "    param_grid['dropout'],\n",
    "    param_grid['hidden_sizes'],\n",
    "    param_grid['alpha']\n",
    "))\n",
    "\n",
    "results = []\n",
    "best_mae = float('inf')\n",
    "\n",
    "# Loop de entrenamiento por combinaci√≥n\n",
    "for lr, dropout, hidden_sizes, alpha in param_combinations:\n",
    "    print(f\"\\nüîß Entrenando con: lr={lr}, dropout={dropout}, hidden_sizes={hidden_sizes}, alpha={alpha}\")\n",
    "\n",
    "    # Crear modelo y mover a dispositivo\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrenamiento corto para tuning\n",
    "    y_true_gs, y_pred_gs = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        n_epochs=8, lr=lr, alpha=alpha, patience=2,penalty_indices=penalty_indices,coefficients=coefficients\n",
    "    )\n",
    "\n",
    "    mae = mean_absolute_error(y_true_gs, y_pred_gs)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'dropout': dropout,\n",
    "        'hidden_sizes': hidden_sizes,\n",
    "        'alpha': alpha,\n",
    "        'mae': mae\n",
    "    })\n",
    "\n",
    "    print(f\"‚úÖ MAE = {mae:.4f}\")\n",
    "\n",
    "    # Guardar modelo si es el mejor\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        torch.save(model.state_dict(), f\"best_model_mae{mae:.4f}_lr{lr}_do{dropout}_a{alpha}.pth\")\n",
    "        print(\"üíæ Modelo guardado (mejor hasta ahora)\")\n",
    "\n",
    "    # Limpiar memoria GPU\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Convertir a DataFrame y mostrar top 5\n",
    "results_df = pd.DataFrame(results).sort_values(by='mae')\n",
    "print(\"\\nüìä Mejores combinaciones:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Guardar resultados a disco\n",
    "results_df.to_csv(\"gridsearch_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# ------------------------\n",
    "# Prepara el dataset final\n",
    "# ------------------------\n",
    "# train_dataset y val_dataset ya deber√≠an estar definidos\n",
    "train_val_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader_full = DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------\n",
    "# Lista de hiperpar√°metros de los 10 mejores modelos\n",
    "# (¬°completa esta lista con tus resultados!)\n",
    "# ------------------------\n",
    "configs = [\n",
    "    {\"lr\": 0.0005, \"dropout\": 0.3, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0,   \"n_epochs\": 8, \"name\": \"modelo_1\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.2, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0,   \"n_epochs\": 4, \"name\": \"modelo_2\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.3, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.1, \"n_epochs\": 8, \"name\": \"modelo_3\"},\n",
    "    {\"lr\": 0.0005, \"dropout\": 0.2, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.3, \"n_epochs\": 6, \"name\": \"modelo_4\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.3, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.3, \"n_epochs\": 8, \"name\": \"modelo_5\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.2, \"hidden_sizes\": [1024, 512, 256],       \"alpha\": 0.7, \"n_epochs\": 8, \"name\": \"modelo_6\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.3, \"hidden_sizes\": [1024, 512, 256],       \"alpha\": 0.1, \"n_epochs\": 8, \"name\": \"modelo_7\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.2, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.7, \"n_epochs\": 5, \"name\": \"modelo_8\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.3, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.3, \"n_epochs\": 5, \"name\": \"modelo_9\"},\n",
    "    {\"lr\": 0.0005, \"dropout\": 0.2, \"hidden_sizes\": [1024, 512, 256],       \"alpha\": 0.1, \"n_epochs\": 4, \"name\": \"modelo_10\"},\n",
    "]\n",
    "\n",
    "# Actualizar epochs\n",
    "for cfg in configs:\n",
    "    cfg['n_epochs'] = int(math.ceil(cfg['n_epochs'] * 1.3))\n",
    "\n",
    "# Mostramos resultado\n",
    "for cfg in configs:\n",
    "    print(cfg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bc99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Entrenamiento y guardado\n",
    "# ------------------------\n",
    "\n",
    "for cfg in configs:\n",
    "    print(f\"\\nüîß Entrenando {cfg['name']} - lr={cfg['lr']} dropout={cfg['dropout']} hidden={cfg['hidden_sizes']} alpha={cfg['alpha']} epochs={cfg['n_epochs']}\")\n",
    "\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg['hidden_sizes'],\n",
    "        dropout=cfg['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrena sobre TODO el set\n",
    "    y_true, y_pred = train_model(\n",
    "        model,\n",
    "        train_loader_full,\n",
    "        val_loader=None,\n",
    "        n_epochs=cfg['n_epochs'],\n",
    "        lr=cfg['lr'],\n",
    "        alpha=cfg['alpha'],\n",
    "        patience=None,  # No early stopping en entrenamiento final\n",
    "        penalty_indices=penalty_indices,\n",
    "        coefficients=coefficients\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{cfg['name']}_final.pt\")\n",
    "    print(f\"üíæ Modelo guardado: {cfg['name']}_final.pt\")\n",
    "\n",
    "print(\"\\n‚úÖ ¬°Entrenamiento y guardado de los 10 mejores modelos finalizado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "print(lightgbm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "# --- SETUP ---\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "feature_cols = [col for col in df_train.columns if col != target_col]\n",
    "\n",
    "X = df_train[feature_cols].values\n",
    "y = df_train[target_col].values\n",
    "\n",
    "# --- OPTUNA OBJECTIVE ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 16),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.15, random_state=trial.number)\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[early_stopping(stopping_rounds=30, verbose=False)],\n",
    "    )\n",
    "    preds = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    return mae\n",
    "\n",
    "\n",
    "# --- OPTIMIZE ---\n",
    "N_MODELS = 10\n",
    "N_TRIALS = 80\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_lgbm, n_trials=N_TRIALS, n_jobs=28)\n",
    "\n",
    "# --- EXTRACT BEST 10 PARAMS ---\n",
    "top_lgbm_trials = study.trials_dataframe().sort_values(\"value\").head(N_MODELS)\n",
    "final_configs = []\n",
    "for i, row in top_lgbm_trials.iterrows():\n",
    "    params = row.filter(like='params_').to_dict()\n",
    "    params = {k.replace('params_', ''): v for k, v in params.items()}\n",
    "    # Asegurar enteros donde corresponde\n",
    "    for p in [\"num_leaves\", \"max_depth\", \"n_estimators\", \"min_child_samples\"]:\n",
    "        params[p] = int(params[p])\n",
    "    params[\"random_state\"] = 42\n",
    "    params[\"n_jobs\"] = -1\n",
    "    final_configs.append(params)\n",
    "\n",
    "print(final_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa797b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48913620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenar train y val\n",
    "df_full_train = pd.concat([df_train, df_val], axis=0).reset_index(drop=True)\n",
    "\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "feature_cols = [col for col in df_full_train.columns if col != target_col]\n",
    "\n",
    "X_full = df_full_train[feature_cols].values\n",
    "y_full = df_full_train[target_col].values\n",
    "\n",
    "# Tus hiperpar√°metros copiados (reemplaz√° si lo vas a cargar de un archivo)\n",
    "param_list = [{'colsample_bytree': 0.9956156662176132, 'learning_rate': 0.150647911151466, 'max_depth': 14, 'min_child_samples': 62, 'min_split_gain': 0.18618312765512604, 'n_estimators': 861, 'num_leaves': 222, 'reg_alpha': 0.0008184678353621553, 'reg_lambda': 0.00181071360138272, 'subsample': 0.8288045639730978, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9900835480198793, 'learning_rate': 0.15285688565610836, 'max_depth': 14, 'min_child_samples': 22, 'min_split_gain': 0.21318221132522686, 'n_estimators': 863, 'num_leaves': 218, 'reg_alpha': 0.005832497428105893, 'reg_lambda': 0.00048332566435359725, 'subsample': 0.966212591402847, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9810381286523686, 'learning_rate': 0.17126526528861452, 'max_depth': 15, 'min_child_samples': 20, 'min_split_gain': 0.18620761002164188, 'n_estimators': 887, 'num_leaves': 233, 'reg_alpha': 7.525959386634591, 'reg_lambda': 0.002932489509766247, 'subsample': 0.9855006418021718, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9991282388618083, 'learning_rate': 0.04595382767022815, 'max_depth': 11, 'min_child_samples': 41, 'min_split_gain': 0.21027442904661392, 'n_estimators': 876, 'num_leaves': 216, 'reg_alpha': 0.0009948983243008944, 'reg_lambda': 0.0007851771585517262, 'subsample': 0.8348381160448405, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9997415495087778, 'learning_rate': 0.207311488631083, 'max_depth': 11, 'min_child_samples': 40, 'min_split_gain': 0.21104245822261147, 'n_estimators': 883, 'num_leaves': 256, 'reg_alpha': 0.0010361594632463222, 'reg_lambda': 1.430438254956134e-07, 'subsample': 0.8317847564939153, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.587328586423979, 'learning_rate': 0.1861100303288766, 'max_depth': 11, 'min_child_samples': 17, 'min_split_gain': 0.003592855256288212, 'n_estimators': 879, 'num_leaves': 226, 'reg_alpha': 0.07837411453395908, 'reg_lambda': 2.5489037442005937e-07, 'subsample': 0.8302404263289589, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.5895955174460985, 'learning_rate': 0.14941003478023016, 'max_depth': 15, 'min_child_samples': 20, 'min_split_gain': 0.19615530216306443, 'n_estimators': 896, 'num_leaves': 221, 'reg_alpha': 5.010927643947572, 'reg_lambda': 0.0007780698199438967, 'subsample': 0.9998177336233038, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.5837101193747908, 'learning_rate': 0.15338979834994065, 'max_depth': 11, 'min_child_samples': 64, 'min_split_gain': 0.19477841520356795, 'n_estimators': 856, 'num_leaves': 217, 'reg_alpha': 0.07087365761730123, 'reg_lambda': 6.887002502845986e-08, 'subsample': 0.8244503737000027, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9891294837026124, 'learning_rate': 0.14710934806711512, 'max_depth': 14, 'min_child_samples': 66, 'min_split_gain': 0.1888950104677536, 'n_estimators': 898, 'num_leaves': 230, 'reg_alpha': 0.045761956230443775, 'reg_lambda': 0.000545124313416254, 'subsample': 0.8224562428405393, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9998748104919317, 'learning_rate': 0.15126642557505776, 'max_depth': 14, 'min_child_samples': 20, 'min_split_gain': 0.25087912345115637, 'n_estimators': 849, 'num_leaves': 219, 'reg_alpha': 0.001191096159067221, 'reg_lambda': 0.0016155410949331161, 'subsample': 0.8372252476108392, 'random_state': 42, 'n_jobs': -1}]\n",
    "\n",
    "models = []\n",
    "for i, params in enumerate(param_list):\n",
    "    print(f\"Entrenando modelo {i+1}/10...\")\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_full, y_full)\n",
    "    models.append(model)\n",
    "    # Guardar modelo\n",
    "    joblib.dump(model, f'lgbm_model_{i+1:02d}.pkl')\n",
    "\n",
    "print(\"¬°Entrenamiento y guardado de modelos finalizado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Cargar los 10 modelos LightGBM entrenados\n",
    "lgbm_models = []\n",
    "for i in range(1, 11):\n",
    "    model = joblib.load(f'lgbm_model_{i:02d}.pkl')\n",
    "    lgbm_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e148e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- Configuraci√≥n de columnas ---\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "feature_cols = [col for col in df_pred.columns if col != target_col]\n",
    "\n",
    "# --- Predicci√≥n LightGBM (10 modelos) ---\n",
    "lgbm_preds = []\n",
    "\n",
    "# Us√° DataFrame, no .values para evitar el warning de feature names\n",
    "X_pred_lgbm = df_pred[feature_cols]\n",
    "\n",
    "for i, model in enumerate(lgbm_models):\n",
    "    print(f\"Prediciendo LightGBM {i+1}/10...\")\n",
    "    preds = model.predict(X_pred_lgbm)\n",
    "    lgbm_preds.append(preds)\n",
    "\n",
    "lgbm_preds = np.stack(lgbm_preds)  # shape (10, N)\n",
    "df_pred['lgbm_ensemble_mean'] = lgbm_preds.mean(axis=0)\n",
    "df_pred['lgbm_ensemble_median'] = np.median(lgbm_preds, axis=0)\n",
    "# Opcional: guardar cada predicci√≥n individual\n",
    "for i in range(10):\n",
    "    df_pred[f'lgbm_pred_LOG1P_Z_{i+1}'] = lgbm_preds[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "\n",
    "# Columnas categ√≥ricas a embeddings\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', \n",
    "            'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', \n",
    "            'PRODUCT_RANK_BIN']\n",
    "\n",
    "# Codificaci√≥n para embeddings\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "    clases_entrenadas = set(le.classes_)\n",
    "    df_val[col] = df_val[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    df_pred[col] = df_pred[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "embedding_sizes = [\n",
    "    (df_train[col].nunique() + 1, min(50, (df_train[col].nunique() + 1) // 2))\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Excluir columnas que no deben ir al modelo\n",
    "excluir = ['PERIODO', 'CUSTOMER_ID', 'PRODUCT_ID', 'CLASE_LOG1P_Z', 'ORDINAL']\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in excluir and col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Predicci√≥n MLP (PyTorch) (10 modelos) ---\n",
    "# Asegurar tipos correctos en el DataFrame\n",
    "for col in cat_cols:\n",
    "    df_pred[col] = df_pred[col].astype(np.int64)\n",
    "for col in feature_cols:\n",
    "    df_pred[col] = df_pred[col].astype(np.float32)\n",
    "\n",
    "X_cats = torch.LongTensor(df_pred[cat_cols].values)\n",
    "X_conts = torch.FloatTensor(df_pred[feature_cols].values)\n",
    "ds_pred = TensorDataset(X_cats, X_conts)\n",
    "pred_loader = DataLoader(ds_pred, batch_size=8192, shuffle=False)\n",
    "\n",
    "def predict_model(model, loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_cats, X_conts in loader:\n",
    "            X_cats = X_cats.to(device)\n",
    "            X_conts = X_conts.to(device)\n",
    "            output = model(X_cats, X_conts)\n",
    "            preds.append(output.cpu().numpy().squeeze())\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "mlp_preds = []\n",
    "\n",
    "for i, cfg in enumerate(configs):\n",
    "    print(f\"Prediciendo MLP {i+1}/10...\")\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg['hidden_sizes'],\n",
    "        dropout=cfg['dropout']\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(f\"{cfg['name']}_final.pt\", map_location=device))\n",
    "    preds = predict_model(model, pred_loader, device)\n",
    "    mlp_preds.append(preds)\n",
    "\n",
    "# Guardar cada predicci√≥n individual\n",
    "for i, cfg in enumerate(configs):\n",
    "    df_pred[f'mlp_pred_LOG1P_Z_{i+1}'] = mlp_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07627ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['CUSTOMER_ID', 'PRODUCT_ID'] + [\n",
    "    col for col in df_pred.columns \n",
    "    if ('mlp_pred_LOG1P_Z_' in col) or ('lgbm_pred_LOG1P_Z_' in col)\n",
    "]\n",
    "df_pred_final = df_pred[cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros usados en la transformaci√≥n original\n",
    "mean_train = 0.4756487705286022\n",
    "std_train = 0.6596333033693771\n",
    "\n",
    "def inverse_log_transform_signed(x):\n",
    "    return np.sign(x) * np.expm1(np.abs(x))\n",
    "\n",
    "def inv_transform(arr):\n",
    "    # Paso 1: desescalar Z-score\n",
    "    log1p_vals = arr * std_train + mean_train\n",
    "    # Paso 2: inversa de log1p con signo\n",
    "    orig_vals = inverse_log_transform_signed(log1p_vals)\n",
    "    return orig_vals\n",
    "\n",
    "# Crear columnas nuevas con sufijo \"_ORIG\"\n",
    "for col in df_pred_final.columns:\n",
    "    if col not in ['CUSTOMER_ID', 'PRODUCT_ID']:\n",
    "        df_pred_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform(df_pred_final[col].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.width', 400)\n",
    "df_pred_final = df_pred_final.loc[:, ~df_pred_final.columns.str.contains('LOG1P_Z')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccion√° solo las columnas de predicci√≥n de modelos individuales\n",
    "pred_cols = [col for col in df_pred_final.columns if col.startswith('lgbm_pred_ORIG_') or col.startswith('mlp_pred_ORIG_')]\n",
    "# Por cada columna pred_cols de df_pred_final hacerla cero si es negativa\n",
    "for col in pred_cols:\n",
    "    df_pred_final[col] = df_pred_final[col].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calcul√° media y mediana fila a fila\n",
    "df_pred_final['df_pred_mean'] = df_pred_final[pred_cols].mean(axis=1)\n",
    "df_pred_final['df_pred_median'] = df_pred_final[pred_cols].median(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Seleccionar solo las columnas LGBM\n",
    "lgbm_cols = [col for col in df_pred_final.columns if col.startswith('lgbm_pred_ORIG_')]\n",
    "\n",
    "# Paso 2: Calcular media y mediana por fila (e.g. por combinaci√≥n CUSTOMER_ID + PRODUCT_ID)\n",
    "df_pred_final['lgbm_pred_mean'] = df_pred_final[lgbm_cols].mean(axis=1)\n",
    "df_pred_final['lgbm_pred_median'] = df_pred_final[lgbm_cols].median(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d767914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_final_mediana = df_pred_final[['CUSTOMER_ID', 'PRODUCT_ID', 'df_pred_median']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f835c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_final_lgbm_mediana = df_pred_final[['CUSTOMER_ID', 'PRODUCT_ID', 'lgbm_pred_median']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_final_lgbm_mediana.to_csv('df_pred_final_lgbm_mediana.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pred_final_lgbm_mediana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986da7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar en df_promedios promedios_tn_no_mueven_agujas.csv\n",
    "df_promedios = pd.read_csv('promedios_tn_no_mueven_aguja.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer DataFrame: de predicciones (mediana del modelo)\n",
    "df1 = df_pred_final_mediana.copy()\n",
    "df1 = df1.rename(columns={'df_pred_median': 'TN'})\n",
    "df1['tipo'] = 'mediana_pred'\n",
    "\n",
    "# Segundo DataFrame: promedios hist√≥ricos\n",
    "df2 = df_promedios.copy()\n",
    "df2 = df2.rename(columns={'TN_MEAN': 'TN'})\n",
    "df2['tipo'] = 'media_train'\n",
    "\n",
    "# Dejar columnas consistentes y unir\n",
    "cols = ['CUSTOMER_ID', 'PRODUCT_ID', 'TN', 'tipo']\n",
    "df_union = pd.concat([df1[cols], df2[cols]], axis=0, ignore_index=True)\n",
    "\n",
    "print(df_union.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por PRODUCT_ID y sumar TN (puede ser mean si prefer√≠s)\n",
    "df_out = (\n",
    "    df_union.groupby('PRODUCT_ID', as_index=False)['TN']\n",
    "    .sum()   # Cambi√° a .mean() si quer√©s promedio\n",
    "    .rename(columns={'PRODUCT_ID': 'product_id', 'TN': 'tn'})\n",
    ")\n",
    "\n",
    "# Guardar a CSV\n",
    "df_out.to_csv('tn_por_producto.csv', index=False, float_format='%.5f')\n",
    "\n",
    "print(df_out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b487252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer DataFrame: de predicciones (mediana del modelo)\n",
    "df4 = df_pred_final_lgbm_mediana.copy()\n",
    "df4 = df4.rename(columns={'lgbm_pred_median': 'TN'})\n",
    "df4['tipo'] = 'mediana_lightGBMpred'\n",
    "print(df4.head(10))\n",
    "print(df_completa.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660df3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, asegurate de que los nombres de las columnas coincidan exactamente\n",
    "df_merge = df4[['CUSTOMER_ID', 'PRODUCT_ID', 'TN']].merge(\n",
    "    df_completa[['CUSTOMER_ID', 'PRODUCT_ID', 'TN_median']],\n",
    "    on=['CUSTOMER_ID', 'PRODUCT_ID'],\n",
    "    how='inner' \n",
    ")\n",
    "\n",
    "print(df_merge.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91867cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Segundo DataFrame: promedios hist√≥ricos\n",
    "df2 = df_promedios.copy()\n",
    "df2 = df2.rename(columns={'TN_MEAN': 'TN'})\n",
    "df2['tipo'] = 'media_train'\n",
    "\n",
    "# Dejar columnas consistentes y unir\n",
    "cols = ['CUSTOMER_ID', 'PRODUCT_ID', 'TN', 'tipo']\n",
    "df_union = pd.concat([df4[cols], df2[cols]], axis=0, ignore_index=True)\n",
    "\n",
    "print(df_union.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bea7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por PRODUCT_ID y sumar TN (puede ser mean si prefer√≠s)\n",
    "df_out = (\n",
    "    df_union.groupby('PRODUCT_ID', as_index=False)['TN']\n",
    "    .sum()   \n",
    "    .rename(columns={'PRODUCT_ID': 'product_id', 'TN': 'tn'})\n",
    ")\n",
    "\n",
    "# Guardar a CSV\n",
    "df_out.to_csv('tn_por_producto_LightGBM.csv', index=False, float_format='%.5f')\n",
    "\n",
    "print(df_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b36305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completa = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')\n",
    "# Agrup√°s y calcul√°s la mediana\n",
    "\n",
    "df_completa = (\n",
    "    df_completa\n",
    "    .groupby(['CUSTOMER_ID', 'PRODUCT_ID'], as_index=False)['TN']\n",
    "    .median()\n",
    "    .rename(columns={'TN': 'TN_median'})\n",
    "    .sort_values('TN_median', ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_completa.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pred_final_lgbm_mediana.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6be0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medianas_lightgbm_e_historicas = pd.merge(\n",
    "    df_completa,\n",
    "    df_pred_final_lgbm_mediana[['CUSTOMER_ID', 'PRODUCT_ID', 'lgbm_pred_median']],\n",
    "    on=['CUSTOMER_ID', 'PRODUCT_ID'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(df_medianas_lightgbm_e_historicas.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6bf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medianas_lightgbm_e_historicas.to_csv('df_medianas_lightgbm_e_historicas.csv', index=False, float_format='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9579ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que df_pred tiene columnas: CUSTOMER_ID, PRODUCT_ID, TN, tipo\n",
    "df_merge = pd.merge(\n",
    "    df_completa,          # hist√≥ricos\n",
    "    df_union,      # predicciones\n",
    "    on=['CUSTOMER_ID', 'PRODUCT_ID'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eed07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['diff_TN_vs_median'] = df_merge['TN'] - df_merge['TN_median']\n",
    "\n",
    "# Tambi√©n pod√©s ver el valor absoluto (absoluto de la diferencia)\n",
    "df_merge['abs_diff_TN_vs_median'] = df_merge['diff_TN_vs_median'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_diff = df_merge.sort_values('abs_diff_TN_vs_median', ascending=False).head(100)\n",
    "print(top_diff[['CUSTOMER_ID', 'PRODUCT_ID', 'TN', 'TN_median', 'abs_diff_TN_vs_median']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
