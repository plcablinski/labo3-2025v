{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/train_val_NN_TORCH.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3938cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar conjuntos\n",
    "df_train = df_full[df_full['PERIODO'] <= 201908].copy()\n",
    "df_val = df_full[(df_full['PERIODO'] >= 201909) & (df_full['PERIODO'] <= 201910)].copy()\n",
    "df_pred = df_full[df_full['PERIODO'] == 201912].copy()\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ffc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "\n",
    "# Columnas categóricas a embeddings\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', \n",
    "            'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', \n",
    "            'PRODUCT_RANK_BIN']\n",
    "\n",
    "# Codificación para embeddings\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "    clases_entrenadas = set(le.classes_)\n",
    "    df_val[col] = df_val[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    df_pred[col] = df_pred[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "embedding_sizes = [\n",
    "    (df_train[col].nunique() + 1, min(50, (df_train[col].nunique() + 1) // 2))\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Excluir columnas que no deben ir al modelo\n",
    "excluir = ['PERIODO', 'CUSTOMER_ID', 'PRODUCT_ID', 'CLASE_LOG1P_Z', 'ORDINAL']\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in excluir and col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No los incluyas en ninguna de estas dos listas\n",
    "assert 'CUSTOMER_ID' not in feature_cols\n",
    "assert 'CUSTOMER_ID' not in cat_cols\n",
    "assert 'PRODUCT_ID' not in feature_cols\n",
    "assert 'PRODUCT_ID' not in cat_cols\n",
    "assert 'PERIODO' not in feature_cols\n",
    "assert 'PERIODO' not in cat_cols\n",
    "assert 'CLASE_LOG1P_Z' not in feature_cols\n",
    "assert 'CLASE_LOG1P_Z' not in cat_cols\n",
    "assert 'ORDINAL' not in feature_cols\n",
    "assert 'ORDINAL' not in cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col=None):\n",
    "        self.cat_data = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
    "        self.num_data = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
    "        self.has_target = target_col is not None\n",
    "        if self.has_target:\n",
    "            self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.cat_data[idx], self.num_data[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.cat_data[idx], self.num_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightedMSELossMultiStable(nn.Module):\n",
    "    def __init__(self, penalty_indices, coefficients, alpha=0.1, debug=False):\n",
    "        \"\"\"\n",
    "        penalty_indices: lista de índices de columnas en x_num (por ejemplo, [10, 11, ..., 20])\n",
    "        coefficients: lista de coeficientes (ordenados igual que penalty_indices)\n",
    "        alpha: fuerza de penalización (más bajo por estabilidad en redes grandes)\n",
    "        debug: si True, imprime el promedio del peso de penalización ocasionalmente\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.penalty_indices = penalty_indices\n",
    "        self.coefficients = coefficients\n",
    "        self.alpha = alpha\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, preds, targets, x_num):\n",
    "        # Penalización por muestra (shape: [batch_size, 1])\n",
    "        penalty = torch.ones_like(targets).float()\n",
    "\n",
    "        for idx, coef in zip(self.penalty_indices, self.coefficients):\n",
    "            val = x_num[:, idx:idx+1]  # shape: [batch_size, 1]\n",
    "            safe_val = torch.tanh(val)  # limitar la magnitud para evitar outliers\n",
    "            penalty += self.alpha * coef * safe_val.abs()  # siempre positivo\n",
    "\n",
    "        if self.debug and torch.rand(1).item() < 0.01:\n",
    "            print(f\"[LossDebug] Mean penalty: {penalty.mean().item():.4f}\")\n",
    "\n",
    "        error = (preds - targets) ** 2\n",
    "        weighted_error = penalty * error\n",
    "\n",
    "        return weighted_error.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad019ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_cols = ['TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z','TN_LAG_05_Z','TN_LAG_06_Z','TN_LAG_07_Z',\n",
    "'TN_LAG_08_Z','TN_LAG_09_Z','TN_LAG_10_Z','TN_LAG_11_Z']\n",
    "penalty_indices = [feature_cols.index(col) for col in penalty_cols]\n",
    "print(penalty_cols)\n",
    "print(penalty_indices)\n",
    "coefficients = [\n",
    "    0.236558,\n",
    "    0.178208,\n",
    "   -0.060031,\n",
    "   -0.161875,\n",
    "   -0.007775,\n",
    "    0.151936,\n",
    "    0.043933,\n",
    "    0.142839,\n",
    "    0.103804,\n",
    "    0.119211,\n",
    "    0.073671\n",
    "]\n",
    "# loss_fn = WeightedMSELossMulti(penalty_indices, coefficients, alpha=0.5)\n",
    "\n",
    "loss_fn = WeightedMSELossMultiStable(\n",
    "    penalty_indices=penalty_indices,\n",
    "    coefficients=coefficients,\n",
    "    alpha=0.1,      # más suave\n",
    "    debug=True      # activalo si querés monitorear internamente\n",
    ")\n",
    "\n",
    "#loss_fn = WeightedMSELossMulti(penalty_indices=penalty_cols, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(col in df_train.columns for col in cat_cols), \"Faltan columnas categóricas\"\n",
    "assert all(col in df_train.columns for col in feature_cols), \"Faltan columnas numéricas\"\n",
    "assert target_col in df_train.columns, \"Falta la variable objetivo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd112c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "\n",
    "train_dataset = TabularDataset(df_train, cat_cols, feature_cols, target_col)\n",
    "val_dataset = TabularDataset(df_val, cat_cols, feature_cols, target_col)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d04175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_cols)\n",
    "print(feature_cols)\n",
    "print(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TabularNNImproved(nn.Module):\n",
    "    def __init__(self, embedding_sizes, num_numerical, hidden_sizes=[512, 512, 256, 128], dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(ni, nf) for ni, nf in embedding_sizes\n",
    "        ])\n",
    "        embedding_dim = sum([nf for _, nf in embedding_sizes])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Total input size after embedding + numerical\n",
    "        input_size = embedding_dim + num_numerical\n",
    "\n",
    "        # Hidden layers\n",
    "        layers = []\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = h\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        x = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = torch.cat([x, x_num], dim=1)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Detectar si hay GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Crear el modelo\n",
    "model = TabularNNImproved(\n",
    "    embedding_sizes=embedding_sizes,\n",
    "    num_numerical=len(feature_cols),\n",
    "    hidden_sizes=[4096,2048,1024,512, 512, 256, 128],\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8314c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader=None, n_epochs=20, lr=1e-3, alpha=0.5, patience=3,\n",
    "    penalty_indices=None, coefficients=None\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = WeightedMSELossMultiStable(\n",
    "        penalty_indices=penalty_indices, coefficients=coefficients, alpha=alpha\n",
    "    )\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for cats, conts, y in train_loader:\n",
    "            cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(cats, conts)\n",
    "            loss = criterion(y_pred, y, conts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validación (solo si hay val_loader)\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            y_true_list = []\n",
    "            y_pred_list = []\n",
    "            with torch.no_grad():\n",
    "                for cats, conts, y in val_loader:\n",
    "                    cats, conts, y = cats.to(device), conts.to(device), y.to(device)\n",
    "                    y_pred = model(cats, conts)\n",
    "                    loss = criterion(y_pred, y, conts)\n",
    "                    val_loss += loss.item() * y.size(0)\n",
    "                    y_true_list.append(y.cpu().numpy())\n",
    "                    y_pred_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            y_true = np.concatenate(y_true_list)\n",
    "            y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | MAE: {mae:.4f} | R²: {r2:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if patience is not None and epochs_without_improvement >= patience:\n",
    "                    print(\"🔴 Early stopping triggered\")\n",
    "                    break\n",
    "        else:\n",
    "            # Si no hay val_loader, solo mostrar train_loss\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Restaurar el mejor modelo solo si hubo validación\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Retornar resultados solo si hubo validación, si no, devolver None\n",
    "    if val_loader is not None:\n",
    "        return y_true, y_pred\n",
    "    else:\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b6b5a",
   "metadata": {},
   "source": [
    "# Búsqueda de hiperparámetros (Grid Search)\n",
    "Probamos distintas combinaciones de hiperparámetros y seleccionamos la que da mejor MAE en validación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040764c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Definir el espacio de búsqueda\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'hidden_sizes': [\n",
    "        [1024, 512, 256],\n",
    "        [2048, 1024, 512, 256]\n",
    "    ],\n",
    "    'alpha': [0,0.1,0.3, 0.5, 0.7,0.9]\n",
    "}\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "param_combinations = list(product(\n",
    "    param_grid['lr'],\n",
    "    param_grid['dropout'],\n",
    "    param_grid['hidden_sizes'],\n",
    "    param_grid['alpha']\n",
    "))\n",
    "\n",
    "results = []\n",
    "best_mae = float('inf')\n",
    "\n",
    "# Loop de entrenamiento por combinación\n",
    "for lr, dropout, hidden_sizes, alpha in param_combinations:\n",
    "    print(f\"\\n🔧 Entrenando con: lr={lr}, dropout={dropout}, hidden_sizes={hidden_sizes}, alpha={alpha}\")\n",
    "\n",
    "    # Crear modelo y mover a dispositivo\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrenamiento corto para tuning\n",
    "    y_true_gs, y_pred_gs = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        n_epochs=8, lr=lr, alpha=alpha, patience=2,penalty_indices=penalty_indices,coefficients=coefficients\n",
    "    )\n",
    "\n",
    "    mae = mean_absolute_error(y_true_gs, y_pred_gs)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'dropout': dropout,\n",
    "        'hidden_sizes': hidden_sizes,\n",
    "        'alpha': alpha,\n",
    "        'mae': mae\n",
    "    })\n",
    "\n",
    "    print(f\"✅ MAE = {mae:.4f}\")\n",
    "\n",
    "    # Guardar modelo si es el mejor\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        torch.save(model.state_dict(), f\"best_model_mae{mae:.4f}_lr{lr}_do{dropout}_a{alpha}.pth\")\n",
    "        print(\"💾 Modelo guardado (mejor hasta ahora)\")\n",
    "\n",
    "    # Limpiar memoria GPU\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Convertir a DataFrame y mostrar top 5\n",
    "results_df = pd.DataFrame(results).sort_values(by='mae')\n",
    "print(\"\\n📊 Mejores combinaciones:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Guardar resultados a disco\n",
    "results_df.to_csv(\"gridsearch_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# ------------------------\n",
    "# Prepara el dataset final\n",
    "# ------------------------\n",
    "# train_dataset y val_dataset ya deberían estar definidos\n",
    "train_val_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader_full = DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------\n",
    "# Lista de hiperparámetros de los 10 mejores modelos\n",
    "# (¡completa esta lista con tus resultados!)\n",
    "# ------------------------\n",
    "configs = [\n",
    "    {\"lr\": 0.0005, \"dropout\": 0.3, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0,   \"n_epochs\": 8, \"name\": \"modelo_1\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.2, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0,   \"n_epochs\": 4, \"name\": \"modelo_2\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.3, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.1, \"n_epochs\": 8, \"name\": \"modelo_3\"},\n",
    "    {\"lr\": 0.0005, \"dropout\": 0.2, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.3, \"n_epochs\": 6, \"name\": \"modelo_4\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.3, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.3, \"n_epochs\": 8, \"name\": \"modelo_5\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.2, \"hidden_sizes\": [1024, 512, 256],       \"alpha\": 0.7, \"n_epochs\": 8, \"name\": \"modelo_6\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.3, \"hidden_sizes\": [1024, 512, 256],       \"alpha\": 0.1, \"n_epochs\": 8, \"name\": \"modelo_7\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.2, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.7, \"n_epochs\": 5, \"name\": \"modelo_8\"},\n",
    "    {\"lr\": 0.001,  \"dropout\": 0.3, \"hidden_sizes\": [2048, 1024, 512, 256], \"alpha\": 0.3, \"n_epochs\": 5, \"name\": \"modelo_9\"},\n",
    "    {\"lr\": 0.0005, \"dropout\": 0.2, \"hidden_sizes\": [1024, 512, 256],       \"alpha\": 0.1, \"n_epochs\": 4, \"name\": \"modelo_10\"},\n",
    "]\n",
    "\n",
    "# Actualizar epochs\n",
    "for cfg in configs:\n",
    "    cfg['n_epochs'] = int(math.ceil(cfg['n_epochs'] * 1.3))\n",
    "\n",
    "# Mostramos resultado\n",
    "for cfg in configs:\n",
    "    print(cfg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bc99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Entrenamiento y guardado\n",
    "# ------------------------\n",
    "\n",
    "for cfg in configs:\n",
    "    print(f\"\\n🔧 Entrenando {cfg['name']} - lr={cfg['lr']} dropout={cfg['dropout']} hidden={cfg['hidden_sizes']} alpha={cfg['alpha']} epochs={cfg['n_epochs']}\")\n",
    "\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg['hidden_sizes'],\n",
    "        dropout=cfg['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    # Entrena sobre TODO el set\n",
    "    y_true, y_pred = train_model(\n",
    "        model,\n",
    "        train_loader_full,\n",
    "        val_loader=None,\n",
    "        n_epochs=cfg['n_epochs'],\n",
    "        lr=cfg['lr'],\n",
    "        alpha=cfg['alpha'],\n",
    "        patience=None,  # No early stopping en entrenamiento final\n",
    "        penalty_indices=penalty_indices,\n",
    "        coefficients=coefficients\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{cfg['name']}_final.pt\")\n",
    "    print(f\"💾 Modelo guardado: {cfg['name']}_final.pt\")\n",
    "\n",
    "print(\"\\n✅ ¡Entrenamiento y guardado de los 10 mejores modelos finalizado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "print(lightgbm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "# --- SETUP ---\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "feature_cols = [col for col in df_train.columns if col != target_col]\n",
    "\n",
    "X = df_train[feature_cols].values\n",
    "y = df_train[target_col].values\n",
    "\n",
    "# --- OPTUNA OBJECTIVE ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 16),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.15, random_state=trial.number)\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[early_stopping(stopping_rounds=30, verbose=False)],\n",
    "    )\n",
    "    preds = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    return mae\n",
    "\n",
    "\n",
    "# --- OPTIMIZE ---\n",
    "N_MODELS = 10\n",
    "N_TRIALS = 80\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_lgbm, n_trials=N_TRIALS, n_jobs=28)\n",
    "\n",
    "# --- EXTRACT BEST 10 PARAMS ---\n",
    "top_lgbm_trials = study.trials_dataframe().sort_values(\"value\").head(N_MODELS)\n",
    "final_configs = []\n",
    "for i, row in top_lgbm_trials.iterrows():\n",
    "    params = row.filter(like='params_').to_dict()\n",
    "    params = {k.replace('params_', ''): v for k, v in params.items()}\n",
    "    # Asegurar enteros donde corresponde\n",
    "    for p in [\"num_leaves\", \"max_depth\", \"n_estimators\", \"min_child_samples\"]:\n",
    "        params[p] = int(params[p])\n",
    "    params[\"random_state\"] = 42\n",
    "    params[\"n_jobs\"] = -1\n",
    "    final_configs.append(params)\n",
    "\n",
    "print(final_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa797b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48913620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenar train y val\n",
    "df_full_train = pd.concat([df_train, df_val], axis=0).reset_index(drop=True)\n",
    "\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "feature_cols = [col for col in df_full_train.columns if col != target_col]\n",
    "\n",
    "X_full = df_full_train[feature_cols].values\n",
    "y_full = df_full_train[target_col].values\n",
    "\n",
    "# Tus hiperparámetros copiados (reemplazá si lo vas a cargar de un archivo)\n",
    "param_list = [{'colsample_bytree': 0.9956156662176132, 'learning_rate': 0.150647911151466, 'max_depth': 14, 'min_child_samples': 62, 'min_split_gain': 0.18618312765512604, 'n_estimators': 861, 'num_leaves': 222, 'reg_alpha': 0.0008184678353621553, 'reg_lambda': 0.00181071360138272, 'subsample': 0.8288045639730978, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9900835480198793, 'learning_rate': 0.15285688565610836, 'max_depth': 14, 'min_child_samples': 22, 'min_split_gain': 0.21318221132522686, 'n_estimators': 863, 'num_leaves': 218, 'reg_alpha': 0.005832497428105893, 'reg_lambda': 0.00048332566435359725, 'subsample': 0.966212591402847, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9810381286523686, 'learning_rate': 0.17126526528861452, 'max_depth': 15, 'min_child_samples': 20, 'min_split_gain': 0.18620761002164188, 'n_estimators': 887, 'num_leaves': 233, 'reg_alpha': 7.525959386634591, 'reg_lambda': 0.002932489509766247, 'subsample': 0.9855006418021718, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9991282388618083, 'learning_rate': 0.04595382767022815, 'max_depth': 11, 'min_child_samples': 41, 'min_split_gain': 0.21027442904661392, 'n_estimators': 876, 'num_leaves': 216, 'reg_alpha': 0.0009948983243008944, 'reg_lambda': 0.0007851771585517262, 'subsample': 0.8348381160448405, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9997415495087778, 'learning_rate': 0.207311488631083, 'max_depth': 11, 'min_child_samples': 40, 'min_split_gain': 0.21104245822261147, 'n_estimators': 883, 'num_leaves': 256, 'reg_alpha': 0.0010361594632463222, 'reg_lambda': 1.430438254956134e-07, 'subsample': 0.8317847564939153, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.587328586423979, 'learning_rate': 0.1861100303288766, 'max_depth': 11, 'min_child_samples': 17, 'min_split_gain': 0.003592855256288212, 'n_estimators': 879, 'num_leaves': 226, 'reg_alpha': 0.07837411453395908, 'reg_lambda': 2.5489037442005937e-07, 'subsample': 0.8302404263289589, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.5895955174460985, 'learning_rate': 0.14941003478023016, 'max_depth': 15, 'min_child_samples': 20, 'min_split_gain': 0.19615530216306443, 'n_estimators': 896, 'num_leaves': 221, 'reg_alpha': 5.010927643947572, 'reg_lambda': 0.0007780698199438967, 'subsample': 0.9998177336233038, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.5837101193747908, 'learning_rate': 0.15338979834994065, 'max_depth': 11, 'min_child_samples': 64, 'min_split_gain': 0.19477841520356795, 'n_estimators': 856, 'num_leaves': 217, 'reg_alpha': 0.07087365761730123, 'reg_lambda': 6.887002502845986e-08, 'subsample': 0.8244503737000027, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9891294837026124, 'learning_rate': 0.14710934806711512, 'max_depth': 14, 'min_child_samples': 66, 'min_split_gain': 0.1888950104677536, 'n_estimators': 898, 'num_leaves': 230, 'reg_alpha': 0.045761956230443775, 'reg_lambda': 0.000545124313416254, 'subsample': 0.8224562428405393, 'random_state': 42, 'n_jobs': -1}, \n",
    "{'colsample_bytree': 0.9998748104919317, 'learning_rate': 0.15126642557505776, 'max_depth': 14, 'min_child_samples': 20, 'min_split_gain': 0.25087912345115637, 'n_estimators': 849, 'num_leaves': 219, 'reg_alpha': 0.001191096159067221, 'reg_lambda': 0.0016155410949331161, 'subsample': 0.8372252476108392, 'random_state': 42, 'n_jobs': -1}]\n",
    "\n",
    "models = []\n",
    "for i, params in enumerate(param_list):\n",
    "    print(f\"Entrenando modelo {i+1}/10...\")\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_full, y_full)\n",
    "    models.append(model)\n",
    "    # Guardar modelo\n",
    "    joblib.dump(model, f'lgbm_model_{i+1:02d}.pkl')\n",
    "\n",
    "print(\"¡Entrenamiento y guardado de modelos finalizado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Cargar los 10 modelos LightGBM entrenados\n",
    "lgbm_models = []\n",
    "for i in range(1, 11):\n",
    "    model = joblib.load(f'lgbm_model_{i:02d}.pkl')\n",
    "    lgbm_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e148e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- Configuración de columnas ---\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "feature_cols = [col for col in df_pred.columns if col != target_col]\n",
    "\n",
    "# --- Predicción LightGBM (10 modelos) ---\n",
    "lgbm_preds = []\n",
    "\n",
    "# Usá DataFrame, no .values para evitar el warning de feature names\n",
    "X_pred_lgbm = df_pred[feature_cols]\n",
    "\n",
    "for i, model in enumerate(lgbm_models):\n",
    "    print(f\"Prediciendo LightGBM {i+1}/10...\")\n",
    "    preds = model.predict(X_pred_lgbm)\n",
    "    lgbm_preds.append(preds)\n",
    "\n",
    "lgbm_preds = np.stack(lgbm_preds)  # shape (10, N)\n",
    "df_pred['lgbm_ensemble_mean'] = lgbm_preds.mean(axis=0)\n",
    "df_pred['lgbm_ensemble_median'] = np.median(lgbm_preds, axis=0)\n",
    "# Opcional: guardar cada predicción individual\n",
    "for i in range(10):\n",
    "    df_pred[f'lgbm_pred_LOG1P_Z_{i+1}'] = lgbm_preds[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "target_col = 'CLASE_LOG1P_Z'\n",
    "\n",
    "# Columnas categóricas a embeddings\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', \n",
    "            'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', \n",
    "            'PRODUCT_RANK_BIN']\n",
    "\n",
    "# Codificación para embeddings\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "    clases_entrenadas = set(le.classes_)\n",
    "    df_val[col] = df_val[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    df_pred[col] = df_pred[col].map(lambda x: le.transform([x])[0] if x in clases_entrenadas else 0)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "embedding_sizes = [\n",
    "    (df_train[col].nunique() + 1, min(50, (df_train[col].nunique() + 1) // 2))\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Excluir columnas que no deben ir al modelo\n",
    "excluir = ['PERIODO', 'CUSTOMER_ID', 'PRODUCT_ID', 'CLASE_LOG1P_Z', 'ORDINAL']\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col not in excluir and col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Predicción MLP (PyTorch) (10 modelos) ---\n",
    "# Asegurar tipos correctos en el DataFrame\n",
    "for col in cat_cols:\n",
    "    df_pred[col] = df_pred[col].astype(np.int64)\n",
    "for col in feature_cols:\n",
    "    df_pred[col] = df_pred[col].astype(np.float32)\n",
    "\n",
    "X_cats = torch.LongTensor(df_pred[cat_cols].values)\n",
    "X_conts = torch.FloatTensor(df_pred[feature_cols].values)\n",
    "ds_pred = TensorDataset(X_cats, X_conts)\n",
    "pred_loader = DataLoader(ds_pred, batch_size=8192, shuffle=False)\n",
    "\n",
    "def predict_model(model, loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_cats, X_conts in loader:\n",
    "            X_cats = X_cats.to(device)\n",
    "            X_conts = X_conts.to(device)\n",
    "            output = model(X_cats, X_conts)\n",
    "            preds.append(output.cpu().numpy().squeeze())\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "mlp_preds = []\n",
    "\n",
    "for i, cfg in enumerate(configs):\n",
    "    print(f\"Prediciendo MLP {i+1}/10...\")\n",
    "    model = TabularNNImproved(\n",
    "        embedding_sizes=embedding_sizes,\n",
    "        num_numerical=len(feature_cols),\n",
    "        hidden_sizes=cfg['hidden_sizes'],\n",
    "        dropout=cfg['dropout']\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(f\"{cfg['name']}_final.pt\", map_location=device))\n",
    "    preds = predict_model(model, pred_loader, device)\n",
    "    mlp_preds.append(preds)\n",
    "\n",
    "# Guardar cada predicción individual\n",
    "for i, cfg in enumerate(configs):\n",
    "    df_pred[f'mlp_pred_LOG1P_Z_{i+1}'] = mlp_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07627ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['CUSTOMER_ID', 'PRODUCT_ID'] + [\n",
    "    col for col in df_pred.columns \n",
    "    if ('mlp_pred_LOG1P_Z_' in col) or ('lgbm_pred_LOG1P_Z_' in col)\n",
    "]\n",
    "df_pred_final = df_pred[cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros usados en la transformación original\n",
    "mean_train = 0.4756487705286022\n",
    "std_train = 0.6596333033693771\n",
    "\n",
    "def inverse_log_transform_signed(x):\n",
    "    return np.sign(x) * np.expm1(np.abs(x))\n",
    "\n",
    "def inv_transform(arr):\n",
    "    # Paso 1: desescalar Z-score\n",
    "    log1p_vals = arr * std_train + mean_train\n",
    "    # Paso 2: inversa de log1p con signo\n",
    "    orig_vals = inverse_log_transform_signed(log1p_vals)\n",
    "    return orig_vals\n",
    "\n",
    "# Crear columnas nuevas con sufijo \"_ORIG\"\n",
    "for col in df_pred_final.columns:\n",
    "    if col not in ['CUSTOMER_ID', 'PRODUCT_ID']:\n",
    "        df_pred_final[col.replace('_LOG1P_Z_', '_ORIG_')] = inv_transform(df_pred_final[col].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.width', 400)\n",
    "df_pred_final = df_pred_final.loc[:, ~df_pred_final.columns.str.contains('LOG1P_Z')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccioná solo las columnas de predicción de modelos individuales\n",
    "pred_cols = [col for col in df_pred_final.columns if col.startswith('lgbm_pred_ORIG_') or col.startswith('mlp_pred_ORIG_')]\n",
    "# Por cada columna pred_cols de df_pred_final hacerla cero si es negativa\n",
    "for col in pred_cols:\n",
    "    df_pred_final[col] = df_pred_final[col].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculá media y mediana fila a fila\n",
    "df_pred_final['df_pred_mean'] = df_pred_final[pred_cols].mean(axis=1)\n",
    "df_pred_final['df_pred_median'] = df_pred_final[pred_cols].median(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Seleccionar solo las columnas LGBM\n",
    "lgbm_cols = [col for col in df_pred_final.columns if col.startswith('lgbm_pred_ORIG_')]\n",
    "\n",
    "# Paso 2: Calcular media y mediana por fila (e.g. por combinación CUSTOMER_ID + PRODUCT_ID)\n",
    "df_pred_final['lgbm_pred_mean'] = df_pred_final[lgbm_cols].mean(axis=1)\n",
    "df_pred_final['lgbm_pred_median'] = df_pred_final[lgbm_cols].median(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d767914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_final_mediana = df_pred_final[['CUSTOMER_ID', 'PRODUCT_ID', 'df_pred_median']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f835c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_final_lgbm_mediana = df_pred_final[['CUSTOMER_ID', 'PRODUCT_ID', 'lgbm_pred_median']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_final_lgbm_mediana.to_csv('df_pred_final_lgbm_mediana.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pred_final_lgbm_mediana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986da7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar en df_promedios promedios_tn_no_mueven_agujas.csv\n",
    "df_promedios = pd.read_csv('promedios_tn_no_mueven_aguja.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer DataFrame: de predicciones (mediana del modelo)\n",
    "df1 = df_pred_final_mediana.copy()\n",
    "df1 = df1.rename(columns={'df_pred_median': 'TN'})\n",
    "df1['tipo'] = 'mediana_pred'\n",
    "\n",
    "# Segundo DataFrame: promedios históricos\n",
    "df2 = df_promedios.copy()\n",
    "df2 = df2.rename(columns={'TN_MEAN': 'TN'})\n",
    "df2['tipo'] = 'media_train'\n",
    "\n",
    "# Dejar columnas consistentes y unir\n",
    "cols = ['CUSTOMER_ID', 'PRODUCT_ID', 'TN', 'tipo']\n",
    "df_union = pd.concat([df1[cols], df2[cols]], axis=0, ignore_index=True)\n",
    "\n",
    "print(df_union.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por PRODUCT_ID y sumar TN (puede ser mean si preferís)\n",
    "df_out = (\n",
    "    df_union.groupby('PRODUCT_ID', as_index=False)['TN']\n",
    "    .sum()   # Cambiá a .mean() si querés promedio\n",
    "    .rename(columns={'PRODUCT_ID': 'product_id', 'TN': 'tn'})\n",
    ")\n",
    "\n",
    "# Guardar a CSV\n",
    "df_out.to_csv('tn_por_producto.csv', index=False, float_format='%.5f')\n",
    "\n",
    "print(df_out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b487252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer DataFrame: de predicciones (mediana del modelo)\n",
    "df4 = df_pred_final_lgbm_mediana.copy()\n",
    "df4 = df4.rename(columns={'lgbm_pred_median': 'TN'})\n",
    "df4['tipo'] = 'mediana_lightGBMpred'\n",
    "print(df4.head(10))\n",
    "print(df_completa.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660df3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, asegurate de que los nombres de las columnas coincidan exactamente\n",
    "df_merge = df4[['CUSTOMER_ID', 'PRODUCT_ID', 'TN']].merge(\n",
    "    df_completa[['CUSTOMER_ID', 'PRODUCT_ID', 'TN_median']],\n",
    "    on=['CUSTOMER_ID', 'PRODUCT_ID'],\n",
    "    how='inner' \n",
    ")\n",
    "\n",
    "print(df_merge.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91867cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Segundo DataFrame: promedios históricos\n",
    "df2 = df_promedios.copy()\n",
    "df2 = df2.rename(columns={'TN_MEAN': 'TN'})\n",
    "df2['tipo'] = 'media_train'\n",
    "\n",
    "# Dejar columnas consistentes y unir\n",
    "cols = ['CUSTOMER_ID', 'PRODUCT_ID', 'TN', 'tipo']\n",
    "df_union = pd.concat([df4[cols], df2[cols]], axis=0, ignore_index=True)\n",
    "\n",
    "print(df_union.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bea7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por PRODUCT_ID y sumar TN (puede ser mean si preferís)\n",
    "df_out = (\n",
    "    df_union.groupby('PRODUCT_ID', as_index=False)['TN']\n",
    "    .sum()   \n",
    "    .rename(columns={'PRODUCT_ID': 'product_id', 'TN': 'tn'})\n",
    ")\n",
    "\n",
    "# Guardar a CSV\n",
    "df_out.to_csv('tn_por_producto_LightGBM.csv', index=False, float_format='%.5f')\n",
    "\n",
    "print(df_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b36305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completa = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')\n",
    "# Agrupás y calculás la mediana\n",
    "\n",
    "df_completa = (\n",
    "    df_completa\n",
    "    .groupby(['CUSTOMER_ID', 'PRODUCT_ID'], as_index=False)['TN']\n",
    "    .median()\n",
    "    .rename(columns={'TN': 'TN_median'})\n",
    "    .sort_values('TN_median', ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_completa.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pred_final_lgbm_mediana.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6be0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medianas_lightgbm_e_historicas = pd.merge(\n",
    "    df_completa,\n",
    "    df_pred_final_lgbm_mediana[['CUSTOMER_ID', 'PRODUCT_ID', 'lgbm_pred_median']],\n",
    "    on=['CUSTOMER_ID', 'PRODUCT_ID'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(df_medianas_lightgbm_e_historicas.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6bf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medianas_lightgbm_e_historicas.to_csv('df_medianas_lightgbm_e_historicas.csv', index=False, float_format='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9579ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que df_pred tiene columnas: CUSTOMER_ID, PRODUCT_ID, TN, tipo\n",
    "df_merge = pd.merge(\n",
    "    df_completa,          # históricos\n",
    "    df_union,      # predicciones\n",
    "    on=['CUSTOMER_ID', 'PRODUCT_ID'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eed07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['diff_TN_vs_median'] = df_merge['TN'] - df_merge['TN_median']\n",
    "\n",
    "# También podés ver el valor absoluto (absoluto de la diferencia)\n",
    "df_merge['abs_diff_TN_vs_median'] = df_merge['diff_TN_vs_median'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_diff = df_merge.sort_values('abs_diff_TN_vs_median', ascending=False).head(100)\n",
    "print(top_diff[['CUSTOMER_ID', 'PRODUCT_ID', 'TN', 'TN_median', 'abs_diff_TN_vs_median']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
