{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41eccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducir memoria automáticamente\n",
    "def optimizar_memoria(df):\n",
    "    for col in df.select_dtypes(include=['int64', 'int32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    for col in df.select_dtypes(include=['float64', 'float32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11bc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar de df_agrupado las filas donde la columna A_PREDECIR sea 'N'\n",
    "df_full = df_full[df_full['A_PREDECIR'] != 'N']\n",
    "df_full = df_full.drop(columns=['A_PREDECIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado = (\n",
    "    df_full.groupby([\n",
    "        'ORDINAL','PERIODO', 'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE',\n",
    "        'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'PRODUCT_ID'\n",
    "    ], as_index=False)[['TN', 'CLASE']].sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las filas de df_agrupado ordernadas por TN de menor a mayor\n",
    "df_agrupado = df_agrupado.sort_values(by='TN', ascending=True)\n",
    "# Mostrar las primeras 10 filas de df_agrupado\n",
    "print(df_agrupado.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "# Calcular los días del mes usando las columnas ANIO y MES\n",
    "\n",
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si ANIO==2019 y MES en [6, 8], y 0 en caso contrario\n",
    "df_agrupado['MES_PROBLEMATICO'] = np.where(\n",
    "       (df_agrupado['ANIO'] == 2019) & (df_agrupado['MES'].isin([6, 8])),\n",
    "       1, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "ROLL_BASE = 12\n",
    "CAIDA_UMBRAL = -100\n",
    "\n",
    "def add_global_features(group):\n",
    "    group = group.sort_values('PERIODO').copy()\n",
    "    group['FECHA'] = pd.to_datetime(group['PERIODO'].astype(str), format='%Y%m')\n",
    "    group['MES'] = group['FECHA'].dt.month\n",
    "    group['IS_FEBRERO'] = (group['MES'] == 2).astype(int)\n",
    "    group['ESTOY_PREDICIENDO_FEBRERO'] = (group['MES'] == 12).astype(int)\n",
    "\n",
    "    group['CAIDA_ABRUPTA'] = (group['TN'].diff(1) < CAIDA_UMBRAL).astype(int)\n",
    "    \n",
    "    # Tomar solo los febreros históricos\n",
    "    febreros = group[group['MES'] == 2][['PERIODO', 'TN']].sort_values('PERIODO').reset_index(drop=True)\n",
    "    # Calcular el rolling promedio (sin el actual, siempre pasado)\n",
    "    febreros['PROM_ULT_3_FEBREROS'] = febreros['TN'].shift(1).rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "    # Hacer merge con el original: para cada PERIODO, ¿cuál fue el último promedio disponible?\n",
    "    # Vamos a usar merge_asof para asignar el promedio más reciente antes o igual al periodo actual\n",
    "    group = group.sort_values('PERIODO').reset_index(drop=True)\n",
    "    group = pd.merge_asof(\n",
    "        group, \n",
    "        febreros[['PERIODO', 'PROM_ULT_3_FEBREROS']],\n",
    "        by=None, left_on='PERIODO', right_on='PERIODO', \n",
    "        direction='backward', \n",
    "        allow_exact_matches=False\n",
    "    )\n",
    "    # Si no hay historial, pone cero (puede cambiar a np.nan si preferís)\n",
    "    group['PROM_ULT_3_FEBREROS'] = group['PROM_ULT_3_FEBREROS'].fillna(0)\n",
    "    group['DIF_TN_VS_FEBREROS_ULT_3'] = group['TN'] - group['PROM_ULT_3_FEBREROS']\n",
    "\n",
    "\n",
    "\n",
    "    # Max histórico HASTA el momento (expanding + shift)\n",
    "    group['TN_MAX_HISTORICO'] = group['TN'].shift(1).expanding().max().fillna(0)\n",
    "    group['TN_DIST_A_MAX_HIST'] = group['TN_MAX_HISTORICO'] - group['TN']\n",
    "    group['TN_RATIO_VS_MAX_HIST'] = (group['TN'] / (group['TN_MAX_HISTORICO'] + 1e-8)).clip(lower=0, upper=5)\n",
    "\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "def calcular_pendientes_grupo(group, periodos_list):\n",
    "    group = add_global_features(group)\n",
    "    group = group.sort_values(by='PERIODO').copy()\n",
    "    n = len(group)\n",
    "    y_values = group['TN'].values\n",
    "    for cant in periodos_list:\n",
    "        means = np.zeros(n)\n",
    "        pendientes = np.zeros(n)\n",
    "        ewmas = np.zeros(n)\n",
    "        medians = np.zeros(n)\n",
    "        minimo = np.zeros(n)\n",
    "        maximo = np.zeros(n)\n",
    "        stds = np.zeros(n)\n",
    "        skews = np.zeros(n)\n",
    "        kurts = np.zeros(n)\n",
    "        growths = np.zeros(n)\n",
    "        iqrs = np.zeros(n)\n",
    "        sums = np.zeros(n)\n",
    "        count_pos = np.zeros(n)\n",
    "        pct_zero = np.zeros(n)\n",
    "        last = np.zeros(n)\n",
    "        last_diff = np.zeros(n)\n",
    "\n",
    "        coef_var = np.zeros(n)\n",
    "        maxmin_ratio = np.zeros(n)\n",
    "        rango = np.zeros(n)\n",
    "        rango_rel = np.zeros(n)\n",
    "        last_vs_median = np.zeros(n)\n",
    "        cambio_ventana = np.zeros(n)\n",
    "        zeros_end = np.zeros(n)\n",
    "        last_pct_sum = np.zeros(n)\n",
    "        pct_90 = np.zeros(n)\n",
    "        pct_10 = np.zeros(n)\n",
    "        pct_width = np.zeros(n)\n",
    "\n",
    "        # --- Comparaciones TN vs rolling stats ---\n",
    "        tn_minus_mean = np.zeros(n)\n",
    "        tn_minus_median = np.zeros(n)\n",
    "        tn_minus_ewma = np.zeros(n)\n",
    "        tn_over_mean = np.zeros(n)\n",
    "        tn_over_median = np.zeros(n)\n",
    "        tn_over_ewma = np.zeros(n)\n",
    "\n",
    "        x = np.arange(cant)\n",
    "        for i in range(n):\n",
    "            # Solo procesar si la ventana está completa\n",
    "            if i >= cant - 1:\n",
    "                y = y_values[i - (cant - 1): i + 1]\n",
    "                means[i] = np.mean(y)\n",
    "                try:\n",
    "                    pendientes[i] = np.polyfit(x, y, 1)[0]\n",
    "                except:\n",
    "                    pendientes[i] = 0\n",
    "                ewmas[i] = pd.Series(y).ewm(span=cant, adjust=False).mean().iloc[-1]\n",
    "                medians[i] = np.median(y)\n",
    "                minimo[i] = np.min(y)\n",
    "                maximo[i] = np.max(y)\n",
    "                std_y = np.std(y)\n",
    "                mean_y = means[i]\n",
    "                stds[i] = std_y\n",
    "                skews[i] = skew(y) if std_y > 0 else 0\n",
    "                kurts[i] = kurtosis(y) if std_y > 0 else 0\n",
    "                growths[i] = (y[-1] - y[0]) / (y[0] + 1e-8) if y[0] != 0 else 0\n",
    "                iqrs[i] = np.percentile(y, 75) - np.percentile(y, 25)\n",
    "                sums[i] = np.sum(y)\n",
    "                count_pos[i] = np.sum(y > 0)\n",
    "                pct_zero[i] = np.mean(y == 0)\n",
    "                last[i] = y[-1]\n",
    "                last_diff[i] = y[-1] - y[-2] if cant > 1 else 0\n",
    "\n",
    "                coef_var[i] = std_y / (mean_y + 1e-8) if mean_y != 0 else 0\n",
    "                maxmin_ratio[i] = maximo[i] / (minimo[i] + 1e-8) if minimo[i] != 0 else 0\n",
    "                rango[i] = maximo[i] - minimo[i]\n",
    "                rango_rel[i] = rango[i] / (minimo[i] + 1e-8) if minimo[i] != 0 else 0\n",
    "                last_vs_median[i] = y[-1] - np.median(y)\n",
    "                # Cambio de ventana anterior a actual\n",
    "                if i >= 2 * cant - 1:\n",
    "                    y_prev = y_values[i - (2 * cant - 1):i - cant + 1]\n",
    "                    mean_prev = np.mean(y_prev)\n",
    "                    cambio_ventana[i] = (mean_y - mean_prev) / (mean_prev + 1e-8) if mean_prev != 0 else 0\n",
    "                zeros_end[i] = np.argmax(y[::-1] != 0)\n",
    "                last_pct_sum[i] = y[-1] / (np.sum(y) + 1e-8) if np.sum(y) != 0 else 0\n",
    "                pct_90[i] = np.percentile(y, 90)\n",
    "                pct_10[i] = np.percentile(y, 10)\n",
    "                pct_width[i] = pct_90[i] - pct_10[i]\n",
    "\n",
    "                # Comparaciones TN vs rolling stats\n",
    "                tn_actual = y_values[i]\n",
    "                tn_minus_mean[i] = tn_actual - means[i]\n",
    "                tn_minus_median[i] = tn_actual - medians[i]\n",
    "                tn_minus_ewma[i] = tn_actual - ewmas[i]\n",
    "                tn_over_mean[i] = tn_actual / (means[i] + 1e-8) if means[i] != 0 else 0\n",
    "                tn_over_median[i] = tn_actual / (medians[i] + 1e-8) if medians[i] != 0 else 0\n",
    "                tn_over_ewma[i] = tn_actual / (ewmas[i] + 1e-8) if ewmas[i] != 0 else 0\n",
    "            # Si la ventana no está completa, todo queda en cero (ya inicializado)\n",
    "\n",
    "        group[f'TN_MEAN_{str(cant).zfill(2)}'] = means\n",
    "        group[f'PENDIENTE_TENDENCIA_{cant}'] = pendientes\n",
    "        group[f'TN_EWMA_{str(cant).zfill(2)}'] = ewmas\n",
    "        group[f'TN_MEDIAN_{str(cant).zfill(2)}'] = medians\n",
    "        group[f'TN_MIN_{str(cant).zfill(2)}'] = minimo\n",
    "        group[f'TN_MAX_{str(cant).zfill(2)}'] = maximo\n",
    "        group[f'TN_STD_{str(cant).zfill(2)}'] = stds\n",
    "        group[f'TN_SKEW_{str(cant).zfill(2)}'] = skews\n",
    "        group[f'TN_KURT_{str(cant).zfill(2)}'] = kurts\n",
    "        group[f'TN_GROWTH_{str(cant).zfill(2)}'] = growths\n",
    "        group[f'TN_IQR_{str(cant).zfill(2)}'] = iqrs\n",
    "        group[f'TN_SUM_{str(cant).zfill(2)}'] = sums\n",
    "        group[f'TN_COUNT_POS_{str(cant).zfill(2)}'] = count_pos\n",
    "        group[f'TN_PCT_ZERO_{str(cant).zfill(2)}'] = pct_zero\n",
    "        group[f'TN_LAST_{str(cant).zfill(2)}'] = last\n",
    "        group[f'TN_LAST_DIFF_{str(cant).zfill(2)}'] = last_diff\n",
    "\n",
    "        group[f'TN_COEF_VAR_{cant}'] = coef_var\n",
    "        group[f'TN_MAXMIN_RATIO_{cant}'] = maxmin_ratio\n",
    "        group[f'TN_RANGO_{cant}'] = rango\n",
    "        group[f'TN_RANGO_REL_{cant}'] = rango_rel\n",
    "        group[f'TN_LAST_VS_MEDIAN_{cant}'] = last_vs_median\n",
    "        group[f'TN_CHANGE_PREV_WINDOW_{cant}'] = cambio_ventana\n",
    "        group[f'TN_ZEROS_END_{cant}'] = zeros_end\n",
    "        group[f'TN_LAST_PCT_SUM_{cant}'] = last_pct_sum\n",
    "        group[f'TN_PCT90_{cant}'] = pct_90\n",
    "        group[f'TN_PCT10_{cant}'] = pct_10\n",
    "        group[f'TN_PCT_WIDTH_{cant}'] = pct_width\n",
    "\n",
    "        # Comparaciones TN vs rolling stats\n",
    "        group[f'TN_MINUS_MEAN_{str(cant).zfill(2)}'] = tn_minus_mean\n",
    "        group[f'TN_MINUS_MEDIAN_{str(cant).zfill(2)}'] = tn_minus_median\n",
    "        group[f'TN_MINUS_EWMA_{str(cant).zfill(2)}'] = tn_minus_ewma\n",
    "        group[f'TN_OVER_MEAN_{str(cant).zfill(2)}'] = tn_over_mean\n",
    "        group[f'TN_OVER_MEDIAN_{str(cant).zfill(2)}'] = tn_over_median\n",
    "        group[f'TN_OVER_EWMA_{str(cant).zfill(2)}'] = tn_over_ewma\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_pendientes_parallel(df, periodos_list=[3, 6, 9, 12, 18], n_jobs=28):\n",
    "    grupos = [group for _, group in df.groupby('PRODUCT_ID')]\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', verbose=10)(\n",
    "        delayed(calcular_pendientes_grupo)(group, periodos_list) for group in grupos\n",
    "    )\n",
    "    df_final = pd.concat(resultados, ignore_index=True)\n",
    "    df_final.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    df_final.fillna(0, inplace=True)\n",
    "    return df_final\n",
    "\n",
    "df_agrupado = calcular_pendientes_parallel(df_agrupado, periodos_list=[3, 6, 9, 12, 18], n_jobs=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir las columnas PRODUCT_ID,PERIODO,TN y las que contienen PENDIENTE de df_agrupado que muestre 100 columnas por pantalla\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Mostrar solo las columnas PRODUCT_ID, PERIODO, TN y las que contienen 'PENDIENTE'\n",
    "cols = [col for col in df_agrupado.columns if col in ['PRODUCT_ID', 'PERIODO', 'TN'] or 'PENDIENTE' in col]\n",
    "# Mostrar las últimas 10 filas de df_agrupado de PRODUCT_ID== 20001\n",
    "display(df_agrupado[cols][df_agrupado['PRODUCT_ID'] == 20001])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lags = [3, 6, 9, 12, 18]\n",
    "\n",
    "# Definí la columna de agrupamiento, por ejemplo:\n",
    "group_cols = ['PRODUCT_ID']  # Cambia esto si tu agrupamiento es por otra cosa\n",
    "\n",
    "# Ordenar el DataFrame por las claves necesarias\n",
    "df_agrupado = df_agrupado.sort_values(group_cols + ['ORDINAL'], ascending=[True, False])\n",
    "\n",
    "for lag in lags:\n",
    "    lag_col = f'TN_LAG_{lag:02d}'\n",
    "    delta_col = f'TN_DELTA_{lag:02d}'\n",
    "    # Calcular el lag\n",
    "    df_agrupado[lag_col] = (\n",
    "        df_agrupado.groupby(group_cols)['TN']\n",
    "        .shift(-lag)  # Sentido descendente en ORDINAL\n",
    "        .fillna(0)\n",
    "    )\n",
    "    # Calcular el delta\n",
    "    df_agrupado[delta_col] = df_agrupado['TN'] - df_agrupado[lag_col]\n",
    "\n",
    "# Si querés, podés ver las primeras filas\n",
    "print(df_agrupado[[f'TN_LAG_{l:02d}' for l in lags] + [f'TN_DELTA_{l:02d}' for l in lags]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "windows = [3, 6, 9, 12, 18]\n",
    "group_cols = ['PRODUCT_ID']  # agregá más claves si corresponde\n",
    "\n",
    "for w in windows:\n",
    "    # CRECIMIENTO PERCENTUAL (respecto al lag de la ventana)\n",
    "    lag_col = f'TN_LAG_{w:02d}'\n",
    "    growth_col = f'TN_GROWTH_{w:02d}'\n",
    "    if lag_col in df_agrupado.columns:\n",
    "        df_agrupado[growth_col] = (df_agrupado['TN'] - df_agrupado[lag_col]) / (df_agrupado[lag_col].replace(0, np.nan))\n",
    "        df_agrupado[growth_col] = df_agrupado[growth_col].fillna(0)\n",
    "    \n",
    "    # CANTIDAD DE CEROS EN LA VENTANA\n",
    "    count_zero_col = f'TN_COUNT_ZERO_{w:02d}'\n",
    "    df_agrupado[count_zero_col] = (\n",
    "        df_agrupado\n",
    "        .groupby(group_cols)['TN']\n",
    "        .rolling(window=w, min_periods=1)\n",
    "        .apply(lambda x: np.sum(x == 0), raw=True)\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    \n",
    "    # NÚMERO DE CAMBIOS DE SIGNO EN LA VENTANA\n",
    "    sign_change_col = f'TN_SIGN_CHANGES_{w:02d}'\n",
    "    df_agrupado[sign_change_col] = (\n",
    "        df_agrupado\n",
    "        .groupby(group_cols)['TN']\n",
    "        .rolling(window=w, min_periods=2)\n",
    "        .apply(lambda x: np.sum(np.diff(np.sign(x)) != 0), raw=True)\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    \n",
    "    # COEFICIENTE DE VARIACIÓN EN LA VENTANA\n",
    "    coef_var_col = f'TN_COEF_VAR_{w:02d}'\n",
    "    mean_col = f'TN_MEAN_{w:02d}'\n",
    "    std_col = f'TN_STD_{w:02d}'\n",
    "    # Si ya tenés mean y std, sólo calculás la división, si no, calculás aquí\n",
    "    if mean_col in df_agrupado.columns and std_col in df_agrupado.columns:\n",
    "        df_agrupado[coef_var_col] = df_agrupado[std_col] / (df_agrupado[mean_col].replace(0, np.nan))\n",
    "        df_agrupado[coef_var_col] = df_agrupado[coef_var_col].fillna(0)\n",
    "    else:\n",
    "        df_agrupado[mean_col] = (\n",
    "            df_agrupado.groupby(group_cols)['TN']\n",
    "            .rolling(window=w, min_periods=1).mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        df_agrupado[std_col] = (\n",
    "            df_agrupado.groupby(group_cols)['TN']\n",
    "            .rolling(window=w, min_periods=1).std().fillna(0)\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        df_agrupado[coef_var_col] = df_agrupado[std_col] / (df_agrupado[mean_col].replace(0, np.nan))\n",
    "        df_agrupado[coef_var_col] = df_agrupado[coef_var_col].fillna(0)\n",
    "    \n",
    "    # PROPORCIÓN DE OUTLIERS (> 2 * std en la ventana)\n",
    "    outlier_col = f'TN_OUTLIER_PROP_{w:02d}'\n",
    "    def outlier_prop(x):\n",
    "        m = np.nanmean(x)\n",
    "        s = np.nanstd(x)\n",
    "        if s == 0:\n",
    "            return 0\n",
    "        return np.mean(np.abs(x - m) > 2 * s)\n",
    "    df_agrupado[outlier_col] = (\n",
    "        df_agrupado\n",
    "        .groupby(group_cols)['TN']\n",
    "        .rolling(window=w, min_periods=2)\n",
    "        .apply(outlier_prop, raw=True)\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # PROPORCIÓN DE VENTAS MAYORES A CERO\n",
    "    pct_pos_col = f'TN_PCT_POS_{w:02d}'\n",
    "    df_agrupado[pct_pos_col] = (\n",
    "        df_agrupado\n",
    "        .groupby(group_cols)['TN']\n",
    "        .rolling(window=w, min_periods=1)\n",
    "        .apply(lambda x: np.mean(x > 0), raw=True)\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Si querés ver un resumen de las nuevas columnas:\n",
    "cols_new = [\n",
    "    f'TN_GROWTH_{w:02d}' for w in windows\n",
    "] + [\n",
    "    f'TN_COUNT_ZERO_{w:02d}' for w in windows\n",
    "] + [\n",
    "    f'TN_SIGN_CHANGES_{w:02d}' for w in windows\n",
    "] + [\n",
    "    f'TN_COEF_VAR_{w:02d}' for w in windows\n",
    "] + [\n",
    "    f'TN_OUTLIER_PROP_{w:02d}' for w in windows\n",
    "] + [\n",
    "    f'TN_PCT_POS_{w:02d}' for w in windows\n",
    "]\n",
    "print(df_agrupado[cols_new].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_agrupado.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer un EDA de la columna  CLASE en  df_agrupado\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Estadísticas descriptivas ---\n",
    "print(\"Estadísticas básicas de CLASE:\")\n",
    "print(df_agrupado['CLASE'].describe())\n",
    "\n",
    "print(\"\\nPrimeros valores únicos (si son pocos):\", df_agrupado['CLASE'].unique()[:10])\n",
    "print(\"Cantidad de valores únicos:\", df_agrupado['CLASE'].nunique())\n",
    "\n",
    "print(\"\\nCantidad de nulos:\", df_agrupado['CLASE'].isna().sum())\n",
    "\n",
    "# --- Histograma ---\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df_agrupado['CLASE'], bins=30, kde=True)\n",
    "plt.title(\"Histograma de CLASE\")\n",
    "plt.xlabel(\"CLASE\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n",
    "\n",
    "# --- Boxplot para ver outliers ---\n",
    "plt.figure(figsize=(8,2))\n",
    "sns.boxplot(x=df_agrupado['CLASE'])\n",
    "plt.title(\"Boxplot de CLASE\")\n",
    "plt.show()\n",
    "\n",
    "# --- Distribución log-transformada (útil si hay muchos valores chicos y pocos grandes) ---\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(np.log1p(df_agrupado['CLASE']), bins=30, kde=True)\n",
    "plt.title(\"Histograma de log(1+CLASE)\")\n",
    "plt.xlabel(\"log(1+CLASE)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n",
    "\n",
    "# --- Percentiles y cuantiles ---\n",
    "print(\"\\nPercentiles de CLASE:\")\n",
    "print(df_agrupado['CLASE'].quantile([0, 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99, 1]))\n",
    "\n",
    "# --- Outliers extremos (más allá de 3 desviaciones estándar) ---\n",
    "media = df_agrupado['CLASE'].mean()\n",
    "std = df_agrupado['CLASE'].std()\n",
    "outliers = df_agrupado[(df_agrupado['CLASE'] > media + 3*std) | (df_agrupado['CLASE'] < media - 3*std)]\n",
    "print(f\"\\nCantidad de outliers (>3 desvíos): {len(outliers)}\")\n",
    "\n",
    "# --- Relación con otras variables (ejemplo: PERIODO) ---\n",
    "if 'PERIODO' in df_agrupado.columns:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.lineplot(x='PERIODO', y='CLASE', data=df_agrupado, ci=None)\n",
    "    plt.title(\"CLASE a lo largo del tiempo (PERIODO)\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado['CLASE_LOG1P'] = np.log1p(df_agrupado['CLASE'])\n",
    "# Y para invertir:\n",
    "#df_agrupado['CLASE'] = np.expm1(df_full['CLASE_LOG1P'])\n",
    "\n",
    "# Eliminar la columna CLASE \n",
    "df_agrupado = df_agrupado.drop(columns=['CLASE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas CLASE  para evitar confusiones\n",
    "df_agrupado.drop(columns=['CLASE'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_agrupado.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame en parquet\n",
    "df_agrupado.to_parquet('./data/product_interm_LGBM.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d75fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado = pd.read_parquet('./data/product_interm_LGBM.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f799e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir la variable de clase\n",
    "cols_to_check = df_agrupado.columns.difference(['CLASE_LOG1P'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_agrupado[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = df_agrupado.columns.difference(['CLASE_LOG1P'])\n",
    "\n",
    "# Reemplaza NaN e infinitos por 0 solo en las columnas seleccionadas\n",
    "df_agrupado[cols_to_check] = df_agrupado[cols_to_check].replace([np.nan, np.inf, -np.inf], 0)\n",
    "\n",
    "# Excluir la variable de clase\n",
    "cols_to_check = df_agrupado.columns.difference(['CLASE_LOG1P'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_agrupado[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"📊 DataFrame final con {df_agrupado.shape[0]:,} filas y {df_agrupado.shape[1]} columnas:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc780ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia de seguridad del DataFrame\n",
    "df = df_agrupado.copy()\n",
    "\n",
    "# === Binning (en deciles) ===\n",
    "df['PRODUCT_RANK_BIN'] = pd.qcut(df['PRODUCT_ID'], q=10, labels=False)\n",
    "df['PRODUCT_RANK_BIN'] = df['PRODUCT_RANK_BIN'].astype('category')\n",
    "\n",
    "# Reemplazar en el DataFrame principal\n",
    "df_agrupado = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_agrupado.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar de df_agrupado la columna FECHA\n",
    "df_agrupado = df_agrupado.drop(columns=['FECHA'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE','MES_PROBLEMATICO',\n",
    "            'PRODUCT_RANK_BIN','IS_FEBRERO', 'ESTOY_PREDICIENDO_FEBRERO','CAIDA_ABRUPTA']\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in df_agrupado.columns:\n",
    "        df_agrupado[col] = df_agrupado[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be20467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrupado.to_parquet('./data/product_interm_LGBM.parquet', index=False, engine='fastparquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
