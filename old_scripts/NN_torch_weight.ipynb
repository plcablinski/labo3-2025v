{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c9b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import optuna\n",
    "import sqlite3\n",
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Rutas de entrada y salida\n",
    "input_path = './data/l_vm_completa_normalizada_fe.parquet'\n",
    "output_train = './data/df_train.parquet'\n",
    "output_val = './data/df_val.parquet'\n",
    "\n",
    "# Periodos para divisi√≥n\n",
    "periodo_train_max = 201908\n",
    "periodos_val = [201909, 201910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4257dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_limpio = './data/df_train_limpio.parquet'\n",
    "output_val_limpio = './data/df_val_limpio.parquet'\n",
    "\n",
    "df_train_limpio = pd.read_parquet(output_train_limpio, engine='fastparquet')\n",
    "df_val_limpio = pd.read_parquet(output_val_limpio, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a5692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data for TabularNeuralNetTorchModel has: 6730977 examples, 381 features (381 vector, 0 embedding)\n",
      "Training on GPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=381, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training tabular neural network for up to 25 epochs...\n",
      "Epoch 1 (Update 1095).\tTrain loss: 5.7042, Val mean_absolute_error: -5.7107, Best Epoch: 1\n",
      "Epoch 2 (Update 2190).\tTrain loss: 5.7089, Val mean_absolute_error: -5.7107, Best Epoch: 2\n",
      "Epoch 3 (Update 3285).\tTrain loss: 5.7089, Val mean_absolute_error: -5.7107, Best Epoch: 3\n",
      "Epoch 4 (Update 4380).\tTrain loss: 4.3351, Val mean_absolute_error: -0.1192, Best Epoch: 4\n",
      "Epoch 5 (Update 5475).\tTrain loss: 0.0818, Val mean_absolute_error: -0.1302, Best Epoch: 4\n",
      "Epoch 6 (Update 6570).\tTrain loss: 0.0679, Val mean_absolute_error: -0.1229, Best Epoch: 4\n",
      "Epoch 7 (Update 7665).\tTrain loss: 0.0643, Val mean_absolute_error: -0.1258, Best Epoch: 4\n",
      "Epoch 8 (Update 8760).\tTrain loss: 0.0621, Val mean_absolute_error: -0.124, Best Epoch: 4\n",
      "Epoch 9 (Update 9855).\tTrain loss: 0.0617, Val mean_absolute_error: -0.1274, Best Epoch: 4\n",
      "Epoch 10 (Update 10950).\tTrain loss: 0.0605, Val mean_absolute_error: -0.1306, Best Epoch: 4\n",
      "Epoch 11 (Update 12045).\tTrain loss: 0.0609, Val mean_absolute_error: -0.1193, Best Epoch: 4\n",
      "Epoch 12 (Update 13140).\tTrain loss: 0.0597, Val mean_absolute_error: -0.1299, Best Epoch: 4\n",
      "Epoch 13 (Update 14235).\tTrain loss: 0.0602, Val mean_absolute_error: -0.1314, Best Epoch: 4\n",
      "Epoch 14 (Update 15330).\tTrain loss: 0.0604, Val mean_absolute_error: -0.1267, Best Epoch: 4\n",
      "Epoch 15 (Update 16425).\tTrain loss: 0.0597, Val mean_absolute_error: -0.1255, Best Epoch: 4\n",
      "Epoch 16 (Update 17520).\tTrain loss: 0.0595, Val mean_absolute_error: -0.1243, Best Epoch: 4\n",
      "Epoch 17 (Update 18615).\tTrain loss: 0.0592, Val mean_absolute_error: -0.1157, Best Epoch: 17\n",
      "Epoch 18 (Update 19710).\tTrain loss: 0.0597, Val mean_absolute_error: -0.1158, Best Epoch: 17\n",
      "Epoch 19 (Update 20805).\tTrain loss: 0.0579, Val mean_absolute_error: -0.1345, Best Epoch: 17\n",
      "Epoch 20 (Update 21900).\tTrain loss: 0.0584, Val mean_absolute_error: -0.133, Best Epoch: 17\n",
      "Epoch 21 (Update 22995).\tTrain loss: 0.0572, Val mean_absolute_error: -0.1293, Best Epoch: 17\n",
      "Epoch 22 (Update 24090).\tTrain loss: 0.0569, Val mean_absolute_error: -0.1297, Best Epoch: 17\n",
      "Epoch 23 (Update 25185).\tTrain loss: 0.0567, Val mean_absolute_error: -0.1264, Best Epoch: 17\n",
      "Epoch 24 (Update 26280).\tTrain loss: 0.0567, Val mean_absolute_error: -0.1299, Best Epoch: 17\n",
      "Epoch 25 (Update 27375).\tTrain loss: 0.0566, Val mean_absolute_error: -0.1275, Best Epoch: 17\n",
      "Best model found on Epoch 17 (Update 18615). Val mean_absolute_error: -0.11567343771457672\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/NeuralNetTorchGPU_Full_Weighted/model.pkl\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/utils/attr/NeuralNetTorchGPU_Full_Weighted/y_pred_proba_val.pkl\n",
      "\t-0.1157\t = Validation score   (-mean_absolute_error)\n",
      "\t1149.59s\t = Training   runtime\n",
      "\t12.71s\t = Validation runtime\n",
      "\t41316.3\t = Inference  throughput (rows/s | 525116 batch size)\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/trainer.pkl\n",
      "Loading: /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/utils/attr/NeuralNetTorchGPU_Full_Weighted/y_pred_proba_val.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 711.03s of the 5947.97s of remaining time.\n",
      "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 28\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Loading: /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Ensemble size: 1\n",
      "Ensemble indices: [np.int64(0)]\n",
      "Ensemble weights: \n",
      "[1.]\n",
      "\t0.29s\t= Estimated out-of-fold prediction time...\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/WeightedEnsemble_L2/model.pkl\n",
      "\tEnsemble Weights: {'NeuralNetTorchGPU_Full_Weighted': 1.0}\n",
      "\t-0.1157\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\t41308.8\t = Inference  throughput (rows/s | 525116 batch size)\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/trainer.pkl\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/trainer.pkl\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/trainer.pkl\n",
      "AutoGluon training complete, total runtime = 1259.94s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 41308.8 rows/s (525116 batch size)\n",
      "Loading: /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/trainer.pkl\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/trainer.pkl\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/learner.pkl\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/predictor.pkl\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/version.txt with contents \"1.3.1\"\n",
      "Saving /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/metadata.json\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# ============================\n",
    "# Paso 1: Agregar sample_weight\n",
    "# ============================\n",
    "\n",
    "alpha = 1.0  # Pod√©s ajustar este valor seg√∫n la dispersi√≥n de tu variable\n",
    "\n",
    "max_weight = 10.0\n",
    "df_train_limpio['sample_weight'] = (1 + alpha * df_train_limpio['CLASE_DELTA_ZSCORE'].abs()).clip(upper=max_weight)\n",
    "df_val_limpio['sample_weight'] = (1 + alpha * df_val_limpio['CLASE_DELTA_ZSCORE'].abs()).clip(upper=max_weight)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Paso 2: Entrenar el predictor\n",
    "# ============================\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label='CLASE_DELTA_ZSCORE',\n",
    "    problem_type='regression',\n",
    "    eval_metric='mean_absolute_error',\n",
    "    verbosity=4,\n",
    "    path='AutogluonModels/nn_gpu_full_train_v2_weighted'\n",
    ").fit(\n",
    "    train_data=df_train_limpio,\n",
    "    tuning_data=df_val_limpio,\n",
    "    time_limit=7200,  # 2 horas\n",
    "    use_bag_holdout=False,\n",
    "    presets='medium',\n",
    "    hyperparameters={\n",
    "        'NN_TORCH': [{\n",
    "            'num_epochs': 25,\n",
    "            'learning_rate': 0.005,\n",
    "            'dropout_prob': 0.1,\n",
    "            'batch_size': 6144,\n",
    "            'hidden_size': 1024,\n",
    "            'ag_args': {'name_suffix': 'GPU_Full_Weighted'},\n",
    "            'ag_args_fit': {\n",
    "                'num_gpus': 1,\n",
    "                'sample_weight': 'sample_weight'  # <--- Esto es lo correcto\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50069cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/NeuralNetTorchGPU_Full_Weighted/model.pkl\n",
      "Loading: /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/WeightedEnsemble_L2/model.pkl\n",
      "Model scores:\n",
      "{'NeuralNetTorchGPU_Full_Weighted': -0.11567343771457672, 'WeightedEnsemble_L2': -0.11567343771457672}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             model  score_test  mean_absolute_error  median_absolute_error       r2  score_val          eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  NeuralNetTorchGPU_Full_Weighted   -0.115673            -0.115673              -0.016925  0.89115  -0.115673  mean_absolute_error       12.732528      12.709661  1149.585235                12.732528               12.709661        1149.585235            1       True          1\n",
      "1              WeightedEnsemble_L2   -0.115673            -0.115673              -0.016925  0.89115  -0.115673  mean_absolute_error       12.736080      12.711959  1149.594618                 0.003552                0.002298           0.009383            2       True          2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===  Evaluaci√≥n del modelo ===\n",
    "\n",
    "lb = predictor.leaderboard(df_val_limpio, extra_metrics=['mean_absolute_error', 'median_absolute_error', 'r2'], silent=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe25ee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: /home/pablo/Documentos/labo3-2025v/AutogluonModels/nn_gpu_full_train_v2_weighted/models/NeuralNetTorchGPU_Full_Weighted/model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 25, 'epochs_wo_improve': None, 'activation': 'relu', 'embedding_size_factor': 1.0, 'embed_exponent': 0.56, 'max_embedding_dim': 100, 'y_range': None, 'y_range_extend': 0.05, 'dropout_prob': 0.1, 'optimizer': 'adam', 'learning_rate': 0.005, 'weight_decay': 1e-06, 'proc.embed_min_categories': 4, 'proc.impute_strategy': 'median', 'proc.max_category_levels': 100, 'proc.skew_threshold': 0.99, 'use_ngram_features': False, 'num_layers': 4, 'hidden_size': 1024, 'max_batch_size': 512, 'use_batchnorm': False, 'loss_function': 'auto', 'batch_size': 6144}\n"
     ]
    }
   ],
   "source": [
    "# Acced√© al modelo dentro del predictor\n",
    "model = predictor._trainer.load_model('NeuralNetTorchGPU_Full_Weighted')\n",
    "\n",
    "# Inspeccionar los argumentos usados al entrenar\n",
    "print(model.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4247ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = predictor.feature_importance(df_val_limpio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52685527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imprimir las 20 caracter√≠sticas m√°s importantes\n",
    "print(feature_importance.tail(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce344269",
   "metadata": {},
   "source": [
    "‚úÖ Paso 1: Calcular errores de predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Predicciones y error absoluto\n",
    "df_val_limpio['y_true'] = df_val_limpio['CLASE_DELTA_ZSCORE']\n",
    "# Ignorar la columna 'CLASE_DELTA_ZSCORE' \n",
    "df_val = df_val_limpio.drop(columns=['CLASE_DELTA_ZSCORE'])\n",
    "df_val_limpio['y_pred'] = predictor.predict(df_val_limpio)\n",
    "df_val_limpio['error_abs'] = abs(df_val_limpio['y_true'] - df_val_limpio['y_pred'])\n",
    "df_val_limpio['error_signed'] = df_val_limpio['y_pred'] - df_val_limpio['y_true']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47114e7",
   "metadata": {},
   "source": [
    "üìä Paso 2: Histogramas de error absoluto y error signado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df_val_limpio['error_abs'], bins=50, kde=True)\n",
    "plt.title('Distribuci√≥n del Error Absoluto')\n",
    "plt.xlabel('|y_pred - y_true|')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df_val_limpio['error_signed'], bins=50, kde=True)\n",
    "plt.title('Distribuci√≥n del Error Signado')\n",
    "plt.xlabel('y_pred - y_true')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18881d9e",
   "metadata": {},
   "source": [
    "üìà Paso 3: Error vs. Valor real (dispersi√≥n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df_val_limpio['y_true'], y=df_val_limpio['error_signed'], alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Error Signado vs. Valor Real')\n",
    "plt.xlabel('Valor real')\n",
    "plt.ylabel('Error (pred - real)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c603b",
   "metadata": {},
   "source": [
    "üß© Paso 4: Promedio de error por grupo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_errores_producto = df_val.groupby('PRODUCT_ID')['error_abs'].mean().sort_values(ascending=False).head(20)\n",
    "top_errores_producto.plot(kind='bar', figsize=(10,4), title='Top 20 PRODUCT_ID con mayor error promedio')\n",
    "plt.ylabel('Error Absoluto Medio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_errores_producto = df_val.groupby('CUSTOMER_ID')['error_abs'].mean().sort_values(ascending=False).head(20)\n",
    "top_errores_producto.plot(kind='bar', figsize=(10,4), title='Top 20 CUSTOMER_ID con mayor error promedio')\n",
    "plt.ylabel('Error Absoluto Medio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_train_limpio, df_val_limpio], ignore_index=True)\n",
    "del df_train_limpio, df_val_limpio\n",
    "gc.collect()\n",
    "predictor_full = predictor_nn_lightfast.refit_full(train_data=df_full)\n",
    "predictor_full.save(\"AutogluonModels/nn_lightfast_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac970579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Combinar entrenamiento + validaci√≥n\n",
    "df_full = pd.concat([df_train, df_val], axis=0)\n",
    "del df_train, df_val\n",
    "gc.collect() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31acb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reentrenar el mejor modelo con TODOS los datos disponibles\n",
    "# predictor_full = predictor.refit_full(train_data=df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar los modelos disponibles (el mejor ahora tiene el sufijo '_FULL')\n",
    "print(\"Modelos disponibles luego del refit completo:\")\n",
    "print(predictor.leaderboard(silent=True)['model'].tolist())\n",
    "# Eliminar modelos intermedios para liberar espacio\n",
    "predictor.delete_models(models_to_keep='best', dry_run=False)\n",
    "\n",
    "# Confirmar que solo queda el modelo reentrenado\n",
    "print(\"\\nModelos restantes despu√©s de eliminar los intermedios:\")\n",
    "print(predictor.leaderboard(silent=True)['model'].tolist())\n",
    "\n",
    "# (Opcional) Guardar el predictor final si quer√©s usarlo luego sin volver a cargar todo\n",
    "predictor.save('./data/modelo_final_autogluon')\n",
    "\n",
    "# ---  Liberar memoria ---\n",
    "del df_full\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos sobre los que quiero hacer predicciones\n",
    "df_pred_full = pd.read_parquet('./data/l_vm_completa_normalizada_fe.parquet', engine='fastparquet')\n",
    "# Dejo solo los datos del periodo 201910 y que A_PREDECIR sea True\n",
    "# Filtrar solo los datos del periodo 201910 y donde A_PREDECIR sea True\n",
    "df_pred_full = df_pred_full[\n",
    "    (df_pred_full['PERIODO'] == 201910) & (df_pred_full['A_PREDECIR'] == True)\n",
    "].drop(columns=['CLASE_ZSCORE', 'CLASE_DELTA_ZSCORE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7da41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar las predicciones usando el predictor original\n",
    "predictions = predictor.predict(df_pred_full)\n",
    "# Agregar las predicciones al DataFrame original\n",
    "df_pred_full['CLASE_DELTA_ZSCORE'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la lista de columas del DataFrame con las predicciones\n",
    "print(\"Columnas del DataFrame con las predicciones:\")\n",
    "print(df_pred_full.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d459184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dernormalizar la columna CLASE_DELTA_ZSCORE\n",
    "df_pred_full['CLASE_DELTA'] = df_pred_full['CLASE_DELTA_ZSCORE'] * df_pred_full['CLASE_DELTA_STD'] + df_pred_full['CLASE_DELTA_MEAN']\n",
    "df_pred_full['TN'] = df_pred_full['TN_ZSCORE'] * df_pred_full['TN_STD'] + df_pred_full['TN_MEAN']\n",
    "# Agregar la columna TN_PREDICT que sea la suma de TN y CLASE_DELTA y si es menor que cero, poner cero\n",
    "df_pred_full['TN_PREDICT'] = df_pred_full['TN'] + df_pred_full['CLASE_DELTA']\n",
    "df_pred_full['TN_PREDICT'] = df_pred_full['TN_PREDICT'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d822ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar Dataframe que contenga por cada PRODUCT_ID la suma de TN_PREDICT\n",
    "df_final = df_pred_full.groupby('PRODUCT_ID').agg({'TN_PREDICT': 'sum'}).reset_index()\n",
    "df_final = df_final.rename(columns={'PRODUCT_ID': 'product_id', 'TN_PREDICT': 'tn'})\n",
    "# Guardar el DataFrame df_final en un archivo CSV\n",
    "df_final.to_csv('./modelos/autoglun_normalizando_clase_delta.csv', index=False)\n",
    "df_final.shape\n",
    "\n",
    "# Para instalar AutoGluon con soporte para `autogluon.core.space` en conda, ejecuta:\n",
    "# \n",
    "# conda install -c conda-forge autogluon\n",
    "# \n",
    "# O si prefieres usar pip dentro de tu entorno conda:\n",
    "# \n",
    "# pip install autogluon\n",
    "# \n",
    "# Luego podr√°s usar:\n",
    "# from autogluon.core import space as ag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaboIII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
