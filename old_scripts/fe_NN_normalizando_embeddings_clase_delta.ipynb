{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41eccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9628af",
   "metadata": {},
   "source": [
    "El objetivo de este Notebook es preparar los datos para modelar con redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cd76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducir memoria autom√°ticamente\n",
    "def optimizar_memoria(df):\n",
    "    for col in df.select_dtypes(include=['int64', 'int32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    for col in df.select_dtypes(include=['float64', 'float32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d1dd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')\n",
    "df_pendientes = pd.read_parquet('./data/l_vm_completa_train_pendientes.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1990a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar los DataFrames df_full y df_pendientes por PRODUCT_ID,CUSTOMER_ID y PERIODO\n",
    "df_full = pd.merge(\n",
    "    df_full,\n",
    "    df_pendientes,\n",
    "    on=['PRODUCT_ID', 'CUSTOMER_ID', 'PERIODO'],\n",
    "    how='inner'  # solo filas que existen en ambos DataFrames\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ae79444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERIODO', 'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID', 'PRODUCT_ID', 'PLAN_PRECIOS_CUIDADOS', 'CUST_REQUEST_QTY', 'CUST_REQUEST_TN', 'TN', 'STOCK_FINAL', 'MEDIA_MOVIL_3M_CLI_PROD', 'MEDIA_MOVIL_6M_CLI_PROD', 'MEDIA_MOVIL_12M_CLI_PROD', 'DESVIO_MOVIL_3M_CLI_PROD', 'DESVIO_MOVIL_6M_CLI_PROD', 'DESVIO_MOVIL_12M_CLI_PROD', 'MEDIA_MOVIL_3M_PROD', 'MEDIA_MOVIL_6M_PROD', 'MEDIA_MOVIL_12M_PROD', 'DESVIO_MOVIL_3M_PROD', 'DESVIO_MOVIL_6M_PROD', 'DESVIO_MOVIL_12M_PROD', 'MEDIA_MOVIL_3M_CLI', 'MEDIA_MOVIL_6M_CLI', 'MEDIA_MOVIL_12M_CLI', 'DESVIO_MOVIL_3M_CLI', 'DESVIO_MOVIL_6M_CLI', 'DESVIO_MOVIL_12M_CLI', 'TN_LAG_01', 'TN_LAG_02', 'TN_LAG_03', 'TN_LAG_04', 'TN_LAG_05', 'TN_LAG_06', 'TN_LAG_07', 'TN_LAG_08', 'TN_LAG_09', 'TN_LAG_10', 'TN_LAG_11', 'TN_LAG_12', 'TN_LAG_13', 'TN_LAG_14', 'TN_LAG_15', 'CLASE', 'CLASE_DELTA', 'ORDINAL', 'TN_DELTA_01', 'TN_DELTA_02', 'TN_DELTA_03', 'TN_DELTA_04', 'TN_DELTA_05', 'TN_DELTA_06', 'TN_DELTA_07', 'TN_DELTA_08', 'TN_DELTA_09', 'TN_DELTA_10', 'TN_DELTA_11', 'TN_DELTA_12', 'TN_DELTA_13', 'TN_DELTA_14', 'TN_DELTA_15', 'ANTIG_CLIENTE', 'ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER', 'MEDIA_PROD_PER', 'MEDIA_PROD', 'MEDIA_PER', 'A_PREDECIR', 'PENDIENTE_TENDENCIA_3', 'TN_EWMA_03', 'TN_MEDIAN_03', 'TN_MIN_03', 'TN_MAX_03', 'PENDIENTE_TENDENCIA_6', 'TN_EWMA_06', 'TN_MEDIAN_06', 'TN_MIN_06', 'TN_MAX_06', 'PENDIENTE_TENDENCIA_9', 'TN_EWMA_09', 'TN_MEDIAN_09', 'TN_MIN_09', 'TN_MAX_09', 'PENDIENTE_TENDENCIA_12', 'TN_EWMA_12', 'TN_MEDIAN_12', 'TN_MIN_12', 'TN_MAX_12']\n"
     ]
    }
   ],
   "source": [
    "print(df_full.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88dd72eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PERIODO',\n",
       " 'ANIO',\n",
       " 'MES',\n",
       " 'MES_SIN',\n",
       " 'MES_COS',\n",
       " 'TRIMESTRE',\n",
       " 'ID_CAT1',\n",
       " 'ID_CAT2',\n",
       " 'ID_CAT3',\n",
       " 'ID_BRAND',\n",
       " 'SKU_SIZE',\n",
       " 'CUSTOMER_ID',\n",
       " 'PRODUCT_ID',\n",
       " 'PLAN_PRECIOS_CUIDADOS',\n",
       " 'CUST_REQUEST_QTY',\n",
       " 'CUST_REQUEST_TN',\n",
       " 'TN',\n",
       " 'STOCK_FINAL',\n",
       " 'MEDIA_MOVIL_3M_CLI_PROD',\n",
       " 'MEDIA_MOVIL_6M_CLI_PROD',\n",
       " 'MEDIA_MOVIL_12M_CLI_PROD',\n",
       " 'DESVIO_MOVIL_3M_CLI_PROD',\n",
       " 'DESVIO_MOVIL_6M_CLI_PROD',\n",
       " 'DESVIO_MOVIL_12M_CLI_PROD',\n",
       " 'MEDIA_MOVIL_3M_PROD',\n",
       " 'MEDIA_MOVIL_6M_PROD',\n",
       " 'MEDIA_MOVIL_12M_PROD',\n",
       " 'DESVIO_MOVIL_3M_PROD',\n",
       " 'DESVIO_MOVIL_6M_PROD',\n",
       " 'DESVIO_MOVIL_12M_PROD',\n",
       " 'MEDIA_MOVIL_3M_CLI',\n",
       " 'MEDIA_MOVIL_6M_CLI',\n",
       " 'MEDIA_MOVIL_12M_CLI',\n",
       " 'DESVIO_MOVIL_3M_CLI',\n",
       " 'DESVIO_MOVIL_6M_CLI',\n",
       " 'DESVIO_MOVIL_12M_CLI',\n",
       " 'TN_LAG_01',\n",
       " 'TN_LAG_02',\n",
       " 'TN_LAG_03',\n",
       " 'TN_LAG_04',\n",
       " 'TN_LAG_05',\n",
       " 'TN_LAG_06',\n",
       " 'TN_LAG_07',\n",
       " 'TN_LAG_08',\n",
       " 'TN_LAG_09',\n",
       " 'TN_LAG_10',\n",
       " 'TN_LAG_11',\n",
       " 'TN_LAG_12',\n",
       " 'TN_LAG_13',\n",
       " 'TN_LAG_14',\n",
       " 'TN_LAG_15',\n",
       " 'CLASE',\n",
       " 'CLASE_DELTA',\n",
       " 'ORDINAL',\n",
       " 'TN_DELTA_01',\n",
       " 'TN_DELTA_02',\n",
       " 'TN_DELTA_03',\n",
       " 'TN_DELTA_04',\n",
       " 'TN_DELTA_05',\n",
       " 'TN_DELTA_06',\n",
       " 'TN_DELTA_07',\n",
       " 'TN_DELTA_08',\n",
       " 'TN_DELTA_09',\n",
       " 'TN_DELTA_10',\n",
       " 'TN_DELTA_11',\n",
       " 'TN_DELTA_12',\n",
       " 'TN_DELTA_13',\n",
       " 'TN_DELTA_14',\n",
       " 'TN_DELTA_15',\n",
       " 'ANTIG_CLIENTE',\n",
       " 'ANTIG_PRODUCTO',\n",
       " 'CANT_PROD_CLI_PER',\n",
       " 'MEDIA_PROD_PER',\n",
       " 'MEDIA_PROD',\n",
       " 'MEDIA_PER',\n",
       " 'A_PREDECIR',\n",
       " 'PENDIENTE_TENDENCIA_3',\n",
       " 'TN_EWMA_03',\n",
       " 'TN_MEDIAN_03',\n",
       " 'TN_MIN_03',\n",
       " 'TN_MAX_03',\n",
       " 'PENDIENTE_TENDENCIA_6',\n",
       " 'TN_EWMA_06',\n",
       " 'TN_MEDIAN_06',\n",
       " 'TN_MIN_06',\n",
       " 'TN_MAX_06',\n",
       " 'PENDIENTE_TENDENCIA_9',\n",
       " 'TN_EWMA_09',\n",
       " 'TN_MEDIAN_09',\n",
       " 'TN_MIN_09',\n",
       " 'TN_MAX_09',\n",
       " 'PENDIENTE_TENDENCIA_12',\n",
       " 'TN_EWMA_12',\n",
       " 'TN_MEDIAN_12',\n",
       " 'TN_MIN_12',\n",
       " 'TN_MAX_12']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccba47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Combinaciones PRODUCT_ID + CUSTOMER_ID con TN = 0 en todos sus registros: 327068\n",
      "üóëÔ∏è Filas eliminadas de df_full: 6,594,430\n",
      "üóëÔ∏è Filas eliminadas de df_full: 6,594,430\n"
     ]
    }
   ],
   "source": [
    "# Buscar en df_full los product_id, customer_id que solo tienen ceros en TN\n",
    "def buscar_productos_solo_ceros(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    grouped = df.groupby(['PRODUCT_ID', 'CUSTOMER_ID'])['TN'].sum().reset_index()\n",
    "    productos_solo_ceros = grouped[grouped['TN'] == 0]\n",
    "    return productos_solo_ceros\n",
    "\n",
    "productos_solo_ceros = buscar_productos_solo_ceros(df_full)\n",
    "print(f\"üîç Combinaciones PRODUCT_ID + CUSTOMER_ID con TN = 0 en todos sus registros: {len(productos_solo_ceros)}\")\n",
    "\n",
    "# Eliminar del df_full los product_id, customer_id que solo tienen ceros en TN\n",
    "def eliminar_productos_solo_ceros(df: pd.DataFrame, productos_solo_ceros: pd.DataFrame) -> pd.DataFrame:\n",
    "    productos_set = set(zip(productos_solo_ceros['PRODUCT_ID'], productos_solo_ceros['CUSTOMER_ID']))\n",
    "    mask = df.set_index(['PRODUCT_ID', 'CUSTOMER_ID']).index.isin(productos_set)\n",
    "    \n",
    "    cantidad_eliminada = mask.sum()\n",
    "    print(f\"üóëÔ∏è Filas eliminadas de df_full: {cantidad_eliminada:,}\")\n",
    "    \n",
    "    df_filtrado = df[~mask]\n",
    "    return df_filtrado\n",
    "\n",
    "df_full = eliminar_productos_solo_ceros(df_full, productos_solo_ceros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d03193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar de df_full las filas donde la columna A_PREDECIR sea 'N'\n",
    "df_full = df_full[df_full['A_PREDECIR'] != 'N']\n",
    "df_full = df_full.drop(columns=['A_PREDECIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e0fd77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Cantidad de filas por PERIODO:\n",
      "PERIODO\n",
      "201701    153506\n",
      "201702    169292\n",
      "201703    173972\n",
      "201704    175629\n",
      "201705    177198\n",
      "201706    179909\n",
      "201707    184384\n",
      "201708    187774\n",
      "201709    190994\n",
      "201710    195034\n",
      "201711    201377\n",
      "201712    201427\n",
      "201801    201965\n",
      "201802    202278\n",
      "201803    203591\n",
      "201804    207881\n",
      "201805    212721\n",
      "201806    213622\n",
      "201807    215649\n",
      "201808    216883\n",
      "201809    222730\n",
      "201810    226473\n",
      "201811    229728\n",
      "201812    230059\n",
      "201901    230063\n",
      "201902    230976\n",
      "201903    235674\n",
      "201904    244108\n",
      "201905    247448\n",
      "201906    251593\n",
      "201907    256940\n",
      "201908    260099\n",
      "201909    262500\n",
      "201910    262616\n",
      "201911    262721\n",
      "201912    262805\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Por cada PERIODO mostrar la cantidad de filas\n",
    "periodo_counts = df_full['PERIODO'].value_counts().sort_index()\n",
    "print(\"üìÖ Cantidad de filas por PERIODO:\")\n",
    "print(periodo_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "355a135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar una columna que indique la diferencia en ORDINAL entre el ORDINAL actual y el ORDINAL anterior donde TN sea mayor a 0\n",
    "# para ese CUSTOMER_ID y PRODUCT_ID\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_grupo(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_parallel(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = 36  # valor inicial\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'] = df['MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID'].astype('int16')\n",
    "\n",
    "    # Agrupar por cliente y producto\n",
    "    grupos = list(df.groupby(['CUSTOMER_ID', 'PRODUCT_ID']))\n",
    "\n",
    "    # Procesar en paralelo\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_grupo)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    # Concatenar todos los resultados\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_parallel(df_full, n_jobs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8275d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_producto(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_PRODUCT_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_por_producto(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_ID'] = 36\n",
    "    df['MESES_SIN_COMPRAR_PRODUCT_ID'] = df['MESES_SIN_COMPRAR_PRODUCT_ID'].astype('int16')\n",
    "\n",
    "    # Agrupar solo por PRODUCT_ID\n",
    "    grupos = list(df.groupby('PRODUCT_ID'))\n",
    "\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_producto)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_por_producto(df_full, n_jobs=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "651a8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calcular_mejoras_por_cliente(grupo):\n",
    "    grupo = grupo.sort_values('ORDINAL').copy()\n",
    "    ult_ordinal = None\n",
    "    valores = []\n",
    "\n",
    "    for _, row in grupo.iterrows():\n",
    "        if ult_ordinal is None:\n",
    "            valores.append(36)\n",
    "        else:\n",
    "            valores.append(int(row['ORDINAL'] - ult_ordinal))\n",
    "\n",
    "        if row['TN'] > 0:\n",
    "            ult_ordinal = row['ORDINAL']\n",
    "\n",
    "    grupo['MESES_SIN_COMPRAR_CUSTOMER_ID'] = np.array(valores, dtype=np.int16)\n",
    "    return grupo\n",
    "\n",
    "def agregar_diferencia_ordinal_por_cliente(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['MESES_SIN_COMPRAR_CUSTOMER_ID'] = 36\n",
    "    df['MESES_SIN_COMPRAR_CUSTOMER_ID'] = df['MESES_SIN_COMPRAR_CUSTOMER_ID'].astype('int16')\n",
    "\n",
    "    grupos = list(df.groupby('CUSTOMER_ID'))\n",
    "\n",
    "    resultados = Parallel(n_jobs=n_jobs, backend='loky', batch_size=128)(\n",
    "        delayed(calcular_mejoras_por_cliente)(grupo) for _, grupo in grupos\n",
    "    )\n",
    "\n",
    "    df_resultado = pd.concat(resultados, axis=0).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "df_full = agregar_diferencia_ordinal_por_cliente(df_full, n_jobs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba02f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "# Calcular los d√≠as del mes usando las columnas ANIO y MES\n",
    "\n",
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si ANIO==2019 y MES en [6, 8, 10], y 0 en caso contrario\n",
    "df_full['MES_PROBLEMATICO'] = np.where(\n",
    "       (df_full['ANIO'] == 2019) & (df_full['MES'].isin([6, 8, 10])),\n",
    "       1., 0.0\n",
    ")\n",
    "df_full['MES_PROBLEMATICO'] = df_full['MES_PROBLEMATICO'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f26846e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame en parquet\n",
    "df_full.to_parquet('./data/interm_NN_TORCH.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a78d2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERIODO', 'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID', 'PRODUCT_ID', 'PLAN_PRECIOS_CUIDADOS', 'CUST_REQUEST_QTY', 'CUST_REQUEST_TN', 'TN', 'STOCK_FINAL', 'MEDIA_MOVIL_3M_CLI_PROD', 'MEDIA_MOVIL_6M_CLI_PROD', 'MEDIA_MOVIL_12M_CLI_PROD', 'DESVIO_MOVIL_3M_CLI_PROD', 'DESVIO_MOVIL_6M_CLI_PROD', 'DESVIO_MOVIL_12M_CLI_PROD', 'MEDIA_MOVIL_3M_PROD', 'MEDIA_MOVIL_6M_PROD', 'MEDIA_MOVIL_12M_PROD', 'DESVIO_MOVIL_3M_PROD', 'DESVIO_MOVIL_6M_PROD', 'DESVIO_MOVIL_12M_PROD', 'MEDIA_MOVIL_3M_CLI', 'MEDIA_MOVIL_6M_CLI', 'MEDIA_MOVIL_12M_CLI', 'DESVIO_MOVIL_3M_CLI', 'DESVIO_MOVIL_6M_CLI', 'DESVIO_MOVIL_12M_CLI', 'TN_LAG_01', 'TN_LAG_02', 'TN_LAG_03', 'TN_LAG_04', 'TN_LAG_05', 'TN_LAG_06', 'TN_LAG_07', 'TN_LAG_08', 'TN_LAG_09', 'TN_LAG_10', 'TN_LAG_11', 'TN_LAG_12', 'TN_LAG_13', 'TN_LAG_14', 'TN_LAG_15', 'CLASE', 'CLASE_DELTA', 'ORDINAL', 'TN_DELTA_01', 'TN_DELTA_02', 'TN_DELTA_03', 'TN_DELTA_04', 'TN_DELTA_05', 'TN_DELTA_06', 'TN_DELTA_07', 'TN_DELTA_08', 'TN_DELTA_09', 'TN_DELTA_10', 'TN_DELTA_11', 'TN_DELTA_12', 'TN_DELTA_13', 'TN_DELTA_14', 'TN_DELTA_15', 'ANTIG_CLIENTE', 'ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER', 'MEDIA_PROD_PER', 'MEDIA_PROD', 'MEDIA_PER', 'PENDIENTE_TENDENCIA_3', 'TN_EWMA_03', 'TN_MEDIAN_03', 'TN_MIN_03', 'TN_MAX_03', 'PENDIENTE_TENDENCIA_6', 'TN_EWMA_06', 'TN_MEDIAN_06', 'TN_MIN_06', 'TN_MAX_06', 'PENDIENTE_TENDENCIA_9', 'TN_EWMA_09', 'TN_MEDIAN_09', 'TN_MIN_09', 'TN_MAX_09', 'PENDIENTE_TENDENCIA_12', 'TN_EWMA_12', 'TN_MEDIAN_12', 'TN_MIN_12', 'TN_MAX_12', 'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID', 'MESES_SIN_COMPRAR_PRODUCT_ID', 'MESES_SIN_COMPRAR_CUSTOMER_ID', 'MES_PROBLEMATICO']\n"
     ]
    }
   ],
   "source": [
    "print(df_full.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c59f2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conservar las siguientes columnas\n",
    "# #columns_to_keep = ['MES_SIN', 'MES_COS', 'ID_CAT1',\n",
    "#        'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID',\n",
    "#        'PRODUCT_ID', 'CUST_REQUEST_QTY',\n",
    "#        'CUST_REQUEST_TN', 'TN', 'CLASE_DELTA',\n",
    "#        'ANTIG_CLIENTE','ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER',\n",
    "#        'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID','MESES_SIN_COMPRAR_PRODUCT_ID','MESES_SIN_COMPRAR_CUSTOMER_ID',\n",
    "#        'MES_PROBLEMATICO','PERIODO','ORDINAL']\n",
    "# Filtrar el DataFrame para conservar solo las columnas deseadas \n",
    "columns_to_keep = ['PERIODO', 'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', \n",
    "                   'SKU_SIZE', 'CUSTOMER_ID', 'PRODUCT_ID', 'CUST_REQUEST_QTY', 'CUST_REQUEST_TN', 'TN', \n",
    "                   'MEDIA_MOVIL_3M_CLI_PROD', 'MEDIA_MOVIL_6M_CLI_PROD', 'MEDIA_MOVIL_12M_CLI_PROD', \n",
    "                   'DESVIO_MOVIL_3M_CLI_PROD', 'DESVIO_MOVIL_6M_CLI_PROD', 'DESVIO_MOVIL_12M_CLI_PROD', 'MEDIA_MOVIL_3M_PROD', \n",
    "                   'MEDIA_MOVIL_6M_PROD', 'MEDIA_MOVIL_12M_PROD', 'DESVIO_MOVIL_3M_PROD', 'DESVIO_MOVIL_6M_PROD', \n",
    "                   'DESVIO_MOVIL_12M_PROD', 'MEDIA_MOVIL_3M_CLI', 'MEDIA_MOVIL_6M_CLI', 'MEDIA_MOVIL_12M_CLI', \n",
    "                   'DESVIO_MOVIL_3M_CLI', 'DESVIO_MOVIL_6M_CLI', 'DESVIO_MOVIL_12M_CLI', 'TN_LAG_01', 'TN_LAG_02', 'TN_LAG_03', \n",
    "                   'TN_LAG_04', 'TN_LAG_05', 'TN_LAG_06', 'TN_LAG_07', 'TN_LAG_08', 'TN_LAG_09', 'TN_LAG_10', 'TN_LAG_11', \n",
    "                   'TN_LAG_12', 'TN_LAG_13', 'TN_LAG_14', 'TN_LAG_15','CLASE_DELTA', 'ORDINAL', \n",
    "                   'TN_DELTA_01', 'TN_DELTA_02', 'TN_DELTA_03', 'TN_DELTA_04', 'TN_DELTA_05', 'TN_DELTA_06', 'TN_DELTA_07', \n",
    "                   'TN_DELTA_08', 'TN_DELTA_09', 'TN_DELTA_10', 'TN_DELTA_11', 'TN_DELTA_12', 'TN_DELTA_13', 'TN_DELTA_14', \n",
    "                   'TN_DELTA_15', 'ANTIG_CLIENTE', 'ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER', 'MEDIA_PROD_PER', 'MEDIA_PROD', \n",
    "                   'MEDIA_PER', 'PENDIENTE_TENDENCIA_3', 'TN_EWMA_03', 'TN_MEDIAN_03', 'TN_MIN_03', 'TN_MAX_03', \n",
    "                   'PENDIENTE_TENDENCIA_6', 'TN_EWMA_06', 'TN_MEDIAN_06', 'TN_MIN_06', 'TN_MAX_06', 'PENDIENTE_TENDENCIA_9',\n",
    "                   'TN_EWMA_09', 'TN_MEDIAN_09', 'TN_MIN_09', 'TN_MAX_09', 'PENDIENTE_TENDENCIA_12', 'TN_EWMA_12', \n",
    "                   'TN_MEDIAN_12', 'TN_MIN_12', 'TN_MAX_12', 'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID', \n",
    "                   'MESES_SIN_COMPRAR_PRODUCT_ID', 'MESES_SIN_COMPRAR_CUSTOMER_ID', 'MES_PROBLEMATICO']\n",
    "df_full = df_full[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3398b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_signed(x):\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "df_full['CLASE_DELTA_LOG1P'] = log_transform_signed(df_full['CLASE_DELTA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b395843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de CLASE_DELTA_LOG1P_Z: -0.0002512964473318228\n",
      "Desviaci√≥n est√°ndar de CLASE_DELTA_LOG1P_Z: 0.2535427832171512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "df_full['CLASE_DELTA_LOG1P_Z'] = scaler_y.fit_transform(df_full[['CLASE_DELTA_LOG1P']])\n",
    "\n",
    "media_y = scaler_y.mean_[0]\n",
    "std_y = scaler_y.scale_[0]\n",
    "\n",
    "print(f\"Media de CLASE_DELTA_LOG1P_Z: {media_y}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de CLASE_DELTA_LOG1P_Z: {std_y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "597da50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_clase_delta_log1p_z(pred_z, media = media_y, std = std_y):\n",
    "    pred_log1p = pred_z * std + media\n",
    "    return np.sign(pred_log1p) * (np.expm1(np.abs(pred_log1p)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca8671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas CLASE_DELTA y CLASE_DELTA_LOG1P para evitar confusiones\n",
    "df_full.drop(columns=['CLASE_DELTA', 'CLASE_DELTA_LOG1P'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc77790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cat_cols = ['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE','ANIO', 'MES','TRIMESTRE','MES_PROBLEMATICO']\n",
    "\n",
    "for col in cat_cols:\n",
    "    if df_full[col].isnull().any():\n",
    "        df_full[col] = df_full[col].fillna(\"missing\")\n",
    "\n",
    "\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_full.loc[:, col] = le.fit_transform(df_full[col]).astype(int)\n",
    "    encoders[col] = le  # para guardar los mapeos por si necesit√°s revertirlos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9299b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo los encoders en archivos .pkl\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('encoders', exist_ok=True)\n",
    "\n",
    "for col, le in encoders.items():\n",
    "    joblib.dump(le, f'encoders/{col}_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d1dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame en parquet\n",
    "df_full.to_parquet('./data/interm_NN_TORCH.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d75fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/interm_NN_TORCH.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_group(group_key, df_group, columnas, lags, rolling_windows, trend_windows, time_col):\n",
    "    df_group = df_group.sort_values(by=time_col)\n",
    "    df_group_result = df_group.copy()\n",
    "    nuevas_columnas = []\n",
    "\n",
    "    for col in columnas:\n",
    "        col_data = df_group[col]\n",
    "        min_val = col_data[col_data > 0].min() if (col_data > 0).any() else 1\n",
    "\n",
    "        # Lags y Deltas para todas las columnas\n",
    "        for lag in lags:\n",
    "            lag_col = f'{col}_LAG_{lag:02d}'\n",
    "            delta_col = f'{col}_DELTA_{lag:02d}'\n",
    "            delta_pct_col = f'{col}_DELTA_PCT_{lag:02d}'\n",
    "\n",
    "            df_group_result[lag_col] = col_data.shift(lag)\n",
    "            df_group_result[delta_col] = col_data - df_group_result[lag_col]\n",
    "            reemplazo = np.where(df_group_result[lag_col] == 0, min_val, df_group_result[lag_col])\n",
    "            df_group_result[delta_pct_col] = col_data / reemplazo\n",
    "            df_group_result[delta_pct_col] = df_group_result[delta_pct_col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "            nuevas_columnas += [lag_col, delta_col, delta_pct_col]\n",
    "\n",
    "        # Rolling y tendencias solo para 'TN'\n",
    "        if col == 'TN':\n",
    "            for window in rolling_windows:\n",
    "                roll_mean_col = f'{col}_ROLL_MEAN_{window}'\n",
    "                roll_std_col = f'{col}_ROLL_STD_{window}'\n",
    "                roll_z_col = f'{col}_ROLL_Z_{window}'\n",
    "\n",
    "                roll_mean = col_data.rolling(window, min_periods=1).mean()\n",
    "                roll_std = col_data.rolling(window, min_periods=1).std(ddof=0).fillna(0)\n",
    "                roll_z = (col_data - roll_mean) / roll_std.replace(0, 1)\n",
    "\n",
    "                df_group_result[roll_mean_col] = roll_mean\n",
    "                df_group_result[roll_std_col] = roll_std\n",
    "                df_group_result[roll_z_col] = roll_z\n",
    "\n",
    "                nuevas_columnas += [roll_mean_col, roll_std_col, roll_z_col]\n",
    "\n",
    "            for window in trend_windows:\n",
    "                slope_col = f'{col}_SLOPE_{window}'\n",
    "                def slope_func(x):\n",
    "                    if len(x) < 2:\n",
    "                        return 0.0\n",
    "                    x_ = np.arange(len(x))\n",
    "                    return linregress(x_, x).slope\n",
    "                slope = col_data.rolling(window, min_periods=2).apply(slope_func, raw=True)\n",
    "                df_group_result[slope_col] = slope\n",
    "                nuevas_columnas.append(slope_col)\n",
    "\n",
    "    df_group_result[nuevas_columnas] = df_group_result[nuevas_columnas].fillna(0)\n",
    "    return df_group_result\n",
    "\n",
    "def generar_lags_deltas_rolling_parallel_optim(\n",
    "    df: pd.DataFrame,\n",
    "    columnas: list,\n",
    "    lags: list = list(range(1, 4)),\n",
    "    rolling_windows: list = [3, 6],\n",
    "    trend_windows: list = [6],\n",
    "    group_cols: list = ['PRODUCT_ID', 'CUSTOMER_ID'],\n",
    "    time_col: str = 'ORDINAL',\n",
    "    n_workers: int = 20,\n",
    "    batch_size: int = 50\n",
    "):\n",
    "    print(\"üîÑ Iniciando procesamiento optimizado...\")\n",
    "    df = df.sort_values(by=group_cols + [time_col])\n",
    "    grouped = df.groupby(group_cols)\n",
    "    total_groups = grouped.ngroups\n",
    "    print(f\"üìä Procesando {total_groups:,} grupos en lotes de {batch_size}\")\n",
    "    all_results = []\n",
    "    processed_groups = 0\n",
    "    group_iter = iter(grouped)\n",
    "    while processed_groups < total_groups:\n",
    "        batch = []\n",
    "        for _ in range(min(batch_size, total_groups - processed_groups)):\n",
    "            try:\n",
    "                batch.append(next(group_iter))\n",
    "            except StopIteration:\n",
    "                break\n",
    "        if not batch:\n",
    "            break\n",
    "        with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "            futures = []\n",
    "            for name, group in batch:\n",
    "                group_subset = group[group_cols + [time_col] + columnas].copy()\n",
    "                future = executor.submit(\n",
    "                    process_group,\n",
    "                    name,\n",
    "                    group_subset,\n",
    "                    columnas,\n",
    "                    lags,\n",
    "                    rolling_windows,\n",
    "                    trend_windows,\n",
    "                    time_col\n",
    "                )\n",
    "                futures.append(future)\n",
    "            batch_results = []\n",
    "            # Mostrar progreso dentro del batch\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"üì¶ Procesando batch ({processed_groups}/{total_groups})\"):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    batch_results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error procesando grupo: {e}\")\n",
    "        if batch_results:\n",
    "            batch_df = pd.concat(batch_results, ignore_index=True)\n",
    "            all_results.append(batch_df)\n",
    "            del batch_results, batch_df\n",
    "            gc.collect()\n",
    "        processed_groups += len(batch)\n",
    "        print(f\"‚úÖ Procesados {processed_groups:,}/{total_groups:,} grupos ({processed_groups/total_groups*100:.1f}%)\")\n",
    "    print(\"üîÑ Concatenando resultados finales...\")\n",
    "    df_final = pd.concat(all_results, ignore_index=True)\n",
    "    print(\"üîÑ Optimizando tipos de datos...\")\n",
    "    for col in df_final.columns:\n",
    "        if col not in group_cols + [time_col] and pd.api.types.is_numeric_dtype(df_final[col]):\n",
    "            if df_final[col].dtype == 'float64':\n",
    "                df_final[col] = df_final[col].astype(np.float32)\n",
    "            elif df_final[col].dtype in ['int64', 'int32']:\n",
    "                if df_final[col].min() >= -32768 and df_final[col].max() <= 32767:\n",
    "                    df_final[col] = df_final[col].astype(np.int16)\n",
    "    del all_results\n",
    "    gc.collect()\n",
    "    print(f\"‚úÖ Procesamiento completado. Shape final: {df_final.shape}\")\n",
    "    return df_final.sort_index()\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_full = generar_lags_deltas_rolling_parallel_optim(\n",
    "    df=df_full,\n",
    "    columnas=['TN', 'CUST_REQUEST_QTY', 'CUST_REQUEST_TN', 'CANT_PROD_CLI_PER'],\n",
    "    lags=list(range(1, 12)),\n",
    "    rolling_windows=[3, 6],\n",
    "    trend_windows=[6],\n",
    "    n_workers=28,\n",
    "    batch_size=50000  # Bajalo si a√∫n ten√©s problemas de RAM\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff5ded51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame en parquet\n",
    "df_full.to_parquet('./data/interm_NN_TORCH.parquet', index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f799e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENDIENTE_TENDENCIA_12    2777480\n",
      "TN_EWMA_12                2777480\n",
      "TN_MEDIAN_12              2777480\n",
      "TN_MAX_12                 2777480\n",
      "TN_MIN_12                 2777480\n",
      "TN_EWMA_09                2066722\n",
      "PENDIENTE_TENDENCIA_9     2066722\n",
      "TN_MEDIAN_09              2066722\n",
      "TN_MIN_09                 2066722\n",
      "TN_MAX_09                 2066722\n",
      "TN_EWMA_06                1310741\n",
      "TN_MAX_06                 1310741\n",
      "TN_MEDIAN_06              1310741\n",
      "PENDIENTE_TENDENCIA_6     1310741\n",
      "TN_MIN_06                 1310741\n",
      "TN_EWMA_03                 525526\n",
      "TN_MAX_03                  525526\n",
      "PENDIENTE_TENDENCIA_3      525526\n",
      "TN_MEDIAN_03               525526\n",
      "TN_MIN_03                  525526\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Excluir la variable de clase\n",
    "cols_to_check = df_full.columns.difference(['CLASE_DELTA_LOG1P_Z'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_full[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1b1fca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3921255/2542019331.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_3921255/2542019331.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full[col].fillna(media_col, inplace=True)\n",
      "/tmp/ipykernel_3921255/2542019331.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full[col].fillna(media_col, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Para las columnas con NaN, si son PENDIENTE_TENDENCIA_nn reemplazar por 0,\n",
    "# sin son TN_EWMA reemplazar por la media de la columna\n",
    "for col in nan_columns.index:\n",
    "    if 'PENDIENTE_TENDENCIA' in col:\n",
    "        df_full[col].fillna(0, inplace=True)\n",
    "    elif 'TN_EWMA' in col:\n",
    "        media_col = df_full[col].mean()\n",
    "        df_full[col].fillna(media_col, inplace=True)\n",
    "    elif 'TN_MIN' in col or 'TN_MAX' in col or 'TN_MEDIAN' in col:\n",
    "        media_col = df_full[col].mean()\n",
    "        df_full[col].fillna(media_col, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9abb44fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Excluir la variable de clase\n",
    "cols_to_check = df_full.columns.difference(['CLASE_DELTA_LOG1P_Z'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_full[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae6c73c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DataFrame final con 7,781,619 filas y 96 columnas:\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìä DataFrame final con {df_full.shape[0]:,} filas y {df_full.shape[1]} columnas:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c38a24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia de seguridad\n",
    "df = df_full.copy()\n",
    "\n",
    "\n",
    "# === Binning (en deciles) ===\n",
    "df['CUSTOMER_RANK_BIN'] = pd.qcut(df['CUSTOMER_ID'], q=10, labels=False)\n",
    "df['CUSTOMER_RANK_BIN'] = df['CUSTOMER_RANK_BIN'].astype('category')\n",
    "\n",
    "# Validaci√≥n opcional\n",
    "assert df['CUSTOMER_RANK_BIN'].isna().sum() == 0, \"NaNs en qcut\"\n",
    "\n",
    "# Reemplazar en df_full\n",
    "df_full = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbc780ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia de seguridad del DataFrame\n",
    "df = df_full.copy()\n",
    "\n",
    "# === Binning (en deciles) ===\n",
    "df['PRODUCT_RANK_BIN'] = pd.qcut(df['PRODUCT_ID'], q=10, labels=False)\n",
    "df['PRODUCT_RANK_BIN'] = df['PRODUCT_RANK_BIN'].astype('category')\n",
    "\n",
    "# Reemplazar en el DataFrame principal\n",
    "df_full = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26e24452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia del DataFrame original\n",
    "df = df_full.copy()\n",
    "\n",
    "# Columnas categ√≥ricas\n",
    "cat_cols = cat_cols + ['CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7b1f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN']\n"
     ]
    }
   ],
   "source": [
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb34f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERIODO', 'CLASE_DELTA_LOG1P_Z', 'MES_SIN', 'MES_COS', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Columnas a excluir\n",
    "excluir = ['PERIODO','CLASE_DELTA_LOG1P_Z', 'MES_SIN', 'MES_COS'] + cat_cols\n",
    "print(excluir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b288b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columnas num√©ricas a escalar\n",
    "cols_a_escalar = [col for col in df.columns if col not in excluir and pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "# Entrenamiento del scaler SOLO con datos de entrenamiento\n",
    "df_entrenamiento = df[df['PERIODO'] <= 201910].copy()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_entrenamiento[cols_a_escalar])\n",
    "\n",
    "# Aplicaci√≥n del scaler a TODO el dataset\n",
    "valores_escalados = scaler.transform(df[cols_a_escalar])\n",
    "df_scaled = pd.DataFrame(valores_escalados, columns=[col + '_Z' for col in cols_a_escalar], index=df.index)\n",
    "\n",
    "# Reemplazo de columnas originales salvo CUSTOMER_ID y PRODUCT_ID\n",
    "cols_a_escalar = [col for col in cols_a_escalar if col not in ['CUSTOMER_ID', 'PRODUCT_ID']]\n",
    "df.drop(columns=cols_a_escalar, inplace=True)\n",
    "df = pd.concat([df, df_scaled], axis=1)\n",
    "\n",
    "# Guardar scaler\n",
    "joblib.dump(scaler, './encoders/scaler_features_numericas.pkl')\n",
    "\n",
    "# Actualizar df_full\n",
    "df_full = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33c9a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERIODO', 'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID', 'PRODUCT_ID', 'MES_PROBLEMATICO', 'CLASE_DELTA_LOG1P_Z', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN', 'CUSTOMER_ID_Z', 'PRODUCT_ID_Z', 'CUST_REQUEST_QTY_Z', 'CUST_REQUEST_TN_Z', 'TN_Z', 'MEDIA_MOVIL_3M_CLI_PROD_Z', 'MEDIA_MOVIL_6M_CLI_PROD_Z', 'MEDIA_MOVIL_12M_CLI_PROD_Z', 'DESVIO_MOVIL_3M_CLI_PROD_Z', 'DESVIO_MOVIL_6M_CLI_PROD_Z', 'DESVIO_MOVIL_12M_CLI_PROD_Z', 'MEDIA_MOVIL_3M_PROD_Z', 'MEDIA_MOVIL_6M_PROD_Z', 'MEDIA_MOVIL_12M_PROD_Z', 'DESVIO_MOVIL_3M_PROD_Z', 'DESVIO_MOVIL_6M_PROD_Z', 'DESVIO_MOVIL_12M_PROD_Z', 'MEDIA_MOVIL_3M_CLI_Z', 'MEDIA_MOVIL_6M_CLI_Z', 'MEDIA_MOVIL_12M_CLI_Z', 'DESVIO_MOVIL_3M_CLI_Z', 'DESVIO_MOVIL_6M_CLI_Z', 'DESVIO_MOVIL_12M_CLI_Z', 'TN_LAG_01_Z', 'TN_LAG_02_Z', 'TN_LAG_03_Z', 'TN_LAG_04_Z', 'TN_LAG_05_Z', 'TN_LAG_06_Z', 'TN_LAG_07_Z', 'TN_LAG_08_Z', 'TN_LAG_09_Z', 'TN_LAG_10_Z', 'TN_LAG_11_Z', 'TN_LAG_12_Z', 'TN_LAG_13_Z', 'TN_LAG_14_Z', 'TN_LAG_15_Z', 'ORDINAL_Z', 'TN_DELTA_01_Z', 'TN_DELTA_02_Z', 'TN_DELTA_03_Z', 'TN_DELTA_04_Z', 'TN_DELTA_05_Z', 'TN_DELTA_06_Z', 'TN_DELTA_07_Z', 'TN_DELTA_08_Z', 'TN_DELTA_09_Z', 'TN_DELTA_10_Z', 'TN_DELTA_11_Z', 'TN_DELTA_12_Z', 'TN_DELTA_13_Z', 'TN_DELTA_14_Z', 'TN_DELTA_15_Z', 'ANTIG_CLIENTE_Z', 'ANTIG_PRODUCTO_Z', 'CANT_PROD_CLI_PER_Z', 'MEDIA_PROD_PER_Z', 'MEDIA_PROD_Z', 'MEDIA_PER_Z', 'PENDIENTE_TENDENCIA_3_Z', 'TN_EWMA_03_Z', 'TN_MEDIAN_03_Z', 'TN_MIN_03_Z', 'TN_MAX_03_Z', 'PENDIENTE_TENDENCIA_6_Z', 'TN_EWMA_06_Z', 'TN_MEDIAN_06_Z', 'TN_MIN_06_Z', 'TN_MAX_06_Z', 'PENDIENTE_TENDENCIA_9_Z', 'TN_EWMA_09_Z', 'TN_MEDIAN_09_Z', 'TN_MIN_09_Z', 'TN_MAX_09_Z', 'PENDIENTE_TENDENCIA_12_Z', 'TN_EWMA_12_Z', 'TN_MEDIAN_12_Z', 'TN_MIN_12_Z', 'TN_MAX_12_Z', 'MESES_SIN_COMPRAR_PRODUCT_CUSTOMER_ID_Z', 'MESES_SIN_COMPRAR_PRODUCT_ID_Z', 'MESES_SIN_COMPRAR_CUSTOMER_ID_Z']\n"
     ]
    }
   ],
   "source": [
    "print(df_full.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d7910d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Excluir la variable de clase\n",
    "cols_to_check = df_full.columns.difference(['CLASE_DELTA_LOG1P_Z'])\n",
    "\n",
    "# Calcular cantidad de NaNs por columna\n",
    "nan_columns = df_full[cols_to_check].isna().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un NaN\n",
    "nan_columns = nan_columns[nan_columns > 0].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed5652d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7781619, 100)\n"
     ]
    }
   ],
   "source": [
    "print(df_full.shape)\n",
    "# Guardar el DataFrame resultante en un archivo parquet\n",
    "df_full.to_parquet('./data/train_val_NN_TORCH.parquet', engine='fastparquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "777f0d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'ANIO', 'MES', 'TRIMESTRE', 'MES_PROBLEMATICO', 'CUSTOMER_RANK_BIN', 'PRODUCT_RANK_BIN']\n"
     ]
    }
   ],
   "source": [
    "print(cat_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
