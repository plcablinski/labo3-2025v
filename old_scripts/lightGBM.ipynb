{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f82aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import joblib\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Machine Learning ---\n",
    "import optuna\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# --- Parallel Processing ---\n",
    "from joblib import Parallel, delayed\n",
    "from more_itertools import chunked\n",
    "\n",
    "# --- PyTorch (if needed) ---\n",
    "#import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/product_interm_LGBM.parquet', engine='fastparquet')\n",
    "# Separar conjuntos\n",
    "df_train = df_full[df_full['PERIODO'] <= 201812].copy()\n",
    "df_val = df_full[df_full['PERIODO'].between(201901, 201909)].copy()\n",
    "df_test = df_full[(df_full['PERIODO'] == 201910)].copy()\n",
    "df_target_201912 = df_full[df_full['PERIODO'] == 201912].copy()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_forecast_error(y_true, y_pred):\n",
    "    numerador = np.sum(np.abs(y_true - y_pred))\n",
    "    denominador = np.sum(y_true)\n",
    "    if denominador == 0:\n",
    "        return np.nan\n",
    "    return numerador / denominador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --- SETUP ---\n",
    "target_col = 'CLASE_LOG1P'\n",
    "feature_cols = [col for col in df_train.columns if col != target_col]\n",
    "\n",
    "X_tr = df_train[feature_cols]\n",
    "y_tr = df_train[target_col]\n",
    "X_val = df_val[feature_cols]\n",
    "y_val = df_val[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ef116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Definir función objetivo ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0005, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 32),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 3000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 120),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.95),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 0.95),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 100.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 100.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"random_state\": trial.number,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbosity\": -1,\n",
    "        \"objective\": \"mae\",\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[early_stopping(stopping_rounds=100, verbose=False)],\n",
    "    )\n",
    "    preds = model.predict(X_val)\n",
    "    \n",
    "    y_val_orig = inv_transform_log1p(y_val.values)\n",
    "    preds_orig = inv_transform_log1p(preds)\n",
    "\n",
    "    tfe = total_forecast_error(y_val_orig, preds_orig)\n",
    "    trial.set_user_attr(\"mae\", mean_absolute_error(y_val_orig, preds_orig))\n",
    "    gc.collect()\n",
    "    return tfe\n",
    "\n",
    "# --- 2. Crear estudio con almacenamiento en SQLite ---\n",
    "optuna_db_path = \"sqlite:///optuna_lgbm_study.db\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=101),\n",
    "    storage=optuna_db_path,\n",
    "    study_name=\"lgbm_tfe_optim\",\n",
    "    load_if_exists=True  # si ya existe, lo continúa\n",
    ")\n",
    "\n",
    "# --- 3. Ejecutar la optimización ---\n",
    "N_MODELS = 50\n",
    "N_TRIALS = 500\n",
    "\n",
    "study.optimize(objective_lgbm, n_trials=N_TRIALS, n_jobs=28, show_progress_bar=True)\n",
    "\n",
    "# --- 4. Extraer y guardar los mejores parámetros ---\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df[\"mae\"] = [t.user_attrs.get(\"mae\", np.nan) for t in study.trials]\n",
    "\n",
    "top_lgbm_trials = trials_df.sort_values(\"value\").head(N_MODELS)\n",
    "\n",
    "final_configs = []\n",
    "for i, row in top_lgbm_trials.iterrows():\n",
    "    params = row.filter(like='params_').to_dict()\n",
    "    params = {k.replace('params_', ''): v for k, v in params.items()}\n",
    "    for p in [\"num_leaves\", \"max_depth\", \"n_estimators\", \"min_child_samples\"]:\n",
    "        params[p] = int(params[p])\n",
    "    params[\"random_state\"] = int(row[\"number\"])\n",
    "    params[\"n_jobs\"] = -1\n",
    "    params[\"verbosity\"] = -1\n",
    "    final_configs.append(params)\n",
    "\n",
    "# --- 5. Guardar en archivos externos ---\n",
    "top_lgbm_trials.to_csv(\"optuna_lgbm_trials.csv\", index=False)\n",
    "\n",
    "with open(\"lgbm_ensemble_configs.json\", \"w\") as f:\n",
    "    json.dump(final_configs, f, indent=2)\n",
    "\n",
    "# --- 6. Resumen en consola ---\n",
    "print(f\"Top 50 TFE: {top_lgbm_trials['value'].head(50).values}\")\n",
    "print(f\"Top 50 MAE: {top_lgbm_trials['mae'].head(50).values}\")\n",
    "print(final_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93254e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([df_train, df_val], ignore_index=True)[feature_cols]\n",
    "y_full = pd.concat([df_train[target_col], df_val[target_col]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9213cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_lgbm_trials = pd.read_csv(\"optuna_lgbm_trials.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"lgbm_ensemble_configs.json\", \"r\") as f:\n",
    "    final_configs = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0060a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def train_and_save_model(i, params, X_full, y_full):\n",
    "    print(f\"Entrenando modelo {i+1}/50...\")\n",
    "    params = params.copy()  # Para no modificar el original en la lista\n",
    "    params[\"objective\"] = \"mae\"  # Forzar MAE aunque el config no lo tenga\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_full, y_full)\n",
    "    joblib.dump(model, f'lgbm_model_{i+1:02d}.pkl')\n",
    "    return f\"Modelo {i+1} terminado\"\n",
    "\n",
    "\n",
    "results = joblib.Parallel(n_jobs=20)(\n",
    "    joblib.delayed(train_and_save_model)(i, params, X_full, y_full)\n",
    "    for i, params in enumerate(final_configs[:50])\n",
    ")\n",
    "\n",
    "print(results)\n",
    "print(\"¡Entrenamiento y guardado de los 50 modelos finalizado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Cargar los 50 modelos LightGBM entrenados\n",
    "lgbm_models = []\n",
    "for i in range(1, 51):\n",
    "    model = joblib.load(f'lgbm_model_{i:02d}.pkl')\n",
    "    lgbm_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Predicción LightGBM (50 modelos) ---\n",
    "lgbm_preds = []\n",
    "\n",
    "feature_cols = [col for col in df_test.columns if col != target_col]\n",
    "\n",
    "X_pred_lgbm = df_test[feature_cols]  # Si podés, asegurate que están en el mismo orden y tipo que en train\n",
    "\n",
    "for i, model in enumerate(lgbm_models):\n",
    "    print(f\"Prediciendo LightGBM {i+1}/50...\")\n",
    "    preds = model.predict(X_pred_lgbm)\n",
    "    lgbm_preds.append(preds)\n",
    "\n",
    "lgbm_preds = np.stack(lgbm_preds).T  # shape (N, 50)\n",
    "\n",
    "# --- Agregar predicciones LGBM al DataFrame existente con resultados de MLP ---\n",
    "for i in range(50):\n",
    "    df_test[f'lgbm_pred_LOG1P_{i+1}'] = lgbm_preds[:, i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6f807",
   "metadata": {},
   "source": [
    "## 📊 Cálculo de métricas estadísticas sobre el ensemble de modelos LightGBM\n",
    "\n",
    "Una vez agregadas las predicciones individuales de los 50 modelos LightGBM al DataFrame `df_test`, se calculan métricas agregadas por fila para capturar información estadística sobre la dispersión y tendencia central del ensemble.\n",
    "\n",
    "Estas métricas permiten construir predicciones más robustas, como la media o mediana del conjunto de modelos, y analizar la incertidumbre asociada a cada predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aae736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seleccioná las columnas de predicción\n",
    "lgbm_pred_cols = [col for col in df_test.columns if col.startswith('lgbm_pred_LOG1P_')]\n",
    "\n",
    "# Calcula métricas fila a fila\n",
    "df_test['lgbm_pred_mean'] = df_test[lgbm_pred_cols].mean(axis=1)\n",
    "df_test['lgbm_pred_median'] = df_test[lgbm_pred_cols].median(axis=1)\n",
    "df_test['lgbm_pred_std'] = df_test[lgbm_pred_cols].std(axis=1)\n",
    "df_test['lgbm_pred_q25'] = df_test[lgbm_pred_cols].quantile(0.25, axis=1)\n",
    "df_test['lgbm_pred_q75'] = df_test[lgbm_pred_cols].quantile(0.75, axis=1)\n",
    "df_test['lgbm_pred_min'] = df_test[lgbm_pred_cols].min(axis=1)\n",
    "df_test['lgbm_pred_max'] = df_test[lgbm_pred_cols].max(axis=1)\n",
    "df_test['lgbm_pred_iqr'] = df_test['lgbm_pred_q75'] - df_test['lgbm_pred_q25']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rango de productos a analizar\n",
    "productos = range(20001, 20051)\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "box_data = []\n",
    "obs_pos = []\n",
    "obs_vals = []\n",
    "\n",
    "for i, pid in enumerate(productos, 1):\n",
    "    # Todas las predicciones fila a fila para ese producto\n",
    "    vals = df_test[df_test['PRODUCT_ID'] == pid][lgbm_pred_cols].values.flatten()\n",
    "    box_data.append(vals)\n",
    "    # Todos los valores observados para ese producto\n",
    "    vals_obs = df_test[df_test['PRODUCT_ID'] == pid]['CLASE_LOG1P'].values\n",
    "    for obs in vals_obs:\n",
    "        obs_pos.append(i)\n",
    "        obs_vals.append(obs)\n",
    "\n",
    "# Dibujar los boxplots\n",
    "plt.boxplot(box_data, positions=range(1, len(productos)+1), widths=0.6, patch_artist=True, showmeans=True)\n",
    "\n",
    "# Dibujar todos los valores observados como círculos rojos\n",
    "plt.plot(obs_pos, obs_vals, 'ro', markersize=7, label='Valor observado')\n",
    "\n",
    "plt.title('Dispersión de predicciones y observado para PRODUCT_ID 20001 a 20050')\n",
    "plt.ylabel('Predicción (LOG1P)')\n",
    "plt.xlabel('PRODUCT_ID')\n",
    "plt.xticks(range(1, len(productos)+1), [str(pid) for pid in productos], rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6bcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calcula la dispersión (std) para cada producto\n",
    "pred_cols = [c for c in df_test.columns if c.startswith('lgbm_pred_LOG1P_')]\n",
    "df_test['pred_std'] = df_test[pred_cols].std(axis=1)\n",
    "top_n = 20\n",
    "productos_disp = df_test.sort_values('pred_std', ascending=False)['PRODUCT_ID'].unique()[:top_n]\n",
    "\n",
    "# Subset de los productos seleccionados\n",
    "df_plot = df_test[df_test['PRODUCT_ID'].isin(productos_disp)].copy()\n",
    "df_plot = df_plot[['PRODUCT_ID', 'CLASE_LOG1P'] + pred_cols]\n",
    "\n",
    "# Boxplots\n",
    "plt.figure(figsize=(15, 6))\n",
    "data = df_plot[pred_cols].values  # Solo las columnas de predicción\n",
    "positions = np.arange(1, len(df_plot)+1)\n",
    "\n",
    "plt.boxplot(data.T, labels=df_plot['PRODUCT_ID'], showfliers=True)\n",
    "\n",
    "# Marcar el valor observado con un punto rojo para cada producto\n",
    "for idx, (prod, y_true) in enumerate(zip(df_plot['PRODUCT_ID'], df_plot['CLASE_LOG1P'])):\n",
    "    plt.scatter(idx+1, y_true, color='crimson', marker='o', s=100, zorder=5, label='Valor observado' if idx==0 else \"\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('PRODUCT_ID')\n",
    "plt.ylabel('Predicción (LOG1P)')\n",
    "plt.title(f'Dispersión de predicciones para los {top_n} productos más dispersos')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la predicción central y la dispersión\n",
    "df_test['pred_median'] = df_test[pred_cols].median(axis=1)\n",
    "df_test['pred_mean'] = df_test[pred_cols].mean(axis=1)\n",
    "df_test['pred_std'] = df_test[pred_cols].std(axis=1)\n",
    "\n",
    "# Error absoluto respecto al real\n",
    "df_test['abs_err_median'] = (df_test['pred_median'] - df_test['CLASE_LOG1P']).abs()\n",
    "df_test['abs_err_mean'] = (df_test['pred_mean'] - df_test['CLASE_LOG1P']).abs()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df_test['pred_std'], df_test['abs_err_median'], alpha=0.6)\n",
    "plt.xlabel('Dispersión de predicción (std de 50 modelos)')\n",
    "plt.ylabel('Error absoluto (mediana)')\n",
    "plt.title('Dispersión vs Error absoluto por producto')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec29f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de columnas de predicción transformadas\n",
    "pred_cols = [c for c in df_test.columns if c.startswith('lgbm_pred_LOG1P_')]\n",
    "\n",
    "# Inversa de la transformación log1p sobre la variable objetivo\n",
    "df_test['CLASE_TN'] = np.expm1(df_test['CLASE_LOG1P'])\n",
    "\n",
    "# Inversa de log1p sobre las predicciones\n",
    "preds_orig = np.expm1(df_test[pred_cols].values)  # Deshace log1p\n",
    "\n",
    "# Crear nuevas columnas con las predicciones en escala original\n",
    "for i, col in enumerate(pred_cols):\n",
    "    new_col = col.replace('LOG1P', 'TN')\n",
    "    df_test[new_col] = preds_orig[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a027a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las nuevas columnas de predicción en escala original\n",
    "pred_cols_tn = [c for c in df_test.columns if c.startswith('lgbm_pred_TN_')]\n",
    "df_test['pred_std_tn'] = df_test[pred_cols_tn].std(axis=1)\n",
    "df_test['pred_median_tn'] = df_test[pred_cols_tn].median(axis=1)\n",
    "df_test['abs_error_median_tn'] = np.abs(df_test['pred_median_tn'] - df_test['CLASE_TN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85da2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_test['pred_std_tn'], df_test['abs_error_median_tn'], alpha=0.7)\n",
    "\n",
    "# Etiquetar solo los puntos más dispersos\n",
    "umbral_disp = 5  # Cambia el umbral si querés más o menos etiquetas\n",
    "df_label = df_test[df_test['pred_std_tn'] > umbral_disp]\n",
    "\n",
    "for _, row in df_label.iterrows():\n",
    "    plt.text(\n",
    "        row['pred_std_tn'],\n",
    "        row['abs_error_median_tn'],\n",
    "        f\"{int(row['PRODUCT_ID'])}\",  # fuerza a int sin decimales\n",
    "        fontsize=6,                  # más pequeño\n",
    "        color='crimson',\n",
    "        alpha=0.8,\n",
    "        ha='left',\n",
    "        va='bottom'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Dispersión de predicción (std de 50 modelos) [TN]')\n",
    "plt.ylabel('Error absoluto (mediana) [TN]')\n",
    "plt.title('Dispersión vs Error absoluto por producto (escala original)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top N más dispersos\n",
    "top_n = 20\n",
    "productos_disp = df_test.sort_values('pred_std_tn', ascending=False)['PRODUCT_ID'].unique()[:top_n]\n",
    "df_plot = df_test[df_test['PRODUCT_ID'].isin(productos_disp)].copy()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Armar datos para el boxplot\n",
    "data = [row[1:] for row in df_plot[['PRODUCT_ID'] + pred_cols_tn].values]  # omite PRODUCT_ID\n",
    "labels = df_plot['PRODUCT_ID'].astype(str).values\n",
    "\n",
    "# Plotear los boxplots\n",
    "plt.boxplot(data, labels=labels, showfliers=True)\n",
    "\n",
    "# Sobreponer los valores observados (puede haber más de uno por producto, si hay varias filas)\n",
    "for idx, prod_id in enumerate(df_plot['PRODUCT_ID']):\n",
    "    valores_reales = df_test[df_test['PRODUCT_ID'] == prod_id]['CLASE_TN'].values\n",
    "    for vreal in valores_reales:\n",
    "        plt.plot(idx + 1, vreal, 'ro', markersize=7, label='Valor observado' if idx == 0 else \"\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('PRODUCT_ID')\n",
    "plt.ylabel('Predicción (TN)')\n",
    "plt.title(f'Dispersión de predicciones y observados para los {top_n} productos más dispersos (escala original)')\n",
    "\n",
    "# Mostrar solo un label para 'Valor observado'\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "if handles:\n",
    "    plt.legend([handles[0]], ['Valor observado'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 30\n",
    "\n",
    "# Top por dispersión\n",
    "top_disp = df_test.sort_values('pred_std_tn', ascending=False).head(top_n)\n",
    "print(\"Top 30 productos por dispersión:\")\n",
    "print(top_disp[['PRODUCT_ID', 'pred_std_tn', 'abs_error_median_tn', 'CLASE_TN']])\n",
    "\n",
    "# Top por error absoluto\n",
    "top_error = df_test.sort_values('abs_error_median_tn', ascending=False).head(top_n)\n",
    "print(\"\\nTop 30 productos por error absoluto:\")\n",
    "print(top_error[['PRODUCT_ID', 'pred_std_tn', 'abs_error_median_tn', 'CLASE_TN']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Identificá las columnas de predicción\n",
    "pred_cols_tn = [c for c in df_test.columns if c.startswith('lgbm_pred_TN_')]\n",
    "\n",
    "# 2. Calculá media y mediana de las predicciones\n",
    "df_test['ensemble_median'] = df_test[pred_cols_tn].median(axis=1)\n",
    "df_test['ensemble_mean']   = df_test[pred_cols_tn].mean(axis=1)\n",
    "\n",
    "# 3. Calculá el MAE para ambas estrategias\n",
    "mae_median = np.mean(np.abs(df_test['ensemble_median'] - df_test['CLASE_TN']))\n",
    "mae_mean   = np.mean(np.abs(df_test['ensemble_mean']   - df_test['CLASE_TN']))\n",
    "\n",
    "print(f\"MAE usando la **mediana** de 50 modelos: {mae_median:.3f} TN\")\n",
    "print(f\"MAE usando la **media** de 50 modelos:   {mae_mean:.3f} TN\")\n",
    "\n",
    "# 4. Si querés comparar producto por producto:\n",
    "df_test['error_median'] = np.abs(df_test['ensemble_median'] - df_test['CLASE_TN'])\n",
    "df_test['error_mean']   = np.abs(df_test['ensemble_mean']   - df_test['CLASE_TN'])\n",
    "\n",
    "print(\"\\nTop 10 productos donde la media le gana a la mediana:\")\n",
    "print(df_test[df_test['error_mean'] < df_test['error_median']][['PRODUCT_ID', 'error_median', 'error_mean']].sort_values('error_median', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca219c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(df_test['error_median'], df_test['error_mean'], alpha=0.7)\n",
    "\n",
    "plt.plot(\n",
    "    [df_test['error_median'].min(), df_test['error_median'].max()],\n",
    "    [df_test['error_median'].min(), df_test['error_median'].max()],\n",
    "    'r--', label='y=x'\n",
    ")\n",
    "\n",
    "# Etiquetar PRODUCT_ID donde error_median > 75 o error_mean > 75\n",
    "for _, row in df_test.iterrows():\n",
    "    if row['error_median'] > 75 or row['error_mean'] > 75:\n",
    "        plt.text(\n",
    "            row['error_median'],\n",
    "            row['error_mean'],\n",
    "            str(int(row['PRODUCT_ID'])),\n",
    "            fontsize=5,\n",
    "            color='crimson',\n",
    "            alpha=0.85,\n",
    "            ha='left',\n",
    "            va='bottom'\n",
    "        )\n",
    "\n",
    "plt.xlabel('Error absoluto usando MEDIANA')\n",
    "plt.ylabel('Error absoluto usando MEDIA')\n",
    "plt.title('Comparación error absoluto por producto: Media vs Mediana')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ba516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lambdas = np.linspace(0, 1, 21)\n",
    "maes = []\n",
    "\n",
    "for l in lambdas:\n",
    "    pred = l * df_test['ensemble_median'] + (1 - l) * df_test['ensemble_mean']\n",
    "    mae = np.mean(np.abs(pred - df_test['CLASE_TN']))\n",
    "    maes.append(mae)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(lambdas, maes, marker='o')\n",
    "plt.xlabel('Lambda (ponderación de la MEDIANA)')\n",
    "plt.ylabel('MAE (ensemble)')\n",
    "plt.title('Búsqueda de mejor combinación media/mediana')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_lambda = lambdas[np.argmin(maes)]\n",
    "print(f\"Mejor lambda = {best_lambda:.2f} (MAE={min(maes):.3f} TN)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d33d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suponiendo que en tu df_test ya tenés:\n",
    "# 'ensemble_median', 'ensemble_mean', y la observación real 'CLASE_TN'\n",
    "lambdas = np.linspace(0, 1, 21)\n",
    "tfe_scores = []\n",
    "\n",
    "for l in lambdas:\n",
    "    y_pred = l * df_test['ensemble_median'] + (1 - l) * df_test['ensemble_mean']\n",
    "    tfe = total_forecast_error(df_test['CLASE_TN'].values, y_pred.values)\n",
    "    tfe_scores.append(tfe)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(lambdas, tfe_scores, marker='o')\n",
    "plt.xlabel('Lambda (ponderación de la MEDIANA)')\n",
    "plt.ylabel('Total Forecast Error (TFE)')\n",
    "plt.title('Búsqueda de mejor combinación media/mediana (TFE)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Mejor lambda encontrado y su TFE\n",
    "best_idx = np.argmin(tfe_scores)\n",
    "print(f\"Mejor lambda: {lambdas[best_idx]:.2f}  ->  TFE = {tfe_scores[best_idx]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8678105",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Si ensemble_median es UNA predicción por producto (no por fila):\n",
    "# Entonces podés hacer así, suponiendo que tenés una única fila por PRODUCT_ID en df_test:\n",
    "df_test['abs_error_median'] = np.abs(df_test['ensemble_median'] - df_test['CLASE_TN'])\n",
    "\n",
    "# Ordenar por el error absoluto de la mediana, descendente (de mayor a menor error)\n",
    "df_ordenado = df_test.sort_values('abs_error_median', ascending=False)[['PRODUCT_ID', 'ensemble_median', 'CLASE_TN', 'abs_error_median']]\n",
    "\n",
    "# A df_ordenado agregar el desvio estándar de las predicciones\n",
    "df_ordenado['pred_std'] = df_test[pred_cols_tn].std(axis=1)\n",
    "\n",
    "print(df_ordenado.head(50))\n",
    "df_ordenado.to_csv('validacion_lgbm_ordenadas.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a36834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula el TFE por producto usando media y mediana\n",
    "tfe_prod_median = []\n",
    "tfe_prod_mean = []\n",
    "\n",
    "for pid, subdf in df_test.groupby('PRODUCT_ID'):\n",
    "    tfe_median = total_forecast_error(subdf['CLASE_TN'].values, subdf['ensemble_median'].values)\n",
    "    tfe_mean = total_forecast_error(subdf['CLASE_TN'].values, subdf['ensemble_mean'].values)\n",
    "    tfe_prod_median.append((pid, tfe_median))\n",
    "    tfe_prod_mean.append((pid, tfe_mean))\n",
    "\n",
    "df_tfe = pd.DataFrame({\n",
    "    'PRODUCT_ID': [pid for pid, _ in tfe_prod_median],\n",
    "    'TFE_median': [tfe for _, tfe in tfe_prod_median],\n",
    "    'TFE_mean': [tfe for _, tfe in tfe_prod_mean]\n",
    "})\n",
    "df_tfe['TFE_diff'] = df_tfe['TFE_median'] - df_tfe['TFE_mean']\n",
    "\n",
    "# Top 10 productos donde la media mejora más vs la mediana\n",
    "print(df_tfe.sort_values('TFE_diff', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3bf87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el TFE total para el conjunto de test\n",
    "tfe_total_median = total_forecast_error(df_test['CLASE_TN'].values, df_test['ensemble_median'].values)\n",
    "tfe_total_mean = total_forecast_error(df_test['CLASE_TN'].values, df_test['ensemble_mean'].values)\n",
    "print(f\"TFE total usando mediana: {tfe_total_median:.6f}\")\n",
    "print(f\"TFE total usando media: {tfe_total_mean:.6f}\")\n",
    "print(np.sum(df_test['CLASE_TN'].values))  # Para verificar el denominador de TFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Cargar tu DataFrame ---\n",
    "# Debe contener: PRODUCT_ID, CLASE_TN (real), ensemble_median (predicción LGBM)\n",
    "df_test = pd.read_csv(\"validacion_lgbm_ordenadas.csv\")\n",
    "\n",
    "# --- 1. Función y TFE total ---\n",
    "def total_forecast_error(y_true, y_pred):\n",
    "    numerador = np.sum(np.abs(y_true - y_pred))\n",
    "    denominador = np.sum(y_true)\n",
    "    return numerador / denominador if denominador != 0 else np.nan\n",
    "\n",
    "tfe_total = total_forecast_error(df_test[\"CLASE_TN\"], df_test[\"ensemble_median\"])\n",
    "print(f\"TFE total (mediana LGBM): {tfe_total:.4f}\")\n",
    "\n",
    "# --- 2. Error absoluto y aporte individual al TFE ---\n",
    "denominador = df_test[\"CLASE_TN\"].sum()\n",
    "df_test[\"abs_error_median\"] = np.abs(df_test[\"CLASE_TN\"] - df_test[\"ensemble_median\"])\n",
    "df_test[\"tfe_aporte\"] = df_test[\"abs_error_median\"] / denominador          # proporción (0–1)\n",
    "df_test[\"tfe_pct\"]    = df_test[\"tfe_aporte\"] * 100                        # porcentaje\n",
    "\n",
    "# --- 3. Ordenar por aporte y acumulado ---\n",
    "df_test = df_test.sort_values(\"tfe_aporte\", ascending=False).reset_index(drop=True)\n",
    "df_test[\"aporte_acumulado\"] = df_test[\"tfe_aporte\"].cumsum()\n",
    "df_test[\"aporte_acum_pct\"]  = df_test[\"aporte_acumulado\"] * 100\n",
    "\n",
    "# --- 4. Productos que generan el 80 % del TFE ---\n",
    "df_test[\"top_80pct_error\"] = df_test[\"aporte_acumulado\"] <= 0.80\n",
    "productos_criticos = df_test.loc[df_test[\"top_80pct_error\"], \"PRODUCT_ID\"].tolist()\n",
    "\n",
    "# --- 5. Imprimir en formato “PID:XX %” ---\n",
    "print(\"\\nPRODUCT_ID y % del TFE (hasta cubrir el 80 % acumulado):\")\n",
    "formatted = df_test.loc[df_test[\"top_80pct_error\"]] \\\n",
    "                   .apply(lambda r: f\"{int(r['PRODUCT_ID'])}:{r['tfe_pct']:.1f}%\", axis=1)\n",
    "print(\", \".join(formatted))\n",
    "\n",
    "# --- 6. Guardar archivo con todo el detalle ---\n",
    "df_test.to_csv(\"aporte_TFE_por_producto.csv\",\n",
    "               columns=[\"PRODUCT_ID\", \"CLASE_TN\", \"ensemble_median\",\n",
    "                        \"abs_error_median\", \"tfe_pct\", \"aporte_acum_pct\",\n",
    "                        \"top_80pct_error\"],\n",
    "               index=False)\n",
    "\n",
    "print(\"\\nArchivo 'aporte_TFE_por_producto.csv' generado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top productos donde la mediana es mejor que la media\n",
    "print(df_tfe[df_tfe['TFE_diff'] < 0].sort_values('TFE_diff').head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top productos donde la media es mejor que la mediana\n",
    "print(df_tfe[df_tfe['TFE_diff'] > 0].sort_values('TFE_diff').head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63fdcb",
   "metadata": {},
   "source": [
    "Entrenamiento final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c773767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('./data/product_interm_LGBM.parquet', engine='fastparquet')\n",
    "# Separar conjuntos\n",
    "df_train = df_full[df_full['PERIODO'] <= 201812].copy()\n",
    "df_val = df_full[df_full['PERIODO'].between(201901, 201909)].copy()\n",
    "df_test = df_full[(df_full['PERIODO'] == 201910)].copy()\n",
    "df_target_201912 = df_full[df_full['PERIODO'] == 201912].copy()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb638fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([df_train, df_val,df_test], ignore_index=True)[feature_cols]\n",
    "y_full = pd.concat([df_train[target_col], df_val[target_col],df_test[target_col]], ignore_index=True)\n",
    "df_pred = df_pred = df_full[df_full['PERIODO'] == 201912].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def train_and_save_model(i, params, X_full, y_full):\n",
    "    print(f\"Entrenando modelo {i+1}/50...\")\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_full, y_full)\n",
    "    joblib.dump(model, f'lgbm_model_{i+1:02d}.pkl')\n",
    "    return f\"Modelo {i+1} terminado\"\n",
    "\n",
    "results = joblib.Parallel(n_jobs=20)(\n",
    "    joblib.delayed(train_and_save_model)(i, params, X_full, y_full)\n",
    "    for i, params in enumerate(final_configs[:50])\n",
    ")\n",
    "\n",
    "print(results)\n",
    "print(\"¡Entrenamiento y guardado de los 50 modelos finalizado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57598325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Cargar los 50 modelos LightGBM entrenados\n",
    "lgbm_models = []\n",
    "for i in range(1, 51):\n",
    "    model = joblib.load(f'lgbm_model_{i:02d}.pkl')\n",
    "    lgbm_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ced59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pred.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a83dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Predicción LightGBM (50 modelos) ---\n",
    "lgbm_preds = []\n",
    "\n",
    "feature_cols = [col for col in df_pred.columns if col != target_col]\n",
    "\n",
    "X_pred_lgbm = df_pred[feature_cols]  \n",
    "\n",
    "for i, model in enumerate(lgbm_models):\n",
    "    print(f\"Prediciendo LightGBM {i+1}/50...\")\n",
    "    preds = model.predict(X_pred_lgbm)\n",
    "    lgbm_preds.append(preds)\n",
    "\n",
    "lgbm_preds = np.stack(lgbm_preds).T  # shape (N, 50)\n",
    "\n",
    "# --- Agregar predicciones LGBM al DataFrame existente con resultados de MLP ---\n",
    "for i in range(50):\n",
    "    df_pred[f'lgbm_pred_LOG1P_{i+1}'] = lgbm_preds[:, i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pred.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3490e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de columnas de predicción transformadas (log1p)\n",
    "pred_cols = [c for c in df_pred.columns if c.startswith('lgbm_pred_LOG1P_')]\n",
    "\n",
    "# Inversa de la transformación log1p sobre la variable objetivo\n",
    "df_pred['CLASE_TN'] = np.expm1(df_pred['CLASE_LOG1P'].values)\n",
    "\n",
    "# Inversa de log1p sobre las predicciones (matriz n x 50)\n",
    "preds_inv = df_pred[pred_cols].values\n",
    "preds_orig = np.expm1(preds_inv)  # Deshace log1p\n",
    "\n",
    "# Crear nuevas columnas en escala original\n",
    "for i, col in enumerate(pred_cols):\n",
    "    new_col = col.replace('LOG1P', 'TN')\n",
    "    df_pred[new_col] = preds_orig[:, i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfa8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar de df_preds_final las columnas con \"_LOG1P_Z_\"\n",
    "cols_to_remove = [col for col in df_pred.columns if '_LOG1P_' in col]\n",
    "df_pred.drop(columns=cols_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_cols = [c for c in df_pred.columns if c.startswith('lgbm_pred_TN')]\n",
    "print(lgbm_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred['lgbm_mean'] = df_pred[lgbm_cols].mean(axis=1)\n",
    "df_pred['lgbm_median'] = df_pred[lgbm_cols].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred['lgbm_mean'] = df_pred['lgbm_mean'].clip(lower=0)\n",
    "df_pred['lgbm_media'] = df_pred['lgbm_median'].clip(lower=0)\n",
    "\n",
    "\n",
    "df_pred[['PRODUCT_ID', 'lgbm_mean']]\\\n",
    "    .rename(columns={'PRODUCT_ID': 'product_id', 'lgbm_mean': 'tn'})\\\n",
    "    .to_csv('lgbm_predictions_mean.csv', index=False)\n",
    "\n",
    "df_pred[['PRODUCT_ID', 'lgbm_median']]\\\n",
    "    .rename(columns={'PRODUCT_ID': 'product_id', 'lgbm_median': 'tn'})\\\n",
    "    .to_csv('lgbm_predictions_median.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
