{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5d6f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import optuna\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bea074aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cca73795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de df_full después de la unión con df_pendientes:\n",
      "['PERIODO', 'ANIO', 'MES', 'MES_SIN', 'MES_COS', 'TRIMESTRE', 'ID_CAT1', 'ID_CAT2', 'ID_CAT3', 'ID_BRAND', 'SKU_SIZE', 'CUSTOMER_ID', 'PRODUCT_ID', 'PLAN_PRECIOS_CUIDADOS', 'CUST_REQUEST_QTY', 'CUST_REQUEST_TN', 'TN', 'STOCK_FINAL', 'MEDIA_MOVIL_3M_CLI_PROD', 'MEDIA_MOVIL_6M_CLI_PROD', 'MEDIA_MOVIL_12M_CLI_PROD', 'DESVIO_MOVIL_3M_CLI_PROD', 'DESVIO_MOVIL_6M_CLI_PROD', 'DESVIO_MOVIL_12M_CLI_PROD', 'MEDIA_MOVIL_3M_PROD', 'MEDIA_MOVIL_6M_PROD', 'MEDIA_MOVIL_12M_PROD', 'DESVIO_MOVIL_3M_PROD', 'DESVIO_MOVIL_6M_PROD', 'DESVIO_MOVIL_12M_PROD', 'MEDIA_MOVIL_3M_CLI', 'MEDIA_MOVIL_6M_CLI', 'MEDIA_MOVIL_12M_CLI', 'DESVIO_MOVIL_3M_CLI', 'DESVIO_MOVIL_6M_CLI', 'DESVIO_MOVIL_12M_CLI', 'TN_LAG_01', 'TN_LAG_02', 'TN_LAG_03', 'TN_LAG_04', 'TN_LAG_05', 'TN_LAG_06', 'TN_LAG_07', 'TN_LAG_08', 'TN_LAG_09', 'TN_LAG_10', 'TN_LAG_11', 'TN_LAG_12', 'TN_LAG_13', 'TN_LAG_14', 'TN_LAG_15', 'CLASE', 'CLASE_DELTA', 'ORDINAL', 'TN_DELTA_01', 'TN_DELTA_02', 'TN_DELTA_03', 'TN_DELTA_04', 'TN_DELTA_05', 'TN_DELTA_06', 'TN_DELTA_07', 'TN_DELTA_08', 'TN_DELTA_09', 'TN_DELTA_10', 'TN_DELTA_11', 'TN_DELTA_12', 'TN_DELTA_13', 'TN_DELTA_14', 'TN_DELTA_15', 'ANTIG_CLIENTE', 'ANTIG_PRODUCTO', 'CANT_PROD_CLI_PER', 'MEDIA_PROD_PER', 'MEDIA_PROD', 'MEDIA_PER', 'A_PREDECIR', 'PENDIENTE_TENDENCIA_3', 'TN_EWMA_03', 'TN_MEDIAN_03', 'TN_MIN_03', 'TN_MAX_03', 'PENDIENTE_TENDENCIA_6', 'TN_EWMA_06', 'TN_MEDIAN_06', 'TN_MIN_06', 'TN_MAX_06', 'PENDIENTE_TENDENCIA_9', 'TN_EWMA_09', 'TN_MEDIAN_09', 'TN_MIN_09', 'TN_MAX_09', 'PENDIENTE_TENDENCIA_12', 'TN_EWMA_12', 'TN_MEDIAN_12', 'TN_MIN_12', 'TN_MAX_12']\n"
     ]
    }
   ],
   "source": [
    "# Abrir el archivo parquet y cargarlo en un DataFrame data/l_vm_completa_train_pendientes.parquet\n",
    "df_pendientes = pd.read_parquet('./data/l_vm_completa_train_pendientes.parquet', engine='fastparquet')\n",
    "# Reunir los DataFrames df_full y df_pendientes por PRODUCT_ID, CUSTOMER_ID y PERIODO, agregar las \n",
    "# columnas de df_pendientes a df_full\n",
    "df_full = df_full.merge(df_pendientes, on=['PRODUCT_ID', 'CUSTOMER_ID', 'PERIODO'], how='left', suffixes=('', '_features'))\n",
    "# Imprimir las columnas de df_full\n",
    "print(\"Columnas de df_full después de la unión con df_pendientes:\")\n",
    "print(df_full.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2661c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar a df_full una variable categorica MES_PROBLEMATICO que sea 1 si PERIODO es 201906 o 201908 o 201910, y 0 en caso contrario\n",
    "df_full['MES_PROBLEMATICO'] = df_full['PERIODO'].apply(lambda x: 1 if x in [201906, 201908, 201910] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11e06b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizar tipos de datos numéricos\n",
    "for col in df_full.select_dtypes(include=['int64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='integer')\n",
    "for col in df_full.select_dtypes(include=['float64']).columns:\n",
    "    df_full[col] = pd.to_numeric(df_full[col], downcast='float')\n",
    "# Variables categóricas\n",
    "# categorical_features = ['ANIO','MES','TRIMESTRE','ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','SKU_SIZE','CUSTOMER_ID','PRODUCT_ID','PLAN_PRECIOS_CUIDADOS']\n",
    "categorical_features = ['ID_CAT1','ID_CAT2','ID_CAT3','ID_BRAND','PLAN_PRECIOS_CUIDADOS','MES_PROBLEMATICO']\n",
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_full[col] = df_full[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e06cb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13809811"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables predictoras y objetivo\n",
    "# filtrar que en X el periodo sea menor o igual a 201910\n",
    "# En x eliminar la columna 'CLASE' y 'CLASE_DELTA'\n",
    "X = df_full[df_full['PERIODO'] <= 201910].drop(columns=['CLASE', 'CLASE_DELTA']) \n",
    "# Filtrar en y que el periodo sea menor o igual a 201910\n",
    "y = df_full[df_full['PERIODO'] <= 201910]['CLASE_DELTA']\n",
    "# Eliminar df_full para liberar memoria\n",
    "del df_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dec1811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir los periodos de validación 201910\n",
    "#periodos_valid = [201910]\n",
    "periodos_valid = [201909,201910]\n",
    "\n",
    "# Separar train y cinco conjuntos de validación respetando la secuencia temporal\n",
    "X_train = X[X['PERIODO'] < periodos_valid[0]]\n",
    "y_train = y[X['PERIODO'] < periodos_valid[0]]\n",
    "X_val_list = [X[X['PERIODO'] == p] for p in periodos_valid]\n",
    "y_val_list = [y[X['PERIODO'] == p] for p in periodos_valid]\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca270248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hacer un try-except para cargar el modelo de LightGBM para regresión\n",
    "# try:\n",
    "#     # Verificar si el archivo del modelo de regresión existe\n",
    "#     if not os.path.exists('./modelos/lgbm_model_reg.txt'):\n",
    "#         raise FileNotFoundError(\"El modelo de regresión no se encuentra en la ruta especificada.\")\n",
    "#     model_reg = lgb.Booster(model_file='./modelos/lgbm_model_reg.txt')\n",
    "#     print(\"Modelo de regresión cargado exitosamente.\")\n",
    "# except FileNotFoundError:\n",
    "#     model_reg = None\n",
    "# Crear los datasets de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "val_data_list = []\n",
    "for i in range(len(periodos_valid)):\n",
    "    val_dataset = lgb.Dataset(X_val_list[i], label=y_val_list[i], categorical_feature=categorical_features)\n",
    "    # Inyectar PRODUCT_ID como atributo extra\n",
    "    val_dataset.PRODUCT_ID = X_val_list[i]['PRODUCT_ID'].values\n",
    "    val_data_list.append(val_dataset)\n",
    "\n",
    "def mape_sum_lgb(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    product_id = getattr(dataset, 'PRODUCT_ID', None)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    if denom == 0:\n",
    "        return 'mape_sum', 0.0, False\n",
    "    if product_id is not None:\n",
    "        df_pred = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'PRODUCT_ID': product_id})\n",
    "        mape = np.sum(np.abs(df_pred.groupby('PRODUCT_ID')['y_true'].sum() - df_pred.groupby('PRODUCT_ID')['y_pred'].sum())) / denom\n",
    "    else:\n",
    "        mape = np.sum(np.abs(y_true.sum() - y_pred.sum())) / denom\n",
    "    mape = np.nan_to_num(mape, nan=0.0)\n",
    "    return 'mape_sum', mape, False  # False: menor es mejor\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'None',  # Solo métrica personalizada\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 512),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 16),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 1.0),\n",
    "        'verbose': -1,\n",
    "        'feature_pre_filter': False\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=val_data_list,\n",
    "        valid_names=[f'validation_{p}' for p in periodos_valid],\n",
    "        feval=mape_sum_lgb,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, first_metric_only=True),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "    best_score = model.best_score[f'validation_{periodos_valid[0]}']['mape_sum']\n",
    "    print(f\"Trial {trial.number}: mape_sum={best_score:.5f}\")\n",
    "    return best_score\n",
    "\n",
    "# Guardar resultados en base de datos sqlite\n",
    "storage_url = \"sqlite:///./modelos/optuna.db\"\n",
    "study = optuna.create_study(direction='minimize', study_name=\"lgbm_regression_todos_los_productos_con_pendientes_regresion_moviles\", storage=storage_url, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)  # Puedes ajustar n_trials\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Entrena el modelo final con los mejores hiperparámetros encontrados\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'None'\n",
    "\n",
    "model_reg = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=50000,\n",
    "    valid_sets=val_data_list,\n",
    "    valid_names=[f'validation_{p}' for p in periodos_valid],\n",
    "    feval=mape_sum_lgb,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=500, first_metric_only=True),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.makedirs('./modelos', exist_ok=True)\n",
    "model_reg.save_model('./modelos/lgbm_model_reg_todos_los_productos.txt')\n",
    "\n",
    "# {'num_leaves': 439, 'learning_rate': 0.02651028762503521, 'feature_fraction': 0.8720499830243962, 'bagging_fraction': 0.9846969614646556, \n",
    "# 'bagging_freq': 7, 'min_data_in_leaf': 59, 'max_depth': 16, 'lambda_l1': 0.23731324768160023, 'lambda_l2': 3.760900279594698, \n",
    "# 'min_gain_to_split': 0.03595413253429805}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771eeb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params1 = {'num_leaves': 439, 'learning_rate': 0.02651028762503521, 'feature_fraction': 0.8720499830243962, 'bagging_fraction': 0.9846969614646556, \n",
    "'bagging_freq': 7, 'min_data_in_leaf': 59, 'max_depth': 16, 'lambda_l1': 0.23731324768160023, 'lambda_l2': 3.760900279594698, \n",
    "'min_gain_to_split': 0.03595413253429805}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd676f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "model_reg_sin_val = lgb.train(\n",
    "    best_params1,\n",
    "    train_data,\n",
    "    num_boost_round=850)\n",
    "\n",
    "os.makedirs('./modelos', exist_ok=True)\n",
    "model_reg.save_model('./modelos/lgbm_model_reg_sin_val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la importancia de cada variable\n",
    "importancia = model_reg.feature_importance(importance_type='gain')\n",
    "nombres = X_train.columns\n",
    "\n",
    "# Crear un DataFrame ordenado por importancia\n",
    "df_importancia = pd.DataFrame({'feature': nombres, 'importance': importancia})\n",
    "df_importancia = df_importancia.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Mostrar las variables más importantes\n",
    "print(df_importancia)\n",
    "\n",
    "# Si quieres visualizarlo gráficamente:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(df_importancia['feature'], df_importancia['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Importancia de variables LightGBM')\n",
    "plt.xlabel('Importancia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6453469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos sobre los que quiero hacer predicciones\n",
    "\n",
    "gc.collect()\n",
    "df_full = pd.read_parquet('./data/l_vm_completa_train.parquet', engine='fastparquet')\n",
    "df_pred_full = df_full[df_full['PERIODO'] == 201910].drop(columns=['CLASE', 'CLASE_DELTA'])\n",
    "y_obs = df_full[df_full['PERIODO'] == 201910]['CLASE']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las variables categóricas a tipo 'category'\n",
    "for col in categorical_features:\n",
    "    df_pred_full[col] = df_pred_full[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a0458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar del dataframe df_pred_full la columna 'PREDICCIONES'\n",
    "if 'PREDICCIONES' in df_pred_full.columns:\n",
    "    df_pred_full.drop(columns=['PREDICCIONES'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model_reg.predict(df_pred_full) \n",
    "df_pred_full['PREDICCIONES'] = predictions\n",
    "# Hacer que las predicciones sean cero si son negativas\n",
    "df_pred_full['PREDICCIONES'] = np.where(df_pred_full['PREDICCIONES'] < 0, 0, df_pred_full['PREDICCIONES'])\n",
    "df_pred_full['CLASE'] = y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar Dataframe que contenga por cada PRODUCT_ID la suma de las predicciones y la suma de la clase observada\n",
    "df_comp = df_pred_full.groupby('PRODUCT_ID').agg({'PREDICCIONES': 'sum', 'CLASE': 'sum'}).reset_index()\n",
    "df_comp['DIF_ABS'] = np.abs(df_comp['PREDICCIONES'] - df_comp['CLASE'])\n",
    "# ordernar por la diferencia absoluta\n",
    "df_comp = df_comp.sort_values(by='DIF_ABS', ascending=False)\n",
    "df_comp.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para los primeros 30 PRODUCT_ID de df_comp generar graficos como el anterior agregando resaltado en el gráfico en el periodo 201912\n",
    "# el valor PREDICCIONES de DF_COMP para el PRODUCT_ID en el título del gráfico\n",
    "for product_id in df_comp['PRODUCT_ID'].head(30):\n",
    "    df_prod = df_full[df_full['PRODUCT_ID'] == product_id].groupby('PERIODO').agg({'TN': 'sum'}).reset_index()\n",
    "    df_prod['PERIODO'] = df_prod['PERIODO'].astype(str)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_prod, x='PERIODO', y='TN', marker='o')\n",
    "    pred_value = df_comp[df_comp[\"PRODUCT_ID\"] == product_id][\"PREDICCIONES\"].values[0]\n",
    "    dif = df_comp[df_comp[\"PRODUCT_ID\"] == product_id][\"DIF_ABS\"].values[0]\n",
    "    plt.title(f'Evolución de las TN para PRODUCT_ID {product_id} - Predicciones: {pred_value:.2f} - Dif Abs: {dif:.2f}')\n",
    "    plt.xlabel('Periodo')\n",
    "    plt.ylabel('Total de TN')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, df_prod['TN'].max() * 1.1)  # Ajustar el eje y para que comience en cero\n",
    "    plt.scatter('201912', pred_value, color='red', s=100, zorder=5, label='Predicho')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el DataFrame de salida que contiene PRODUCT_ID y la suma de las predicciones por PRODUCT_ID\n",
    "df_output = df_pred_full.groupby('PRODUCT_ID')['PREDICCIONES'].sum().reset_index()\n",
    "# Hacer que la columna de predicciones sea mayor que cero\n",
    "df_output['PREDICCIONES'] = np.where(df_output['PREDICCIONES'] < 0, 0, df_output['PREDICCIONES'])\n",
    "df_output.head(10)\n",
    "# Renombrar las columnas como product_id y tn\n",
    "df_output.columns = ['product_id', 'tn']\n",
    "# Guardar el DataFrame de salida en un archivo CSV\n",
    "df_output.to_csv('./modelos/optuna_lgbm_predictions_sin_val_sin_negativos.csv', index=False)\n",
    "# Contar los valores negativos en df_output\n",
    "negativos = df_output[df_output['tn'] < 0].shape[0]\n",
    "print(f\"Número de valores negativos en las predicciones: {negativos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaboIII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
